
<!DOCTYPE html><html lang=pt-br prefix='og: https://ogp.me/ns# fb: https://ogp.me/ns/fb#'><head><meta charset="utf-8" /><style>p img{\display:block;margin-left:auto;margin-right:auto}.one-page{min-height:1550px;display:block;margin-bottom:50px}.first-page{position:relative;border:3px solid #0a2f5c}.content-first-page{margin:0;position:absolute;top:50%;left:50%;-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%);text-align:center}.content-first-page h1,.content-first-page p{text-align:center;margin-bottom:5em}.summary li{font-size:1.1em;margin-top:.2em}</style></head><body><div class="first-page one-page"><div class=content-first-page><h1> DEEPLEARNING BOOK BY V1pi</h1><p><img width=690 height=318 src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?fit=690%2C318" alt="Deep Learning Book" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?w=780 780w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?resize=300%2C138 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?resize=768%2C354 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?resize=200%2C92 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?resize=690%2C318 690w" sizes="(max-width: 690px) 100vw, 690px" data-attachment-id=113 data-permalink=http://deeplearningbook.com.br/deep-learning-book/deep-learning-book/ data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?fit=780%2C360" data-orig-size=780,360 data-comments-opened=0 data-image-meta={&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;} data-image-title="Deep Learning Book" data-image-description data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?fit=300%2C138" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deep-learning-book.jpg?fit=780%2C360"></p><p class=description>Cópia não oficial do livro disponível no <a href=http://deeplearningbook.com.br/>site</a> da galera do Data Science Academy. Este arquivo é disponibilizado em formato PDF e mobi.</p></div></div><div class="one-page summary"><h1>Sumário</h1>
	<ul>
 <li>
  <a href="#capitulo-1">
   CapÃ­tulo 1 – Deep Learning e a Tempestade Perfeita
  </a>
 </li>
 <li>
  <a href="#capitulo-2">
   Capítulo 2 – Uma Breve História das Redes Neurais Artificiais
  </a>
 </li>
 <li>
  <a href="#capitulo-3">
   Capítulo 3 – O Que São Redes Neurais Artificiais Profundas ou Deep Learning?
  </a>
 </li>
 <li>
  <a href="#capitulo-4">
   Capítulo 4 – O Neurônio, Biológico e Matemático
  </a>
 </li>
 <li>
  <a href="#capitulo-5">
   Capítulo 5 – Usando Redes Neurais Para Reconhecer Dígitos Manuscritos
  </a>
 </li>
 <li>
  <a href="#capitulo-6">
   Capítulo 6 – O Perceptron – Parte 1
  </a>
 </li>
 <li>
  <a href="#capitulo-7">
   Capítulo 7 – O Perceptron – Parte 2
  </a>
 </li>
 <li>
  <a href="#capitulo-8">
   Capítulo 8 – Função de Ativação
  </a>
 </li>
 <li>
  <a href="#capitulo-9">
   Capítulo 9 – A Arquitetura das Redes Neurais
  </a>
 </li>
 <li>
  <a href="#capitulo-10">
   Capítulo 10 – As 10 Principais Arquiteturas de Redes Neurais
  </a>
 </li>
 <li>
  <a href="#capitulo-11">
   Capítulo 11 – Design De Uma Rede Neural Para Reconhecimento de Dígitos
  </a>
 </li>
 <li>
  <a href="#capitulo-12">
   Capítulo 12 – Aprendizado Com a Descida do Gradiente
  </a>
 </li>
 <li>
  <a href="#capitulo-13">
   Capítulo 13 – Construindo Uma Rede Neural Com Linguagem Python
  </a>
 </li>
 <li>
  <a href="#capitulo-14">
   Capítulo 14 – Algoritmo Backpropagation Parte 1 – Grafos Computacionais e Chain Rule
  </a>
 </li>
 <li>
  <a href="#capitulo-15">
   Capítulo 15 – Algoritmo Backpropagation Parte 2 – Treinamento de Redes Neurais
  </a>
 </li>
 <li>
  <a href="#capitulo-16">
   Capítulo 16 – Algoritmo Backpropagation em Python
  </a>
 </li>
 <li>
  <a href="#capitulo-17">
   Capítulo 17 – Cross-Entropy Cost Function
  </a>
 </li>
 <li>
  <a href="#capitulo-18">
   Capítulo 18 – Entropia Cruzada Para Quantificar a Diferença Entre Duas Distribuições de Probabilidade
  </a>
 </li>
 <li>
  <a href="#capitulo-19">
   Capítulo 19 – Overfitting e Regularização – Parte 1
  </a>
 </li>
 <li>
  <a href="#capitulo-20">
   Capítulo 20 – Overfitting e Regularização – Parte 2
  </a>
 </li>
 <li>
  <a href="#capitulo-21">
   Capítulo 21 – Afinal, Por Que a Regularização Ajuda a Reduzir o Overfitting?
  </a>
 </li>
 <li>
  <a href="#capitulo-22">
   Capítulo 22 – Regularização L1
  </a>
 </li>
 <li>
  <a href="#capitulo-23">
   Capítulo 23 – Como Funciona o Dropout?
  </a>
 </li>
 <li>
  <a href="#capitulo-24">
   Capítulo 24 – Expandir Artificialmente os Dados de Treinamento
  </a>
 </li>
 <li>
  <a href="#capitulo-25">
   Capítulo 25 – Inicialização de Pesos em Redes Neurais Artificiais
  </a>
 </li>
 <li>
  <a href="#capitulo-26">
   Capítulo 26 – Como Escolher os Hiperparâmetros de Uma Rede Neural
  </a>
 </li>
 <li>
  <a href="#capitulo-27">
   Capítulo 27 – A Taxa de Aprendizado de Uma Rede Neural
  </a>
 </li>
 <li>
  <a href="#capitulo-28">
   Capítulo 28 – Usando Early Stopping Para Definir o Número de Épocas de Treinamento
  </a>
 </li>
 <li>
  <a href="#capitulo-29">
   Capítulo 29 – Definindo o Tamanho do Mini-Batch
  </a>
 </li>
 <li>
  <a href="#capitulo-30">
   Capítulo 30 – Variações do Stochastic Gradient Descent – Hessian Optimization e Momentum
  </a>
 </li>
 <li>
  <a href="#capitulo-31">
   Capítulo 31 – As Redes Neurais Artificiais Podem Computar Qualquer Função?
  </a>
 </li>
 <li>
  <a href="#capitulo-32">
   Capítulo 32 – Como Uma Rede Neural Artificial Encontra a Aproximação de Uma Função
  </a>
 </li>
 <li>
  <a href="#capitulo-33">
   Capítulo 33 – Por que as Redes Neurais Profundas São Difíceis de Treinar?
  </a>
 </li>
 <li>
  <a href="#capitulo-34">
   Capítulo 34 – O Problema da Dissipação do Gradiente
  </a>
 </li>
 <li>
  <a href="#capitulo-35">
   Capítulo 35 – A Matemática do Problema de Dissipação do Gradiente em Deep Learning
  </a>
 </li>
 <li>
  <a href="#capitulo-36">
   Capítulo 36 – Outros Problemas com o Gradiente em Redes Neurais Artificiais
  </a>
 </li>
 <li>
  <a href="#capitulo-37">
   Capítulo 37 – O Efeito do Batch Size no Treinamento de Redes Neurais Artificiais
  </a>
 </li>
 <li>
  <a href="#capitulo-38">
   Capítulo 38 – O Efeito da Taxa de Aprendizagem no Treinamento de Redes Neurais Artificiais
  </a>
 </li>
 <li>
  <a href="#capitulo-39">
   Capítulo 39 – Relação Entre o Tamanho do Lote e o Cálculo do Gradiente
  </a>
 </li>
 <li>
  <a href="#capitulo-40">
   Capítulo 40 – Introdução as Redes Neurais Convolucionais
  </a>
 </li>
 <li>
  <a href="#capitulo-41">
   Capítulo 41 – Campos Receptivos Locais em Redes Neurais Convolucionais
  </a>
 </li>
 <li>
  <a href="#capitulo-42">
   Capítulo 42 – Compartilhamento de Pesos em Redes Neurais Convolucionais
  </a>
 </li>
 <li>
  <a href="#capitulo-43">
   Capítulo 43 – Camadas de Pooling em Redes Neurais Convolucionais
  </a>
 </li>
 <li>
  <a href="#capitulo-44">
   Capítulo 44 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python – Parte 1
  </a>
 </li>
 <li>
  <a href="#capitulo-45">
   Capítulo 45 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python – Parte 2
  </a>
 </li>
 <li>
  <a href="#capitulo-46">
   Capítulo 46 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python – Parte 3
  </a>
 </li>
 <li>
  <a href="#capitulo-47">
   CapÃ­tulo 47 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python â Parte 4
  </a>
 </li>
 <li>
  <a href="#capitulo-48">
   Capítulo 48 – Redes Neurais Recorrentes
  </a>
 </li>
 <li>
  <a href="#capitulo-49">
   Capítulo 49 – A Matemática do Backpropagation Through Time (BPTT)
  </a>
 </li>
 <li>
  <a href="#capitulo-50">
   Capítulo 50 – A Matemática da Dissipação do Gradiente e Aplicações das RNNs
  </a>
 </li>
 <li>
  <a href="#capitulo-51">
   Capítulo 51 – Arquitetura de Redes Neurais Long Short Term Memory (LSTM)
  </a>
 </li>
 <li>
  <a href="#capitulo-52">
   Capítulo 52 – Arquitetura de Redes Neurais Gated Recurrent Unit (GRU)
  </a>
 </li>
 <li>
  <a href="#capitulo-53">
   Capítulo 53 – Matemática na GRU, Dissipação e Clipping do Gradiente
  </a>
 </li>
 <li>
  <a href="#capitulo-54">
   Capítulo 54 – Introdução às Redes Adversárias Generativas (GANs – Generative Adversarial Networks)
  </a>
 </li>
 <li>
  <a href="#capitulo-55">
   Capítulo 55 – Geração de Variáveis Aleatórias – Uma das Bases dos Modelos Generativos em GANs (Generative Adversarial Networks)
  </a>
 </li>
 <li>
  <a href="#capitulo-56">
   Capítulo 56 – Modelos Generativos – O Diferencial das GANs (Generative Adversarial Networks)
  </a>
 </li>
 <li>
  <a href="#capitulo-57">
   Capítulo 57 – Os Detalhes Matemáticos das GANs (Generative Adversarial Networks)
  </a>
 </li>
 <li>
  <a href="#capitulo-58">
   Capítulo 58 – Introdução aos Autoencoders
  </a>
 </li>
 <li>
  <a href="#capitulo-59">
   Capítulo 59 – Principais Tipos de Redes Neurais Artificiais Autoencoders
  </a>
 </li>
 <li>
  <a href="#capitulo-60">
   Capítulo 60 – Variational Autoencoders (VAEs) – Definição, Redução de Dimensionalidade, Espaço Latente e Regularização
  </a>
 </li>
 <li>
  <a href="#capitulo-61">
   Capítulo 61 – A Matemática dos Variational Autoencoders (VAEs)
  </a>
 </li>
 <li>
  <a href="#capitulo-62">
   Capítulo 62 – O Que é Aprendizagem Por Reforço?
  </a>
 </li>
</ul></div><h1 class="entry-title" id="capitulo-1">
 CapÃ­tulo 1 – Deep Learning e a Tempestade Perfeita
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  O interesse pela Aprendizagem de MÃ¡quina (Machine Learning) explodiu na Ãºltima dÃ©cada. O mundo a nossa volta estÃ¡ passando por uma transformaÃ§Ã£o e vemos uma interaÃ§Ã£o cada vez maior das aplicaÃ§Ãµes de computador com os seres humanos. Softwares de detecÃ§Ã£o de spam, sistemas de recomendaÃ§Ã£o, marcaÃ§Ã£o em fotos de redes sociais, assistentes pessoais ativados por voz, carros autÃ´nomos, smartphones com reconhecimento facial e muito mais.
 </p>
 <p style="text-align: justify;">
  E o interesse por Machine Learning se mostra ainda mais evidente pelo nÃºmero cada vez maior de conferÃªncias, meetups, artigos, livros, cursos, buscas no Google e profissionais e empresas procurando compreender o que Ã© e como usar aprendizagem de mÃ¡quina, embora muitos ainda confundem o que podem fazer com o que desejam fazer. NÃ£o hÃ¡ como ficar indiferente a esta revoluÃ§Ã£o trazida pela aprendizagem de mÃ¡quina e, segundo o Gartner, atÃ© 2020 todos os softwares corporativos terÃ£o alguma funcionalidade ligada a Machine Learning.
 </p>
 <p style="text-align: justify;">
  Fundamentalmente, Machine Learning Ã© a utilizaÃ§Ã£o de algoritmos para extrair informaÃ§Ãµes de dados brutos e representÃ¡-los atravÃ©s de algum tipo de modelo matemÃ¡tico. Usamos entÃ£o este modelo para fazer inferÃªncias a partir de outros conjuntos de dados. Existem muitos algoritmos que permitem fazer isso, mas um tipo em especial vem se destacando, as redes neurais artificiais.
 </p>
 <p style="text-align: justify;">
  As redes neurais artificiais nÃ£o sÃ£o necessariamente novas, existem pelo menos desde a dÃ©cada de 1950. Mas durante vÃ¡rias dÃ©cadas, embora a arquitetura desses modelos tivesse evoluÃ­do, ainda faltavam ingredientes que fizessem os modelos realmente funcionar. E esses ingredientes surgiram quase ao mesmo tempo. Um deles vocÃª jÃ¡ deve ter ouvido: Big Data. O volume de dados, gerado em variedade e velocidade cada vez maiores, permite criar modelos e atingir altos nÃ­veis de precisÃ£o. Mas ainda falta um ingrediente. Faltava! Como processar grandes modelos de Machine Learning com grandes quantidades de dados? As CPUs nÃ£o conseguiam dar conta do recado.
 </p>
 <p style="text-align: justify;">
  Foi quando os gamers e sua avidez por poder computacional e grÃ¡ficos perfeitos, nos ajudaram a encontrar o segundo ingrediente: ProgramaÃ§Ã£o Paralela em GPUs. As unidades de processamento grÃ¡fico, que permitem realizar operaÃ§Ãµes matemÃ¡ticas de forma paralela, principalmente operaÃ§Ãµes com matrizes e vetores, elementos presentes em modelos de redes neurais artificias, formaram a tempestade perfeita, que permitiu a evoluÃ§Ã£o na qual nos encontramos hoje: Big Data + Processamento Paralelo + Modelos de Aprendizagem de MÃ¡quina = InteligÃªncia Artificial.
 </p>
 <p style="text-align: justify;">
  A unidade fundamental de uma rede neural artificial Ã© um nÃ³ (ou neurÃ´nio matemÃ¡tico), que por sua vez Ã© baseado no neurÃ´nio biolÃ³gico. As conexÃµes entre esses neurÃ´nios matemÃ¡ticos tambÃ©m foram inspiradas em cÃ©rebros biolÃ³gicos, especialmente na forma como essas conexÃµes se desenvolvem ao longo do tempo com “treinamento”. Em meados da dÃ©cada de 1980 e inÃ­cio da dÃ©cada de 1990, muitos avanÃ§os importantes na arquitetura das redes neurais artificias ocorreram. No entanto, a quantidade de tempo e dados necessÃ¡rios para obter bons resultados retardou a adoÃ§Ã£o e, portanto, o interesse foi arrefecido, com o que ficou conhecimento como AI Winter (Inverno da IA).
 </p>
 <p style="text-align: justify;">
  No inÃ­cio dos anos 2000, o poder computacional expandiu exponencialmente e o mercado viu uma “explosÃ£o” de tÃ©cnicas computacionais que nÃ£o eram possÃ­veis antes disso. Foi quando o aprendizado profundo (Deep Learning) emergiu do crescimento computacional explosivo dessa dÃ©cada como o principal mecanismo de construÃ§Ã£o de sistemas de InteligÃªncia Artificial, ganhando muitas competiÃ§Ãµes importantes de aprendizagem de mÃ¡quina. O interesse por Deep Learning nÃ£o para de crescer e hoje vemos o termo aprendizado profundo sendo mencionado com frequÃªncia cada vez maior e soluÃ§Ãµes comerciais surgindo a todo momento.
 </p>
 <p style="text-align: justify;">
  Este livro online, gratuito e em portuguÃªs, Ã© uma iniciativa da
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br" rel="noopener" target="_blank">
    Data Science Academy
   </a>
  </span>
  para ajudar aqueles que buscam conhecimento avanÃ§ado e de qualidade em nosso idioma. SerÃ£o mais de 50 capÃ­tulos, publicados no formato de posts e lanÃ§ados semanalmente. Desta forma, esperamos contribuir para o crescimento do Deep Learning e InteligÃªncia Artificial no Brasil.
 </p>
 <p>
  Nos acompanhe nesta incrÃ­vel jornada!
 </p>
 <p>
  Equipe DSA
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br" rel="noopener" target="_blank">
    www.datascienceacademy.com.br
   </a>
  </span>
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-25" href="http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-25" href="http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-25" href="http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-25" href="http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-25-5e0b6a00f2562" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=25&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-25-5e0b6a00f2562" id="like-post-wrapper-140353593-25-5e0b6a00f2562">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-2">
 Capítulo 2 – Uma Breve História das Redes Neurais Artificiais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Para compreender onde estamos hoje, precisamos olhar para o passado e analisar como chegamos até aqui. Vejamos então Uma Breve História das Redes Neurais Artificiais.
 </p>
 <p style="text-align: justify;">
  O cérebro humano é uma máquina altamente poderosa e complexa capaz de processar uma grande quantidade de informações em tempo mínimo. As unidades principais do cérebro são os neurônios e é por meio deles que as informações são transmitidas e processadas. As tarefas realizadas pelo cérebro intrigam os pesquisadores, como por exemplo, a capacidade do cérebro de reconhecer um rosto familiar dentre uma multidão em apenas milésimos de segundo. As respostas sobre alguns enigmas do funcionamento do cérebro ainda não foram respondidas e se perpetuam ate os dias de hoje. O que é conhecido sobre o funcionamento do cérebro é que o mesmo desenvolve suas regras através da experiência adquirida em situações vividas anteriormente.
 </p>
 <p>
  <img alt="Cérebro Humano" class="aligncenter size-medium wp-image-74" data-attachment-id="74" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"1"}' data-image-title="Cérebro Humano" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/cérebro-humano.jpg?fit=660%2C528" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/cérebro-humano.jpg?fit=300%2C240" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/cérebro-humano.jpg?fit=660%2C528" data-orig-size="660,528" data-permalink="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/cerebro-humano/" data-recalc-dims="1" height="240" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/cérebro-humano.jpg?resize=300%2C240" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/cérebro-humano.jpg?resize=300%2C240 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/cérebro-humano.jpg?resize=200%2C160 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/cérebro-humano.jpg?w=660 660w" width="300"/>
 </p>
 <p style="text-align: center;">
  Fig1 – Cérebro humano, a máquina mais fantástica que existe no Planeta Terra.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O desenvolvimento do cérebro humano ocorre principalmente nos dois primeiros anos de vida, mas se arrasta por toda a vida. Inspirando-se neste modelo, diversos pesquisadores tentaram simular o funcionamento do cérebro, principalmente o processo de aprendizagem por experiência, a fim de criar sistemas inteligentes capazes de realizar tarefas como classificação, reconhecimento de padrões, processamento de imagens, entre outras atividades. Como resultado destas pesquisas surgiu o modelo do neurônio artificial e posteriormente um sistema com vários neurônios interconectados, a chamada Rede Neural.
 </p>
 <p style="text-align: justify;">
  Em 1943, o neurofisiologista Warren McCulloch e o matemático Walter Pitts escreveram um artigo sobre como os neurônios poderiam funcionar e para isso, eles modelaram uma rede neural simples usando circuitos elétricos.
 </p>
 <p style="text-align: justify;">
  Warren McCulloch e Walter Pitts criaram um modelo computacional para redes neurais baseadas em matemática e algoritmos denominados lógica de limiar (threshold logic). Este modelo abriu o caminho para a pesquisa da rede neural dividida em duas abordagens: uma abordagem focada em processos biológicos no cérebro, enquanto a outra focada na aplicação de redes neurais à inteligência artificial.
 </p>
 <p style="text-align: justify;">
  Em 1949, Donald Hebb escreveu
  <em>
   The Organization of Behavior
  </em>
  , uma obra que apontou o fato de que os caminhos neurais são fortalecidos cada vez que são usados, um conceito fundamentalmente essencial para a maneira como os humanos aprendem. Se dois nervos dispararem ao mesmo tempo, argumentou, a conexão entre eles é melhorada.
 </p>
 <p style="text-align: justify;">
  À medida que os computadores se tornaram mais avançados na década de 1950, finalmente foi possível simular uma hipotética rede neural. O primeiro passo para isso foi feito por Nathanial Rochester dos laboratórios de pesquisa da IBM. Infelizmente para ele, a primeira tentativa de fazê-lo falhou.
 </p>
 <p style="text-align: justify;">
  No entanto, ao longo deste tempo, os defensores das “máquinas pensantes” continuaram a argumentar suas pesquisas. Em 1956, o Projeto de Pesquisa de Verão de Dartmouth sobre Inteligência Artificial proporcionou um impulso tanto à Inteligência Artificial como às Redes Neurais. Um dos resultados deste processo foi estimular a pesquisa em IA na parte de processamento neural.
 </p>
 <p style="text-align: justify;">
  Nos anos seguintes ao Projeto Dartmouth, John von Neumann sugeriu imitar funções simples de neurônios usando relés telegráficos ou tubos de vácuo. Além disso, Frank Rosenblatt, um neurobiologista, começou a trabalhar no Perceptron. Ele estava intrigado com o funcionamento do olho de uma mosca. Grande parte do processamento feito por uma mosca ao decidir fugir, é feito em seus olhos. O Perceptron, que resultou dessa pesquisa, foi construído em hardware e é a mais antiga rede neural ainda em uso hoje. Um Percetron de camada única foi útil para classificar um conjunto de entradas de valor contínuo em uma de duas classes. O Perceptron calcula uma soma ponderada das entradas, subtrai um limite e passa um dos dois valores possíveis como resultado. Infelizmente, o Perceptron é limitado e foi comprovado como tal durante os “anos desiludidos” por Marvin Minsky e o livro de Seymour Papert de 1969, Perceptrons.
 </p>
 <p>
 </p>
 <p>
  <img alt="Redes Neurais" class="aligncenter size-full wp-image-78" data-attachment-id="78" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Redes Neurais" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?fit=967%2C617" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?fit=300%2C191" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?fit=967%2C617" data-orig-size="967,617" data-permalink="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/neuralnetworks/" data-recalc-dims="1" height="617" sizes="(max-width: 967px) 100vw, 967px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=967%2C617" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?w=967 967w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=300%2C191 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=768%2C490 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=200%2C128 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=690%2C440 690w" width="967"/>
 </p>
 <p style="text-align: center;">
  Fig2 – Algumas Arquiteturas de Redes Neurais
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Em 1959, Bernard Widrow e Marcian Hoff, de Stanford, desenvolveram modelos denominados “ADALINE” e “MADALINE”. Em uma exibição típica do amor de Stanford por siglas, os nomes provêm do uso de múltiplos elementos ADAptive LINear. ADALINE foi desenvolvido para reconhecer padrões binários de modo que, se ele estivesse lendo bits de transmissão de uma linha telefônica, poderia prever o próximo bit. MADALINE foi a primeira rede neural aplicada a um problema do mundo real, usando um filtro adaptativo que elimina ecos nas linhas telefônicas. Embora o sistema seja tão antigo como os sistemas de controle de tráfego aéreo, ele ainda está em uso comercial.
 </p>
 <p style="text-align: justify;">
  Infelizmente, esses sucessos anteriores levaram as pessoas a exagerar o potencial das redes neurais, particularmente à luz da limitação na eletrônica, então disponível na época. Este exagero excessivo, que decorreu do mundo acadêmico e técnico, infectou a literatura geral da época. Muitas promessas foram feitas, mas o resultado foi o desapontamento. Além disso, muitos escritores começaram a refletir sobre o efeito que teria “máquinas pensantes” no homem. A série de Asimov em robôs revelou os efeitos sobre a moral e os valores do homem quando máquinas fossem capazes de fazer todo o trabalho da humanidade. Outros escritores criaram computadores mais sinistros, como HAL do filme 2001.
 </p>
 <p style="text-align: justify;">
  Toda essa discussão sobre o efeito da Inteligência Artificial sobre a vida humana, aliada aos poucos progressos, fizeram vozes respeitadas criticar a pesquisa em redes neurais. O resultado foi a redução drástica de grande parte do financiamento em pesquisas. Esse período de crescimento atrofiado durou até 1981, sendo conhecido como o Inverno da IA (AI Winter).
 </p>
 <p style="text-align: justify;">
  Em 1982, vários eventos provocaram um renovado interesse. John Hopfield da Caltech apresentou um documento à Academia Nacional de Ciências. A abordagem de Hopfield não era simplesmente modelar cérebros, mas criar dispositivos úteis. Com clareza e análise matemática, ele mostrou como essas redes poderiam funcionar e o que poderiam fazer. No entanto, o maior recurso de Hopfield foi seu carisma. Ele era articulado e simpático e isso colaborou bastante para que ele fosse ouvido.
 </p>
 <p style="text-align: justify;">
  Em 1985, o Instituto Americano de Física começou o que se tornou uma reunião anual – Redes Neurais para Computação. Em 1987, a primeira Conferência Internacional sobre Redes Neurais do Institute of Electrical and Electronic Engineer’s (IEEE) atraiu mais de 1.800 participantes.
 </p>
 <p style="text-align: justify;">
  Em 1986, com redes neurais de várias camadas nas notícias, o problema era como estender a regra Widrow-Hoff para várias camadas. Três grupos independentes de pesquisadores, dentre os quais David Rumelhart, ex-membro do departamento de psicologia de Stanford, apresentaram ideias semelhantes que agora são chamadas de redes Backpropagation porque distribuem erros de reconhecimento de padrões em toda a rede. As redes híbridas utilizavam apenas duas camadas, essas redes de Backpropagation utilizam muitas. O resultado é que as redes de Backpropagation “aprendem” de forma mais lenta, pois necessitam, possivelmente, de milhares de iterações para aprender, mas geram um resultado muito preciso.
 </p>
 <p style="text-align: justify;">
  Agora, as redes neurais são usadas em várias aplicações. A ideia fundamental por trás da natureza das redes neurais é que, se ela funcionar na natureza, deve ser capaz de funcionar em computadores. O futuro das redes neurais, no entanto, reside no desenvolvimento de hardware. As redes neurais rápidas e eficientes dependem do hardware especificado para seu eventual uso.
 </p>
 <p style="text-align: justify;">
  O diagrama abaixo mostra alguns marcos importantes na evolução e pesquisa das redes neurais artificiais. O fato, é que ainda estamos escrevendo esta história e muita evolução está ocorrendo neste momento, através do trabalho de milhares de pesquisadores e profissionais de Inteligência Artificial em todo mundo. E você, não quer ajudar a escrever esta história?
 </p>
 <p>
  <img alt="Timeline das Redes Neurais" class="aligncenter wp-image-76 size-full" data-attachment-id="76" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Timeline das Redes Neurais" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?fit=1024%2C481" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?fit=300%2C141" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?fit=2133%2C1002" data-orig-size="2133,1002" data-permalink="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/nn_timeline/" data-recalc-dims="1" height="550" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=1170%2C550" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?w=2133 2133w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=300%2C141 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=768%2C361 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=1024%2C481 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=200%2C94 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=690%2C324 690w" width="1170"/>
 </p>
 <p style="text-align: center;">
  Fig3 – Marcos no desenvolvimento das redes neurais.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Podemos resumir assim os principais marcos na pesquisa e evolução das redes neurais artificiais até chegarmos ao Deep Learning:
 </p>
 <p style="text-align: justify;">
  <strong>
   1943
  </strong>
  : Warren McCulloch e Walter Pitts criam um modelo computacional para redes neurais baseadas em matemática e algoritmos denominados lógica de limiar.
 </p>
 <p style="text-align: justify;">
  <strong>
   1958
  </strong>
  : Frank Rosenblatt cria o Perceptron, um algoritmo para o reconhecimento de padrões baseado em uma rede neural computacional de duas camadas usando simples adição e subtração. Ele também propôs camadas adicionais com notações matemáticas, mas isso não seria realizado até 1975.
 </p>
 <p style="text-align: justify;">
  <strong>
   1980
  </strong>
  : Kunihiko Fukushima propõe a Neoconitron, uma rede neural de hierarquia, multicamada, que foi utilizada para o reconhecimento de caligrafia e outros problemas de reconhecimento de padrões.
 </p>
 <p style="text-align: justify;">
  <strong>
   1989
  </strong>
  : os cientistas conseguiram criar algoritmos que usavam redes neurais profundas, mas os tempos de treinamento para os sistemas foram medidos em dias, tornando-os impraticáveis ​​para o uso no mundo real.
 </p>
 <p style="text-align: justify;">
  <strong>
   1992
  </strong>
  : Juyang Weng publica o Cresceptron, um método para realizar o reconhecimento de objetos 3-D automaticamente a partir de cenas desordenadas.
 </p>
 <p style="text-align: justify;">
  <strong>
   Meados dos anos 2000
  </strong>
  : o termo “aprendizagem profunda” começa a ganhar popularidade após um artigo de Geoffrey Hinton e Ruslan Salakhutdinov mostrar como uma rede neural de várias camadas poderia ser pré-treinada uma camada por vez.
 </p>
 <p style="text-align: justify;">
  <strong>
   2009
  </strong>
  : acontece o NIPS Workshop sobre Aprendizagem Profunda para Reconhecimento de Voz e descobre-se que com um conjunto de dados suficientemente grande, as redes neurais não precisam de pré-treinamento e as taxas de erro caem significativamente.
 </p>
 <p style="text-align: justify;">
  <strong>
   2012
  </strong>
  : algoritmos de reconhecimento de padrões artificiais alcançam desempenho em nível humano em determinadas tarefas. E o algoritmo de aprendizagem profunda do Google é capaz de identificar gatos.
 </p>
 <p style="text-align: justify;">
  <strong>
   2014
  </strong>
  : o Google compra a Startup de Inteligência Artificial chamada DeepMind, do Reino Unido, por £ 400m
 </p>
 <p style="text-align: justify;">
  <strong>
   2015
  </strong>
  : Facebook coloca a tecnologia de aprendizado profundo – chamada DeepFace – em operação para marcar e identificar automaticamente usuários do Facebook em fotografias. Algoritmos executam tarefas superiores de reconhecimento facial usando redes profundas que levam em conta 120 milhões de parâmetros.
 </p>
 <p style="text-align: justify;">
  <strong>
   2016
  </strong>
  : o algoritmo do Google DeepMind, AlphaGo, mapeia a arte do complexo jogo de tabuleiro Go e vence o campeão mundial de Go, Lee Sedol, em um torneio altamente divulgado em Seul.
 </p>
 <p style="text-align: justify;">
  <strong>
   2017
  </strong>
  : adoção em massa do Deep Learning em diversas aplicações corporativas e mobile, além do avanço em pesquisas. Todos os eventos de tecnologia ligados a Data Science, IA e Big Data, apontam Deep Learning como a principal tecnologia para criação de sistemas inteligentes.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A promessa do aprendizado profundo não é que os computadores comecem a pensar como seres humanos. Isso é como pedir uma maçã para se tornar uma laranja. Em vez disso, demonstra que, dado um conjunto de dados suficientemente grande, processadores rápidos e um algoritmo suficientemente sofisticado, os computadores podem começar a realizar tarefas que até então só podiam ser realizadas apenas por seres humanos, como reconhecer imagens e voz, criar obras de arte ou tomar decisões por si mesmo.
 </p>
 <p style="text-align: justify;">
  Os estudos sobre as redes neurais sofreram uma grande revolução a partir dos anos 80 e esta área de estudos tem se destacado, seja pelas promissoras características apresentadas pelos modelos de redes neurais propostos, seja pelas condições tecnológicas atuais de implementação que permitem desenvolver arrojadas implementações de arquiteturas neurais paralelas em hardwares dedicado, obtendo assim ótimas performances destes sistemas (bastante superiores aos sistemas convencionais). A evolução natural das redes neurais, são as redes neurais profundas (ou Deep Learning). Mas isso é o que vamos discutir no próximo capítulo! Até lá.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  Christopher D. Manning. (2015). Computational Linguistics and Deep Learning Computational Linguistics, 41(4), 701–707.
 </p>
 <p>
  F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.
 </p>
 <p>
  W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115–133, 1943.
 </p>
 <p>
  The organization of behavior: A neuropsychological theory. D. O. Hebb. John Wiley And Sons, Inc., New York, 1949
 </p>
 <p>
  B. Widrow et al. Adaptive ”Adaline” neuron using chemical ”memistors”. Number Technical Report 1553-2. Stanford Electron. Labs., Stanford, CA, October 1960.
 </p>
 <p>
  “New Navy Device Learns By Doing”, New York Times, July 8, 1958.
 </p>
 <p>
  Perceptrons. An Introduction to Computational Geometry. MARVIN MINSKY and SEYMOUR PAPERT. M.I.T. Press, Cambridge, Mass., 1969.
 </p>
 <p>
  Minsky, M. (1952). A neural-analogue calculator based upon a probability model of reinforcement. Harvard University Pychological Laboratories internal report.
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-58" href="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-58" href="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-58" href="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-58" href="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-58-5e0dd03f69491" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=58&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-58-5e0dd03f69491" id="like-post-wrapper-140353593-58-5e0dd03f69491">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-3">
 Capítulo 3 – O Que São Redes Neurais Artificiais Profundas ou Deep Learning?
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Aprendizagem Profunda ou Deep Learning, é uma sub-área da Aprendizagem de Máquina, que emprega algoritmos para processar dados e imitar o processamento feito pelo cérebro humano. Mas O Que São Redes Neurais Artificiais Profundas ou Deep Learning? É o que veremos neste capítulo. Não se preocupe se alguns termos mais técnicos não fizerem sentido agora. Todos eles serão estudados ao longo deste livro online.
 </p>
 <p style="text-align: justify;">
  Deep Learning  usa camadas de neurônios matemáticos para processar dados, compreender a fala humana e reconhecer objetos visualmente. A informação é passada através de cada camada, com a saída da camada anterior fornecendo entrada para a próxima camada. A primeira camada em uma rede é chamada de camada de entrada, enquanto a última é chamada de camada de saída. Todas as camadas entre as duas são referidas como camadas ocultas. Cada camada é tipicamente um algoritmo simples e uniforme contendo um tipo de função de ativação.
 </p>
 <p>
 </p>
 <p>
  <img alt="Neural Network" class="aligncenter size-full wp-image-87" data-attachment-id="87" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Neural Network" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?fit=690%2C259" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?fit=300%2C113" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?fit=690%2C259" data-orig-size="690,259" data-permalink="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/neural/" data-recalc-dims="1" height="259" sizes="(max-width: 690px) 100vw, 690px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?resize=690%2C259" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?w=690 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?resize=300%2C113 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?resize=200%2C75 200w" width="690"/>
 </p>
 <p style="text-align: center;">
  Fig4 – Rede Neural Simples e Rede Neural Profunda (Deep Learning)
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A aprendizagem profunda é responsável por avanços recentes em visão computacional, reconhecimento de fala, processamento de linguagem natural e reconhecimento de áudio. O aprendizado profundo é baseado no conceito de redes neurais artificiais, ou sistemas computacionais que imitam a maneira como o cérebro humano funciona.
 </p>
 <p style="text-align: justify;">
  A extração de recursos é outro aspecto da Aprendizagem Profunda. A extração de recursos usa um algoritmo para construir automaticamente “recursos” significativos dos dados para fins de treinamento, aprendizado e compreensão. Normalmente, o Cientista de Dados, ou Engenheiro de IA, é responsável pela extração de recursos.
 </p>
 <p style="text-align: justify;">
  O aumento rápido e o aparente domínio do aprendizado profundo sobre os métodos tradicionais de aprendizagem de máquina em uma variedade de tarefas tem sido surpreendente de testemunhar e, às vezes, difícil de explicar. Deep Learning é uma evolução das Redes Neurais, que por sua vez possuem uma história fascinante que remonta à década de 1940, cheia de altos e baixos, voltas e reviravoltas, amigos e rivais, sucessos e fracassos. Em uma história digna de um filme dos anos 90, uma ideia que já foi uma espécie de patinho feio floresceu para se tornar a bola da vez.
 </p>
 <p style="text-align: justify;">
  Consequentemente, o interesse em aprendizagem profunda tem disparado, com cobertura constante na mídia popular. A pesquisa de aprendizagem profunda agora aparece rotineiramente em revistas como Science, Nature, Nature Methods e Forbes apenas para citar alguns. O aprendizado profundo conquistou Go, aprendeu a dirigir um carro, diagnosticou câncer de pele e autismo, tornou-se um falsificador de arte mestre e pode até alucinar imagens fotorrealistas.
 </p>
 <p style="text-align: justify;">
  Os primeiros algoritmos de aprendizagem profunda que possuíam múltiplas camadas de características não-lineares podem ser rastreados até Alexey Grigoryevich Ivakhnenko (desenvolveu o Método do Grupo de Manipulação de Dados) e Valentin Grigor’evich Lapa (autor de Cybernetics and Forecasting Techniques) em 1965 (Figura 5), que usaram modelos finos mas profundos com funções de ativação polinomial os quais eles analisaram com métodos estatísticos. Em cada camada, eles selecionavam os melhores recursos através de métodos estatísticos e encaminhavam para a próxima camada. Eles não usaram Backpropagation para treinar a rede de ponta a ponta, mas utilizaram mínimos quadrados camada-por-camada, onde as camadas anteriores foram independentemente instaladas em camadas posteriores (um processo lento e manual).
 </p>
 <p>
  <img alt="GMDH-network" class="aligncenter size-full wp-image-86" data-attachment-id="86" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="GMDH-network" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?fit=662%2C501" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?fit=300%2C227" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?fit=662%2C501" data-orig-size="662,501" data-permalink="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/gmdh-network/" data-recalc-dims="1" height="501" sizes="(max-width: 662px) 100vw, 662px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?resize=662%2C501" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?w=662 662w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?resize=300%2C227 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?resize=200%2C151 200w" width="662"/>
 </p>
 <p style="text-align: center;">
  Fig5 – Arquitetura da primeira rede profunda conhecida treinada por Alexey Grigorevich Ivakhnenko em 1965.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  No final da década de 1970, o primeiro inverno de AI começou, resultado de promessas que não poderiam ser mantidas. O impacto desta falta de financiamento limitou a pesquisa em Redes Neurais Profundas e Inteligência Artificial. Felizmente, houve indivíduos que realizaram a pesquisa sem financiamento.
 </p>
 <p style="text-align: justify;">
  As primeiras “redes neurais convolutivas” foram usadas por Kunihiko Fukushima. Fukushima concebeu redes neurais com múltiplas camadas de agrupamento e convoluções. Em 1979, ele desenvolveu uma rede neural artificial, chamada Neocognitron, que usava um design hierárquico e multicamadas. Este design permitiu ao computador “aprender” a reconhecer padrões visuais. As redes se assemelhavam a versões modernas, mas foram treinadas com uma estratégia de reforço de ativação recorrente em múltiplas camadas, que ganhou força ao longo do tempo. Além disso, o design de Fukushima permitiu que os recursos importantes fossem ajustados manualmente aumentando o “peso” de certas conexões.
 </p>
 <p style="text-align: justify;">
  Muitos dos conceitos de Neocognitron continuam a ser utilizados. O uso de conexões de cima para baixo e novos métodos de aprendizagem permitiram a realização de uma variedade de redes neurais. Quando mais de um padrão é apresentado ao mesmo tempo, o Modelo de Atenção Seletiva pode separar e reconhecer padrões individuais deslocando sua atenção de um para o outro (o mesmo processo que usamos em multitarefa). Um Neocognitron moderno não só pode identificar padrões com informações faltantes (por exemplo, um número 5 desenhado de maneira incompleta), mas também pode completar a imagem adicionando as informações que faltam. Isso pode ser descrito como “inferência”.
 </p>
 <p style="text-align: justify;">
  O Backpropagation, o uso de erros no treinamento de modelos de Deep Learning, evoluiu significativamente em 1970. Foi quando Seppo Linnainmaa escreveu sua tese de mestrado, incluindo um código FORTRAN para Backpropagation. Infelizmente, o conceito não foi aplicado às redes neurais até 1985. Foi quando Rumelhart, Williams e Hinton demonstraram o Backpropagation em uma rede neural que poderia fornecer representações de distribuição “interessantes”. Filosoficamente, essa descoberta trouxe à luz a questão dentro da psicologia cognitiva de saber se a compreensão humana depende da lógica simbólica (computacionalismo) ou de representações distribuídas (conexão). Em 1989, Yann LeCun forneceu a primeira demonstração prática de Backpropagation no Bell Labs. Ele combinou redes neurais convolutivas com Backpropagation para ler os dígitos “manuscritos” (assunto do próximo capítulo). Este sistema foi usado para ler o número de cheques manuscritos.
 </p>
 <p>
  <img alt="Deep Learning" class="aligncenter size-full wp-image-111" data-attachment-id="111" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Deep Learning" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?fit=1024%2C768" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?fit=300%2C225" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?fit=2048%2C1536" data-orig-size="2048,1536" data-permalink="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/deeplearningpioneersatnipsconference2014inmontreal/" data-recalc-dims="1" height="878" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=1170%2C878" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?w=2048 2048w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=768%2C576 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=1024%2C768 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=200%2C150 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=690%2C518 690w" width="1170"/>
 </p>
 <p style="text-align: center;">
  Fig6 – Os pioneiros da Inteligência Artificial. Da esquerda para a direita: Yann LeCun, Geoffrey Hinton, Yoshua Bengio e Andrew Ng
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Porém, tivemos neste período o que ficou conhecido como segundo Inverno da IA, que ocorreu entre 1985-1990, que também afetou pesquisas em Redes Neurais e Aprendizagem Profunda. Vários indivíduos excessivamente otimistas haviam exagerado o potencial “imediato” da Inteligência Artificial, quebrando as expectativas e irritando os investidores. A raiva era tão intensa, que a frase Inteligência Artificial atingiu o status de pseudociência. Felizmente, algumas pessoas continuaram trabalhando em IA e Deep Learning, e alguns avanços significativos foram feitos. Em 1995, Dana Cortes e Vladimir Vapnik desenvolveram a máquina de vetor de suporte ou Support Vector Machine (um sistema para mapear e reconhecer dados semelhantes). O LSTM (Long-Short Term Memory) para redes neurais recorrentes foi desenvolvido em 1997, por Sepp Hochreiter e Juergen Schmidhuber.
 </p>
 <p style="text-align: justify;">
  O próximo passo evolutivo significativo para Deep Learning ocorreu em 1999, quando os computadores começaram a se tornar mais rápidos no processamento de dados e GPUs (unidades de processamento de gráfico) foram desenvolvidas. O uso de GPUs significou um salto no tempo de processamento, resultando em um aumento das velocidades computacionais em 1000 vezes ao longo de um período de 10 anos. Durante esse período, as redes neurais começaram a competir com máquinas de vetor de suporte. Enquanto uma rede neural poderia ser lenta em comparação com uma máquina de vetor de suporte, as redes neurais ofereciam melhores resultados usando os mesmos dados. As redes neurais também têm a vantagem de continuar a melhorar à medida que mais dados de treinamento são adicionados.
 </p>
 <p style="text-align: justify;">
  Em torno do ano 2000, apareceu o problema conhecido como Vanishing Gradient. Foi descoberto que as “características” aprendidas em camadas mais baixas não eram aprendidas pelas camadas superiores, pois nenhum sinal de aprendizado alcançou essas camadas. Este não era um problema fundamental para todas as redes neurais, apenas aquelas com métodos de aprendizagem baseados em gradientes. A origem do problema acabou por ser certas funções de ativação. Uma série de funções de ativação condensavam sua entrada, reduzindo, por sua vez, a faixa de saída de forma um tanto caótica. Isso produziu grandes áreas de entrada mapeadas em uma faixa extremamente pequena. Nessas áreas de entrada, uma grande mudança será reduzida a uma pequena mudança na saída, resultando em um gradiente em queda. Duas soluções utilizadas para resolver este problema foram o pré-treino camada-a-camada e o desenvolvimento de uma memória longa e de curto prazo.
 </p>
 <p style="text-align: justify;">
  Em 2001, um relatório de pesquisa do Grupo META (agora chamado Gartner) descreveu os desafios e oportunidades no crescimento do volume de dados. O relatório descreveu o aumento do volume de dados e a crescente velocidade de dados como o aumento da gama de fontes e tipos de dados. Este foi um apelo para se preparar para a investida do Big Data, que estava apenas começando.
 </p>
 <p style="text-align: justify;">
  Em 2009, Fei-Fei Li, professora de IA em Stanford na Califórnia, lançou o ImageNet e montou uma base de dados gratuita de mais de 14 milhões de imagens etiquetadas. Eram necessárias imagens marcadas para “treinar” as redes neurais. A professora Li disse: “Nossa visão é que o Big Data mudará a maneira como a aprendizagem de máquina funciona. Data drives learning.”. Ela acertou em cheio!
 </p>
 <p style="text-align: justify;">
  Até 2011, a velocidade das GPUs aumentou significativamente, possibilitando a formação de redes neurais convolutivas “sem” o pré-treino camada por camada. Com o aumento da velocidade de computação, tornou-se óbvio que Deep Learning tinha vantagens significativas em termos de eficiência e velocidade. Um exemplo é a AlexNet, uma rede neural convolutiva, cuja arquitetura ganhou várias competições internacionais durante 2011 e 2012. As unidades lineares retificadas foram usadas para melhorar a velocidade.
 </p>
 <p style="text-align: justify;">
  Também em 2012, o Google Brain lançou os resultados de um projeto incomum conhecido como The Cat Experiment. O projeto de espírito livre explorou as dificuldades de “aprendizagem sem supervisão”. A Aprendizagem profunda usa “aprendizagem supervisionada”, o que significa que a rede neural convolutiva é treinada usando dados rotulados. Usando a aprendizagem sem supervisão, uma rede neural convolucional é alimentada com dados não marcados, e é então solicitada a busca de padrões recorrentes.
 </p>
 <p style="text-align: justify;">
  O Cat Experiment usou uma rede neural distribuída por mais de 1.000 computadores. Dez milhões de imagens “sem etiqueta” foram tiradas aleatoriamente do YouTube, mostradas ao sistema e, em seguida, o software de treinamento foi autorizado a ser executado. No final do treinamento, um neurônio na camada mais alta foi encontrado para responder fortemente às imagens de gatos. Andrew Ng, o fundador do projeto, disse: “Nós também encontramos um neurônio que respondeu fortemente aos rostos humanos”. A aprendizagem não supervisionada continua a ser um um campo ativo de pesquisa em Aprendizagem Profunda.
 </p>
 <p style="text-align: justify;">
  Atualmente, o processamento de Big Data e a evolução da Inteligência Artificial são ambos dependentes da Aprendizagem Profunda. Com Deep Learning podemos construir sistemas inteligentes e estamos nos aproximando da criação de uma IA totalmente autônoma. Isso vai gerar impacto em todas os segmentos da sociedade e aqueles que souberem trabalhar com a tecnologia, serão os líderes desse novo mundo que se apresenta diante de nós.
 </p>
 <p style="text-align: justify;">
  No próximo capítulo você vai começar a compreender tecnicamente como funciona a Aprendizagem Profunda. Até o capítulo 4.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  Deep Learning in a Nutshell: History and Training from NVIDIA
 </p>
 <p>
  Linnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master’s thesis, Univ. Helsinki.
 </p>
 <p>
  P. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University, Cambridge, MA, 1974.
 </p>
 <p>
  Werbos, P.J. (2006). Backwards differentiation in AD and neural nets: Past links and new opportunities. In Automatic Differentiation: Applications, Theory, and Implementations, pages 15-34. Springer.
 </p>
 <p>
  Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533–536.
 </p>
 <p>
  Widrow, B., &amp; Lehr, M. (1990). 30 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proceedings of the IEEE, 78(9), 1415-1442.
 </p>
 <p>
  D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1, David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group (Eds.). MIT Press, Cambridge, MA, USA 318-362
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-60" href="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-60" href="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-60" href="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-60" href="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-60-5e0dd041939bb" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=60&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-60-5e0dd041939bb" id="like-post-wrapper-140353593-60-5e0dd041939bb">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-4">
 Capítulo 4 – O Neurônio, Biológico e Matemático
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Para compreender a lógica de funcionamento das redes neurais, alguns conceitos básicos referentes ao funcionamento do cérebro humano e seus componentes, os neurônios, são de fundamental importância. A formação das conexões entre as células e algumas considerações sobre como se concebe teoricamente o funcionamento matemático, ajudam a entender as bases da aprendizagem de máquina e das redes neurais. Vejamos como funciona o neurônio biológico deixando Machine Learning de lado por um instante!
 </p>
 <p>
 </p>
 <h2>
  O Neurônio Biológico
 </h2>
 <p style="text-align: justify;">
  O neurônio é a unidade básica do cérebro humano, sendo uma célula especializada na transmissão de informações, pois nelas estão introduzidas propriedades de excitabilidade e condução de mensagens nervosas. O neurônio é constituído por 3 partes principais: a soma ou corpo celular, do qual emanam algumas ramificações denominadas de dendritos, e por uma outra ramificação descendente da soma, porém mais extensa, chamada de axônio. Nas extremidades dos axônios estão os nervos terminais, pelos quais é realizada a transmissão das informações para outros neurônios. Esta transmissão é conhecida como sinapse.
 </p>
 <p>
 </p>
 <p>
  <img alt="Neurônio Biológico" class="aligncenter size-full wp-image-126" data-attachment-id="126" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Neurônio Biológico" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?fit=800%2C357" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?fit=300%2C134" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?fit=800%2C357" data-orig-size="800,357" data-permalink="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/neuronio/" data-recalc-dims="1" height="357" sizes="(max-width: 800px) 100vw, 800px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=800%2C357" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=300%2C134 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=768%2C343 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=200%2C89 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=690%2C308 690w" width="800"/>
 </p>
 <p style="text-align: center;">
  Fig7 – Representação Simplificada do Neurônio Biológico
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Nosso cérebro é formado por bilhões de neurônios. Mas eles não estão isolados. Pelo contrário, existem centenas de bilhões de conexões entre eles, formando uma enorme rede de comunicação, a rede neural. Cada neurônio possui um corpo central, diversos dendritos e um axônio. Os dendritos recebem sinais elétricos de outros neurônios através das sinapses, que constitui o processo de comunicação entre neurônios. O corpo celular processa a informação e envia para outro neurônio.
 </p>
 <p style="text-align: justify;">
  Observe que a soma e os dendritos formam a superfície de entrada do neurônio e o axônio a superfície de saída do fluxo de informação (esse fluxo de informação é importante para compreender o neurônio matemático daqui a pouco). A informação transmitida pelos neurônios na realidade são impulsos elétricos. O impulso elétrico é a mensagem que os neurônios transmitem uns aos outros, ou seja, é a propagação de um estímulo ao longo dos neurônios que pode ser qualquer sinal captado pelos receptores nervosos.
 </p>
 <p style="text-align: justify;">
  Os dendritos têm como função, receber informações, ou impulsos nervosos, oriundos de outros neurônios e conduzi-los até o corpo celular. Ali, a informação é processada e novos impulsos são gerados. Estes impulsos são transmitidos a outros neurônios, passando pelo axônio e atingindo os dendritos dos neurônios seguintes. O corpo do neurônio é responsável por coletar e combinar informações vindas de outros neurônios.
 </p>
 <p style="text-align: justify;">
  O ponto de contato entre a terminação axônica de um neurônio e o dendrito de outro é chamado sinapse. É pelas sinapses que os neurônios se unem funcionalmente, formando as redes neurais. As sinapses funcionam como válvulas, sendo capazes de controlar a transmissão de impulsos, isto é, o fluxo da informação entre os neurônios na rede neural. O efeito das sinapses é variável e é esta variação que dá ao neurônio capacidade de adaptação.
 </p>
 <p style="text-align: justify;">
  Sinais elétricos gerados nos sensores (retina ocular, papilas gustativas, etc…) caminham pelos axônios. Se esses sinais forem superiores a um limiar de disparo (threshold), seguem pelo axônio. Caso contrário, são bloqueados e não prosseguem (são considerados irrelevantes). A passagem desses sinais não é elétrica, mas química (através da substância serotonina). Se o sinal for superior a certo limite (threshold), vai em frente; caso contrário é bloqueado e não segue. Estamos falando aqui do neurônio biológico e preste bastante atenção a palavra threshold, pois ela é a essência do neurônio matemático.
 </p>
 <p style="text-align: justify;">
  Um neurônio recebe sinais através de inúmeros dendritos, os quais são ponderados e enviados para o axônio, podendo ou não seguir adiante (threshold). Na passagem por um neurônio, um sinal pode ser amplificado ou atenuado, dependendo do dendrito de origem, pois a cada condutor, está associado um peso pelo qual o sinal é multiplicado. Os pesos são o que chamamos de memória.
 </p>
 <p style="text-align: justify;">
  Cada região do cérebro é especializada em uma dada função, como processamento de sinais auditivos, sonoros, elaboração de pensamentos, desejos, etc… Esse processamento se dá através de redes particulares interligadas entre si, realizando processamento paralelo. Cada região do cérebro possui uma arquitetura de rede diferente: varia o número de neurônios, de sinapses por neurônio, valor dos thresholds e dos pesos, etc…Os valores dos pesos são estabelecidos por meio de treinamento recebido pelo cérebro durante a vida útil. É a memorização.
 </p>
 <p style="text-align: justify;">
  Inspirados no neurônio biológico, os pesquisadores desenvolveram um modelo de neurônio matemático que se tornou a base da Inteligência Artificial. A ideia era simples: “Se redes neurais formam a inteligência humana, vamos reproduzir isso e criar Inteligência Artificial”. E assim nasceu o neurônio matemático, o qual descrevemos abaixo.
 </p>
 <p>
 </p>
 <h2>
  O Neurônio Matemático
 </h2>
 <p style="text-align: justify;">
  A partir da estrutura e funcionamento do neurônio biológico, pesquisadores tentaram simular este sistema em computador. O modelo mais bem aceito foi proposto por Warren McCulloch e Walter Pitts em 1943, o qual implementa de maneira simplificada os componentes e o funcionamento de um neurônio biológico. Em termos simples, um neurônio matemático de uma rede neural artificial é um componente que calcula a soma ponderada de vários inputs, aplica uma função e passa o resultado adiante.
 </p>
 <p style="text-align: justify;">
  Neste modelo de neurônio matemático, os impulsos elétricos provenientes de outros neurônios são representados pelos chamados sinais de entrada (a letra x nesse diagrama abaixo, que nada mais são do que os dados que alimentam seu modelo de rede neural artificial). Dentre os vários estímulos recebidos, alguns excitarão mais e outros menos o neurônio receptor e essa medida de quão excitatório é o estímulo é representada no modelo de Warren McCulloch e Walter Pitts através dos pesos sinápticos. Quanto maior o valor do peso, mais excitatório é o estímulo. Os pesos sinápticos são representados por wkn neste diagrama abaixo, onde k representa o índice do neurônio em questão e n se refere ao terminal de entrada da sinapse a qual o peso sináptico se refere.
 </p>
 <p style="text-align: justify;">
  A soma ou corpo da célula é representada por uma composição de dois módulos, o primeiro é uma junção aditiva, somatório dos estímulos (sinais de entrada) multiplicado pelo seu fator excitatório (pesos sinápticos), e posteriormente uma função de ativação, que definirá com base nas entradas e pesos sinápticos, qual será a saída do neurônio. O axônio é aqui representado pela saída (yk) obtida pela aplicação da função de ativação. Assim como no modelo biológico, o estímulo pode ser excitatório ou inibitório, representado pelo peso sináptico positivo ou negativo respectivamente.
 </p>
 <p>
 </p>
 <p>
  <img alt="Neurônio Matemático" class="aligncenter size-full wp-image-128" data-attachment-id="128" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Neurônio Matemático" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?fit=543%2C300" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?fit=300%2C166" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?fit=543%2C300" data-orig-size="543,300" data-permalink="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/neuronio-matematico/" data-recalc-dims="1" height="300" sizes="(max-width: 543px) 100vw, 543px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?resize=543%2C300" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?w=543 543w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?resize=300%2C166 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?resize=200%2C110 200w" width="543"/>
 </p>
 <p style="text-align: center;">
  Fig8 – Representação Simplificada do Neurônio Matemático
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O modelo proposto possui uma natureza binária. Tanto os sinais de entrada quanto a saída, são valores binários. McCulloch acreditava que o funcionamento do sistema nervoso central possuía um carater binário, ou seja, um neurônio infuencia ou não outro neurônio, mas posteriormente mostrou-se que não era dessa forma.
 </p>
 <p style="text-align: justify;">
  O neurônio matemático é um modelo simplificado do neurônio biológico. Tais modelos inspirados a partir da análise da geração e propagação de impulsos elétricos pela membrana celular dos neurônios. O neurônio matemático recebe um ou mais sinais de entrada e devolve um único sinal de saída, que pode ser distribuído como sinal de saída da rede, ou como sinal de entrada para um ou vários outros neurônios da camada posterior (que formam a rede neural artificial). Os dendritos e axônios são representados matematicamente apenas pelas sinapses, e a intensidade da ligação é representada por uma grandeza denominada peso sináptico, simbolizada pela letra w. Quando as entradas, x são apresentadas ao neurônio, elas são multiplicadas pelos pesos sinápticos correspondentes, gerando as entradas ponderadas, ou seja, x1 que multiplica w1, etc… Isso descreve uma das bases matemáticas do funcionamento de uma rede neural artificial, a multiplicação de matrizes:
 </p>
 <p>
  <img alt="Matriz" class="aligncenter wp-image-130 size-medium" data-attachment-id="130" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Matriz" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?fit=668%2C328" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?fit=300%2C147" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?fit=668%2C328" data-orig-size="668,328" data-permalink="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/matriz/" data-recalc-dims="1" height="147" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?resize=300%2C147" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?resize=300%2C147 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?resize=200%2C98 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?w=668 668w" width="300"/>
 </p>
 <p style="text-align: center;">
  Fig9 – Multiplicação de Matrizes Entre Sinais de Entrada x e Pesos Sinápticos w (versão simplificada)
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O neurônio então totaliza todos os produtos gerando um único resultado. A esta função se denomina função de combinação. Este valor é então apresentado a uma função de ativação ou função de transferência, que tem, dentre outras, a finalidade de evitar o acréscimo progressivo dos valores de saída ao longo das camadas da rede, visto que tais funções possuem valores máximos e mínimos contidos em intervalos determinados. O uso de funções de transferência não-lineares torna a rede neural uma ferramenta poderosa. Sabe-se que uma rede perceptron de duas camadas com função de transferência não-linear como a função sigmóide (que veremos mais adiante), é denominada de aproximador universal.
 </p>
 <p style="text-align: justify;">
  Um neurônio dispara quando a soma dos impulsos que ele recebe ultrapassa o seu limiar de excitação chamado de threshold. O corpo do neurônio, por sua vez, é emulado por um mecanismo simples que faz a soma dos valores xi e wi recebidos pelo neurônio (soma ponderada) e decide se o neurônio deve ou não disparar (saída igual a 1 ou a 0) comparando a soma obtida ao limiar ou threshold do neurônio. A ativação do neurônio é obtida através da aplicação de uma “função de ativação”, que ativa a saída ou não, dependendo do valor da soma ponderada das suas entradas.
 </p>
 <p style="text-align: justify;">
  Note que este modelo matemático simplificado de um neurônio é estático, ou seja, não considera a dinâmica do neurônio natural. No neurônio biológico, os sinais são enviados em pulsos e alguns componentes dos neurônios biológicos, a exemplo do axônio, funcionam como filtros de frequência.
 </p>
 <p style="text-align: justify;">
  O modelo do neurônio matemático também pode incluir uma polarização ou bias de entrada. Esta variável é incluída ao somatório da função de ativação, com o intuito de aumentar o grau de liberdade desta função e, consequentemente, a capacidade de aproximação da rede. O valor do bias é ajustado da mesma forma que os pesos sinápticos. O bias possibilita que um neurônio apresente saída não nula ainda que todas as suas entradas sejam nulas. Por exemplo, caso não houvesse o bias e todas as entradas de um neurônio fossem nulas, então o valor da função de ativação seria nulo. Desta forma não poderíamos, por exemplo, fazer com o que o neurônio aprendesse a relação pertinente ao ”ou exclusivo” da lógica. Em resumo, temos esses componentes em um neurônio matemático:
 </p>
 <p>
 </p>
 <p>
  <img alt="Resumo do Neurônio" class="aligncenter size-medium wp-image-135" data-attachment-id="135" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Resumo do Neurônio" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?fit=800%2C365" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?fit=300%2C137" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?fit=800%2C365" data-orig-size="800,365" data-permalink="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/neuronio-2/" data-recalc-dims="1" height="137" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=300%2C137" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=300%2C137 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=768%2C350 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=200%2C91 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=690%2C315 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?w=800 800w" width="300"/>
 </p>
 <p style="text-align: center;">
  Fig10 – Representação do Neurônio Matemático
 </p>
 <p>
 </p>
 <ul>
  <li style="text-align: justify;">
   Sinais de entrada { X1, X2, …, Xn }: São os sinais externos normalmente normalizados para incrementar a eficiência computacional dos algoritmos de aprendizagem. São os dados que alimentam seu modelo preditivo.
  </li>
  <li style="text-align: justify;">
   Pesos sinápticos { W1, W2, …, Wn }: São valores para ponderar os sinais de cada entrada da rede. Esses valores são aprendidos durante o treinamento.
  </li>
  <li style="text-align: justify;">
   Combinador linear { Σ }: Agregar todos sinais de entrada que foram ponderados pelos respectivos pesos sinápticos a fim de produzir um potencial de ativação.
  </li>
  <li style="text-align: justify;">
   Limiar de ativação { Θ }: Especifica qual será o patamar apropriado para que o resultado produzido pelo combinador linear possa gerar um valor de disparo de ativação.
  </li>
  <li style="text-align: justify;">
   Potencial de ativação { u }: É o resultado obtido pela diferença do valor produzido entre o combinador linear e o limiar de ativação. Se o valor for positivo, ou seja, se u ≥ 0 então o neurônio produz um potencial excitatório; caso contrário, o potencial será inibitório.
  </li>
  <li style="text-align: justify;">
   Função de ativação { g }: Seu objetivo é limitar a saída de um neurônio em um intervalo valores.
  </li>
  <li style="text-align: justify;">
   Sinal de saída { y}: É o valor final de saída podendo ser usado como entrada de outros neurônios que estão sequencialmente interligados.
  </li>
 </ul>
 <p style="text-align: justify;">
  Os modelos baseados em redes neurais artificiais são os que mais ganharam atenção nos últimos anos por conseguirem resolver problemas de IA nos quais se conseguia pouco avanço com outras técnicas. A partir da concepção do neurônio matemático, várias arquiteturas e modelos com diferentes combinações entre esses neurônios, e aplicando diferentes técnicas matemáticas e estatísticas, surgiram e propiciaram a criação de arquiteturas avançadas de Deep Learning como Redes Neurais Convolucionais, Redes Neurais Recorrentes, Auto Encoders, Generative Adversarial Networks, Memory Networks, entre outras, que estudaremos ao longo deste livro online.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.khanacademy.org/science/biology/human-biology/neuron-nervous-system/v/anatomy-of-a-neuron" rel="noopener" target="_blank">
    Anatomia de um Neurônio
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://datascienceacademy.com.br/blog/bibliografia-machine-learning-e-inteligencia-artificial/" rel="noopener" target="_blank">
    Bibliografia Machine Learning e IA
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/abs/1404.7828" rel="noopener" target="_blank">
    Deep Learning in Neural Networks: An Overview
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.manning.com/books/grokking-deep-learning" rel="noopener" target="_blank">
    Grokking Deep Learning
   </a>
  </span>
 </p>
 <p>
  HAYKIN, S. Redes Neurais, princípios e práticas. Porto Alegre: Bookman, 2001.
 </p>
 <p>
  JAIN, A. K, MAO, J., MOHIUDDIN, K.M. Artificial neural networks: a tutorial. IEEE Computer, v. 29, n. 3, p. 56-63, 1996.
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-124" href="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-124" href="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-124" href="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-124" href="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-124-5e0dd044ad0f9" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=124&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-124-5e0dd044ad0f9" id="like-post-wrapper-140353593-124-5e0dd044ad0f9">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-5">
 Capítulo 5 – Usando Redes Neurais Para Reconhecer Dígitos Manuscritos
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  O sistema visual humano é uma das maravilhas do mundo. Considere a seguinte sequência de dígitos manuscritos:
 </p>
 <p>
 </p>
 <p>
  <img alt="Dígitos" class="aligncenter wp-image-91 size-medium" data-attachment-id="91" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Dígitos" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?fit=623%2C128" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?fit=300%2C62" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?fit=623%2C128" data-orig-size="623,128" data-permalink="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/digits/" data-recalc-dims="1" height="62" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?resize=300%2C62" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?resize=300%2C62 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?resize=200%2C41 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?w=623 623w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A maioria das pessoas reconhece sem esforço esses dígitos como 504192. Essa facilidade é enganosa. Em cada hemisfério do nosso cérebro, os seres humanos têm um córtex visual primário, também conhecido como V1, contendo 140 milhões de neurônios, com dezenas de bilhões de conexões entre eles. E, no entanto, a visão humana envolve não apenas V1, mas uma série inteira de cortices visuais – V2, V3, V4 e V5 – fazendo processamento de imagem progressivamente mais complexo. Nós carregamos em nossas cabeças um supercomputador, sintonizado pela evolução ao longo de centenas de milhões de anos, e soberbamente adaptado para entender o mundo visual. Reconhecer os dígitos manuscritos não é fácil. Em vez disso, nós humanos somos estupendos, surpreendentemente bons, em entender o que nossos olhos nos mostram. Mas quase todo esse trabalho é feito inconscientemente. E, portanto, geralmente não apreciamos o quão difícil é o problema dos nossos sistemas visuais.
 </p>
 <p style="text-align: justify;">
  A dificuldade de reconhecimento do padrão visual torna-se evidente se você tentar escrever um programa de computador para reconhecer dígitos como os acima. O que parece fácil quando nós seres humanos fazemos, de repente, se torna extremamente difícil. Intuições simples sobre como reconhecemos formas – “um 9 tem um loop no topo e um curso vertical no canto inferior direito” – não é tão simples de se expressar algoritmicamente. Quando você tenta construir essas regras de forma precisa, você se perde rapidamente em diversas exceções, ressalvas e casos especiais. É meio desesperador.
 </p>
 <p style="text-align: justify;">
  As redes neurais abordam o problema de uma maneira diferente. A ideia é tomar uma grande quantidade de dígitos manuscritos, conhecidos como exemplos de treinamento, e em seguida, desenvolver um sistema que possa aprender com esses exemplos de treinamento. Em outras palavras, a rede neural usa os exemplos para inferir automaticamente regras para o reconhecimento de dígitos manuscritos. Além disso, ao aumentar o número de exemplos de treinamento, a rede pode aprender mais sobre a caligrafia, e assim melhorar sua precisão. Podemos construir um reconhecedor de dígitos manuscritos melhor usando milhares, milhões ou bilhões de exemplos de treinamento.
 </p>
 <p>
 </p>
 <p>
  <img alt="MNIST" class="aligncenter size-full wp-image-93" data-attachment-id="93" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="MNIST" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?fit=255%2C204" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?fit=255%2C204" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?fit=255%2C204" data-orig-size="255,204" data-permalink="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/mnist_100_digits/" data-recalc-dims="1" height="204" sizes="(max-width: 255px) 100vw, 255px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?resize=255%2C204" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?w=255 255w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?resize=200%2C160 200w" width="255"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Ao longo dos próximos capítulos começaremos nossa jornada rumo às arquiteturas mais avançadas de Deep Learning, desenvolvendo um programa de computador implementando uma rede neural que aprende a reconhecer os dígitos manuscritos. O programa não usará bibliotecas de redes neurais especiais (usaremos apenas linguagem Python). Mas este programa pode reconhecer dígitos com uma precisão de mais de 96%, sem intervenção humana. Além disso, em capítulos posteriores, desenvolveremos ideias que podem melhorar a precisão para mais de 99%. Na verdade, as melhores redes neurais comerciais são agora tão boas que são usadas pelos bancos para processar cheques e por agências de correio para reconhecer endereços.
 </p>
 <p style="text-align: justify;">
  Estamos nos concentrando no reconhecimento de manuscrito porque é um excelente problema protótipo para aprender sobre redes neurais em geral. Como um protótipo, ele atinge um ponto interessante: é desafiador – não é tão simples reconhecer os dígitos manuscritos – mas também não é tão difícil e nem requer uma solução extremamente complicada, ou um tremendo poder computacional. Além disso, é uma ótima maneira de desenvolver técnicas mais avançadas, como a aprendizagem profunda. E assim, ao longo do livro, retornaremos repetidamente ao problema do reconhecimento de dígitos manuscritos. Mais tarde, no livro, vamos discutir como essas ideias podem ser aplicadas a outros problemas em visão computacional, e também em reconhecimento da fala, processamento de linguagem natural e outras áreas.
 </p>
 <p style="text-align: justify;">
  Ao longo do caminho, desenvolveremos muitas ideias-chave sobre as redes neurais, incluindo dois tipos importantes de neurônios artificiais (o perceptron e o neurônio sigmóide) e o algoritmo de aprendizagem padrão para redes neurais, conhecido como descida estocástica do gradiente. Explicaremos porque as coisas são feitas da maneira que elas são e na construção de sua intuição de redes neurais. Isso requer uma discussão mais longa do que apenas apresentar a mecânica básica do que está acontecendo, mas vale a pena para o entendimento mais profundo que você alcançará. E ao final deste livro, você terá uma boa compreensão do que é aprendizado profundo e como isso está transformando o mundo!
 </p>
 <p>
  Caso você não tenha conhecimento em linguagem Python, recomendamos o curso gratuito
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener" target="_blank">
    Python Fundamentos Para Análise de Dados
   </a>
  </span>
  . Ele vai fornecer uma ótima base de tudo que você precisa para começar a desenvolver suas redes neurais.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
  (alguns trechos extraídos e traduzidos com autorização do autor
  <span style="text-decoration: underline;">
   <a href="http://michaelnielsen.org/" rel="noopener" target="_blank">
    Michael Nielsen
   </a>
  </span>
  )
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-62" href="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-62" href="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-62" href="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-62" href="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-62-5e0dd04676444" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=62&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-62-5e0dd04676444" id="like-post-wrapper-140353593-62-5e0dd04676444">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-6">
 Capítulo 6 – O Perceptron – Parte 1
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;">
   Você sabe quais são as principais arquiteturas de redes neurais artificias? Não. Então analise cuidadosamente a imagem abaixo (excelente trabalho criado pela equipe do Asimov Institute, cujo link você encontra na seção de referências ao final deste capítulo):
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="Neural Network Zoo" class="aligncenter size-large wp-image-154" data-attachment-id="154" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Neural Network Zoo" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?fit=683%2C1024" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?fit=200%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?fit=2000%2C3000" data-orig-size="2000,3000" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-1/neuralnetworks-2/" data-recalc-dims="1" height="1024" sizes="(max-width: 683px) 100vw, 683px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=683%2C1024" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=683%2C1024 683w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=200%2C300 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=768%2C1152 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=690%2C1035 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?w=2000 2000w" width="683"/>
 </p>
 <p style="text-align: justify;">
  Incrível, não? São diversas arquiteturas, usadas para resolver diferentes tipos de problemas, como por exemplo as arquiteturas de redes neurais convolucionais usadas em problemas de Visão Computacional e as redes neurais recorrentes usadas em problemas de Processamento de Linguagem Natural. Estudaremos quase todas essas arquiteturas aqui neste livro. Sim, isso mesmo que você leu. Estamos apenas começando!! Caso queira aprender a construir modelos e projetos usando essas arquiteturas e trabalhando com linguagem Python e Google TensorFlow, clique
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/cursos-online/" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  .
 </p>
 <p style="text-align: justify;">
  Embora todas essas arquiteturas sejam de redes neurais artificias, nem todas são de Deep Learning. O que caracteriza modelos de aprendizagem profunda, como o nome sugere, são redes neurais artificias com muitas camadas ocultas (ou intermediárias). Mas antes de chegarmos lá, precisamos passar pela arquitetura mais simples de uma rede neural artificial, o Perceptron. Como diz o ditado: “Toda grande caminhada começa pelo primeiro passo”.
 </p>
 <p style="text-align: justify;">
  O Modelo Perceptron foi desenvolvido nas décadas de 1950 e 1960 pelo cientista Frank Rosenblatt, inspirado em trabalhos anteriores de Warren McCulloch e Walter Pitts. Hoje, é mais comum usar outros modelos de neurônios artificiais, mas o Perceptron permite uma compreensão clara de como funciona uma rede neural em termos matemáticos, sendo uma excelente introdução.
 </p>
 <p style="text-align: justify;">
  Então, como funcionam os Perceptrons? Um Perceptron é um modelo matemático que recebe várias entradas, x1, x2, … e produz uma única saída binária:
 </p>
 <p>
 </p>
 <p>
  <img alt="Perceptron" class="aligncenter size-full wp-image-97" data-attachment-id="97" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Perceptron" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?fit=280%2C138" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?fit=280%2C138" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?fit=280%2C138" data-orig-size="280,138" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-1/perceptron/" data-recalc-dims="1" height="138" sizes="(max-width: 280px) 100vw, 280px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?resize=280%2C138" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?w=280 280w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?resize=200%2C99 200w" width="280"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  No exemplo mostrado, o Perceptron possui três entradas: x1, x2, x3. Rosenblatt propôs uma regra simples para calcular a saída. Ele introduziu pesos, w1, w2, …, números reais expressando a importância das respectivas entradas para a saída. A saída do neurônio, 0 ou 1, é determinada pela soma ponderada,
  <strong>
   Σjwjxj
  </strong>
  , menor ou maior do que algum valor limiar (threshold). Assim como os pesos, o threshold é um número real que é um parâmetro do neurônio. Para colocá-lo em termos algébricos mais precisos:
 </p>
 <p>
 </p>
 <p>
  <img alt="Output" class="aligncenter size-full wp-image-99" data-attachment-id="99" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Output" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?fit=762%2C186" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?fit=300%2C73" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?fit=762%2C186" data-orig-size="762,186" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-1/output/" data-recalc-dims="1" height="186" sizes="(max-width: 762px) 100vw, 762px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=762%2C186" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?w=762 762w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=300%2C73 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=200%2C49 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=690%2C168 690w" width="762"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Esse é o modelo matemático básico. Uma maneira de pensar sobre o Perceptron é que é um dispositivo que toma decisões ao comprovar evidências. Deixe-me dar um exemplo. Não é um exemplo muito realista, mas é fácil de entender, e logo chegaremos a exemplos mais realistas. Suponha que o fim de semana esteja chegando e você ouviu falar que haverá um festival de queijo em sua cidade. Você gosta de queijo e está tentando decidir se deve ou não ir ao festival. Você pode tomar sua decisão pesando três fatores:
 </p>
 <ul>
  <li style="text-align: justify;">
   O tempo está bom?
  </li>
  <li style="text-align: justify;">
   Seu namorado ou namorada quer acompanhá-lo(a)?
  </li>
  <li style="text-align: justify;">
   O festival está perto de transporte público? (Você não possui um carro)
  </li>
 </ul>
 <p style="text-align: justify;">
  Podemos representar estes três fatores pelas variáveis binárias correspondentes x1, x2 e x3. Por exemplo, teríamos x1 = 1 se o tempo estiver bom e x1 = 0 se o tempo estiver ruim. Da mesma forma, x2 = 1 se seu namorado ou namorada quiser ir ao festival com você, e x2 = 0, se não. E similarmente para x3 e transporte público.
 </p>
 <p style="text-align: justify;">
  Agora, suponha que você adore queijo e está disposto a ir ao festival, mesmo que seu namorado ou namorada não esteja interessado e o festival fica em um lugar de difícil acesso e sem transporte público amplamente disponível. Além disso, você realmente detesta mau tempo, e não há como ir ao festival se o tempo estiver ruim. Você pode usar Perceptrons para modelar esse tipo de tomada de decisão.
 </p>
 <p style="text-align: justify;">
  Uma maneira de fazer isso é escolher um peso w1 = 6 para o tempo e w2 = 2 e w3 = 2 para as outras condições. O valor maior de w1 indica que o tempo é muito importante para você, muito mais do que se seu namorado ou namorada vai acompanhá-lo(a) ou se o festival é próximo do transporte público. Finalmente, suponha que você escolha um threshold de 5 para o Perceptron. Com essas escolhas, o Perceptron implementa o modelo de tomada de decisão desejado, produzindo 1 sempre que o tempo estiver bom e 0 sempre que o tempo estiver ruim. Não faz diferença para o resultado se seu namorado ou namorada quer ir, ou se o transporte público está acessível.
 </p>
 <p style="text-align: justify;">
  Variando os pesos e o limiar, podemos obter diferentes modelos de tomada de decisão. Por exemplo, suponha que escolhemos um threshold de 3. Então, o Perceptron decidirá que você deveria ir ao festival sempre que o tempo estiver bom ou quando o festival estiver perto do transporte público e seu namorado ou namorada estiver disposto a se juntar a você. Em outras palavras, seria um modelo diferente de tomada de decisão. Reduzir o threshold significa que você está mais propenso a ir ao festival.
 </p>
 <p style="text-align: justify;">
  Obviamente, o Perceptron não é um modelo completo de tomada de decisão humana! Mas o que o exemplo ilustra é como um Perceptron pode pesar diferentes tipos de evidências para tomar decisões. E deve parecer plausível que uma rede complexa de Perceptrons possa tomar decisões bastante sutis.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="Rede" class="aligncenter size-full wp-image-100" data-attachment-id="100" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Rede" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?fit=540%2C211" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?fit=300%2C117" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?fit=540%2C211" data-orig-size="540,211" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-1/rede/" data-recalc-dims="1" height="211" sizes="(max-width: 540px) 100vw, 540px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?resize=540%2C211" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?w=540 540w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?resize=300%2C117 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?resize=200%2C78 200w" width="540"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Nesta rede, a primeira coluna de Perceptrons – o que chamaremos de primeira camada de Perceptrons – está tomando três decisões muito simples, pesando a evidência de entrada. E quanto aos Perceptrons na segunda camada? Cada um desses Perceptrons está tomando uma decisão ponderando os resultados da primeira camada de tomada de decisão. Desta forma, um Perceptron na segunda camada pode tomar uma decisão em um nível mais complexo e mais abstrato do que os Perceptrons na primeira camada. E as decisões ainda mais complexas podem ser feitas pelos Perceptrons na terceira camada. Desta forma, uma rede de Perceptrons de várias camadas pode envolver-se em uma tomada de decisão sofisticada.
 </p>
 <p style="text-align: justify;">
  Aliás, quando definimos os Perceptrons, dissemos que um Perceptron possui apenas uma saída. Na rede acima, os Perceptrons parecem ter múltiplos resultados. Na verdade, eles ainda são de saída única. As setas de saída múltiplas são meramente uma maneira útil de indicar que a saída de um Perceptron está sendo usada como entrada para vários outros Perceptrons.
 </p>
 <p style="text-align: justify;">
  Vamos simplificar a maneira como descrevemos os Perceptrons. No limite de condição
  <strong>
   Σjwjxj &gt; threshold
  </strong>
  podemos fazer duas mudanças de notação para simplificá-lo. A primeira mudança é escrever Σjwjxj como um produto (dot product), w⋅x≡Σjwjxj, onde w e x são vetores cujos componentes são os pesos e entradas, respectivamente. A segunda mudança é mover o threshold para o outro lado da equação e substituí-lo pelo que é conhecido como o viés (bias) do Perceptron, ou b ≡ -threshold. Usando o viés em vez do threshold, a regra Perceptron pode ser reescrita:
 </p>
 <p>
 </p>
 <p>
  <img alt="Fórmula Perceptron" class="aligncenter size-full wp-image-157" data-attachment-id="157" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Fórmula Perceptron" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?fit=295%2C81" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?fit=295%2C81" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?fit=295%2C81" data-orig-size="295,81" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-1/formula/" data-recalc-dims="1" height="81" sizes="(max-width: 295px) 100vw, 295px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?resize=295%2C81" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?w=295 295w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?resize=200%2C55 200w" width="295"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Você pode pensar no viés como uma medida de quão fácil é obter o Perceptron para produzir um 1. Ou para colocá-lo em termos mais biológicos, o viés é uma medida de quão fácil é fazer com que o Perceptron dispare. Para um Perceptron com um viés realmente grande, é extremamente fácil para o Perceptron emitir um 1. Mas se o viés é muito negativo, então é difícil para o Perceptron emitir um 1. Obviamente, a introdução do viés é apenas uma pequena mudança em como descrevemos Perceptrons, mas veremos mais adiante que isso leva a outras simplificações de notação. Por isso, no restante do livro, não usaremos o threshold, usaremos sempre o viés.
 </p>
 <p style="text-align: justify;">
  Agora começa a ficar mais fácil compreender o conceito por trás das redes neurais artificiais e isso será muito útil quando estudarmos arquiteturas mais avançadas! Um Perceptron segue o modelo “feed-forward”, o que significa que as entradas são enviadas para o neurônio, processadas e resultam em uma saída. No diagrama abaixo, isso significa que a rede (um neurônio) lê da esquerda para a direita.
 </p>
 <p>
 </p>
 <p>
  <img alt="Neurônio" class="aligncenter wp-image-162 size-full" data-attachment-id="162" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Neurônio" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?fit=945%2C417" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?fit=300%2C132" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?fit=945%2C417" data-orig-size="945,417" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-1/neuronio-4/" data-recalc-dims="1" height="417" sizes="(max-width: 945px) 100vw, 945px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=945%2C417" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?w=945 945w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=300%2C132 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=768%2C339 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=200%2C88 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=690%2C304 690w" width="945"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O processo de treinamento de um modelo Perceptron consiste em fazer com que o modelo aprenda os valores ideais de pesos e bias. Apresentamos ao modelo os dados de entrada e as possíveis saídas, treinamos o modelo e pesos e bias são aprendidos. Com o modelo treinado, podemos apresentar novos dados de entrada e o modelo será capaz de prever a saída. Veremos isso em breve quando criarmos nosso primeiro modelo usando linguagem Python.
 </p>
 <p style="text-align: justify;">
  Perceptron é uma rede neural de camada única e um Perceptron de várias camadas é chamado de Rede Neural Artificial. O Perceptron é um classificador linear (binário). Além disso, é usado na aprendizagem supervisionada e pode ser usado para classificar os dados de entrada fornecidos.
 </p>
 <p>
  Mas o Perceptron tem ainda outras características importantes, como a representação de condicionais lógicos (and, or, xor), problemas com dados não linearmente separáveis e as funções de ativação. Mas esses são temas para o próximo capítulo. Até lá!
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.asimovinstitute.org/neural-network-zoo/" rel="noopener" target="_blank">
    The Neural Network Zoo
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
  (alguns trechos extraídos e traduzidos com autorização do autor
  <span style="text-decoration: underline;">
   <a href="http://michaelnielsen.org/" rel="noopener" target="_blank">
    Michael Nielsen
   </a>
  </span>
  )
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-95" href="http://deeplearningbook.com.br/o-perceptron-parte-1/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-95" href="http://deeplearningbook.com.br/o-perceptron-parte-1/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-95" href="http://deeplearningbook.com.br/o-perceptron-parte-1/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-95" href="http://deeplearningbook.com.br/o-perceptron-parte-1/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-perceptron-parte-1/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-perceptron-parte-1/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-95-5e0dd04872547" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=95&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-95-5e0dd04872547" id="like-post-wrapper-140353593-95-5e0dd04872547">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-7">
 Capítulo 7 – O Perceptron – Parte 2
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  O Perceptron é um modelo matemático de um neurônio biológico. Enquanto nos neurônios reais o dendrito recebe sinais elétricos dos axônios de outros neurônios, no Perceptron estes sinais elétricos são representados como valores numéricos. Nas sinapses entre dendritos e axônio, os sinais elétricos são modulados em várias quantidades. Isso também é modelado no Perceptron multiplicando cada valor de entrada por um valor chamado peso. Um neurônio real dispara um sinal de saída somente quando a força total dos sinais de entrada excede um certo limiar. Nós modelamos esse fenômeno em um Perceptron calculando a soma ponderada das entradas para representar a força total dos sinais de entrada e aplicando uma função de ativação na soma para determinar sua saída. Tal como nas redes neurais biológicas, esta saída é alimentada em outros Perceptrons. Estudamos tudo isso no capítulo anterior. Agora vamos continuar nossa discussão sobre o Perceptron compreendendo mais alguns conceitos, que serão fundamentais mais a frente quando estudarmos as arquiteturas avançadas de Deep Learning.
 </p>
 <p style="text-align: justify;">
  Antes de iniciar, vamos definir dois conceitos que você vai ver com frequência daqui em diante, vetor de entrada e vetor de pesos:
 </p>
 <p style="text-align: justify;">
  <strong>
   Vetor de entrada
  </strong>
  –  todos os valores de entrada de cada Perceptron são coletivamente chamados de vetor de entrada desse Perceptron. Esses são seus dados de entrada.
 </p>
 <p style="text-align: justify;">
  <strong>
   Vetor de pesos
  </strong>
  – de forma semelhante, todos os valores de peso de cada Perceptron são coletivamente chamados de vetor de peso desse Perceptron. Iniciamos nossa rede neural artificial com valores aleatórios de pesos e durante o treinamento a rede neural aprende os valores de peso ideais. Como veremos, existem muitas formas de realizar esse processo.
 </p>
 <p style="text-align: justify;">
  Boa parte do trabalho de uma rede neural vai girar em torno das operações algébricas entre o vetor de entrada e o vetor de pesos. Em seguida, vamos adicionando outras camadas matemáticas ou estatísticas para realizar diferentes operações, de acordo com o problema que estamos tentando resolver com o modelo de rede neural. Você vai perceber que tudo não passa de Matemática, que pode ser implementada com linguagens de programação, grandes conjuntos de dados e processamento paralelo, para formar sistemas de Inteligência Artificial.
 </p>
 <p>
 </p>
 <h2 style="text-align: justify;">
  Mas o que um Perceptron pode fazer afinal?
 </h2>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/capitulo-6-o-perceptron-parte-1/" rel="noopener" target="_blank">
    No capítulo anterior
   </a>
  </span>
  descrevemos os Perceptrons como um método para pesar evidências a fim de tomar decisões. Outra forma em que os Perceptrons podem ser usados é para calcular as funções lógicas elementares tais como AND, OR e NAND (caso tenha dúvidas sobre as operações lógicas, consulte as referências ao final deste capítulo). Por exemplo, suponha que tenhamos um Perceptron com duas entradas, cada uma com peso -2 e um viés de 3. Aqui está o nosso Perceptron:
 </p>
 <p>
 </p>
 <p>
  <img alt="Perceptron" class="aligncenter size-full wp-image-177" data-attachment-id="177" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Perceptron" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?fit=250%2C104" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?fit=250%2C104" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?fit=250%2C104" data-orig-size="250,104" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-2/perceptron-2/" data-recalc-dims="1" height="104" sizes="(max-width: 250px) 100vw, 250px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?resize=250%2C104" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?w=250 250w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?resize=200%2C83 200w" width="250"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Então vemos que a entrada 00 produziria a saída 1, uma vez que (-2) * 0 + (- 2) * 0 + 3 = 3, é positivo (resultado positivo, gera saída 1 do Perceptron, lembra do capítulo anterior?). Aqui, incluímos o símbolo * para tornar as multiplicações explícitas. Cálculos similares mostram que as entradas 01 e 10 produzem a saída 1. Mas a entrada 11 produz a saída 0, uma vez que (-2) * 1 + (- 2) * 1 + 3 = -1, é negativo. E assim nosso Perceptron implementa um “portão” NAND, ou uma operação lógica binária NAND.
 </p>
 <p style="text-align: justify;">
  O exemplo NAND mostra que podemos usar Perceptrons para calcular funções lógicas simples. Na verdade, podemos usar redes de Perceptrons para calcular qualquer função lógica. A razão é que o portão NAND é universal para computação, ou seja, podemos construir qualquer computação com portões NAND.
 </p>
 <p style="text-align: justify;">
  Uma rede de Perceptrons pode ser usada para simular um circuito contendo muitos portões NAND. E como os portões NAND são universais para a computação, segue-se que os Perceptrons também são universais para a computação. Considerando que o Perceptron é o modelo mais simples de rede neural, imagine o que pode ser feito com modelos bem mais avançados! Acertou se você pensou em Inteligência Artificial.
 </p>
 <p style="text-align: justify;">
  A universalidade computacional dos Perceptrons é simultaneamente reconfortante e decepcionante. É reconfortante porque nos diz que redes de Perceptrons podem ser tão poderosas como qualquer outro dispositivo de computação. Mas também é decepcionante, porque parece que os Perceptrons são meramente um novo tipo de portão NAND. Isso não é uma grande noticia!
 </p>
 <p style="text-align: justify;">
  No entanto, a situação é melhor do que esta visão sugere. Acontece que podemos conceber algoritmos de aprendizado que podem ajustar automaticamente os pesos e os vieses de uma rede de neurônios artificiais. Este ajuste ocorre em resposta a estímulos externos, sem intervenção direta de um programador. Esses algoritmos de aprendizagem nos permitem usar neurônios artificiais de uma maneira que é radicalmente diferente dos portões lógicos convencionais. Em vez de colocar explicitamente um circuito de NAND e outros portões, nossas redes neurais podem simplesmente aprender a resolver problemas, às vezes problemas em que seriam extremamente difíceis de projetar diretamente usando um circuito convencional de lógica.
 </p>
 <p>
 </p>
 <h2>
  Operações Lógicas e Regiões Linearmente Separáveis
 </h2>
 <p style="text-align: justify;">
  Conforme mencionado acima, um Perceptron calcula a soma ponderada dos valores de entrada. Por simplicidade, suponhamos que existem dois valores de entrada, x e y para um certo Perceptron P. Vamos definir os pesos de x e y, como sendo A e B, respectivamente. A soma ponderada pode ser representada como: A x + B y.
 </p>
 <p style="text-align: justify;">
  Uma vez que o Perceptron produz um valor não-zero somente quando a soma ponderada excede um certo limite C, pode-se escrever a saída deste Perceptron da seguinte maneira:
 </p>
 <p>
  <img alt="Regra Perceptron" class="aligncenter wp-image-192 size-full" data-attachment-id="192" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Regra Perceptron" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?fit=882%2C256" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?fit=300%2C87" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?fit=882%2C256" data-orig-size="882,256" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-2/regra/" data-recalc-dims="1" height="256" sizes="(max-width: 882px) 100vw, 882px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=882%2C256" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?w=882 882w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=300%2C87 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=768%2C223 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=200%2C58 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=690%2C200 690w" width="882"/>
 </p>
 <p style="text-align: justify;">
  Considerando que A x + B y &gt; C e A x + B y &lt; C são as duas regiões no plano xy separadas pela linha A x + B y + C = 0, e se considerarmos ainda a entrada (x, y) como um ponto em um plano, então o Perceptron realmente nos diz qual região no plano a que esse ponto pertence. Tais regiões, uma vez que são separadas por uma única linha, são chamadas de regiões linearmente separáveis.
 </p>
 <p style="text-align: justify;">
  Um único Perceptron consegue resolver somente funções linearmente separáveis. Em funções não linearmente separáveis, o Perceptron não consegue gerar um hiperplano, esta linha nos gráficos abaixo, para separar os dados. A questão é que no mundo real raramente os dados são linearmente separáveis, fazendo com o que o Perceptron não seja muito útil para atividades práticas (mas sendo ideal para iniciar o estudo em redes neurais artificiais). E como separamos os dados não linearmente separáveis? Continue acompanhando este livro e você irá descobrir.
 </p>
 <p>
  <img alt="Linear e Não-Linear" class="aligncenter wp-image-194 size-full" data-attachment-id="194" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Linear e Não-Linear" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?fit=771%2C369" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?fit=300%2C144" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?fit=771%2C369" data-orig-size="771,369" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-2/linsep_new/" data-recalc-dims="1" height="369" sizes="(max-width: 771px) 100vw, 771px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=771%2C369" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?w=771 771w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=300%2C144 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=768%2C368 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=200%2C96 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=690%2C330 690w" width="771"/>
 </p>
 <p style="text-align: justify;">
  Mas ainda assim o Perceptron tem sua utilidade, porque resulta em algumas funções lógicas, como os operadores booleanos AND, OR e NOT, que são linearmente separáveis, isto é, eles podem ser realizadas usando um único Perceptron. Podemos ilustrar porque eles são linearmente separáveis ao traçar cada um deles em um gráfico:
 </p>
 <p>
 </p>
 <p>
  <img alt="Funções Lógicas" class="aligncenter wp-image-183 size-medium" data-attachment-id="183" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Funções Lógicas" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?fit=816%2C172" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?fit=300%2C63" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?fit=816%2C172" data-orig-size="816,172" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-2/functions/" data-recalc-dims="1" height="63" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=300%2C63" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=300%2C63 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=768%2C162 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=200%2C42 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=690%2C145 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?w=816 816w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Nos gráficos acima, os dois eixos são as entradas que podem levar o valor de 0 ou 1 e os números no gráfico são a saída esperada para uma entrada específica. Usando um vetor de peso apropriado para cada caso, um único Perceptron pode executar todas essas funções.
 </p>
 <p style="text-align: justify;">
  No entanto, nem todos os operadores de lógica são linearmente separáveis. Por exemplo, o operador XOR não é linearmente separável e não pode ser alcançado por um único Perceptron. No entanto, esse problema poderia ser superado usando mais de um Perceptron organizado em redes neurais feed-forward, que veremos mais a frente nos próximos capítulos.
 </p>
 <p>
 </p>
 <p>
  <img alt="xor" class="aligncenter size-full wp-image-184" data-attachment-id="184" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="xor" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?fit=108%2C71" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?fit=108%2C71" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?fit=108%2C71" data-orig-size="108,71" data-permalink="http://deeplearningbook.com.br/o-perceptron-parte-2/xor/" data-recalc-dims="1" height="71" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?resize=108%2C71" width="108"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Uma vez que é impossível desenhar uma linha para dividir as regiões contendo 1 ou 0, a função XOR não é linearmente separável, conforme pode ser visto no gráfico acima.
 </p>
 <p style="text-align: justify;">
  Agora fica mais fácil compreender porque precisamos de arquiteturas mais avançadas de redes neurais artificiais, uma vez que temos problemas complexos no mundo real, como Visão Computacional, Processamento de Linguagem Natural, Tradução, Detecção de Fraudes, Classificação e muitos outros. E veremos essas arquiteturas em detalhes. Mas antes, precisamos falar sobre um componente fundamental das redes neurais, a Função de Ativação. Não perca o próximo capítulo. Até lá.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Porta_AND" rel="noopener" target="_blank">
    Operação Lógica AND
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Porta_OR" rel="noopener" target="_blank">
    Operação Lógica OR
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Porta_NAND" rel="noopener" target="_blank">
    Operação Lógica NAND
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Porta_XOR" rel="noopener" target="_blank">
    Operação Lógica XOR
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
  (alguns trechos extraídos e traduzidos com autorização do autor
  <span style="text-decoration: underline;">
   <a href="http://michaelnielsen.org/" rel="noopener" target="_blank">
    Michael Nielsen
   </a>
  </span>
  )
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-175" href="http://deeplearningbook.com.br/o-perceptron-parte-2/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-175" href="http://deeplearningbook.com.br/o-perceptron-parte-2/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-175" href="http://deeplearningbook.com.br/o-perceptron-parte-2/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-175" href="http://deeplearningbook.com.br/o-perceptron-parte-2/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-perceptron-parte-2/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-perceptron-parte-2/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-175-5e0dd04a5505a" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=175&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-175-5e0dd04a5505a" id="like-post-wrapper-140353593-175-5e0dd04a5505a">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-8">
 Capítulo 8 – Função de Ativação
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Neste capítulo estudaremos um importante componente de uma rede neural artificial, a Função de Ativação. Este capítulo é uma introdução ao tema e voltaremos a ele mais adiante quando estudarmos as arquiteturas avançadas de
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/capitulo-1-deep-learning-a-tempestade-perfeita/" rel="noopener noreferrer" target="_blank">
    Deep Learning
   </a>
  </span>
  . Este capítulo pode ser um pouco desafiador, pois começaremos a introduzir conceitos mais avançados, que serão muito úteis na sequência dos capítulos. Relaxe, faça a leitura e aprenda um pouco mais sobre redes neurais artificiais.
 </p>
 <p style="text-align: justify;">
  Antes de mergulhar nos detalhes das funções de ativação, vamos fazer uma pequena revisão do que são
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/capitulo-3-o-que-sao-redes-neurais-artificiais-profundas/" rel="noopener noreferrer" target="_blank">
    redes neurais artificiais
   </a>
  </span>
  e como funcionam. Uma rede neural é um mecanismo de aprendizado de máquina (Machine Learning) muito poderoso que imita basicamente como um cérebro humano aprende. O cérebro recebe o estímulo do mundo exterior, faz o processamento e gera o resultado. À medida que a tarefa se torna complicada, vários neurônios formam uma rede complexa, transmitindo informações entre si. Usando uma rede neural artificial, tentamos imitar um comportamento semelhante. A rede que você vê abaixo é uma rede neural artificial composta de neurônios interligados.
 </p>
 <p>
 </p>
 <p>
  <img alt="Neural Network" class="aligncenter wp-image-215 size-full" data-attachment-id="215" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Neural Network" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?fit=572%2C312" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?fit=300%2C164" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?fit=572%2C312" data-orig-size="572,312" data-permalink="http://deeplearningbook.com.br/funcao-de-ativacao/nn/" data-recalc-dims="1" height="312" sizes="(max-width: 572px) 100vw, 572px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?resize=572%2C312" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?w=572 572w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?resize=300%2C164 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?resize=200%2C109 200w" width="572"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Os círculos negros na imagem acima são neurônios. Cada neurônio é caracterizado pelo peso, bias e a função de ativação. Os dados de entrada são alimentados na camada de entrada. Os neurônios fazem uma transformação linear na entrada pelos pesos e bias. A transformação não linear é feita pela função de ativação. A informação se move da camada de entrada para as camadas ocultas. As camadas ocultas fazem o processamento e enviam a saída final para a camada de saída. Este é o movimento direto da informação conhecido como propagação direta. Mas e se o resultado gerado estiver longe do valor esperado? Em uma rede neural, atualizaríamos os pesos e bias dos neurônios com base no erro. Este processo é conhecido como backpropagation. Uma vez que todos os dados passaram por este processo, os pesos e bias finais são usados para previsões.
 </p>
 <p style="text-align: justify;">
  Calma, calma, calma. Muita informação em um único parágrafo, eu sei! Vamos por partes. As entradas, os pesos e bias nós já discutimos nos capítulos anteriores. A função de ativação vamos discutir agora e a propagação direta e o backpropagation discutimos nos próximos capítulos!
 </p>
 <p>
 </p>
 <h2>
  Função de Ativação
 </h2>
 <p style="text-align: justify;">
  Os algoritmos de aprendizagem são fantásticos. Mas como podemos elaborar esses algoritmos para uma rede neural artificial? Suponhamos que tenhamos uma rede de Perceptrons que gostaríamos de usar para aprender a resolver algum problema. Por exemplo, as entradas para a rede poderiam ser os dados de pixel de uma imagem digitalizada, escrita à mão, de um dígito. Gostaríamos que a rede aprendesse pesos e bias para que a saída da rede classifique corretamente o dígito. Para ver como a aprendizagem pode funcionar, suponha que façamos uma pequena alteração em algum peso (ou bias) na rede. O que queremos é que esta pequena mudança de peso cause apenas uma pequena alteração correspondente na saída da rede. Como veremos em um momento, esta propriedade tornará possível a aprendizagem. Esquematicamente, aqui está o que queremos (obviamente, esta rede é muito simples para fazer reconhecimento de escrita, mas fique tranquilo que veremos redes bem mais complexas).
 </p>
 <p>
 </p>
 <p>
  <img alt="Esquema" class="aligncenter size-full wp-image-207" data-attachment-id="207" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Esquema" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?fit=487%2C270" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?fit=300%2C166" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?fit=487%2C270" data-orig-size="487,270" data-permalink="http://deeplearningbook.com.br/funcao-de-ativacao/tikz8/" data-recalc-dims="1" height="270" sizes="(max-width: 487px) 100vw, 487px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?resize=487%2C270" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?w=487 487w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?resize=300%2C166 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?resize=200%2C111 200w" width="487"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Se fosse verdade que uma pequena alteração em um peso (ou bias) fizesse com que tivéssemos apenas uma pequena alteração no resultado, então poderíamos usar esse fato para modificar os pesos e os valores de bias para que a nossa rede se comporte mais da maneira que queremos. Por exemplo, suponha que a rede classificasse equivocadamente uma imagem como “8” quando deveria ser um “9”. Podemos descobrir como fazer uma pequena mudança nos pesos e bias para que a rede fique um pouco mais próxima da classificação da imagem como “9”. E então, repetiríamos isso, mudando os pesos e os valores de bias repetidamente para produzir melhor e melhor resultado. A rede estaria aprendendo.
 </p>
 <p style="text-align: justify;">
  O problema é que isso não é o que acontece quando nossa rede contém apenas Perceptrons, conforme estudamos nos capítulos anteriores. De fato, uma pequena alteração nos pesos de um único Perceptron na rede pode, por vezes, fazer com que a saída desse Perceptron mude completamente, digamos de 0 a 1. Essa mudança pode então causar o comportamento do resto da rede mudar completamente de uma maneira muito complicada. Então, enquanto o seu “9” pode agora ser classificado corretamente, o comportamento da rede em todas as outras imagens provavelmente mudará completamente de maneira difícil de controlar. Talvez haja uma maneira inteligente de resolver esse problema. Sim, há. E é conhecida como função de ativação.
 </p>
 <p style="text-align: justify;">
  Podemos superar esse problema através da introdução de um componente matemático em nosso neurônio artificial, chamado função de ativação. As funções de ativação permitem que pequenas mudanças nos pesos e bias causem apenas uma pequena alteração no output. Esse é o fato crucial que permitirá que uma rede de neurônios artificiais aprenda.
 </p>
 <p style="text-align: justify;">
  Vejamos como isso funciona:
 </p>
 <p>
 </p>
 <p>
  <img alt="Função de Ativação" class="aligncenter wp-image-211 size-large" data-attachment-id="211" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Função de Ativação" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?fit=1024%2C426" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?fit=300%2C125" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?fit=1100%2C458" data-orig-size="1100,458" data-permalink="http://deeplearningbook.com.br/funcao-de-ativacao/activation-function-2/" data-recalc-dims="1" height="426" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=1024%2C426" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=1024%2C426 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=300%2C125 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=768%2C320 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=200%2C83 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=690%2C287 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?w=1100 1100w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  As funções de ativação são um elemento extremamente importante das redes neurais artificiais. Elas basicamente decidem se um neurônio deve ser ativado ou não. Ou seja, se a informação que o neurônio está recebendo é relevante para a informação fornecida ou deve ser ignorada. Veja na fórmula abaixo como a função de ativação é mais uma camada matemática no processamento.
 </p>
 <p>
 </p>
 <p>
  <img alt="Função de Ativação" class="aligncenter wp-image-217 size-full" data-attachment-id="217" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Função de Ativação" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?fit=406%2C49" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?fit=300%2C36" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?fit=406%2C49" data-orig-size="406,49" data-permalink="http://deeplearningbook.com.br/funcao-de-ativacao/act/" data-recalc-dims="1" height="49" sizes="(max-width: 406px) 100vw, 406px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?resize=406%2C49" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?w=406 406w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?resize=300%2C36 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?resize=200%2C24 200w" width="406"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A função de ativação é a transformação não linear que fazemos ao longo do sinal de entrada. Esta saída transformada é então enviada para a próxima camada de neurônios como entrada. Quando não temos a função de ativação, os pesos e bias simplesmente fazem uma transformação linear. Uma equação linear é simples de resolver, mas é limitada na sua capacidade de resolver problemas complexos. Uma rede neural sem função de ativação é essencialmente apenas um modelo de regressão linear. A função de ativação faz a transformação não-linear nos dados de entrada, tornando-o capaz de aprender e executar tarefas mais complexas. Queremos que nossas redes neurais funcionem em tarefas complicadas, como traduções de idiomas (Processamento de Linguagem Natural) e classificações de imagens (Visão Computacional). As transformações lineares nunca seriam capazes de executar tais tarefas.
 </p>
 <p style="text-align: justify;">
  As funções de ativação tornam possível a propagação posterior desde que os gradientes sejam fornecidos juntamente com o erro para atualizar os pesos e bias. Sem a função não linear diferenciável, isso não seria possível. Caso o termo gradiente não seja familiar, aguarde os próximos capítulos, quando vamos explicar este conceito em detalhes, visto que ele é a essência do processo de aprendizagem em redes neurais artificiais.
 </p>
 <p style="text-align: justify;">
  Mas não existe apenas um tipo de função de ativação. Na verdade existem vários, cada qual a ser usado em diferentes situações. Vamos a uma breve descrição dos tipos mais populares.
 </p>
 <p>
 </p>
 <h2>
  Tipos Populares de Funções de Ativação
 </h2>
 <p style="text-align: justify;">
  A função de ativação é um componente matemático incluído na estrutura de redes neurais artificiais a fim de permitir a solução de problemas complexos. Existem diversos tipos de funções de ativação e esta é uma área de pesquisa ativa, à medida que a Inteligência Artificial evolui (não é maravilhoso estar participando desta evolução, que vai transformar completamente o mundo?). Vejamos quais são os tipos mais populares.
 </p>
 <p>
 </p>
 <h3>
  Função de Etapa Binária (Binary Step Function)
 </h3>
 <p style="text-align: justify;">
  A primeira coisa que vem à nossa mente quando temos uma função de ativação seria um classificador baseado em limiar (threshold), ou seja, se o neurônio deve ou não ser ativado. Se o valor Y estiver acima de um valor de limite determinado, ative o neurônio senão deixa desativado. Simples! Essa seria a regra:
 </p>
 <p>
  <strong>
   f(x) = 1, x&gt;=0
  </strong>
 </p>
 <p>
  <strong>
   f(x) = 0, x&lt;0
  </strong>
 </p>
 <p style="text-align: justify;">
  A função de etapa binária é isso mesmo, extremamente simples. Ela pode ser usada ao criar um classificador binário. Quando simplesmente precisamos dizer sim ou não para uma única classe, a função de etapa seria a melhor escolha, pois ativaria o neurônio ou deixaria zero.
 </p>
 <p style="text-align: justify;">
  A função é mais teórica do que prática, pois, na maioria dos casos, classificamos os dados em várias classes do que apenas uma única classe. A função de etapa não seria capaz de fazer isso.
 </p>
 <p style="text-align: justify;">
  Além disso, o gradiente da função de etapa é zero. Isso faz com que a função de etapa não seja tão útil durante o backpropagation quando os gradientes das funções de ativação são enviados para cálculos de erro para melhorar e otimizar os resultados. O gradiente da função de etapa reduz tudo para zero e a melhoria dos modelos realmente não acontece. Lembrando, mais uma vez, que veremos em detalhes os conceitos de gradiente e backpropagation mais adiante, nos próximos capítulos!
 </p>
 <p>
 </p>
 <h3>
  Função Linear
 </h3>
 <p style="text-align: justify;">
  Nós vimos o problema com a função step, o gradiente sendo zero, é impossível atualizar o gradiente durante a backpropagation. Em vez de uma função de passo simples, podemos tentar usar uma função linear. Podemos definir a função como:
 </p>
 <p style="text-align: justify;">
  <strong>
   f(x) = ax
  </strong>
 </p>
 <p style="text-align: justify;">
  A derivada de uma função linear é constante, isto é, não depende do valor de entrada x. Isso significa que toda vez que fazemos backpropagation, o gradiente seria o mesmo. E este é um grande problema, não estamos realmente melhorando o erro, já que o gradiente é praticamente o mesmo. E não apenas suponha que estamos tentando realizar uma tarefa complicada para a qual precisamos de múltiplas camadas em nossa rede. Agora, se cada camada tiver uma transformação linear, não importa quantas camadas nós tenhamos, a saída final não é senão uma transformação linear da entrada. Portanto, a função linear pode ser ideal para tarefas simples, onde a interpretabilidade é altamente desejada.
 </p>
 <p>
 </p>
 <h3>
  Sigmóide
 </h3>
 <p>
  Sigmóide é uma função de ativação amplamente utilizada. É da forma:
 </p>
 <p>
  <strong>
   f (x) = 1 / (1 + e ^ -x)
  </strong>
 </p>
 <p style="text-align: justify;">
  Esta é uma função suave e é continuamente diferenciável. A maior vantagem sobre a função de etapa e a função linear é que não é linear. Esta é uma característica incrivelmente interessante da função sigmóide. Isto significa essencialmente que quando eu tenho vários neurônios com função sigmóide como função de ativação – a saída também não é linear. A função varia de 0 a 1 tendo um formato S.
 </p>
 <p style="text-align: justify;">
  A função essencialmente tenta empurrar os valores de Y para os extremos. Esta é uma qualidade muito desejável quando tentamos classificar os valores para uma classe específica.
 </p>
 <p style="text-align: justify;">
  A função sigmóide ainda é amplamente utilizada até hoje, mas ainda temos problemas que precisamos abordar. Com a sigmóide temos problemas quando os gradientes se tornam muito pequenos. Isso significa que o gradiente está se aproximando de zero e a rede não está realmente aprendendo.
 </p>
 <p style="text-align: justify;">
  Outro problema que a função sigmóide sofre é que os valores variam apenas de 0 a 1. Esta medida que a função sigmóide não é simétrica em torno da origem e os valores recebidos são todos positivos. Nem sempre desejamos que os valores enviados ao próximo neurônio sejam todos do mesmo sinal. Isso pode ser abordado pela ampliação da função sigmóide. Isso é exatamente o que acontece na função tanh.
 </p>
 <p>
 </p>
 <h3>
  Tanh
 </h3>
 <p>
  A função tanh é muito semelhante à função sigmóide. Na verdade, é apenas uma versão escalonada da função sigmóide.
 </p>
 <p>
  <strong>
   Tanh (x) = 2sigmoides (2x) -1
  </strong>
 </p>
 <p>
  Pode ser escrito diretamente como:
 </p>
 <p>
  <strong>
   tanh (x) = 2 / (1 + e ^ (- 2x)) -1
  </strong>
 </p>
 <p>
  Tanh funciona de forma semelhante à função sigmóide, mas sim simétrico em relação à origem. varia de -1 a 1.
 </p>
 <p style="text-align: justify;">
  Basicamente, soluciona o nosso problema dos valores, sendo todos do mesmo sinal. Todas as outras propriedades são as mesmas da função sigmoide. É contínuo e diferenciável em todos os pontos. A função não é linear, então podemos fazer o backpropagation facilmente nos erros.
 </p>
 <h3>
 </h3>
 <h3>
  ReLU
 </h3>
 <p>
  A função ReLU é a unidade linear rectificada. É definida como:
 </p>
 <p>
  <strong>
   f(x) = max (0, x)
  </strong>
 </p>
 <p style="text-align: justify;">
  ReLU é a função de ativação mais amplamente utilizada ao projetar redes neurais atualmente. Primeiramente, a função ReLU é não linear, o que significa que podemos facilmente copiar os erros para trás e ter várias camadas de neurônios ativados pela função ReLU.
 </p>
 <p style="text-align: justify;">
  A principal vantagem de usar a função ReLU sobre outras funções de ativação é que ela não ativa todos os neurônios ao mesmo tempo. O que isto significa ? Se você olhar para a função ReLU e a entrada for negativa, ela será convertida em zero e o neurônio não será ativado. Isso significa que, ao mesmo tempo, apenas alguns neurônios são ativados, tornando a rede esparsa e eficiente e fácil para a computação.
 </p>
 <p style="text-align: justify;">
  Mas ReLU também pode ter problemas com os gradientes que se deslocam em direção a zero. Mas quando temos um problema, sempre podemos pensar em uma solução. Aliás, isso é o que as empresas mais procuram nos dias de hoje: “resolvedores de problemas”. Seja um e sua empregabilidade estará garantida!
 </p>
 <h3>
 </h3>
 <h3>
  Leaky ReLU
 </h3>
 <p style="text-align: justify;">
  A função Leaky ReLU não passa de uma versão melhorada da função ReLU. Na função ReLU, o gradiente é 0 para x &lt; 0, o que fez os neurônios morrerem por ativações nessa região. Leaky ReLU ajuda a resolver este problema. Em vez de definir a função Relu como 0 para x inferior a 0, definimos como um pequeno componente linear de x. Pode ser definido como:
 </p>
 <p style="text-align: justify;">
  <strong>
   f(x) = ax, x &lt; 0
  </strong>
  <br/>
  <strong>
   f(x) = x, x &gt; = 0
  </strong>
 </p>
 <p style="text-align: justify;">
  O que fizemos aqui é que simplesmente substituímos a linha horizontal por uma linha não-zero, não horizontal. Aqui um é um valor pequeno como 0,01 ou algo parecido. A principal vantagem de substituir a linha horizontal é remover o gradiente zero.
 </p>
 <h3>
 </h3>
 <h3>
  Softmax
 </h3>
 <p style="text-align: justify;">
  A função softmax também é um tipo de função sigmóide, mas é útil quando tentamos lidar com problemas de classificação. A função sigmóide como vimos anteriormente é capaz de lidar com apenas duas classes. O que devemos fazer quando estamos tentando lidar com várias classes? Apenas classificar sim ou não para uma única classe não ajudaria. A função softmax transforma as saídas para cada classe para valores entre 0 e 1 e também divide pela soma das saídas. Isso essencialmente dá a probabilidade de a entrada estar em uma determinada classe. Pode ser definido como:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="Softmax" class="aligncenter size-full wp-image-219" data-attachment-id="219" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Softmax" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?fit=274%2C56" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?fit=274%2C56" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?fit=274%2C56" data-orig-size="274,56" data-permalink="http://deeplearningbook.com.br/funcao-de-ativacao/softmax/" data-recalc-dims="1" height="56" sizes="(max-width: 274px) 100vw, 274px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?resize=274%2C56" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?w=274 274w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?resize=200%2C41 200w" width="274"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Digamos, por exemplo, que temos as saídas como [1.2, 0.9, 0.75], quando aplicamos a função softmax, obteríamos [0.42, 0.31, 0.27]. Então, agora podemos usá-los como probabilidades de que o valor seja de cada classe.
 </p>
 <p style="text-align: justify;">
  A função softmax é idealmente usada na camada de saída do classificador, onde realmente estamos tentando gerar as probabilidades para definir a classe de cada entrada.
 </p>
 <p>
 </p>
 <h3>
  Escolhendo a Função de Ativação Correta
 </h3>
 <p style="text-align: justify;">
  Ufa! Muita coisa, não? E ainda não vimos as questões matemáticas envolvidas nessas funções. Mas não tenhamos pressa, não existe atalho para o aprendizado e estudaremos tudo passo a passo, item a item, no padrão dos cursos na
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br" rel="noopener noreferrer" target="_blank">
    Data Science Academy
   </a>
  </span>
  .
 </p>
 <p style="text-align: justify;">
  Agora que já vimos tantas funções de ativação, precisamos de alguma lógica/heurística para saber qual função de ativação deve ser usada em qual situação. Não há uma regra de ouro e a escolha depende do problema no qual você estiver trabalhando.
 </p>
 <p style="text-align: justify;">
  No entanto, dependendo das propriedades do problema, poderemos fazer uma melhor escolha para uma convergência fácil e rápida da rede neural.
 </p>
 <ul>
  <li style="text-align: justify;">
   Funções Sigmóide e suas combinações geralmente funcionam melhor no caso de classificadores.
  </li>
  <li style="text-align: justify;">
   Funções Sigmóide e Tanh às vezes são evitadas devido ao problema de Vanishing Gradient (que estudaremos no capítulo sobre redes neurais recorrentes).
  </li>
  <li style="text-align: justify;">
   A função ReLU é uma função de ativação geral e é usada na maioria dos casos atualmente.
  </li>
  <li style="text-align: justify;">
   Se encontrarmos um caso de neurônios deficientes em nossas redes, a função Leaky ReLU é a melhor escolha.
  </li>
  <li style="text-align: justify;">
   Tenha sempre em mente que a função ReLU deve ser usada apenas nas camadas ocultas.
  </li>
  <li style="text-align: justify;">
   Como regra geral, você pode começar usando a função ReLU e depois passar para outras funções de ativação no caso da ReLU não fornecer resultados ótimos.
  </li>
 </ul>
 <p style="text-align: justify;">
  Está começando a sentir a vibração em trabalhar com Inteligência Artificial? Então continue acompanhando, pois estamos apenas no começo! Até o próximo capítulo!
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide" rel="noopener noreferrer" target="_blank">
    Função Sigmóide
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener noreferrer" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener noreferrer" target="_blank">
    Vanishing Gradient Problem
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener noreferrer" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
  (alguns trechos extraídos e traduzidos com autorização do autor
  <span style="text-decoration: underline;">
   <a href="http://michaelnielsen.org/" rel="noopener noreferrer" target="_blank">
    Michael Nielsen
   </a>
  </span>
  )
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-206" href="http://deeplearningbook.com.br/funcao-de-ativacao/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-206" href="http://deeplearningbook.com.br/funcao-de-ativacao/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-206" href="http://deeplearningbook.com.br/funcao-de-ativacao/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-206" href="http://deeplearningbook.com.br/funcao-de-ativacao/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/funcao-de-ativacao/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/funcao-de-ativacao/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-206-5e0dd04c1c665" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=206&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-206-5e0dd04c1c665" id="like-post-wrapper-140353593-206-5e0dd04c1c665">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-9">
 Capítulo 9 – A Arquitetura das Redes Neurais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  No capítulo 11 vamos desenvolver uma rede neural para classificação de dígitos manuscritos, usando linguagem Python (caso ainda não saiba trabalhar com a linguagem, comece agora mesmo com nosso curso online totalmente gratuito
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener" target="_blank">
    Python Fundamentos Para Análise de Dados
   </a>
  </span>
  ). Mas antes, vamos compreender a terminologia que será muito útil quando estivermos desenvolvendo nosso modelo, estudando a Arquitetura das Redes Neurais. Suponha que tenhamos a rede abaixo:
 </p>
 <p>
 </p>
 <p>
  <img alt="Rede" class="aligncenter wp-image-236 size-full" data-attachment-id="236" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Rede" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?fit=396%2C211" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?fit=300%2C160" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?fit=396%2C211" data-orig-size="396,211" data-permalink="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/rede-2/" data-recalc-dims="1" height="211" sizes="(max-width: 396px) 100vw, 396px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?resize=396%2C211" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?w=396 396w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?resize=300%2C160 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?resize=200%2C107 200w" width="396"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A camada mais à esquerda nesta rede é chamada de camada de entrada e os neurônios dentro da camada são chamados de neurônios de entrada. A camada mais à direita ou a saída contém os neurônios de saída ou, como neste caso, um único neurônio de saída. A camada do meio é chamada de camada oculta, já que os neurônios nessa camada não são entradas ou saídas. O termo “oculto” talvez soe um pouco misterioso – a primeira vez que ouvi o termo, pensei que devesse ter algum significado filosófico ou matemático profundo – mas isso realmente não significa nada mais do que “uma camada que não é entrada ou saída”. A rede acima tem apenas uma única camada oculta, mas algumas redes possuem múltiplas camadas ocultas. Por exemplo, a seguinte rede de quatro camadas tem duas camadas ocultas:
 </p>
 <p>
 </p>
 <p>
  <img alt="Rede" class="aligncenter wp-image-237 size-full" data-attachment-id="237" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Rede" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?fit=597%2C324" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?fit=300%2C163" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?fit=597%2C324" data-orig-size="597,324" data-permalink="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/rede2/" data-recalc-dims="1" height="324" sizes="(max-width: 597px) 100vw, 597px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?resize=597%2C324" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?w=597 597w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?resize=300%2C163 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?resize=200%2C109 200w" width="597"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Tais redes de camadas múltiplas são chamados de Perceptrons Multicamadas ou MLPs (Multilayer Perceptrons), ou seja, uma rede neural formada por Perceptrons (embora na verdade seja uma rede de neurônios sigmóides, como veremos mais adiante).
 </p>
 <p style="text-align: justify;">
  O design das camadas de entrada e saída em uma rede geralmente é direto. Por exemplo, suponha que estamos tentando determinar se uma imagem manuscrita representa um “9” ou não. Uma maneira natural de projetar a rede é codificar as intensidades dos pixels da imagem nos neurônios de entrada. Se a imagem for uma imagem em escala de cinza 64 x 64, teríamos 64 × 64 = 4.096  neurônios de entrada, com as intensidades dimensionadas adequadamente entre 0 e 1. A camada de saída conterá apenas um único neurônio com valores inferiores a 0,5 indicando que “a imagem de entrada não é um 9” e valores maiores que 0,5 indicando que “a imagem de entrada é um 9”.
 </p>
 <p style="text-align: justify;">
  Embora o design das camadas de entrada e saída de uma rede neural seja frequentemente direto, pode haver bastante variação para o design das camadas ocultas. Em particular, não é possível resumir o processo de design das camadas ocultas com poucas regras simples. Em vez disso, pesquisadores de redes neurais desenvolveram muitas heurísticas de design para as camadas ocultas, que ajudam as pessoas a obter o comportamento que querem de suas redes. Conheceremos várias heurísticas de design desse tipo mais adiante ao longo dos próximos capítulos. O design das camadas ocultas é um dos pontos cruciais em modelos de Deep Learning.
 </p>
 <p style="text-align: justify;">
  Até agora, estamos discutindo redes neurais onde a saída de uma camada é usada como entrada para a próxima camada. Essas redes são chamadas de redes neurais feedforward. Isso significa que não há loops na rede – as informações sempre são alimentadas para a frente, nunca são enviadas de volta. Se tivéssemos loops, acabaríamos com situações em que a entrada para a função σ dependeria da saída. Isso seria difícil de entender e, portanto, não permitimos tais loops.
 </p>
 <p style="text-align: justify;">
  No entanto, existem outros modelos de redes neurais artificiais em que os circuitos de feedback são possíveis. Esses modelos são chamados de redes neurais recorrentes. A ideia nestes modelos é ter neurônios que disparem por algum período de tempo limitado. Disparar pode estimular outros neurônios, que podem disparar um pouco mais tarde, também por uma duração limitada. Isso faz com que ainda mais neurônios disparem e, ao longo do tempo, conseguimos uma cascata de disparos de neurônios. Loops não causam problemas em tal modelo, uma vez que a saída de um neurônio afeta apenas sua entrada em algum momento posterior, não instantaneamente.
 </p>
 <p style="text-align: justify;">
  Geralmente, as arquiteturas de redes neurais podem ser colocadas em 3 categorias específicas:
 </p>
 <h3 style="text-align: justify;">
  1 – Redes Neurais Feed-Forward
 </h3>
 <p style="text-align: justify;">
  Estes são o tipo mais comum de rede neural em aplicações práticas. A primeira camada é a entrada e a última camada é a saída. Se houver mais de uma camada oculta, nós as chamamos de redes neurais “profundas” (ou Deep Learning). Esses tipos de redes neurais calculam uma série de transformações que alteram as semelhanças entre os casos. As atividades dos neurônios em cada camada são uma função não-linear das atividades na camada anterior.
 </p>
 <h3 style="text-align: justify;">
  2 – Redes Recorrentes
 </h3>
 <p style="text-align: justify;">
  Estes tipos de redes neurais têm ciclos direcionados em seu grafo de conexão. Isso significa que às vezes você pode voltar para onde você começou seguindo as setas. Eles podem ter uma dinâmica complicada e isso pode torná-los muito difíceis de treinar. Entretanto, estes tipos são mais biologicamente realistas.
 </p>
 <p style="text-align: justify;">
  Atualmente, há muito interesse em encontrar formas eficientes de treinamento de redes recorrentes. As redes neurais recorrentes são uma maneira muito natural de modelar dados sequenciais. Eles são equivalentes a redes muito profundas com uma camada oculta por fatia de tempo; exceto que eles usam os mesmos pesos em cada fatia de tempo e recebem entrada em cada fatia. Eles têm a capacidade de lembrar informações em seu estado oculto por um longo período de tempo, mas é muito difícil treiná-las para usar esse potencial.
 </p>
 <h3 style="text-align: justify;">
  3 – Redes Conectadas Simetricamente
 </h3>
 <p style="text-align: justify;">
  Estas são como redes recorrentes, mas as conexões entre as unidades são simétricas (elas têm o mesmo peso em ambas as direções). As redes simétricas são muito mais fáceis de analisar do que as redes recorrentes. Elas também são mais restritas no que podem fazer porque obedecem a uma função de energia. As redes conectadas simetricamente sem unidades ocultas são chamadas de “Redes Hopfield”. As redes conectadas simetricamente com unidades ocultas são chamadas de “Máquinas de Boltzmann”.
 </p>
 <hr/>
 <p>
  Dentre estas 3 categorias, podemos listar 10 arquiteturas principais de redes neurais:
 </p>
 <ul>
  <li>
   Redes Multilayer Perceptron
  </li>
  <li>
   Redes Neurais Convolucionais
  </li>
  <li>
   Redes Neurais Recorrentes
  </li>
  <li>
   Long Short-Term Memory (LSTM)
  </li>
  <li>
   Redes de Hopfield
  </li>
  <li>
   Máquinas de Boltzmann
  </li>
  <li>
   Deep Belief Network
  </li>
  <li>
   Deep Auto-Encoders
  </li>
  <li>
   Generative Adversarial Network
  </li>
  <li>
   Deep Neural Network Capsules (este é um tipo completamente novo de rede neural, lançado no final de 2017)
  </li>
 </ul>
 <p style="text-align: justify;">
  Quer aprender a construir essas arquiteturas de redes neurais de forma eficiente, profissional e totalmente prática, com mini-projetos para solução de problemas do mundo real, em visão computacional, processamento de linguagem natural, detecção de fraudes, previsão de séries temporais e muito mais? Então confira os únicos cursos online do Brasil, 100% em português, onde você aprende tudo sobre essas arquiteturas. Clique nos links abaixo para acessar os programas completos:
 </p>
 <p>
 </p>
 <h1 style="text-align: center;">
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-i" rel="noopener" target="_blank">
    Deep Learning I
   </a>
  </span>
 </h1>
 <p>
 </p>
 <h1 style="text-align: center;">
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
    Deep Learning II
   </a>
  </span>
 </h1>
 <p>
 </p>
 <p style="text-align: justify;">
  No próximo capítulo, daremos a você uma visão geral sobre cada uma dessas 10 arquiteturas e ao longo dos capítulos seguintes, estudaremos todas elas. Cada umas dessas arquiteturas tem sido usada para resolver diferentes problemas e criar sistemas de Inteligência Artificial. Saber trabalhar com IA de forma eficiente, será determinante para seu futuro profissional.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide" rel="noopener" target="_blank">
    Função Sigmóide
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener" target="_blank">
    Vanishing Gradient Problem
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
  (alguns trechos extraídos e traduzidos com autorização do autor
  <span style="text-decoration: underline;">
   <a href="http://michaelnielsen.org/" rel="noopener" target="_blank">
    Michael Nielsen
   </a>
  </span>
  )
 </p>
 <p>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-235" href="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-235" href="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-235" href="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-235" href="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-235-5e0dd04e044c7" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=235&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-235-5e0dd04e044c7" id="like-post-wrapper-140353593-235-5e0dd04e044c7">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-10">
 Capítulo 10 – As 10 Principais Arquiteturas de Redes Neurais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  O Aprendizado de Máquina (
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
  ) é necessário para resolver tarefas que são muito complexas para os humanos. Algumas tarefas são tão complexas que é impraticável, senão impossível, que os seres humanos consigam explicar todas as nuances envolvidas. Então, em vez disso, fornecemos uma grande quantidade de dados para um algoritmo de aprendizado de máquina e deixamos que o algoritmo funcione, explorando esses dados e buscando um modelo que alcance o que os
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Cientistas de Dados
   </a>
  </span>
  e
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Engenheiros de IA
   </a>
  </span>
  determinaram como objetivo. Vejamos estes dois exemplos:
 </p>
 <p>
 </p>
 <ul>
  <li style="text-align: justify;">
   É muito difícil escrever programas que solucionem problemas como reconhecer um objeto tridimensional a partir de um novo ponto de vista em novas condições de iluminação em uma cena desordenada. Nós não sabemos qual programa de computador escrever porque não sabemos como ocorre o processo em nosso cérebro. Mesmo se tivéssemos uma boa ideia sobre como fazê-lo, o programa poderia ser incrivelmente complicado.
  </li>
 </ul>
 <p>
 </p>
 <ul>
  <li style="text-align: justify;">
   É difícil escrever um programa para calcular a probabilidade de uma transação de cartão de crédito ser fraudulenta. Pode não haver regras que sejam simples e confiáveis. Precisamos combinar um número muito grande de regras fracas. A fraude é um alvo em movimento, mas o programa precisa continuar mudando.
  </li>
 </ul>
 <p>
 </p>
 <p style="text-align: justify;">
  É onde Machine Learning pode ser aplicado com sucesso. Em vez de escrever um programa à mão para cada tarefa específica, nós coletamos muitos exemplos que especificam a saída correta para uma determinada entrada. Um algoritmo de aprendizagem de máquina recebe esses exemplos e produz um programa que faz o trabalho. O programa produzido pelo algoritmo de aprendizagem pode parecer muito diferente de um programa típico escrito à mão. Pode conter milhões de números. Se o fizermos corretamente, o programa funciona para novos casos (novos dados). Se os dados mudarem, o programa também pode mudar ao treinar em novos dados. E com a redução de custos de computação (principalmente usando processamento em nuvem), grande quantidade de dados (Big Data) e processamento paralelo em GPU, temos as condições perfeitas para a evolução de Machine Learning. O maior problema, por incrível que pareça, será a falta de profissionais qualificados em número suficiente para atender as demandas do mercado.
 </p>
 <p>
  Alguns exemplos de tarefas melhor resolvidas pela aprendizagem de máquina incluem:
 </p>
 <ul>
  <li style="text-align: justify;">
   Reconhecimento de padrões: objetos em cenas reais, identidades faciais ou expressões faciais, palavras escritas ou faladas.
  </li>
  <li style="text-align: justify;">
   Detecção de anomalias: sequências incomuns de transações de cartão de crédito, padrões incomuns de leituras de sensores em máquinas de uma indústria têxtil.
  </li>
  <li style="text-align: justify;">
   Previsão: preços de ações futuros ou taxas de câmbio, quais filmes uma pessoa gostaria de assistir, previsão de vendas.
  </li>
 </ul>
 <p style="text-align: justify;">
  Machine Learning é um campo abrangente dentro da Inteligência Artificial. Mas uma sub-área de Machine Learning, o
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/capitulo-3-o-que-sao-redes-neurais-artificiais-profundas/" rel="noopener" target="_blank">
    Deep Learning
   </a>
  </span>
  (ou Redes Neurais Profundas), vem conseguindo resultados no estado da arte para as tarefas acima mencionadas. Neste capítulo você encontra As 10 Principais Arquiteturas de Redes Neurais, dentre elas as principais arquiteturas de Deep Learning.
 </p>
 <p>
 </p>
 <h2>
  1- Redes Multilayer Perceptrons
 </h2>
 <p style="text-align: justify;">
  O Perceptron, conforme estudamos nos capítulos anteriores, é um algoritmo simples destinado a realizar a classificação binária; isto é, prevê se a entrada pertence a uma determinada categoria de interesse ou não: fraude ou não_fraude, gato ou não_gato.
 </p>
 <p>
  <img alt="Multilayer Perceptrons (MLP)" class="aligncenter wp-image-258 size-large" data-attachment-id="258" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Multilayer Perceptrons (MLP)" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?fit=1024%2C583" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?fit=300%2C171" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?fit=1024%2C583" data-orig-size="1024,583" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/mlp/" data-recalc-dims="1" height="583" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=1024%2C583" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?w=1024 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=300%2C171 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=768%2C437 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=200%2C114 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=690%2C393 690w" width="1024"/>
 </p>
 <p style="text-align: justify;">
  Um Perceptron é um classificador linear; ou seja, é um algoritmo que classifica a entrada separando duas categorias com uma linha reta. A entrada geralmente é um vetor de recursos
  <strong>
   x
  </strong>
  multiplicado por pesos
  <strong>
   w
  </strong>
  e adicionado a um viés (ou bias)
  <strong>
   b
  </strong>
  . Aqui um exemplo do Perceptron: y = w * x + b. Um Perceptron produz uma única saída com base em várias entradas de valor real, formando uma combinação linear usando os pesos (e às vezes passando a saída através de uma função de ativação não linear).
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt" rel="noopener" target="_blank">
    Rosenblatt
   </a>
  </span>
  construiu um Perceptron de uma camada. Ou seja, seu algoritmo não inclui múltiplas camadas, o que permite que as redes neurais modelem uma hierarquia de recursos. Isso impede que o Perceptron consiga realizar classificação não linear, como a função XOR (um disparador do operador XOR quando a entrada exibe uma característica ou outra, mas não ambas, significa “OR exclusivo” “), como
  <span style="text-decoration: underline;">
   <a href="https://mitpress.mit.edu/books/perceptrons" rel="noopener" target="_blank">
    Minsky e Papert
   </a>
  </span>
  mostraram em seu livro.
 </p>
 <p style="text-align: justify;">
  Um Multilayer Perceptron (MLP) é uma rede neural artificial composta por mais de um Perceptron. Eles são compostos por uma camada de entrada para receber o sinal, uma camada de saída que toma uma decisão ou previsão sobre a entrada, e entre esses dois, um número arbitrário de camadas ocultas que são o verdadeiro mecanismo computacional do MLP. MLPs com uma camada oculta são capazes de aproximar qualquer função contínua.
 </p>
 <p style="text-align: justify;">
  O Multilayer Perceptron é uma espécie de “Hello World” da aprendizagem profunda: uma boa forma de começar quando você está aprendendo sobre Deep Learning.
 </p>
 <p style="text-align: justify;">
  Os MLPs são frequentemente aplicados a problemas de aprendizagem supervisionados: treinam em um conjunto de pares entrada-saída e aprendem a modelar a correlação (ou dependências) entre essas entradas e saídas. O treinamento envolve o ajuste dos parâmetros, ou os pesos e bias, do modelo para minimizar o erro. O backpropagation é usado para fazer os ajustes dos pesos e de bias em relação ao erro, e o próprio erro pode ser medido de várias maneiras, inclusive pelo erro quadrático médio (MSE – Mean Squared Error).
 </p>
 <p style="text-align: justify;">
  As redes feed forward, como MLPs, são como ping-pong. Elas são principalmente envolvidas em dois movimentos, uma constante de ida e volta.  Na passagem para a frente, o fluxo de sinal se move da camada de entrada através das camadas ocultas para a camada de saída e a decisão da camada de saída é medida em relação às saídas esperadas.
 </p>
 <p style="text-align: justify;">
  Na passagem para trás, usando o backpropagation e a regra da cadeia (Chain Rule), derivadas parciais da função de erro dos vários pesos e bias são reproduzidos através do MLP. Esse ato de diferenciação nos dá um gradiente, ao longo do qual os parâmetros podem ser ajustados à medida que movem o MLP um passo mais perto do erro mínimo. Isso pode ser feito com qualquer algoritmo de otimização baseado em gradiente, como descida estocástica do gradiente. A rede continua jogando aquele jogo de ping-pong até que o erro não possa mais ser reduzido (chegou ao mínimo possível). Este estado é conhecido como convergência.
 </p>
 <p style="text-align: justify;">
  Parece muita coisa? Sim, é. Veremos esse processo em mais detalhes aqui mesmo neste livro e caso queira aprender a construir modelos MLP para aplicações práticas, através de vídeos em português, clique
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-i" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  .
 </p>
 <p>
 </p>
 <h2>
  2- Redes Neurais Convolucionais
 </h2>
 <p style="text-align: justify;">
  Em 1998,
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/" rel="noopener" target="_blank">
    Yann LeCun
   </a>
  </span>
  e seus colaboradores desenvolveram um reconhecedor, realmente bom, para dígitos manuscritos chamado
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/lenet/" rel="noopener" target="_blank">
    LeNet
   </a>
  </span>
  . Ele usou o backpropagation em uma rede feed forward com muitas camadas ocultas, muitos mapas de unidades replicadas em cada camada, agrupando as saídas de unidades próximas, formando uma rede ampla que pode lidar com vários caracteres ao mesmo tempo, mesmo se eles se sobrepõem e uma inteligente maneira de treinar um sistema completo, não apenas um reconhecedor. Mais tarde, esta arquitetura foi formalizada sob o nome de redes neurais convolucionais.
 </p>
 <p style="text-align: justify;">
  As Redes Neurais Convolucionais (ConvNets ou CNNs) são redes neurais artificiais profundas que podem ser usadas para classificar imagens, agrupá-las por similaridade (busca de fotos) e realizar reconhecimento de objetos dentro de cenas. São algoritmos que podem identificar rostos, indivíduos, sinais de rua, cenouras, ornitorrincos e muitos outros aspectos dos dados visuais.
 </p>
 <p style="text-align: justify;">
  As redes convolucionais realizam o reconhecimento óptico de caracteres (OCR) para digitalizar texto e tornar possível o processamento de linguagem natural em documentos analógicos e manuscritos, onde as imagens são símbolos a serem transcritos. CNNs também podem ser aplicadas a arquivos de áudio quando estes são representados visualmente como um espectrograma. Mais recentemente, as redes convolucionais foram aplicadas diretamente à análise de texto, bem como dados gráficos.
 </p>
 <p style="text-align: justify;">
  A eficácia das redes convolucionais no reconhecimento de imagem é uma das principais razões pelas quais o mundo testemunhou a eficácia do aprendizado profundo. Este tipo de rede está impulsionando grandes avanços em
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-visao-computacional-e-reconhecimento-de-imagens" rel="noopener" target="_blank">
    Visão Computacional
   </a>
  </span>
  , que tem aplicações óbvias em carros autônomos, robótica, drones, segurança, diagnósticos médicos e tratamentos para deficientes visuais.
 </p>
 <p style="text-align: justify;">
  As redes convolucionais ingerem e processam imagens como tensores e tensores são matrizes de números com várias dimensões. Eles podem ser difíceis de visualizar, então vamos abordá-los por analogia. Um escalar é apenas um número, como 7; um vetor é uma lista de números (por exemplo, [7,8,9]); e uma matriz é uma grade retangular de números que ocupam várias linhas e colunas como uma planilha. Geometricamente, se um escalar é um ponto de dimensão zero, então um vetor é uma linha unidimensional, uma matriz é um plano bidimensional, uma pilha de matrizes é um cubo tridimensional e quando cada elemento dessas matrizes tem uma pilha de mapas de recursos ligados a ele, você entra na quarta dimensão. Calma, não se desespere (ainda). Veremos isso mais a frente com calma, quando estudarmos exclusivamente esta arquitetura. Em nossos cursos na Data Science Academy incluímos aulas completas sobre Álgebra Linear, onde escalares, vetores, matrizes e tensores são estudados na teoria e prática, pois este conhecimento é fundamental na construção de redes neurais profundas.
 </p>
 <p>
  A primeira coisa a saber sobre redes convolucionais é que elas não percebem imagens como os humanos. Portanto, você terá que pensar de uma maneira diferente sobre o que uma imagem significa quando é alimentada e processada por uma rede convolucional.
 </p>
 <p>
  <img alt="CNN" class="aligncenter size-full wp-image-260" data-attachment-id="260" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="CNN" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?fit=1000%2C341" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?fit=300%2C102" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?fit=1000%2C341" data-orig-size="1000,341" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/cnn/" data-recalc-dims="1" height="341" sizes="(max-width: 1000px) 100vw, 1000px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=1000%2C341" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?w=1000 1000w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=300%2C102 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=768%2C262 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=200%2C68 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=690%2C235 690w" width="1000"/>
 </p>
 <p style="text-align: justify;">
  As redes convolucionais percebem imagens como volumes; isto é, objetos tridimensionais, em vez de estruturas planas a serem medidas apenas por largura e altura. Isso porque as imagens de cores digitais têm uma codificação vermelho-verde-azul (RGB – Red-Green-Blue), misturando essas três cores para produzir o espectro de cores que os seres humanos percebem. Uma rede convolucional recebe imagens como três estratos separados de cores empilhados um em cima do outro.
 </p>
 <p style="text-align: justify;">
  Assim, uma rede convolucional recebe uma imagem como uma caixa retangular cuja largura e altura são medidas pelo número de pixels ao longo dessas dimensões e cuja profundidade é de três camadas profundas, uma para cada letra em RGB. Essas camadas de profundidade são referidas como canais.
 </p>
 <p style="text-align: justify;">
  À medida que as imagens se movem através de uma rede convolucional, descrevemos em termos de volumes de entrada e saída, expressando-as matematicamente como matrizes de múltiplas dimensões dessa forma: 30x30x3. De camada em camada, suas dimensões mudam à medida que atravessam a rede neural convolucional até gerar uma série de probabilidades na camada de saída, sendo uma probabilidade para cada possível classe de saída. Aquela com maior probabilidade, será a classe definida para a imagem de entrada, um pássaro por exemplo.
 </p>
 <p style="text-align: justify;">
  Você precisará prestar muita atenção às medidas de cada dimensão do volume da imagem, porque elas são a base das operações de álgebra linear usadas para processar imagens. Poderíamos dedicar dois capítulos inteiros somente a esta arquitetura. Aliás, é o que faremos mais à frente aqui no livro e o que já fazemos na prática
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-i" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  .
 </p>
 <p>
 </p>
 <h2>
  3- Redes Neurais Recorrentes
 </h2>
 <p style="text-align: justify;">
  As redes recorrentes são um poderoso conjunto de algoritmos de redes neurais artificiais especialmente úteis para o processamento de dados sequenciais, como som, dados de séries temporais ou linguagem natural. Uma versão de redes recorrentes foi usada pelo
  <span style="text-decoration: underline;">
   <a href="https://deepmind.com/" rel="noopener" target="_blank">
    DeepMind
   </a>
  </span>
  no projeto de videogames com agentes autônomos.
 </p>
 <p style="text-align: justify;">
  As redes recorrentes diferem das redes feed forward porque incluem um loop de feedback, pelo qual a saída do passo n-1 é alimentada de volta à rede para afetar o resultado do passo n, e assim por diante para cada etapa subsequente. Por exemplo, se uma rede é exposta a uma palavra letra por letra, e é solicitado a adivinhar cada letra a seguir, a primeira letra de uma palavra ajudará a determinar o que uma rede recorrente pensa que a segunda letra pode ser.
 </p>
 <p style="text-align: justify;">
  Isso difere de uma rede feed forward, que aprende a classificar cada número manuscrito por exemplo, independentemente, e de acordo com os pixels de que é exposto a partir de um único exemplo, sem se referir ao exemplo anterior para ajustar suas previsões. As redes feed forward aceitam uma entrada por vez e produzem uma saída. As redes recorrentes não enfrentam a mesma restrição um-para-um.
 </p>
 <p style="text-align: justify;">
  Embora algumas formas de dados, como imagens, não pareçam ser sequenciais, elas podem ser entendidas como sequências quando alimentadas em uma rede recorrente. Considere uma imagem de uma palavra manuscrita. Assim como as redes recorrentes processam a escrita manual, convertendo cada imagem em uma letra e usando o início de uma palavra para adivinhar como essa palavra terminará, então as redes podem tratar parte de qualquer imagem como letras em uma sequência. Uma rede neural que percorre uma imagem grande pode aprender a partir de cada região, o que as regiões vizinhas, são mais prováveis ​​de ser.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="Redes Neurais Recorrentes" class="aligncenter size-full wp-image-262" data-attachment-id="262" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Redes Neurais Recorrentes" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?fit=400%2C318" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?fit=300%2C239" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?fit=400%2C318" data-orig-size="400,318" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/rnn/" data-recalc-dims="1" height="318" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?resize=400%2C318" width="400"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  As redes recorrentes e as redes feed forward “lembram” algo sobre o mundo, modelando os dados que estão expostos. Mas elas se lembram de maneiras muito diferentes. Após o treinamento, a rede feed forward produz um modelo estático dos dados e esse modelo pode então aceitar novos exemplos e classificá-los ou agrupá-los com precisão.
 </p>
 <p style="text-align: justify;">
  Em contraste, as redes recorrentes produzem modelos dinâmicos – ou seja, modelos que mudam ao longo do tempo – de formas que produzem classificações precisas dependentes do contexto dos exemplos que estão expostos.
 </p>
 <p style="text-align: justify;">
  Para ser preciso, um modelo recorrente inclui o estado oculto que determinou a classificação anterior em uma série. Em cada etapa subsequente, esse estado oculto é combinado com os dados de entrada do novo passo para produzir a) um novo estado oculto e, em seguida, b) uma nova classificação. Cada estado oculto é reciclado para produzir seu sucessor modificado.
 </p>
 <p style="text-align: justify;">
  As memórias humanas também são conscientes do contexto, reciclando a consciência de estados anteriores para interpretar corretamente novos dados. Por exemplo, vamos considerar dois indivíduos. Um está ciente de que ele está perto da casa de Bob. O outro está ciente de que entrou em um avião. Eles interpretarão os sons “Oi Bob!” de duas formas muito diferentes, precisamente porque retém um estado oculto afetado por suas memórias de curto prazo e sensações precedentes.
 </p>
 <p style="text-align: justify;">
  Diferentes lembranças de curto prazo devem ser recontadas em momentos diferentes, a fim de atribuir o significado certo à entrada atual. Algumas dessas memórias terão sido forjadas recentemente e outras memórias terão forjado muitos passos antes de serem necessários. A rede recorrente que efetivamente associa memórias e entrada remota no tempo é chamada de Memória de Longo Prazo (LSTM), a qual veremos em seguida.
 </p>
 <p>
 </p>
 <h2>
  4- Long Short-Term Memory (LSTM)
 </h2>
 <p style="text-align: justify;">
  Em meados dos anos 90, a proposta dos pesquisadores alemães
  <a href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener" target="_blank">
   Sepp Hochreiter e Juergen Schmidhuber
  </a>
  apresentou uma variação da rede recorrente com as chamadas unidades de Long Short-Term Memory, como uma solução para o problema do vanishing gradient, problema comum em redes neurais recorrentes.
 </p>
 <p style="text-align: justify;">
  Os LSTMs ajudam a preservar o erro que pode ser copiado por tempo e camadas. Ao manter um erro mais constante, eles permitem que as redes recorrentes continuem aprendendo durante vários passos de tempo (mais de 1000), abrindo assim um canal para vincular causas e efeitos remotamente. Este é um dos desafios centrais para a aprendizagem de máquina e a IA, uma vez que os algoritmos são frequentemente confrontados por ambientes onde os sinais de recompensa são escassos e atrasados, como a própria vida. (Os pensadores religiosos abordaram este mesmo problema com ideias de karma ou recompensas divinas, teorizando consequências invisíveis e distantes para nossas ações).
 </p>
 <p style="text-align: justify;">
  Os LSTMs contêm informações fora do fluxo normal da rede recorrente em uma célula fechada. As informações podem ser armazenadas, escritas ou lidas a partir de uma célula, como dados na memória de um computador. A célula toma decisões sobre o que armazenar, e quando permitir leituras, gravações e exclusões, através de portões abertos e fechados. Ao contrário do armazenamento digital em computadores, no entanto, esses portões são analógicos, implementados com a multiplicação de elementos por sigmóides, que estão todos na faixa de 0-1. Analógico tem a vantagem sobre o digital de ser diferenciável e, portanto, adequado para backpropagation.
 </p>
 <p style="text-align: justify;">
  Esses portões atuam sobre os sinais que recebem e, de forma semelhante aos nós da rede neural, eles bloqueiam ou transmitem informações com base em sua força e importação, que eles filtram com seus próprios conjuntos de pesos. Esses pesos, como os pesos que modulam a entrada e estados ocultos, são ajustados através do processo de aprendizagem das redes recorrentes. Ou seja, as células aprendem quando permitir que os dados entrem, saiam ou sejam excluídos através do processo iterativo de fazer suposições, calculando o erro durante o backpropagation e ajustando pesos através da descida do gradiente.
 </p>
 <p style="text-align: justify;">
  O diagrama abaixo ilustra como os dados fluem através de uma célula de memória e são controlados por seus portões.
 </p>
 <p>
  <img alt="Long Short-Term Memory" class="aligncenter wp-image-269 size-full" data-attachment-id="269" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Long Short-Term Memory" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?fit=793%2C453" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?fit=300%2C171" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?fit=793%2C453" data-orig-size="793,453" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/lstm/" data-recalc-dims="1" height="453" sizes="(max-width: 793px) 100vw, 793px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=793%2C453" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?w=793 793w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=300%2C171 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=768%2C439 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=200%2C114 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=690%2C394 690w" width="793"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Os LSTM’s possuem muitas aplicações práticas, incluindo processamento de linguagem natural, geração automática de texto e análise de séries temporais. Caso queira ver esses exemplos na prática, clique
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  . Teremos um capítulo inteiro dedicado aos LSTM’s aqui no livro.
 </p>
 <p>
 </p>
 <h2>
  5- Redes de Hopfield
 </h2>
 <p style="text-align: justify;">
  Redes recorrentes de unidades não lineares geralmente são muito difíceis de analisar. Elas podem se comportar de muitas maneiras diferentes: se estabelecer em um estado estável, oscilar ou seguir trajetórias caóticas que não podem ser preditas no futuro. Uma Rede Hopfield é composta por unidades de limite binário com conexões recorrentes entre elas. Em 1982, John Hopfield percebeu que, se as conexões são simétricas, existe uma função de energia global. Cada “configuração” binária de toda a rede possui energia, enquanto a regra de decisão do limite binário faz com que a rede se conforme com um mínimo desta função de energia. Uma excelente maneira de usar esse tipo de computação é usar memórias como energia mínima para a rede neural. Usar mínimos de energia para representar memórias resulta em uma memória endereçável ao conteúdo. Um item pode ser acessado por apenas conhecer parte do seu conteúdo. É robusto contra danos no hardware.
 </p>
 <p>
 </p>
 <p>
  <img alt="Hopfield" class="aligncenter size-full wp-image-279" data-attachment-id="279" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Hopfield" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?fit=520%2C366" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?fit=300%2C211" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?fit=520%2C366" data-orig-size="520,366" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/hopfield/" data-recalc-dims="1" height="366" sizes="(max-width: 520px) 100vw, 520px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?resize=520%2C366" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?w=520 520w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?resize=300%2C211 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?resize=200%2C141 200w" width="520"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Cada vez que memorizamos uma configuração, esperamos criar um novo mínimo de energia. Mas e se dois mínimos próximos estão em um local intermediário? Isso limita a capacidade de uma Rede Hopfield. Então, como aumentamos a capacidade de uma Rede Hopfield? Os físicos adoram a ideia de que a matemática que eles já conhecem pode explicar como o cérebro funciona. Muitos artigos foram publicados em revistas de física sobre Redes Hopfield e sua capacidade de armazenamento. Eventualmente,
  <span style="text-decoration: underline;">
   <a href="http://www.baginsky.de/eli/eg_portr.html" rel="noopener" target="_blank">
    Elizabeth Gardner
   </a>
  </span>
  descobriu que havia uma regra de armazenamento muito melhor que usa a capacidade total dos pesos. Em vez de tentar armazenar vetores de uma só vez, ela percorreu o conjunto de treinamento muitas vezes e usou o procedimento de convergência Perceptron para treinar cada unidade para ter o estado correto, dado os estados de todas as outras unidades nesse vetor.
  <strong>
   Os estatísticos chamam essa técnica de “pseudo-probabilidade”
  </strong>
  .
 </p>
 <p style="text-align: justify;">
  Existe outro papel computacional para as Redes Hopfield. Em vez de usar a rede para armazenar memórias, usamos para construir interpretações de entrada sensorial. A entrada é representada pelas unidades visíveis, a interpretação é representada pelos estados das unidades ocultas e o erro da interpretação é representado pela energia.
 </p>
 <p>
 </p>
 <h2>
  6- Máquinas de Boltzmann
 </h2>
 <p style="text-align: justify;">
  Uma Máquina de Boltzmann é um tipo de rede neural recorrente estocástica. Pode ser visto como a contrapartida estocástica e generativa das Redes Hopfield. Foi uma das primeiras redes neurais capazes de aprender representações internas e é capaz de representar e resolver problemas combinatórios difíceis.
 </p>
 <p style="text-align: justify;">
  O objetivo do aprendizado do algoritmo da Máquina de Boltzmann é maximizar o produto das probabilidades que a Máquina de Boltzmann atribui aos vetores binários no conjunto de treinamento. Isso equivale a maximizar a soma das probabilidades de log que a Máquina de Boltzmann atribui aos vetores de treinamento. Também é equivalente a maximizar a probabilidade de obtermos exatamente os N casos de treinamento se fizéssemos o seguinte: 1) Deixar a rede se estabelecer em sua distribuição estacionária no tempo N diferente, sem entrada externa e 2) Mudar o vetor visível uma vez em cada passada.
 </p>
 <p style="text-align: justify;">
  Um procedimento eficiente de aprendizado de mini-lote foi proposto para as Máquinas de Boltzmann por
  <span style="text-decoration: underline;">
   <a href="http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf" rel="noopener" target="_blank">
    Salakhutdinov e Hinton em 2012
   </a>
  </span>
  .
 </p>
 <p>
  <img alt="Boltzmann Machine Network" class="aligncenter wp-image-281 size-large" data-attachment-id="281" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Boltzmann Machine Network" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?fit=1024%2C576" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?fit=300%2C169" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?fit=1280%2C720" data-orig-size="1280,720" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/maquinas/" data-recalc-dims="1" height="576" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=1024%2C576" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=1024%2C576 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=300%2C169 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=768%2C432 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=200%2C113 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=690%2C388 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?w=1280 1280w" width="1024"/>
 </p>
 <p style="text-align: justify;">
  Em uma Máquina de Boltzmann geral, as atualizações estocásticas de unidades precisam ser sequenciais. Existe uma arquitetura especial que permite alternar atualizações paralelas que são muito mais eficientes (sem conexões dentro de uma camada, sem conexões de camada ignorada). Este procedimento de mini-lote torna as atualizações da Máquina de Boltzmann mais paralelas. Isso é chamado de Deep Boltzmann Machines (DBM), uma Máquina de Boltzmann geral, mas com muitas conexões ausentes.
 </p>
 <p style="text-align: justify;">
  Em 2014, Salakhutdinov e Hinton apresentaram outra atualização para seu modelo, chamando-o de Máquinas Boltzmann Restritas. Elas restringem a conectividade para facilitar a inferência e a aprendizagem (apenas uma camada de unidades escondidas e sem conexões entre unidades ocultas). Em um RBM, é preciso apenas um passo para alcançar o equilíbrio.
 </p>
 <p>
 </p>
 <h2>
  7- Deep Belief Network
 </h2>
 <p class="graf graf--p graf-after--figure" id="1989" style="text-align: justify;">
  O backpropagation é considerado o método padrão em redes neurais artificiais para calcular a contribuição de erro de cada neurônio após processar um lote de dados (teremos um capítulo inteiro sobre isso). No entanto, existem alguns problemas importantes no backpropagation. Em primeiro lugar, requer dados de treinamento rotulados; enquanto quase todos os dados estão sem rótulos. Em segundo lugar, o tempo de aprendizagem não escala bem, o que significa que é muito lento em redes com múltiplas camadas ocultas. Em terceiro lugar, pode ficar preso em um “local optima”. Portanto, para redes profundas, o backpropagation está longe de ser ótimo.
 </p>
 <p class="graf graf--p graf-after--p" id="a0aa" style="text-align: justify;">
  Para superar as limitações do backpropagation, os pesquisadores consideraram o uso de abordagens de aprendizado sem supervisão. Isso ajuda a manter a eficiência e a simplicidade de usar um método de gradiente para ajustar os pesos, mas também usá-lo para modelar a estrutura da entrada sensorial. Em particular, eles ajustam os pesos para maximizar a probabilidade de um modelo gerador ter gerado a entrada sensorial. A questão é que tipo de modelo generativo devemos aprender? Pode ser um modelo baseado em energia como uma Máquina de Boltzmann? Ou um modelo causal feito de neurônios? Ou um híbrido dos dois?
 </p>
 <p>
 </p>
 <p>
  <img alt="Deep Belief Network" class="aligncenter size-large wp-image-283" data-attachment-id="283" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Deep Belief Network" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?fit=1024%2C528" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?fit=300%2C155" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?fit=1600%2C825" data-orig-size="1600,825" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/dbn/" data-recalc-dims="1" height="528" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=1024%2C528" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=1024%2C528 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=300%2C155 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=768%2C396 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=200%2C103 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=690%2C356 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?w=1600 1600w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Uma Deep Belief Network pode ser definida como uma pilha de Máquinas de Boltzmann Restritas (RBM – Restricted Boltzmann Machines), em que cada camada RBM se comunica com as camadas anterior e posterior. Os nós de qualquer camada única não se comunicam lateralmente.
 </p>
 <p style="text-align: justify;">
  Esta pilha de RBMs pode terminar com uma camada Softmax para criar um classificador, ou simplesmente pode ajudar a agrupar dados não gravados em um cenário de aprendizado sem supervisão.
 </p>
 <p style="text-align: justify;">
  Com a exceção das camadas inicial e final, cada camada em uma Deep Belief Network tem uma função dupla: ela serve como a camada oculta para os nós que vem antes, e como a camada de entrada (ou “visível”) para a nós que vem depois. É uma rede construída de redes de camada única.
 </p>
 <p style="text-align: justify;">
  As Deep Belief Networks são usadas para reconhecer, agrupar e gerar imagens, sequências de vídeos e dados de captura de movimento.
  <span style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;">
   Outra aplicação das Deep Belief Networks é no
  </span>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-processamento-de-linguagem-natural-e-reconhecimento-de-voz" rel="noopener" target="_blank">
    Processamento de Linguagem Natural
   </a>
  </span>
  <span style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;">
   . Esse tipo de rede foi apresentado por Geoff Hinton e seus alunos em 2006.
  </span>
 </p>
 <p>
 </p>
 <h2>
  8- Deep Auto-Encoders
 </h2>
 <p style="text-align: justify;">
  Um Deep Auto-Encoder é composto por duas redes simétricas Deep Belief que tipicamente têm quatro ou cinco camadas rasas que representam a metade da codificação (encoder) da rede e o segundo conjunto de quatro ou cinco camadas que compõem a metade da decodificação (decoder).
 </p>
 <p style="text-align: justify;">
  As camadas são Máquinas de Boltzmann Restritas, os blocos de construção das Deep Belief Networks, com várias peculiaridades que discutiremos abaixo. Aqui está um esquema simplificado da estrutura de um Deep Auto-Encoder:
 </p>
 <p>
 </p>
 <p>
  <img alt="Deep Auto-Encoder" class="aligncenter size-full wp-image-285" data-attachment-id="285" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Deep Auto-Encoder" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?fit=540%2C365" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?fit=300%2C203" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?fit=540%2C365" data-orig-size="540,365" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/deep_autoencoder/" data-recalc-dims="1" height="365" sizes="(max-width: 540px) 100vw, 540px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?resize=540%2C365" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?w=540 540w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?resize=300%2C203 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?resize=200%2C135 200w" width="540"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Os Deep Auto-Encoders são uma maneira muito agradável de reduzir a dimensionalidade não linear devido a alguns motivos: eles fornecem mapeamentos flexíveis em ambos os sentidos. O tempo de aprendizagem é linear (ou melhor) no número de casos de treinamento. E o modelo de codificação final é bastante compacto e rápido. No entanto, pode ser muito difícil otimizar Deep Auto-Encoders usando backpropagation. Com pequenos pesos iniciais, o gradiente do backpropagation morre. Mas temos maneiras de otimizá-las, usando o pré-treinamento camada-por-camada sem supervisão ou apenas inicializando os pesos com cuidado.
 </p>
 <p style="text-align: justify;">
  Os Deep Auto-Encoders são úteis na modelagem de tópicos ou modelagem estatística de tópicos abstratos que são distribuídos em uma coleção de documentos. Isso, por sua vez, é um passo importante em sistemas de perguntas e respostas como o IBM Watson.
 </p>
 <p style="text-align: justify;">
  Em resumo, cada documento em uma coleção é convertido em um Bag-of-Words (ou seja, um conjunto de contagens de palavras) e essas contagens de palavras são dimensionadas para decimais entre 0 e 1, o que pode ser pensado como a probabilidade de uma palavra ocorrer no documento.
 </p>
 <p style="text-align: justify;">
  As contagens de palavras em escala são então alimentadas em uma Deep Belief Network, uma pilha de Máquinas de Boltzmann Restritas, que elas mesmas são apenas um subconjunto de Autoencoders. Essas Deep Belief Networks, ou DBNs, comprimem cada documento para um conjunto de 10 números através de uma série de transformações sigmóides que o mapeiam no espaço de recursos.
 </p>
 <p style="text-align: justify;">
  O conjunto de números de cada documento, ou vetor, é então introduzido no mesmo espaço vetorial, e sua distância de qualquer outro vetor de documento medido. Em termos aproximados, os vetores de documentos próximos se enquadram no mesmo tópico. Por exemplo, um documento poderia ser a “pergunta” e outros poderiam ser as “respostas”, uma combinação que o software faria usando medidas de espaço vetorial.
 </p>
 <p style="text-align: justify;">
  Em resumo, existem agora muitas maneiras diferentes de fazer pré-treinamento camada-por-camada de recursos. Para conjuntos de dados que não possuem um grande número de casos rotulados, o pré-treinamento ajuda a aprendizagem discriminativa subsequente. Para conjuntos de dados muito grandes e rotulados, não é necessário inicializar os pesos utilizados na aprendizagem supervisionada usando pré-treinamento não supervisionado, mesmo para redes profundas. O pré-treinamento foi o primeiro bom caminho para inicializar os pesos para redes profundas, mas agora existem outras formas. Mas se construímos redes muito maiores, precisaremos de pré-treinamento novamente! Se quiser aprender a construir Deep Auto-Encoders em Python, clique
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  .
 </p>
 <p>
 </p>
 <h2>
  9- Generative Adversarial Network
 </h2>
 <p style="text-align: justify;">
  As Generative Adversarial Networks (GANs) são arquiteturas de redes neurais profundas compostas por duas redes, colocando uma contra a outra (daí o nome, “adversária”).
 </p>
 <p style="text-align: justify;">
  Os GANs foram introduzidos em um artigo de Ian Goodfellow e outros pesquisadores da Universidade de Montreal no Canadá, incluindo Yoshua Bengio, em 2014. Referindo-se aos GANs, o diretor de pesquisa de IA do Facebook, Yann LeCun, chamou de treinamento adversário “a ideia mais interessante nos últimos 10 anos em Machine Learning”.
 </p>
 <p style="text-align: justify;">
  O potencial de GANs é enorme, porque eles podem aprender a imitar qualquer distribuição de dados. Ou seja, os GANs podem ser ensinados a criar mundos estranhamente semelhantes aos nossos em qualquer domínio: imagens, música, fala, prosa. Eles são artistas robôs em um sentido, e sua produção é impressionante – até mesmo pungente.
 </p>
 <p style="text-align: justify;">
  Para entender os GANs, você deve saber como os algoritmos geradores funcionam, e para isso, contrastá-los com algoritmos discriminatórios é útil. Os algoritmos discriminatórios tentam classificar dados de entrada; isto é, dados os recursos de uma instância de dados, eles predizem um rótulo ou categoria a que esses dados pertencem.
 </p>
 <p style="text-align: justify;">
  Por exemplo, tendo em conta todas as palavras em um e-mail, um algoritmo discriminatório pode prever se a mensagem é spam ou not_spam. O spam é um dos rótulos, e o saco de palavras (Bag of Words) coletadas do e-mail são os recursos que constituem os dados de entrada. Quando este problema é expresso matematicamente, o rótulo é chamado y e os recursos são chamados de x. A formulação p (y | x) é usada para significar “a probabilidade de y dado x”, que neste caso seria traduzido para “a probabilidade de um email ser spam com as palavras que contém”.
 </p>
 <p style="text-align: justify;">
  Portanto, algoritmos discriminatórios mapeiam recursos para rótulos. Eles estão preocupados apenas com essa correlação. Uma maneira de pensar sobre algoritmos generativos é que eles fazem o contrário. Em vez de prever um rótulo com determinados recursos, eles tentam prever os recursos com um determinado rótulo.
 </p>
 <p style="text-align: justify;">
  A questão que um algoritmo gerador tenta responder é: assumir que este e-mail é spam, qual a probabilidade dos recursos? Enquanto os modelos discriminativos se preocupam com a relação entre y e x, os modelos generativos se preocupam com “como você obtém x”. Eles permitem que você capture p (x | y), a probabilidade de x dado y, ou a probabilidade de características oferecidas em uma classe . (Dito isto, os algoritmos geradores também podem ser usados ​​como classificadores, embora eles podem fazer mais do que categorizar dados de entrada.)
 </p>
 <p style="text-align: justify;">
  Outra maneira de pensar sobre isso é distinguir discriminativo de gerador assim:
 </p>
 <ul>
  <li style="text-align: justify;">
   Modelos discriminativos aprendem o limite entre as classes
  </li>
  <li style="text-align: justify;">
   Modelos generativos modelam a distribuição de classes individuais
  </li>
 </ul>
 <p>
 </p>
 <p>
  <img alt="Generative Adversarial Network" class="aligncenter size-large wp-image-270" data-attachment-id="270" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Generative Adversarial Network" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?fit=1024%2C447" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?fit=300%2C131" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?fit=1213%2C529" data-orig-size="1213,529" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/gans/" data-recalc-dims="1" height="447" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=1024%2C447" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=1024%2C447 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=300%2C131 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=768%2C335 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=200%2C87 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=690%2C301 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?w=1213 1213w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Uma rede neural, chamada de gerador, gera novas instâncias de dados, enquanto a outra, o discriminador, as avalia por autenticidade; ou seja, o discriminador decide se cada instância de dados que revisa pertence ao conjunto de dados de treinamento real ou não.
 </p>
 <p style="text-align: justify;">
  Digamos que estamos tentando fazer algo mais banal do que imitar a Mona Lisa. Vamos gerar números escritos à mão como os encontrados no conjunto de dados
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/mnist/" rel="noopener" target="_blank">
    MNIST
   </a>
  </span>
  , que é retirado do mundo real. O objetivo do discriminador, quando mostrado uma instância do verdadeiro conjunto de dados MNIST, é reconhecê-los como autênticos.
 </p>
 <p style="text-align: justify;">
  Enquanto isso, o gerador está criando novas imagens que passa para o discriminador. Isso acontece com a esperança de que eles, também, sejam considerados autênticos, embora sejam falsos. O objetivo do gerador é gerar dígitos ​​escritos à mão por si mesmo. O objetivo do discriminador é identificar as imagens provenientes do gerador como falsas.
 </p>
 <p style="text-align: justify;">
  Aqui estão os passos que um GAN realiza:
 </p>
 <ul>
  <li style="text-align: justify;">
   O gerador recebe números aleatórios e retorna uma imagem.
  </li>
  <li style="text-align: justify;">
   Essa imagem gerada é alimentada no discriminador ao lado de um fluxo de imagens tiradas do conjunto de dados real.
  </li>
  <li style="text-align: justify;">
   O discriminador assume imagens reais e falsas e retorna probabilidades, um número entre 0 e 1, com 1 representando uma previsão de autenticidade e 0 representando falsas.
  </li>
 </ul>
 <p style="text-align: justify;">
  Então você tem um loop de feedback duplo:
 </p>
 <ul>
  <li style="text-align: justify;">
   O discriminador está em um loop de feedback com as imagens verdadeiras, que conhecemos.
  </li>
  <li style="text-align: justify;">
   O gerador está em um loop de feedback com o discriminador.
  </li>
 </ul>
 <p style="text-align: justify;">
  Quer aprender como construir GANs, uma das arquiteturas mais incríveis de Deep Learning, 100% em português e 100% online, para gerar imagens de forma automática? Clique
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  .
 </p>
 <p>
 </p>
 <h2>
  10- Deep Neural Network Capsules
 </h2>
 <p style="text-align: justify;">
  No final de 2017, Geoffrey Hinton e sua equipe publicaram dois artigos que introduziram um novo tipo de rede neural chamada
  <em>
   <strong>
    Capsules
   </strong>
  </em>
  . Além disso, a equipe publicou um algoritmo, denominado roteamento dinâmico entre cápsulas, que permite treinar essa rede.
 </p>
 <p style="text-align: justify;">
  Para todos na comunidade de Deep Learning, esta é uma grande notícia, e por várias razões. Em primeiro lugar, Hinton é um dos fundadores do Deep Learning e um inventor de inúmeros modelos e algoritmos que hoje são amplamente utilizados. Em segundo lugar, esses artigos apresentam algo completamente novo, e isso é muito emocionante porque provavelmente estimulará a onda adicional de pesquisas e aplicativos muito inovadores.
 </p>
 <p>
 </p>
 <p>
  <img alt="Capsule" class="aligncenter size-full wp-image-271" data-attachment-id="271" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Capsule" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?fit=1000%2C303" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?fit=300%2C91" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?fit=1000%2C303" data-orig-size="1000,303" data-permalink="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/capsule/" data-recalc-dims="1" height="303" sizes="(max-width: 1000px) 100vw, 1000px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=1000%2C303" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?w=1000 1000w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=300%2C91 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=768%2C233 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=200%2C61 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=690%2C209 690w" width="1000"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  As
  <em>
   <strong>
    Capsules
   </strong>
  </em>
  introduzem um novo bloco de construção que pode ser usado na aprendizagem profunda para modelar melhor as relações hierárquicas dentro da representação do conhecimento interno de uma rede neural. A intuição por trás deles é muito simples e elegante.
 </p>
 <p style="text-align: justify;">
  Hinton e sua equipe propuseram uma maneira de treinar essa rede composta de cápsulas e treinou-a com êxito em um conjunto de dados simples, alcançando desempenho de ponta. Isso é muito encorajador. No entanto, há desafios. As implementações atuais são muito mais lentas do que outros modelos modernos de aprendizado profundo. O tempo mostrará se as redes
  <em>
   <strong>
    Capsules
   </strong>
  </em>
  podem ser treinadas de forma rápida e eficiente. Além disso, precisamos ver se elas funcionam bem em conjuntos de dados mais difíceis e em diferentes domínios.
 </p>
 <p style="text-align: justify;">
  Em qualquer caso, a rede
  <em>
   <strong>
    Capsule
   </strong>
  </em>
  é um modelo muito interessante e já funcionando, que definitivamente se desenvolverá ao longo do tempo e contribuirá para uma maior expansão de aplicações de aprendizagem profunda.
 </p>
 <p style="text-align: justify;">
  Incluímos as Capsules entre as 10 principais arquiteturas de redes neurais, pois elas representam a inovação e o avanço na incrível e vibrante área de Deep Learning e sistemas de Inteligência Artificial. Profissionais que realmente desejem abraçar IA como carreira, devem estar atentos aos movimentos e inovações na área.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Esta não é uma lista definitiva de arquiteturas e existem outras, tais como Word2Vec, Doc2vec, Neural Embeddings e variações das arquiteturas aqui apresentadas, como Denoising Autoencoders, Variational Autoencoders, além de outras categorias como Deep Reinforcement Learning. Exatamente para auxiliar aqueles que buscam conhecimento de ponta 100% em português e 100% online, que nós criamos a
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
  , o único programa do Brasil completo, com todas as ferramentas que o aluno precisa para aprender a trabalhar com IA de forma eficiente. O aluno aprende programação paralela em GPU, Deep Learning e seus frameworks, estuda as principais arquiteturas com aplicações práticas e desenvolve aplicações de Visão Computacional e Processamento de Linguagem Natural. Clique
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  e veja mais detalhes sobre o programa.
 </p>
 <p>
  Isso conclui a primeira parte deste livro, com uma introdução ao universo do Deep Learning. No próximo capítulo começaremos a ver as redes neurais em ação. Até lá.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://deeplearning4j.org/multilayerperceptron.html" rel="noopener" target="_blank">
    Multilayer Perceptrons
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://people.idsia.ch/~juergen/rnn.html" rel="noopener" target="_blank">
    Long Short-Term Memory
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener" target="_blank">
    Long Short-Term Memory Neural Computation
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener" target="_blank">
    The Unreasonable Effectiveness of Recurrent Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://mitpress.mit.edu/books/perceptrons" rel="noopener" target="_blank">
    Perceptrons, Expanded Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/@james_aka_yale/the-8-neural-network-architectures-machine-learning-researchers-need-to-learn-2f5c4e61aeeb" rel="noopener" target="_blank">
    The 8 Neural Network Architectures Machine Learning Researchers Need to Learn
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/abs/1710.09829v1" rel="noopener" target="_blank">
    Dynamic Routing Between Capsules
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.pnas.org/content/pnas/79/8/2554.full.pdf" rel="noopener" target="_blank">
    Neural networks and physical systems with emergent collective computational abilities
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf" rel="noopener" target="_blank">
    Deep Boltzmann Machines
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.cs.toronto.edu/~hinton/absps/ruhijournal.pdf" rel="noopener" target="_blank">
    Application of Deep Belief Networks for Natural Language Understanding
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf" rel="noopener" target="_blank">
    A fast learning algorithm for deep belief nets
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://deeplearning4j.org/deepautoencoder" rel="noopener" target="_blank">
    A Beginner’s Guide to Deep Autoencoders
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://openreview.net/pdf?id=HJWLfGWRb" rel="noopener" target="_blank">
    Matrix Capsules With RM Routing
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf" rel="noopener" target="_blank">
    Generative Adversarial Networks (GANs)
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-255" href="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-255" href="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-255" href="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-255" href="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-255-5e0dd04fd8297" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=255&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-255-5e0dd04fd8297" id="like-post-wrapper-140353593-255-5e0dd04fd8297">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-11">
 Capítulo 11 – Design De Uma Rede Neural Para Reconhecimento de Dígitos
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Na primeira parte deste livro online, durante os 10 primeiros capítulos, definimos e estudamos o universo das redes neurais artificias. Neste ponto você já deve ter uma boa compreensão sobre que são estes algoritmos e como podem ser usados, além da importância das redes neurais para a construção de sistemas de Inteligência Artificial. Estamos prontos para iniciar a construção de redes neurais e na sequência estudaremos as arquiteturas mais avançadas. Vamos começar definindo o Design De Uma Rede Neural Para Reconhecimento de Dígitos.
 </p>
 <p style="text-align: justify;">
  Nossa primeira tarefa será construir uma rede neural para reconhecer caligrafia, ou seja, dígitos escritos à mão que foram digitalizados em imagens no computador. Por que vamos começar com este tipo de tarefa? Porque ela permite percorrer todas as etapas e procedimentos matemáticos de uma rede neural, sendo portanto uma excelente introdução. Vamos começar?
 </p>
 <p style="text-align: justify;">
  Se você acompanha os cursos na
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br" rel="noopener" target="_blank">
    Data Science Academy
   </a>
  </span>
  já sabe que: antes de pensar em escrever sua primeira linha de código, é preciso definir claramente o problema a ser resolvido. A tecnologia existe para resolver problemas e a definição clara do objetivo é o ponto de partida de qualquer projeto de sucesso! Neste capítulo definiremos o problema a ser resolvido, nesse caso o reconhecimento de dígitos manuscritos.
 </p>
 <p style="text-align: justify;">
  Podemos dividir o problema de reconhecer os dígitos manuscritos em dois sub-problemas. Primeiro, precisamos encontrar uma maneira de quebrar uma imagem que contenha muitos dígitos em uma sequência de imagens separadas, cada uma contendo um único dígito. Por exemplo, gostaríamos de quebrar a imagem:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter wp-image-309 size-medium" data-attachment-id="309" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="digits" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?fit=623%2C128" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?fit=300%2C62" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?fit=623%2C128" data-orig-size="623,128" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/digits-2/" data-recalc-dims="1" height="62" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?resize=300%2C62" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?resize=300%2C62 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?resize=200%2C41 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?w=623 623w" width="300"/>
 </p>
 <p>
 </p>
 <p>
  em seis imagens separadas:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-medium wp-image-310" data-attachment-id="310" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="digits_separate" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?fit=630%2C99" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?fit=300%2C47" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?fit=630%2C99" data-orig-size="630,99" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/digits_separate/" data-recalc-dims="1" height="47" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?resize=300%2C47" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?resize=300%2C47 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?resize=200%2C31 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?w=630 630w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Nós, humanos, resolvemos esse problema de segmentação com facilidade, mas é um desafio para um programa de computador dividir corretamente a imagem. Uma vez que a imagem foi segmentada, o programa precisa classificar cada dígito individual. Então, por exemplo, gostaríamos que nosso programa reconhecesse automaticamente que o primeiro dígito acima é um 5:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-full wp-image-311" data-attachment-id="311" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="mnist_first_digit" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?fit=31%2C35" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?fit=31%2C35" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?fit=31%2C35" data-orig-size="31,35" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_first_digit/" data-recalc-dims="1" height="35" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?resize=31%2C35" width="31"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Vamos nos concentrar em escrever um programa para resolver o segundo problema, isto é, classificar dígitos individuais. O problema da segmentação não é tão difícil de resolver, uma vez que você tenha uma boa maneira de classificar os dígitos individuais. Existem muitas abordagens para resolver o problema de segmentação. Uma abordagem é testar muitas maneiras diferentes de segmentar a imagem, usando o classificador de dígitos individuais para marcar cada segmentação de teste. Uma segmentação de teste obtém uma pontuação alta se o classificador de dígitos individuais estiver confiante de sua classificação em todos os segmentos e uma pontuação baixa se o classificador tiver muitos problemas em um ou mais segmentos. A ideia é que, se o classificador estiver tendo problemas em algum lugar, provavelmente está tendo problemas porque a segmentação foi escolhida incorretamente. Essa ideia e outras variações podem ser usadas para resolver o problema de segmentação. Então, em vez de se preocupar com a segmentação, nos concentraremos no desenvolvimento de uma rede neural que pode resolver o problema mais interessante e difícil, ou seja, reconhecer dígitos individuais manuscritos.
 </p>
 <p>
  Para reconhecer dígitos individuais, usaremos uma rede neural de três camadas:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter wp-image-312 size-full" data-attachment-id="312" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tikz12" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?fit=537%2C447" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?fit=300%2C250" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?fit=537%2C447" data-orig-size="537,447" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/tikz12/" data-recalc-dims="1" height="447" sizes="(max-width: 537px) 100vw, 537px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?resize=537%2C447" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?w=537 537w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?resize=300%2C250 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?resize=200%2C166 200w" width="537"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A camada de entrada da rede contém neurônios que codificam os valores dos pixels de entrada. Conforme iremos discutir no próximo capítulo, nossos dados de treinamento para a rede consistirão em muitas imagens de 28 por 28 pixels de dígitos manuscritos digitalizados e, portanto, a camada de entrada contém 28 × 28 = 784 neurônios (Nota: uma imagem nada mais é do que uma matriz, nesse caso de dimensões 28×28, que iremos converter em um vetor cujo tamanho será 784, onde cada item representa um pixel na imagem). Os pixels de entrada são de escala de cinza, com um valor de 0.0 representando branco e um valor de 1.0 representando preto. Valores intermediários representam tonalidades gradualmente escurecidas de cinza.
 </p>
 <p style="text-align: justify;">
  A segunda camada da rede é uma camada oculta. Representaremos o número de neurônios nesta camada oculta por n, e vamos experimentar diferentes valores para n. O exemplo mostrado acima ilustra uma pequena camada oculta, contendo apenas n = 15 neurônios.
 </p>
 <p style="text-align: justify;">
  A camada de saída da rede contém 10 neurônios. Se o primeiro neurônio “disparar” (for ativado), ou seja, tiver uma saída ≈ 1, então isso indicará que a rede acha que o dígito é 0. Se o segundo neurônio “disparar” (for ativado), isso indicará que a rede pensa que o dígito é um 1. E assim por diante. Em resumo, vamos numerar os neurônios de saída de 0 a 9 e descobrimos qual neurônio possui o maior valor de ativação. Se esse neurônio é, digamos, neurônio número 6, então nossa rede adivinhará que o dígito de entrada era um 6. E assim por diante para os outros neurônios de saída.
 </p>
 <p style="text-align: justify;">
  Você pode se perguntar por que usamos 10 neurônios de saída. Afinal, o objetivo da rede é nos dizer qual dígito (0,1,2, …, 9) corresponde à imagem de entrada. Uma maneira aparentemente natural de fazer isso é usar apenas 4 neurônios de saída, tratando cada neurônio como assumindo um valor binário, dependendo se a saída do neurônio está mais próxima de 0 ou 1. Quatro neurônios são suficientes para codificar a resposta, desde que 2ˆ4 = 16 é mais do que os 10 valores possíveis para o dígito de entrada. Por que nossa rede deve usar 10 neurônios em vez disso? Isso não é ineficiente? A justificativa final é empírica: podemos experimentar ambos os projetos de rede, e verifica-se que, para este problema específico, a rede com 10 neurônios de saída aprende a reconhecer dígitos melhor do que a rede com 4 neurônios de saída. Mas isso ainda deixa a pergunta por que o uso de 10 neurônios de saída funciona melhor. Existe alguma heurística que nos diga com antecedência que devemos usar a codificação de 10 saídas em vez da codificação de 4 saídas?
 </p>
 <p style="text-align: justify;">
  Entender porque fazemos isso, ajuda a pensar sobre o que a rede neural está realmente fazendo. Considere primeiro o caso em que usamos 10 neurônios de saída. Vamos nos concentrar no primeiro neurônio de saída, aquele que está tentando decidir se o dígito é ou não 0. Ele faz isso pesando evidências da camada oculta dos neurônios. O que esses neurônios ocultos estão fazendo? Bem, vamos supor que o primeiro neurônio na camada oculta detecta ou não uma imagem como a seguinte:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter wp-image-313 size-thumbnail" data-attachment-id="313" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="mnist_top_left_feature" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?fit=497%2C511" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?fit=292%2C300" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?fit=497%2C511" data-orig-size="497,511" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_top_left_feature/" data-recalc-dims="1" height="150" sizes="(max-width: 150px) 100vw, 150px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?resize=150%2C150" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?resize=150%2C150 150w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?resize=280%2C280 280w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?zoom=3&amp;resize=150%2C150 450w" width="150"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Isso pode ser feito pesando fortemente pixels de entrada que se sobrepõem à imagem e apenas ponderam ligeiramente as outras entradas. De forma semelhante, suponhamos que o segundo, terceiro e quarto neurônios na camada oculta detectem se as seguintes imagens estão ou não presentes:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-medium wp-image-314" data-attachment-id="314" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="mnist_other_features" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?fit=636%2C203" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?fit=300%2C96" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?fit=636%2C203" data-orig-size="636,203" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_other_features/" data-recalc-dims="1" height="96" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?resize=300%2C96" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?resize=300%2C96 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?resize=200%2C64 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?w=636 636w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Como você pode ter adivinhado, essas quatro imagens juntas compõem a imagem 0 que vimos na linha de dígitos mostrada anteriormente:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-medium wp-image-315" data-attachment-id="315" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="mnist_complete_zero" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?fit=495%2C497" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?fit=300%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?fit=495%2C497" data-orig-size="495,497" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_complete_zero/" data-recalc-dims="1" height="300" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=300%2C300" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=300%2C300 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=150%2C150 150w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=200%2C201 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=280%2C280 280w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?w=495 495w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Então, se todos os quatro neurônios ocultos estão disparando, podemos concluir que o dígito é um 0. Claro, esse não é o único tipo de evidência que podemos usar para concluir que a imagem era um 0 – podemos legitimamente obter um 0 em muitas outras maneiras (por exemplo, através de traduções das imagens acima, ou pequenas distorções). Mas parece seguro dizer que, pelo menos neste caso, concluiríamos que a entrada era um 0.
 </p>
 <p style="text-align: justify;">
  Supondo que a rede neural funciona assim, podemos dar uma explicação plausível sobre porque é melhor ter 10 saídas da rede, em vez de 4. Se tivéssemos 4 saídas, o primeiro neurônio de saída tentaria decidir o que mais um bit significativo do dígito representa. E não existe uma maneira fácil de relacionar esse bit mais significativo com formas simples, como as mostradas acima. As formas componentes do dígito estarão intimamente relacionadas com (digamos) o bit mais significativo na saída.
 </p>
 <p style="text-align: justify;">
  Isso tudo é apenas uma heurística. Nada diz que a rede neural de três camadas tem que operar da maneira que descrevemos, com os neurônios ocultos detectando formas de componentes simples. Talvez um algoritmo de aprendizado inteligente encontre alguma atribuição de pesos que nos permita usar apenas 4 neurônios de saída. Mas, usar uma boa heurística pode economizar muito tempo na concepção de boas arquiteturas de redes neurais.
 </p>
 <p style="text-align: justify;">
  Já temos então um design para a nossa rede neural. Agora precisamos definir como será o processo de aprendizagem do algoritmo, antes de começar a codificar nossa rede em linguagem Python. Usaremos o treinamento com Gradiente Descendente, assunto do próximo capítulo, que aliás eu não perderia por nada, se fosse você, pois aí está a “magia” por trás das redes neurais. Até lá!
 </p>
 <p style="text-align: justify;">
  Para acompanhar os próximos capítulos e reproduzir os exemplos, você deve ter o Anaconda Python instalado no seu computador com Python versão 3.6.x. Acesse o capítulo 1 do curso gratuito
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener" target="_blank">
    Python Fundamentos Para Análise de Dados
   </a>
  </span>
  , para aprender como instalar o Anaconda.
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide" rel="noopener" target="_blank">
    Função Sigmóide
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
  (alguns trechos extraídos e traduzidos com autorização do autor
  <span style="text-decoration: underline;">
   <a href="http://michaelnielsen.org/" rel="noopener" target="_blank">
    Michael Nielsen
   </a>
  </span>
  )
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-308" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-308" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-308" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-308" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-308-5e0dd051d548e" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=308&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-308-5e0dd051d548e" id="like-post-wrapper-140353593-308-5e0dd051d548e">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-12">
 Capítulo 12 – Aprendizado Com a Descida do Gradiente
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  No capítulo anterior definimos o design para a nossa rede neural e agora podemos começar o processo de aprendizado de máquina. Neste capítulo você vai compreender o que é o Aprendizado Com a Descida do Gradiente.
 </p>
 <p style="text-align: justify;">
  A primeira coisa que precisamos é um conjunto de dados para o treinamento da rede. Usaremos o conjunto de dados
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/mnist/" rel="noopener" target="_blank">
    MNIST
   </a>
  </span>
  , que contém dezenas de milhares de imagens digitalizadas de dígitos manuscritos, juntamente com suas classificações corretas. O nome MNIST vem do fato de que é um subconjunto modificado de dois conjuntos de dados coletados pelo NIST, o Instituto Nacional de Padrões e Tecnologia dos Estados Unidos. Aqui estão algumas imagens do MNIST:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-medium wp-image-328" data-attachment-id="328" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="digits_separate" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?fit=630%2C99" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?fit=300%2C47" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?fit=630%2C99" data-orig-size="630,99" data-permalink="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/digits_separate-2/" data-recalc-dims="1" height="47" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?resize=300%2C47" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?resize=300%2C47 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?resize=200%2C31 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?w=630 630w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O MNIST tem duas partes. A primeira parte contém 60.000 imagens para serem usadas como dados de treinamento. Essas imagens são amostras de manuscritos escaneados de 250 pessoas, metade dos quais funcionários do Bureau do Censo dos EUA e metade dos estudantes do ensino médio. As imagens estão em escala de cinza e 28 por 28 pixels de tamanho. A segunda parte do conjunto de dados MNIST tem 10.000 imagens a serem usadas como dados de teste, também 28 por 28 pixels em escala de cinza. Usaremos os dados do teste para avaliar o quão bem a nossa rede neural aprendeu a reconhecer os dígitos. Para fazer deste um bom teste de desempenho, os dados de teste foram retirados de um conjunto diferente de 250 pessoas em relação aos dados de treinamento originais (embora ainda seja um grupo dividido entre funcionários do Census Bureau e alunos do ensino médio). Isso nos ajuda a confiar que nosso sistema pode reconhecer dígitos de pessoas cuja escrita não viu durante o treinamento.
 </p>
 <p style="text-align: justify;">
  Usaremos a notação x para indicar uma entrada (input) de treinamento. Será conveniente considerar cada entrada de treinamento x (cada imagem) como um vetor de 784 posições (28 x 28 pixels). A imagem abaixo representa como este vetor é construído:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-medium wp-image-345" data-attachment-id="345" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="pixels" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?fit=470%2C434" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?fit=300%2C277" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?fit=470%2C434" data-orig-size="470,434" data-permalink="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/pixels/" data-recalc-dims="1" height="277" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?resize=300%2C277" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?resize=300%2C277 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?resize=200%2C185 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?w=470 470w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Cada entrada no vetor representa o valor de cinza para um único pixel na imagem. Vamos indicar a saída correspondente desejada por y = y(x), onde y é um vetor com dimensão 10. Por exemplo, se uma imagem de treinamento particular, x, representa um 3, então y(x) = (0,0,0,1,0,0,0,0,0,0)T é a saída desejada da rede . Observe que T aqui é a operação de transposição, transformando um vetor de linha em um vetor comum (coluna). Vamos deixar isso mais claro. Observe a figura abaixo:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter wp-image-346 size-full" data-attachment-id="346" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="label" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?fit=615%2C124" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?fit=300%2C60" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?fit=615%2C124" data-orig-size="615,124" data-permalink="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/label/" data-recalc-dims="1" height="124" sizes="(max-width: 615px) 100vw, 615px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?resize=615%2C124" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?w=615 615w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?resize=300%2C60 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?resize=200%2C40 200w" width="615"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Vamos usar os pixels de imagem correspondentes ao fluxo inteiro chamado “features”. Os rótulos são One-Hot Encoded 1-hot. O rótulo que representa a classe de saída da imagem com dígito 3 torna-se “0001000000” uma vez que temos 10 classes para os 10 dígitos possíveis, onde o primeiro índice corresponde ao dígito “0” e o último corresponde ao dígito “9”.
 </p>
 <p style="text-align: justify;">
  O que queremos é um algoritmo que nos permita encontrar pesos e bias para que a saída da rede se aproxime de y(x) para todas as entradas de treinamento x. Para quantificar o quão bem estamos alcançando esse objetivo, definimos uma função de custo:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-medium wp-image-329" data-attachment-id="329" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="custo" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?fit=518%2C166" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?fit=300%2C96" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?fit=518%2C166" data-orig-size="518,166" data-permalink="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/2018-03-14_1123/" data-recalc-dims="1" height="96" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?resize=300%2C96" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?resize=300%2C96 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?resize=200%2C64 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?w=518 518w" width="300"/>
 </p>
 <p style="text-align: center;">
  <span style="font-size: 10pt;">
   Função de Custo Quadrático
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Na fórmula acima, w indica a coleta de todos os pesos na rede, b todos os bias (viés), n é o número total de entradas de treinamento, a é o vetor de saídas da rede (quando x é entrada) e a soma é sobre todas as entradas de treinamento x. Claro, a saída a depende de x, w e b, mas para manter a notação simples, eu não indiquei explicitamente essa dependência. A notação ‖v‖ apenas indica a função de comprimento usual para um vetor v. Chamaremos C a função de custo quadrático, que também é conhecido como o erro quadrático médio ou apenas o MSE (Mean Squared Error). Inspecionando a forma da função de custo quadrático, vemos que C (w, b) não é negativo, pois cada termo na soma não é negativo. Além disso, o custo C (w, b) torna-se pequeno, isto é, C (w, b) ≈ 0, precisamente quando y(x) é aproximadamente igual à saída, a, para todas as entradas de treinamento x.
 </p>
 <p style="text-align: justify;">
  Portanto, nosso algoritmo de treinamento faz um bom trabalho se ele pode encontrar pesos e bias para que C (w, b) ≈ 0. Isso significa basicamente que nosso modelo fez as previsões corretas, ou seja, cada vez que apresentamos ao modelo uma imagem com dígito 3, ele é capaz de reconhecer que se trata do número 3.
 </p>
 <p style="text-align: justify;">
  Em contraste, o algoritmo não terá boa performance, quando C (w, b) for um valor maior que 0 – isso significaria que nosso algoritmo não está conseguindo fazer as previsões, ou seja, quando apresentado a imagem com o dígito 3, ele não é capaz de prever que se trata de um número 3. Isso ocorre, porque a diferença entre o valor real da saída e o valor previsto pelo modelo, é muito alta. Assim, o objetivo do nosso algoritmo de treinamento será minimizar o custo C(w, b) em função dos pesos e dos bias. Em outras palavras, queremos encontrar um conjunto de pesos e bias que tornem o custo o menor possível. Vamos fazer isso usando um algoritmo conhecido como Descida do Gradiente (Gradient Descent).
 </p>
 <p style="text-align: justify;">
  Mas antes, uma pergunta. Por que introduzir o custo quadrático? Afinal, não nos interessamos principalmente pelo número de imagens corretamente classificadas pela rede? Por que não tentar maximizar esse número diretamente, em vez de minimizar uma medida, como o custo quadrático? O problema com isso é que o número de imagens corretamente classificadas não é uma “smooth function” dos pesos e bias na rede. Geralmente, fazer pequenas mudanças nos pesos e bias não causará nenhuma alteração no número de imagens de treinamento classificadas corretamente. Isso torna difícil descobrir como mudar os pesos e os bias para melhorar o desempenho. Se, em vez disso, usamos uma “smooth cost function”, como o custo quadrático, revela-se fácil descobrir como fazer pequenas mudanças nos pesos e nos bias para obter uma melhoria no custo. É por isso que nos concentramos primeiro na minimização do custo quadrático e somente depois examinaremos a precisão da classificação.
 </p>
 <p style="text-align: justify;">
  Mesmo considerando que queremos usar uma “smooth cost function”, você ainda pode se perguntar por que escolhemos a função quadrática. Talvez se escolhêssemos uma função de custo diferente, obteríamos um conjunto totalmente diferente de pesos e bias? Esta é uma preocupação válida e, mais tarde, revisitaremos a função de custo e faremos algumas modificações. No entanto, a função de custo quadrático mostrada anteriormente funciona perfeitamente para entender os conceitos básicos de aprendizagem em redes neurais, então ficaremos com isso por enquanto.
 </p>
 <p style="text-align: justify;">
  Recapitulando, nosso objetivo na construção de uma rede neural é encontrar pesos e bias que minimizem a função de custo quadrático C (w, b).
 </p>
 <p>
 </p>
 <h3>
  Descida do Gradiente
 </h3>
 <p style="text-align: justify;">
  A maioria das tarefas em Machine Learning são na verdade problemas de otimização e um dos algoritmos mais usados para isso é o Algoritmo de Descida do Gradiente. Para um iniciante, o nome Algoritmo de Descida do Gradiente pode parecer intimidante, mas espero que depois de ler o que está logo abaixo, isso deixe de ser um mistério para você.
 </p>
 <p style="text-align: justify;">
  A Descida do Gradiente é uma ferramenta padrão para otimizar funções complexas iterativamente dentro de um programa de computador. Seu objetivo é: dada alguma função arbitrária, encontrar um mínimo. Para alguns pequenos subconjuntos de funções – aqueles que são convexos – há apenas um único
  <em>
   minumum
  </em>
  que também acontece de ser global. Para as funções mais realistas, pode haver muitos mínimos, então a maioria dos mínimos são locais. Certifique-se de que a otimização encontre o “melhor”
  <em>
   minimum
  </em>
  e não fique preso em mínimos sub-otimistas (um problema comum durante o treinamento do algoritmo).
 </p>
 <p style="text-align: justify;">
  Para compreender a intuição da Descida do Gradiente, vamos simplificar um pouco as coisas. Vamos imaginar que simplesmente recebemos uma função de muitas variáveis e queremos minimizar essa função. Vamos desenvolver a técnica chamada Descida do Gradiente que pode ser usada para resolver tais problemas de minimização. Então, voltaremos para a função específica que queremos minimizar para as redes neurais.
 </p>
 <p style="text-align: justify;">
  Ok, suponhamos que estamos tentando minimizar alguma função, C(v). Esta poderia ser qualquer função de valor real de muitas variáveis, onde v = v1, v2, …. Observe que eu substitui a notação w e b por v para enfatizar que esta poderia ser qualquer função – não estamos mais pensando especificamente no contexto das redes neurais apenas. Para minimizar C (v), vamos imaginar C como uma função de apenas duas variáveis, que chamaremos v1 e v2, conforme pode ser visto na figura abaixo:
 </p>
 <p>
  <img alt="Descida do Gradiente" class="aligncenter wp-image-330 size-full" data-attachment-id="330" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Descida do Gradiente" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?fit=812%2C612" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?fit=812%2C612" data-orig-size="812,612" data-permalink="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/valley/" data-recalc-dims="1" height="612" sizes="(max-width: 812px) 100vw, 812px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=812%2C612" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?w=812 812w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=768%2C579 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=690%2C520 690w" width="812"/>
 </p>
 <p style="text-align: justify;">
  O que queremos é encontrar onde C atinge seu mínimo global. Fica claro, que para a função traçada no gráfico acima, podemos observar facilmente o gráfico e encontrar o mínimo. Mas uma função geral, C, pode ser uma função complicada de muitas variáveis, e geralmente não será possível apenas observar o gráfico para encontrar o mínimo.
 </p>
 <p style="text-align: justify;">
  Uma maneira de atacar o problema é usar Cálculo (especificamente Álgebra Linear) para tentar encontrar o mínimo de forma analítica. Podemos calcular
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Derivada" rel="noopener" target="_blank">
    derivadas
   </a>
  </span>
  e depois tentar usá-las para encontrar lugares onde C é um
  <em>
   extremum
  </em>
  . Isso pode funcionar quando C é uma função de apenas uma ou algumas variáveis. Mas vai se transformar em um pesadelo quando tivermos muitas outras variáveis. E para as redes neurais, muitas vezes queremos muito mais variáveis – as maiores redes neurais têm funções de custo que dependem de bilhões de pesos e bias de uma maneira extremamente complicada. Usando “apenas” Cálculo para minimizar isso, não funcionará e precisamos de algo mais! Precisamos de um algoritmo de otimização capaz de minimizar C (v).
 </p>
 <p style="text-align: justify;">
  Felizmente, há uma analogia que nos ajuda a compreender como encontrar a solução. Começamos por pensar em nossa função como uma espécie de vale e imaginamos uma bola rolando pela encosta do vale, conforme pode ser visto na figura abaixo. Nossa experiência diária nos diz que a bola acabará rolando para o fundo do vale. Talvez possamos usar essa ideia como forma de encontrar um mínimo para a função? Escolheríamos aleatoriamente um ponto de partida para uma bola (imaginária), e então simularíamos o movimento da bola enquanto ela rola até o fundo do vale. Poderíamos fazer essa simulação simplesmente por derivadas de computação da função C – essas derivadas nos diriam tudo o que precisamos saber sobre a “forma” local do vale, e, portanto, como nossa bola deve rolar.
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter wp-image-347 size-large" data-attachment-id="347" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="ball" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?fit=1024%2C555" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?fit=300%2C163" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?fit=1958%2C1061" data-orig-size="1958,1061" data-permalink="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/ball/" data-recalc-dims="1" height="555" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=1024%2C555" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=1024%2C555 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=300%2C163 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=768%2C416 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=200%2C108 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=690%2C374 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?w=1958 1958w" width="1024"/>
 </p>
 <p style="text-align: center;">
  <span style="font-size: 10pt;">
   Representação da Descida do Gradiente (com o objetivo de minimizar a função de custo)
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Ou seja, a Descida do Gradiente é um algoritmo de otimização usado para encontrar os valores de parâmetros (coeficientes ou se preferir w e b – weight e bias) de uma função que minimizam uma função de custo. A Descida do Gradiente é melhor usada quando os parâmetros não podem ser calculados analiticamente (por exemplo, usando álgebra linear) e devem ser pesquisados por um algoritmo de otimização.
 </p>
 <p style="text-align: justify;">
  O procedimento começa com valores iniciais para o coeficiente ou coeficientes da função. Estes poderiam ser 0.0 ou um pequeno valor aleatório (a inicialização dos coeficiente é parte crítica do processo e diversas técnicas podem ser usadas, ficando a escolha a cargo do
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Cientista de Dados
   </a>
  </span>
  e do problema a ser resolvido com o modelo). Poderíamos iniciar assim nossos coeficientes (valores de w e b):
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   coeficiente = 0,0
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O custo dos coeficientes é avaliado ligando-os à função e calculando o custo.
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   custo = f (coeficiente)
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  ou
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   custo = avaliar (f (coeficiente))
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A derivada do custo é calculada. A derivada é um conceito de Cálculo e refere-se à inclinação da função em um determinado ponto. Precisamos conhecer a inclinação para que possamos conhecer a direção (sinal) para mover os valores dos coeficientes para obter um custo menor na próxima iteração.
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   delta = derivado (custo)
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Agora que sabemos da derivada em que direção está em declive, podemos atualizar os valores dos coeficientes. Um parâmetro de taxa de aprendizagem (alfa) deve ser especificado e controla o quanto os coeficientes podem mudar em cada atualização.
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   coeficiente = coeficiente – (alfa * delta)
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Este processo é repetido até que o
  <strong>
   custo dos coeficientes
  </strong>
  (
  <strong>
   função de custo
  </strong>
  ) seja 0,0 ou próximo o suficiente de zero, indicando que as saídas da rede estão cada vez mais próximas dos valores reais (saídas desejadas).
 </p>
 <p style="text-align: justify;">
  A Descida do Gradiente é simples, mas exige que seja calculado o gradiente da função de custo ou a função que você está otimizando, mas além disso, é muito direto. Em resumo:
 </p>
 <blockquote>
  <p style="text-align: justify;">
   Você divide seus dados em amostras e a cada amostra (sample), você passa as entradas pela rede, multiplica pelos pesos, soma, e no final você vai ter sua saida (a previsão da rede). Você então compara a saída da sua rede com o a resposta certa, calcula o erro, e então retroage esse erro (backpropagation), ajustando os pesos de cada neurônio de cada camada. Quando você acabar de fazer a atualização dos pesos, uma nova amostra é introduzida e ela será multiplicada pelos pesos já atualizados. Esse processo de atualizar os pesos é que é chamado de “aprendizado”.
  </p>
  <p style="text-align: justify;">
   Se você observar os algoritmos mais atuais, todos trabalham dentro de um conceito relativamente novo chamado de mini-lotes (mini-batches). Para otimizar a performance, o que se faz é passar pela rede múltiplas amostras (por exemplo 128 amostras), calcular o erro médio delas e então realizar o backpropagation e a atualização dos pesos. Do ponto de vista da atualização dos pesos, 1 amostra = 128 amostras. Esse é um conceito mais novo, necessário principalmente no treinamento de grandes modelos de Deep Learning.
  </p>
 </blockquote>
 <p>
  Em seguida, veremos como podemos usar isso em algoritmos de aprendizado de máquina.
 </p>
 <h3>
 </h3>
 <h3 style="text-align: justify;">
  Batch Gradient Descent em Aprendizado de Máquina
 </h3>
 <p style="text-align: justify;">
  O objetivo de todos os algoritmos supervisionados de aprendizagem de máquina é estimar uma função de destino (f) que mapeia dados de entrada (X) para as variáveis ​​de saída (Y). Isso descreve todos os problemas de classificação e regressão (aprendizagem supervisionada).
 </p>
 <p style="text-align: justify;">
  Alguns algoritmos de aprendizagem de máquina têm coeficientes que caracterizam a estimativa de algoritmos para a função alvo (f). Diferentes algoritmos têm diferentes representações e diferentes coeficientes, mas muitos deles requerem um processo de otimização para encontrar o conjunto de coeficientes que resultam na melhor estimativa da função alvo. Os exemplos comuns de algoritmos com coeficientes que podem ser otimizados usando descida do gradiente são Regressão linear e Regressão logística.
 </p>
 <p style="text-align: justify;">
  A avaliação de quão próximo um modelo de aprendizagem de máquina estima a função de destino pode ser calculada de várias maneiras, muitas vezes específicas para o algoritmo de aprendizagem de máquina. A função de custo envolve a avaliação dos coeficientes no modelo de aprendizagem de máquina calculando uma previsão para o modelo para cada instância de treinamento no conjunto de dados e comparando as previsões com os valores de saída reais e calculando uma soma ou erro médio (como a Soma de Residuais Quadrados ou SSR no caso de regressão linear).
 </p>
 <p style="text-align: justify;">
  A partir da função de custo, uma derivada pode ser calculada para cada coeficiente para que ele possa ser atualizado usando exatamente a equação de atualização descrita acima.
 </p>
 <p style="text-align: justify;">
  O custo é calculado para um algoritmo de aprendizado de máquina em todo o conjunto de dados de treinamento para cada iteração do algoritmo de descida de gradiente. Uma iteração do algoritmo é chamada de um lote e esta forma de descida do gradiente é referida como descida do gradiente em lote (Batch Gradient Descent).
 </p>
 <p style="text-align: justify;">
  A descida do gradiente em lote é a forma mais comum de descida do gradiente em Machine Learning.
 </p>
 <h3>
 </h3>
 <h3 style="text-align: justify;">
  Stochastic Gradient Descent em Aprendizado de Máquina
 </h3>
 <p style="text-align: justify;">
  A Descida do Gradiente pode ser lenta para executar em conjuntos de dados muito grandes. Como uma iteração do algoritmo de descida do gradiente requer uma previsão para cada instância no conjunto de dados de treinamento, pode demorar muito quando você tem muitos milhões de instâncias.
 </p>
 <p style="text-align: justify;">
  Em situações em que você possui grandes quantidades de dados, você pode usar uma variação da descida do gradiente chamada Stochastic Gradient Descent.
 </p>
 <p style="text-align: justify;">
  Nesta variação, o procedimento de descida do gradiente descrito acima é executado, mas a atualização para os coeficientes é realizada para cada instância de treinamento, em vez do final do lote de instâncias.
 </p>
 <p style="text-align: justify;">
  O primeiro passo do procedimento exige que a ordem do conjunto de dados de treinamento seja randomizada. Isto é, misturar a ordem que as atualizações são feitas para os coeficientes. Como os coeficientes são atualizados após cada instância de treinamento, as atualizações serão barulhentas saltando por todo o lado, e assim o custo correspondente funcionará. Ao misturar a ordem para as atualizações dos coeficientes, ela aproveita essa caminhada aleatória e evita que ela fique “distraída” ou presa.
 </p>
 <p style="text-align: justify;">
  O procedimento de atualização para os coeficientes é o mesmo que o anterior, exceto que o custo não é somado em todos os padrões de treinamento, mas sim calculado para um padrão de treinamento.
 </p>
 <p style="text-align: justify;">
  A aprendizagem pode ser muito mais rápida com descida de gradiente estocástica para conjuntos de dados de treinamento muito grandes e muitas vezes você só precisa de um pequeno número de passagens através do conjunto de dados para alcançar um conjunto de coeficientes bom o suficiente.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Ufa, você ainda está aí? Entende agora porque Cientistas de Dados e Engenheiros de IA devem ser muito bem remunerados? Eles são os “magos” que estão ajudando a transformar o mundo com Machine Learning. E este capítulo foi apenas uma breve introdução! Voltaremos a este assunto mais a frente no livro, quando estudarmos outros algoritmos. Mas caso você queira aprender em detalhes como tudo isso funciona e criar seus modelos usando linguagens R, Python, Scala ou Java, para aplicações comerciais, confira:
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
  ,
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-com-linguagem-scala" rel="noopener" target="_blank">
    Machine Learning com Scala e Spark
   </a>
  </span>
  ,
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-i" rel="noopener" target="_blank">
    Deep Learning
   </a>
  </span>
  e
  <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-em-java" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Análise Preditiva com Machine Learning em Jav
   </span>
   a
  </a>
  .
 </p>
 <p style="text-align: justify;">
  Tenho certeza que você está ansioso para criar e treinar sua primeira rede neural. Então, não perca o próximo capítulo!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-com-linguagem-scala" rel="noopener" target="_blank">
    Machine Learning com Scala e Spark
   </a>
  </span>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-em-java" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Análise Preditiva com Machine Learning em Jav
   </span>
   a
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/mnist/" rel="noopener" target="_blank">
    MNIST
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Derivada" rel="noopener" target="_blank">
    Derivada
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ruder.io/optimizing-gradient-descent/" rel="noopener" target="_blank">
    An overview of gradient descent optimization algorithms
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/" rel="noopener" target="_blank">
    Optimization: Stochastic Gradient Descent
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html" rel="noopener" target="_blank">
    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-327" href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-327" href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-327" href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-327" href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-327-5e0dd053c2c97" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=327&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-327-5e0dd053c2c97" id="like-post-wrapper-140353593-327-5e0dd053c2c97">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-13">
 Capítulo 13 – Construindo Uma Rede Neural Com Linguagem Python
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Ok. Chegou a hora. Vamos escrever um programa em linguagem Python que aprenda como reconhecer dígitos manuscritos, usando Stochastic Gradient Descent e o dataset de treinamento MNIST. Se você chegou até aqui sem ler os capítulos anteriores, então pare imediatamente, leia os últimos 12 capítulos e depois volte aqui! Não tenha pressa! Não existe atalho para o aprendizado!
 </p>
 <p>
 </p>
 <p>
  ******************************** Atenção ********************************
 </p>
 <p style="text-align: justify;">
  Este capítulo considera que você já tem o interpretador Python (versão 3.6.x) instalado no seu computador, seja ele com sistema operacional Windows, MacOS ou Linux. Recomendamos que você instale o
  <span style="text-decoration: underline;">
   <a href="https://www.anaconda.com/download/#macos" rel="noopener" target="_blank">
    Anaconda
   </a>
  </span>
  e que já possua conhecimentos em linguagem Python. Se esse não for seu caso, antes de ler este capítulo e executar os exemplos aqui fornecidos, acesse o curso gratuito
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener" target="_blank">
    Python Fundamentos Para Análise de Dados
   </a>
  </span>
  .
 </p>
 <p style="text-align: justify;">
  <strong>
   Usaremos Python 3 e os scripts podem ser encontrados no repositório do livro no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener" target="_blank">
     GitHub
    </a>
   </span>
   . Vamos começar!
  </strong>
 </p>
 <p>
  *************************************************************************
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Quando descrevemos o dataset MNIST anteriormente, dissemos que ele estava dividido em 60.000 imagens de treinamento e 10.000 imagens de teste. Essa é a descrição oficial do MNIST. Mas vamos dividir os dados de forma um pouco diferente. Deixaremos as imagens de teste como está, mas dividiremos o conjunto de treinamento MNIST de 60.000 imagens em duas partes: um conjunto de 50.000 imagens, que usaremos para treinar nossa rede neural e um conjunto separado de validação de 10.000 imagens. Não utilizaremos os dados de validação neste capítulo, porém mais tarde, aqui mesmo no livro, usaremos este dataset quando estivermos configurando certos hiperparâmetros da rede neural, como a taxa de aprendizado por exemplo. Embora os dados de validação não façam parte da especificação MNIST original, muitas pessoas usam o MNIST desta forma e o uso de dados de validação é comum em redes neurais. Quando eu me referir aos “dados de treinamento MNIST” de agora em diante, vou me referir ao nosso conjunto de dados de 50.000 imagens, e não ao conjunto de dados de 60.000 imagens. Fique atento!
 </p>
 <p style="text-align: justify;">
  Além dos dados MNIST, também precisamos de uma biblioteca Python chamada Numpy, para álgebra linear. Se você instalou o Anaconda, não precisa se preocupar, pois o Numpy já está instalado. Caso contrário, será necessário fazer a instalação do pacote.
 </p>
 <p style="text-align: justify;">
  Mas antes de carregar e dividir os dados, vamos compreender os principais recursos do nosso código para construção de uma rede neural. A peça central é uma classe chamada
  <strong>
   Network
  </strong>
  , que usamos para representar uma rede neural. Abaixo a classe Network e seu construtor:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-large wp-image-371" data-attachment-id="371" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="classe1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?fit=1024%2C201" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?fit=300%2C59" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?fit=1242%2C244" data-orig-size="1242,244" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/classe1-2/" data-recalc-dims="1" height="201" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=1024%2C201" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=1024%2C201 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=300%2C59 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=768%2C151 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=200%2C39 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=690%2C136 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?w=1242 1242w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Neste código, o parâmetro
  <strong>
   sizes
  </strong>
  contêm o número de neurônios nas respectivas camadas, sendo um objeto do tipo lista em Python. Então, por exemplo, se queremos criar um objeto da classe Network com 2 neurônios na primeira camada, 3 neurônios na segunda camada e 1 neurônio na camada final, aqui está o código que usamos para instanciar um objeto da classe Network::
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <span style="font-size: 18pt;">
   <strong>
    rede1 = Network([2, 3, 1])
   </strong>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Os bias e pesos no objeto rede1 são todos inicializados aleatoriamente, usando a função Numpy
  <strong>
   np.random.randn
  </strong>
  para gerar distribuições gaussianas com 0 de média e desvio padrão 1. Esta inicialização aleatória dá ao nosso algoritmo de descida do gradiente estocástico um local para começar. Em capítulos posteriores, encontraremos melhores maneiras de inicializar os pesos e os bias. Observe que o código de inicialização de rede assume que a primeira camada de neurônios é uma camada de entrada e omite a definição de quaisquer bias para esses neurônios, uma vez que os bias são usados apenas para calcular as saídas de camadas posteriores.
 </p>
 <p style="text-align: justify;">
  Observe também que os bias e pesos são armazenados como listas de matrizes Numpy. Assim, por exemplo, rede1.weights[1] é uma matriz Numpy armazenando os pesos conectando a segunda e terceira camadas de neurônios. (Não é a primeira e segunda camadas, uma vez que a indexação da lista em Python começa em 0.) Uma vez que rede1.weights[1] é bastante detalhado, vamos apenas indicar essa matriz w. É uma matriz tal que wjk é o peso para a conexão entre o neurônio kth na segunda camada e o neurônio jth na terceira camada. Essa ordenação dos índices j e k pode parecer estranha – certamente teria mais sentido trocar os índices j e k? A grande vantagem de usar essa ordenação é que isso significa que o vetor de ativações da terceira camada de neurônios é:
 </p>
 <p>
 </p>
 <p>
  <img alt="Form" class="aligncenter size-full wp-image-367" data-attachment-id="367" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Form" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?fit=137%2C43" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?fit=137%2C43" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?fit=137%2C43" data-orig-size="137,43" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/form-2/" data-recalc-dims="1" height="43" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?resize=137%2C43" width="137"/>
 </p>
 <p style="text-align: center;">
  <span style="font-size: 8pt;">
   Equação 1
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Onde,
  <strong>
   a
  </strong>
  é o vetor de ativações da segunda camada de neurônios. Para obter um
  <strong>
   a’
  </strong>
  multiplicamos
  <strong>
   a
  </strong>
  pela matriz de peso
  <strong>
   w
  </strong>
  , e adicionamos o vetor
  <strong>
   b
  </strong>
  com os bias (se você leu os capítulos anteriores, isso não deve ser novidade agora). Em seguida, aplicamos a função
  <strong>
   σ
  </strong>
  de forma elementar a cada entrada no vetor
  <strong>
   wa + b
  </strong>
  . (Isto é chamado de vetorizar a função σ.)
 </p>
 <p>
  Com tudo isso em mente, é fácil escrever código que computa a saída de uma instância de rede. Começamos definindo a função sigmoide:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-full wp-image-372" data-attachment-id="372" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?fit=492%2C108" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?fit=300%2C66" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?fit=492%2C108" data-orig-size="492,108" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/func/" data-recalc-dims="1" height="108" sizes="(max-width: 492px) 100vw, 492px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?resize=492%2C108" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?w=492 492w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?resize=300%2C66 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?resize=200%2C44 200w" width="492"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Observe que quando a entrada z é um vetor ou uma matriz Numpy, Numpy aplica automaticamente a função sigmoid elementwise, ou seja, na forma vetorizada.
 </p>
 <p style="text-align: justify;">
  Em seguida, adicionamos um método feedforward à classe Network, que, dada a entrada a para a rede, retorna a saída corresponente. Basicamente o método feedforward aplica a Equação 1 mostrada acima, para cada camada:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-large wp-image-373" data-attachment-id="373" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func2" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?fit=1024%2C320" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?fit=300%2C94" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?fit=1240%2C388" data-orig-size="1240,388" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/func2/" data-recalc-dims="1" height="320" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=1024%2C320" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=1024%2C320 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=300%2C94 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=768%2C240 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=200%2C63 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=690%2C216 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?w=1240 1240w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A principal atividade que queremos que nossos objetos da classe Network façam é aprender. Para esse fim, criaremos um método SGD (Stochastic Gradient Descent). Aqui está o código. É um pouco misterioso em alguns lugares, mas vamos explicar em detalhes mais abaixo:
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="aligncenter size-large wp-image-383" data-attachment-id="383" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?fit=1024%2C682" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?fit=300%2C200" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?fit=1484%2C988" data-orig-size="1484,988" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/func2-2/" data-recalc-dims="1" height="682" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=1024%2C682" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=1024%2C682 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=300%2C200 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=768%2C511 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=200%2C133 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=690%2C459 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?w=1484 1484w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O
  <strong>
   training_data
  </strong>
  é uma lista de tuplas (x, y) que representam as entradas de treinamento e as correspondentes saídas desejadas. As variáveis
  <strong>
   epochs
  </strong>
  e
  <strong>
   mini_batch_size
  </strong>
  são o que você esperaria – o número de épocas para treinar e o tamanho dos mini-lotes a serem usados durante a amostragem, enquanto
  <strong>
   eta
  </strong>
  é a taxa de aprendizagem, η. Se o argumento opcional test_data for fornecido, o programa avaliará a rede após cada período de treinamento e imprimirá progresso parcial. Isso é útil para rastrear o progresso, mas retarda substancialmente as coisas.
 </p>
 <p style="text-align: justify;">
  O código funciona da seguinte forma. Em cada época, ele começa arrastando aleatoriamente os dados de treinamento e, em seguida, particiona-os em mini-lotes de tamanho apropriado. Esta é uma maneira fácil de amostragem aleatória dos dados de treinamento. Então, para cada mini_batch, aplicamos um único passo de descida do gradiente. Isso é feito pelo código self.update_mini_batch (mini_batch, eta), que atualiza os pesos e os bias da rede de acordo com uma única iteração de descida de gradiente, usando apenas os dados de treinamento em mini_batch. Aqui está o código para o método update_mini_batch:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="" class="aligncenter size-large wp-image-375" data-attachment-id="375" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="classe4" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?fit=1024%2C976" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?fit=300%2C286" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?fit=1442%2C1374" data-orig-size="1442,1374" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/classe4/" data-recalc-dims="1" height="976" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=1024%2C976" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=1024%2C976 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=300%2C286 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=768%2C732 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=200%2C191 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=690%2C657 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?w=1442 1442w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A maior parte do trabalho é feita pela linha delta_nabla_b, delta_nabla_w = self.backprop (x, y). Isso invoca algo chamado algoritmo de backpropagation, que é uma maneira rápida de calcular o gradiente da função de custo. Portanto, update_mini_batch funciona simplesmente calculando esses gradientes para cada exemplo de treinamento no mini_batch e, em seguida, atualizando self.weights e self.biases adequadamente.
 </p>
 <p style="text-align: justify;">
  Abaixo você encontra o código para self.backprop, mas não estudaremos ele agora. Estudaremos em detalhes como funciona o backpropagation no próximo capítulo, incluindo o código para self.backprop. Por hora, basta assumir que ele se comporta conforme indicado, retornando o gradiente apropriado para o custo associado ao exemplo de treinamento x.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="" class="aligncenter size-large wp-image-376" data-attachment-id="376" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="back" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?fit=1024%2C852" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?fit=300%2C250" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?fit=1192%2C992" data-orig-size="1192,992" data-permalink="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/back/" data-recalc-dims="1" height="852" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=1024%2C852" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=1024%2C852 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=300%2C250 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=768%2C639 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=200%2C166 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=690%2C574 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?w=1192 1192w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  No programa completo disponível no
  <span style="text-decoration: underline;">
   <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener" target="_blank">
    Github
   </a>
  </span>
  você encontra comentários explicando como ocorre todo o processo. Além do self.backprop, o programa é auto-explicativo – todo o levantamento pesado é feito em self.SGD e self.update_mini_batch, que já discutimos. O método self.backprop faz uso de algumas funções extras para ajudar no cálculo do gradiente, nomeadamente sigmoid_prime, que calcula a derivada da função σ e self.cost_derivative.
 </p>
 <p style="text-align: justify;">
  A classe Network é em essência nosso algoritmo de rede neural. A partir dela criamos uma instância (como rede1), alimentamos com os dados de treinamento e realizamos o treinamento. Avaliamos então a performance da rede com dados de teste e repetimos todo o processo até alcançar o nível de acurácia desejado em nosso projeto. Quando o modelo final estiver pronto, usamos para realizar as previsões para as quais o modelo foi criado, apresentando a ele novos conjuntos de dados e extraindo as previsões. Perceba que este é um algoritmo de rede neural bem simples, mas que permite compreender como funcionam as redes neurais e mais tarde, aqui mesmo no livro, as redes neurais profundas ou Deep Learning.
 </p>
 <p style="text-align: justify;">
  No próximo capítulo vamos continuar trabalhando com este algoritmo e compreender como funciona o Backpropagation. Na sequência, vamos carregar os dados, treinar e testar nossa rede neural e então usá-la para reconhecer dígitos manuscritos. Até lá.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener" target="_blank">
    Python Fundamentos Para Análise de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-com-linguagem-scala" rel="noopener" target="_blank">
    Machine Learning com Scala e Spark
   </a>
  </span>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-em-java" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Análise Preditiva com Machine Learning em Jav
   </span>
   a
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/mnist/" rel="noopener" target="_blank">
    MNIST
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Derivada" rel="noopener" target="_blank">
    Derivada
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ruder.io/optimizing-gradient-descent/" rel="noopener" target="_blank">
    An overview of gradient descent optimization algorithms
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/" rel="noopener" target="_blank">
    Optimization: Stochastic Gradient Descent
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html" rel="noopener" target="_blank">
    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-363" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-363" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-363" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-363" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-363-5e0dd055b69ab" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=363&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-363-5e0dd055b69ab" id="like-post-wrapper-140353593-363-5e0dd055b69ab">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-14">
 Capítulo 14 – Algoritmo Backpropagation Parte 1 – Grafos Computacionais e Chain Rule
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  No último capítulo, vimos como as redes neurais podem aprender seus pesos e bias usando o algoritmo de gradiente descendente. Houve, no entanto, uma lacuna na nossa explicação: não discutimos como calcular o gradiente da função de custo. Neste capítulo, explicaremos sobre um algoritmo usado para calcular esses gradientes, um algoritmo conhecido como backpropagation. Como esse tema é a essência do treinamento de redes neurais, vamos dividí-lo em dois capítulos. Vamos começar com Algoritmo Backpropagation Parte 1 – Grafos Computacionais e Chain Rule.
 </p>
 <p style="text-align: justify;">
  O backpropagation é indiscutivelmente o algoritmo mais importante na história das redes neurais – sem backpropagation, seria quase impossível treinar redes de aprendizagem profunda da forma que vemos hoje. O backpropagation pode ser considerado a pedra angular das redes neurais modernas e consequentemente do Deep Learning.
 </p>
 <p style="text-align: justify;">
  O algoritmo backpropagation foi originalmente introduzido na década de 1970, mas sua importância não foi totalmente apreciada até um famoso
  <span style="text-decoration: underline;">
   <a href="https://www.nature.com/articles/323533a0" rel="noopener" target="_blank">
    artigo de 1986 de David Rumelhart, Geoffrey Hinton e Ronald Williams
   </a>
  </span>
  . Esse artigo descreve várias redes neurais em que o backpropagation funciona muito mais rapidamente do que as abordagens anteriores de aprendizado, possibilitando o uso de redes neurais para resolver problemas que antes eram insolúveis.
 </p>
 <p style="text-align: justify;">
  O backpropagation é o algoritmo-chave que faz o treinamento de modelos profundos algo computacionalmente tratável. Para as redes neurais modernas, ele pode tornar o treinamento com gradiente descendente até dez milhões de vezes mais rápido, em relação a uma implementação ingênua. Essa é a diferença entre um modelo que leva algumas horas ou dias para treinar e e outro que poderia levar anos (sem exagero).
 </p>
 <p style="text-align: justify;">
  Além de seu uso em Deep Learning, o backpropagation é uma poderosa ferramenta computacional em muitas outras áreas, desde previsão do tempo até a análise da estabilidade numérica. De fato, o algoritmo foi reinventado pelo menos dezenas de vezes em diferentes campos. O nome geral, independente da aplicação, é “diferenciação no modo reverso”.
 </p>
 <p style="text-align: justify;">
  Fundamentalmente, backpropagation é uma técnica para calcular derivadas rapidamente (não sabe o que é derivada? Consulte o link para um excelente vídeo em português explicando esse conceito em detalhes nas referências ao final deste capítulo). E é um truque essencial, não apenas em Deep Learning, mas em uma ampla variedade de situações de computação numérica. E para compreender backpropagation de forma efetiva, vamos primeiro compreender o conceito de grafo computacional e chain rule.
 </p>
 <p>
 </p>
 <h2>
  Grafo Computacional
 </h2>
 <p style="text-align: justify;">
  Grafos computacionais são uma boa maneira de pensar em expressões matemáticas. O conceito de grafo foi introduzido por
  <span style="text-decoration: underline;">
   <a href="https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/birth-graph-theory-leonhard-euler-and-konigsberg-bridge-problem" rel="noopener" target="_blank">
    Leonhard Euler
   </a>
  </span>
  em 1736 para tentar resolver o problema das
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Sete_pontes_de_K%C3%B6nigsberg" rel="noopener" target="_blank">
    Pontes de Konigsberg
   </a>
  </span>
  . Grafos são modelos matemáticos para resolver problemas práticos do dia a dia, com várias aplicações no mundo real tais como: circuitos elétricos, redes de distribuição, relações de parentesco entre pessoas, análise de redes sociais, logística, redes de estradas, redes de computadores e muito mais. Grafos são muito usados para modelar problemas em computação.
 </p>
 <p style="text-align: justify;">
  Um Grafo é um modelo matemático que representa relações entre objetos. Um grafo G = (V, E) consiste de um conjunto de vértices
  <strong>
   V
  </strong>
  (também chamados de nós), ligados por um conjunto de bordas ou arestas
  <strong>
   E
  </strong>
  . Para aprender sobre grafos em mais detalhes, clique
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-analise-em-grafos-para-big-data" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  .
 </p>
 <p>
  Por exemplo, considere a expressão:
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   e = (a + b) ∗ (b + 1)
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Existem três operações: duas adições e uma multiplicação. Para facilitar a compreensão sobre isso, vamos introduzir duas variáveis intermediárias c e d para que a saída de cada função tenha uma variável. Nós agora temos:
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   c = a+b
  </strong>
  <br/>
  <strong>
   d = b+1
  </strong>
  <br/>
  <strong>
   e = c∗d
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Para criar um grafo computacional, fazemos cada uma dessas operações nos
  <em>
   <strong>
    nós
   </strong>
  </em>
  , juntamente com as variáveis de entrada. Quando o valor de um nó é a entrada para outro nó, uma seta vai de um para outro e temos nesse caso um grafo direcionado.
 </p>
 <p>
 </p>
 <div>
 </div>
 <div>
 </div>
 <div>
  <img alt="tree-def" class="aligncenter wp-image-396 size-large" data-attachment-id="396" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tree-def" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?fit=1024%2C592" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?fit=300%2C174" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?fit=1383%2C800" data-orig-size="1383,800" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-def/" data-recalc-dims="1" height="592" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=1024%2C592" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=1024%2C592 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=300%2C174 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=768%2C444 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=200%2C116 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=690%2C399 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?w=1383 1383w" width="1024"/>
 </div>
 <div>
 </div>
 <div>
 </div>
 <p>
 </p>
 <p style="text-align: justify;">
  Esses tipos de grafos surgem o tempo todo em Ciência da Computação, especialmente ao falar sobre programas funcionais. Eles estão intimamente relacionados com as noções de grafos de dependência e grafos de chamadas. Eles também são a principal abstração por trás do popular framework de Deep Learning, o
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-frameworks" rel="noopener" target="_blank">
    TensorFlow
   </a>
  </span>
  .
 </p>
 <p style="text-align: justify;">
  Podemos avaliar a expressão definindo as variáveis de entrada para determinados valores e computando os nós através do grafo. Por exemplo, vamos definir a = 2 e b = 1:
 </p>
 <p>
 </p>
 <p>
  <img alt="tree-eval" class="aligncenter size-large wp-image-397" data-attachment-id="397" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tree-eval" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?fit=1024%2C592" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?fit=300%2C173" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?fit=1403%2C811" data-orig-size="1403,811" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-eval/" data-recalc-dims="1" height="592" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-1024x592.png?resize=1024%2C592" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=1024%2C592 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=300%2C173 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=768%2C444 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=200%2C116 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=690%2C399 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?w=1403 1403w" width="1024"/>
 </p>
 <p>
 </p>
 <p>
  A expressão, nesse exemplo, é avaliada como 6.
 </p>
 <p>
 </p>
 <h2 style="text-align: justify;">
  Derivadas em Grafos Computacionais
 </h2>
 <p style="text-align: justify;">
  Se alguém quiser entender derivadas em um grafo computacional, a chave é entender as derivadas nas bordas (arestas que conectam os nós no grafo). Se
  <strong>
   a
  </strong>
  afeta diretamente
  <strong>
   c
  </strong>
  , então queremos saber como isso afeta
  <strong>
   c
  </strong>
  . Se
  <strong>
   a
  </strong>
  muda um pouco, como
  <strong>
   c
  </strong>
  muda? Chamamos isso de derivada parcial de
  <strong>
   c
  </strong>
  em relação a
  <strong>
   a
  </strong>
  .
 </p>
 <p style="text-align: justify;">
  Para avaliar as derivadas parciais neste grafo, precisamos da regra da soma e da regra do produto:
 </p>
 <p>
 </p>
 <p>
  <img alt="derivada" class="aligncenter wp-image-398 size-full" data-attachment-id="398" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="derivada" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?fit=229%2C119" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?fit=229%2C119" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?fit=229%2C119" data-orig-size="229,119" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/derivada/" data-recalc-dims="1" height="119" sizes="(max-width: 229px) 100vw, 229px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?resize=229%2C119" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?w=229 229w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?resize=200%2C104 200w" width="229"/>
 </p>
 <p>
 </p>
 <p>
  Abaixo, o grafo tem a derivada em cada borda (aresta) rotulada.
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="tree-eval-derivs" class="aligncenter size-large wp-image-399" data-attachment-id="399" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tree-eval-derivs" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1024%2C578" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=300%2C169" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1405%2C793" data-orig-size="1405,793" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-eval-derivs/" data-recalc-dims="1" height="578" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=300%2C169 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=768%2C433 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=200%2C113 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=690%2C389 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?w=1405 1405w" width="1024"/>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  E se quisermos entender como os nós que não estão diretamente conectados afetam uns aos outros? Vamos considerar como
  <strong>
   e
  </strong>
  é afetado por
  <strong>
   a
  </strong>
  . Se mudarmos
  <strong>
   a
  </strong>
  uma velocidade de 1,
  <strong>
   c
  </strong>
  também muda a uma velocidade de 1. Por sua vez,
  <strong>
   c
  </strong>
  mudando a uma velocidade de 1 faz com que
  <strong>
   e
  </strong>
  mude a uma velocidade de 2. Então
  <strong>
   e
  </strong>
  muda a uma taxa de 1 ∗ 2 em relação a
  <strong>
   a
  </strong>
  (analise o diagrama acima para visualizar isso).
 </p>
 <p style="text-align: justify;">
  A regra geral é somar todos os caminhos possíveis de um nó para o outro, multiplicando as derivadas em cada aresta do caminho. Por exemplo, para obter a derivada de
  <strong>
   e
  </strong>
  em relação a
  <strong>
   b
  </strong>
  , obtemos:
 </p>
 <p>
  <img alt="form" class="aligncenter size-full wp-image-400" data-attachment-id="400" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?fit=164%2C72" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?fit=164%2C72" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?fit=164%2C72" data-orig-size="164,72" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/form-3/" data-recalc-dims="1" height="72" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?resize=164%2C72" width="164"/>
 </p>
 <p style="text-align: justify;">
  Isso explica como
  <strong>
   b
  </strong>
  afeta
  <strong>
   e
  </strong>
  através de
  <strong>
   c
  </strong>
  e também como isso afeta
  <strong>
   d
  </strong>
  .
 </p>
 <p style="text-align: justify;">
  Essa regra geral de “soma sobre caminhos” é apenas uma maneira diferente de pensar sobre a regra da cadeia multivariada ou
  <strong>
   chain rule
  </strong>
  .
 </p>
 <p>
 </p>
 <h2>
 </h2>
 <h2>
  Fatorando os Caminhos
 </h2>
 <p style="text-align: justify;">
  O problema com apenas “somar os caminhos” é que é muito fácil obter uma explosão combinatória no número de caminhos possíveis.
 </p>
 <p>
 </p>
 <p>
  <img alt="chain-def-greek" class="aligncenter wp-image-414 size-medium" data-attachment-id="414" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="chain-def-greek" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?fit=1024%2C248" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?fit=300%2C73" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?fit=1660%2C402" data-orig-size="1660,402" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/chain-def-greek/" data-recalc-dims="1" height="73" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=300%2C73" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=300%2C73 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=768%2C186 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=1024%2C248 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=200%2C48 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=690%2C167 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?w=1660 1660w" width="300"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  No diagrama acima, existem três caminhos de X a Y, e mais três caminhos de Y a Z. Se quisermos obter a derivada ∂Z/∂X somando todos os caminhos, precisamos calcular 3 ∗ 3 = 9 caminhos:
 </p>
 <p>
 </p>
 <p>
  <img alt="form2" class="aligncenter size-full wp-image-415" data-attachment-id="415" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?fit=402%2C58" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?fit=300%2C43" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?fit=402%2C58" data-orig-size="402,58" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/form2-3/" data-recalc-dims="1" height="58" sizes="(max-width: 402px) 100vw, 402px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?resize=402%2C58" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?w=402 402w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?resize=300%2C43 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?resize=200%2C29 200w" width="402"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O exemplo acima só tem nove caminhos, mas seria fácil o número de caminhos crescer exponencialmente à medida que o grafo se torna mais complicado. Em vez de apenas ingenuamente somar os caminhos, seria muito melhor fatorá-los:
 </p>
 <p>
 </p>
 <p>
  <img alt="form3" class="aligncenter size-full wp-image-416" data-attachment-id="416" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?fit=228%2C66" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?fit=228%2C66" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?fit=228%2C66" data-orig-size="228,66" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/form3-2/" data-recalc-dims="1" height="66" sizes="(max-width: 228px) 100vw, 228px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?resize=228%2C66" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?w=228 228w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?resize=200%2C58 200w" width="228"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  É aí que entram a “diferenciação de modo de avanço” (forward-mode differentiation ou forward pass) e a “diferenciação de modo reverso” (reverse-mode differentiation ou backpropagation). Eles são algoritmos para calcular a soma de forma eficiente fatorando os caminhos. Em vez de somar todos os caminhos explicitamente, eles calculam a mesma soma de forma mais eficiente, mesclando os caminhos juntos novamente em cada nó. De fato, os dois algoritmos tocam cada borda exatamente uma vez!
 </p>
 <p style="text-align: justify;">
  A diferenciação do modo de avanço inicia em uma entrada para o grafo e se move em direção ao final. Em cada nó, soma todos os caminhos que se alimentam. Cada um desses caminhos representa uma maneira na qual a entrada afeta esse nó. Ao adicioná-los, obtemos a maneira total em que o nó é afetado pela entrada, isso é a derivada.
 </p>
 <p>
 </p>
 <p>
  <img alt="chain-forward-greek" class="aligncenter wp-image-418 size-large" data-attachment-id="418" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="chain-forward-greek" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?fit=1024%2C345" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?fit=300%2C101" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?fit=1660%2C560" data-orig-size="1660,560" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/chain-forward-greek/" data-recalc-dims="1" height="345" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=1024%2C345" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=1024%2C345 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=300%2C101 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=768%2C259 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=200%2C67 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=690%2C233 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?w=1660 1660w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Embora você provavelmente não tenha pensado nisso em termos de grafos, a diferenciação no modo de avanço é muito parecida com o que você aprendeu implicitamente caso tenha feito alguma introdução a Cálculo.
 </p>
 <p style="text-align: justify;">
  A diferenciação no modo reverso, por outro lado, começa na saída do grafo e se move em direção ao início (ou seja, se retropropaga ou backpropagation). Em cada nó, ele mescla todos os caminhos originados nesse nó.
 </p>
 <p>
 </p>
 <p>
  <img alt="chain-backward-greek" class="aligncenter size-large wp-image-419" data-attachment-id="419" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="chain-backward-greek" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?fit=1024%2C356" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?fit=300%2C104" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?fit=1660%2C577" data-orig-size="1660,577" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/chain-backward-greek/" data-recalc-dims="1" height="356" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=1024%2C356" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=1024%2C356 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=300%2C104 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=768%2C267 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=200%2C70 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=690%2C240 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?w=1660 1660w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A diferenciação do modo de avanço rastreia como uma entrada afeta todos os nós. A diferenciação no modo reverso rastreia como cada nó afeta uma saída. Ou seja, a diferenciação de modo de avanço aplica o operador ∂/∂X a cada nó, enquanto a diferenciação de modo reverso aplica o operador ∂Z/∂ a cada nó. Se isso parece o conceito de programação dinâmica, é porque é exatamente isso! (acesse um material sobre programação dinâmica nas referências ao final do capítulo)
 </p>
 <p style="text-align: justify;">
  Nesse ponto, você pode se perguntar porque alguém se importaria com a diferenciação no modo reverso. Parece uma maneira estranha de fazer a mesma coisa que o modo de avanço. Existe alguma vantagem? Vamos considerar nosso exemplo original novamente:
 </p>
 <p>
 </p>
 <p>
  <img alt="tree-eval-derivs" class="aligncenter size-large wp-image-399" data-attachment-id="399" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tree-eval-derivs" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1024%2C578" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=300%2C169" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1405%2C793" data-orig-size="1405,793" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-eval-derivs/" data-recalc-dims="1" height="578" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=300%2C169 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=768%2C433 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=200%2C113 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=690%2C389 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?w=1405 1405w" width="1024"/>
 </p>
 <p>
 </p>
 <p>
  Podemos usar a diferenciação de modo de avanço de
  <strong>
   b
  </strong>
  para cima. Isso nos dá a derivada de cada nó em relação a
  <strong>
   b
  </strong>
  .
 </p>
 <p>
 </p>
 <p>
  <img alt="tree-forwradmode" class="aligncenter size-large wp-image-420" data-attachment-id="420" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tree-forwradmode" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?fit=1024%2C583" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?fit=300%2C171" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?fit=1415%2C806" data-orig-size="1415,806" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-forwradmode/" data-recalc-dims="1" height="583" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=1024%2C583" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=1024%2C583 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=300%2C171 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=768%2C437 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=200%2C114 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=690%2C393 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?w=1415 1415w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Nós calculamos ∂e/∂b, a derivada de nossa saída em relação a um de nossos inputs. E se fizermos a diferenciação de modo reverso de
  <strong>
   e
  </strong>
  para baixo? Isso nos dá a derivada de
  <strong>
   e
  </strong>
  em relação a todos os nós:
 </p>
 <p>
  <img alt="tree-backprop" class="aligncenter size-large wp-image-421" data-attachment-id="421" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tree-backprop" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?fit=1024%2C606" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?fit=300%2C177" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?fit=1408%2C833" data-orig-size="1408,833" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-backprop/" data-recalc-dims="1" height="606" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=1024%2C606" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=1024%2C606 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=300%2C177 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=768%2C454 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=200%2C118 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=690%2C408 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?w=1408 1408w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Quando digo que a diferenciação no modo reverso nos dá a derivada de
  <strong>
   e
  </strong>
  em relação a cada nó, eu realmente quero dizer cada nó. Temos tanto ∂e/∂a quanto ∂e/∂b, as derivadas de
  <strong>
   e
  </strong>
  em relação a ambas as entradas. A diferenciação no modo de avanço nos deu a derivada de nossa saída em relação a uma única entrada, mas a diferenciação no modo reverso nos dá todos eles.
 </p>
 <p style="text-align: justify;">
  Para este grafo, isso é apenas um fator de duas velocidades, mas imagine uma função com um milhão de entradas e uma saída. A diferenciação no modo de avanço exigiria que passássemos pelo grafo um milhão de vezes para obter as derivadas. Diferenciação no modo reverso pode fazer isso em uma só passada! Uma aceleração de um fator de um milhão é bem legal e explica porque conseguimos treinar um modelo de rede neural profunda em tempo razoável.
 </p>
 <p style="text-align: justify;">
  Ao treinar redes neurais, pensamos no custo (um valor que descreve o quanto uma rede neural é ruim) em função dos parâmetros (números que descrevem como a rede se comporta). Queremos calcular as derivadas do custo em relação a todos os parâmetros, para uso em descida do gradiente. Entretanto, muitas vezes, há milhões ou até dezenas de milhões de parâmetros em uma rede neural. Então, a diferenciação no modo reverso, chamada de backpropagation no contexto das redes neurais, nos dá uma velocidade enorme!
 </p>
 <p style="text-align: justify;">
  Existem casos em que a diferenciação de modo de avanço faz mais sentido? Sim, existem! Onde o modo reverso fornece as derivadas de uma saída em relação a todas as entradas, o modo de avanço nos dá as derivadas de todas as saídas em relação a uma entrada. Se tiver uma função com muitas saídas, a diferenciação no modo de avanço pode ser muito, muito mais rápida.
 </p>
 <p>
 </p>
 <h2>
  Agora faz sentido?
 </h2>
 <p style="text-align: justify;">
  Quando aprendemos pela primeira vez o que é backpropagation, a reação é: “Oh, essa é apenas a regra da cadeia (chain rule)! Como demoramos tanto tempo para descobrir?”
 </p>
 <p style="text-align: justify;">
  Na época em que o backpropagation foi inventado, as pessoas não estavam muito focadas nas redes neurais feedforward. Também não era óbvio que as derivadas eram o caminho certo para treiná-las. Esses são apenas óbvios quando você percebe que pode calcular rapidamente derivadas. Houve uma dependência circular.
 </p>
 <p style="text-align: justify;">
  Treinar redes neurais com derivadas? Certamente você ficaria preso em mínimos locais. E obviamente seria caro computar todas essas derivadas. O fato é que só porque sabemos que essa abordagem funciona é que não começamos imediatamente a listar os motivos que provavelmente não funcionaria. Já sabemos que funciona, mas novas abordagens vem sendo propostas no avanço das pesquisas em Deep Learning e Inteligência Artificial.
 </p>
 <p>
 </p>
 <h2>
  Conclusão da Parte 1
 </h2>
 <p style="text-align: justify;">
  O backpropagation também é útil para entender como as derivadas fluem através de um modelo. Isso pode ser extremamente útil no raciocínio sobre porque alguns modelos são difíceis de otimizar. O exemplo clássico disso é o problema do desaparecimento de gradientes em redes neurais recorrentes, que discutiremos mais diante neste livro.
 </p>
 <p style="text-align: justify;">
  Por fim, há uma lição algorítmica ampla a ser retirada dessas técnicas. Backpropagation e forward-mode differentiation usam um poderoso par de truques (linearização e programação dinâmica) para computar derivadas de forma mais eficiente do que se poderia imaginar. Se você realmente entende essas técnicas, pode usá-las para calcular com eficiência várias outras expressões interessantes envolvendo derivadas.
 </p>
 <p style="text-align: justify;">
  Mas este capítulo teve como objetivo apenas ajudá-lo a compreender o algoritmo, já que praticamente não existe documentação sobre isso em português. Falta ainda compreender como o backpropagation é aplicado no treinamento das redes neurais. Ansioso por isso? Então acompanhe o próximo capítulo!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A Data Science Academy oferece um programa completo, onde esses e vários outros conceitos são estudados em detalhes e com várias aplicações práticas e usando TensorFlow. A Formação Inteligência Artificial é composta de 9 cursos, tudo 100% online e 100% em português, que aliam teoria e prática na medida certa, com aplicações reais de Inteligência Artificial. Confira o programa completo dos cursos:
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial.
   </a>
  </span>
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.youtube.com/watch?v=CQxb5ZXeY3E" rel="noopener" target="_blank">
    Me Salva! Cálculo – O que é uma derivada?
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/birth-graph-theory-leonhard-euler-and-konigsberg-bridge-problem" rel="noopener" target="_blank">
    The Birth Of Graph Theory: Leonhard Euler And The Königsberg Bridge Problem
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.nature.com/articles/323533a0" rel="noopener" target="_blank">
    Learning representations by back-propagating errors
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener" target="_blank">
    Chain Rule
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://colah.github.io/posts/2015-08-Backprop/" rel="noopener" target="_blank">
    Calculus on Computational Graphs: Backpropagation
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener" target="_blank">
    How the backpropagation algorithm works
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener" target="_blank">
    Dynamic programming
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  Nota: parte das imagens usadas neste capítulo foram extraídas no excelente post (citado nas referências acima) de
  <span style="text-decoration: underline;">
   <a href="http://colah.github.io/about.html" rel="noopener" target="_blank">
    Christopher Olah
   </a>
  </span>
  , pesquisador de Machine Learning do Google Brain, e com a devida autorização do autor.
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-390" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-390" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-390" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-390" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-390-5e0dd0578dfc5" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=390&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-390-5e0dd0578dfc5" id="like-post-wrapper-140353593-390-5e0dd0578dfc5">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-15">
 Capítulo 15 – Algoritmo Backpropagation Parte 2 – Treinamento de Redes Neurais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O backpropagation é indiscutivelmente o algoritmo mais importante na história das redes neurais – sem backpropagation (eficiente), seria impossível treinar redes de aprendizagem profunda da forma que vemos hoje. O backpropagation pode ser considerado a pedra angular das redes neurais modernas e aprendizagem profunda. Neste capítulo, vamos compreender como o backpropagation é usado no treinamento das redes neurais: Algoritmo Backpropagation Parte 2 – Treinamento de Redes Neurais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O algoritmo de backpropagation consiste em duas fases:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   1. O passo para frente (forward pass), onde nossas entradas são passadas através da rede e as previsões de saída obtidas (essa etapa também é conhecida como fase de propagação).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   2. O passo para trás (backward pass), onde calculamos o gradiente da função de perda na camada final (ou seja, camada de previsão) da rede e usamos esse gradiente para aplicar recursivamente a regra da cadeia (chain rule) para atualizar os pesos em nossa rede (etapa também conhecida como fase de atualização de pesos ou retro-propagação).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos analisar cada uma dessas fases e compreender como funciona o backpropagation no treinamento nas redes neurais. No próximo capítulo, voltaremos ao script em Python para compreender como é a implementação do algoritmo. Let’s begin!
  </span>
 </p>
 <p>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
  </span>
 </h2>
 <h2>
  <span style="color: #000000;">
   Forward Pass
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O propósito do passo para frente é propagar nossas entradas (os dados de entrada) através da rede aplicando uma série de
   <em>
    dot products
   </em>
   (multiplicação entre os vetores) e ativações até chegarmos à camada de saída da rede (ou seja, nossas previsões). Para visualizar esse processo, vamos primeiro considerar a tabela abaixo. Podemos ver que cada entrada X na matriz é 2-dim (2 dimensões), onde cada ponto de dado é representado por dois números. Por exemplo, o primeiro ponto de dado é representado pelo vetor de recursos (0, 0), o segundo ponto de dado por (0, 1), etc. Em seguida, temos nossos valores de saída Y como a coluna da direita. Nossos valores de saída são os rótulos de classe. Dada uma entrada da matriz, nosso objetivo é prever corretamente o valor de saída desejado. Em resumo, X representa as entradas e Y a saída.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="table" class="aligncenter size-full wp-image-438" data-attachment-id="438" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="table" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?fit=538%2C370" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?fit=300%2C206" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?fit=538%2C370" data-orig-size="538,370" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/table/" data-recalc-dims="1" height="370" sizes="(max-width: 538px) 100vw, 538px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?resize=538%2C370" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?w=538 538w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?resize=300%2C206 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?resize=200%2C138 200w" width="538"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para obter uma precisão de classificação perfeita nesse problema, precisamos de uma rede neural feedforward com pelo menos uma camada oculta. Podemos então começar com uma arquitetura de 2-2-1 conforme a imagem abaixo.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="rede1" class="aligncenter size-full wp-image-439" data-attachment-id="439" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?fit=742%2C322" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?fit=300%2C130" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?fit=742%2C322" data-orig-size="742,322" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/rede1/" data-recalc-dims="1" height="322" sizes="(max-width: 742px) 100vw, 742px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=742%2C322" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?w=742 742w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=300%2C130 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=200%2C87 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=690%2C299 690w" width="742"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Este é um bom começo, no entanto, estamos esquecendo de incluir o bias. Existem duas maneiras de incluir o bias b em nossa rede. Nós podemos:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   1. Usar uma variável separada.
  </span>
  <br/>
  <span style="color: #000000;">
   2. Tratar o bias como um parâmetro treinável dentro da matriz, inserindo uma coluna de
   <strong>
    1s
   </strong>
   nos vetores de recursos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Inserir uma coluna de 1s no nosso vetor de recursos é feito de forma programática, mas para garantir a didática, vamos atualizar nossa matriz para ver isso explicitamente, conforme tabela abaixo. Como você pode ver, uma coluna de
   <strong>
    1s
   </strong>
   foi adicionada aos nossos vetores de recursos. Na prática você pode inserir essa coluna em qualquer lugar que desejar, mas normalmente a colocamos como a primeira entrada no vetor de recursos ou a última entrada no vetor de recursos.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="table2" class="aligncenter size-full wp-image-447" data-attachment-id="447" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="table2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?fit=718%2C372" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?fit=300%2C155" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?fit=718%2C372" data-orig-size="718,372" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/table2-2/" data-recalc-dims="1" height="372" sizes="(max-width: 718px) 100vw, 718px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=718%2C372" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?w=718 718w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=300%2C155 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=200%2C104 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=690%2C357 690w" width="718"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como nós mudamos o tamanho do nosso vetor de recursos de entrada (normalmente o que é realizado dentro da implementação da rede em si, para que não seja necessário modificar explicitamente a nossa matriz), isso muda nossa arquitetura de rede de 2-2-1 para uma arquitetura 3-3-1, conforme imagem abaixo. Ainda nos referimos a essa arquitetura de rede como 2-2-1, mas quando se trata de implementação, na verdade, é 3-3-1 devido à adição do termo de bias incorporado na matriz.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="rede2" class="aligncenter size-full wp-image-442" data-attachment-id="442" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?fit=794%2C502" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?fit=300%2C190" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?fit=794%2C502" data-orig-size="794,502" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/rede2-2/" data-recalc-dims="1" height="502" sizes="(max-width: 794px) 100vw, 794px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=794%2C502" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?w=794 794w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=300%2C190 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=768%2C486 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=200%2C126 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=690%2C436 690w" width="794"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Finalmente, lembre-se de que tanto nossa camada de entrada quanto todas as camadas ocultas exigem um termo de bias. No entanto, a camada de saída final não requer um bias. O bias agora é um parâmetro treinável dentro da matriz de peso, tornando o treinamento mais eficiente e substancialmente mais fácil de implementar. Para ver o forward pass em ação, primeiro inicializamos os pesos em nossa rede, conforme figura abaixo. Observe como cada seta na matriz de peso tem um valor associado a ela – esse é o valor de peso atual para um determinado nó e significa o valor em que uma determinada entrada é amplificada ou diminuída. Este valor de peso será então atualizado durante a fase de backpropgation (lembre-se que ainda estamos no forward pass). Existem várias formas de inicializar o vetor de pesos e isso pode influenciar diretamente no treinamento da rede, como veremos mais abaixo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na extrema esquerda da figura abaixo, apresentamos o vetor de recursos (0, 1, 1) e também o valor de saída 1 para a rede, pois depois precisamos calcular os erros de previsão. Aqui podemos ver que 0,1 e 1 foram atribuídos aos três nós de entrada na rede. Para propagar os valores através da rede e obter a classificação final, nós precisamos do
   <em>
    dot product
   </em>
   entre as entradas e os valores de peso, seguido pela aplicação de um função de ativação (neste caso, a função
   <strong>
    sigmóide s
   </strong>
   ). Vamos calcular as entradas para os três nós nas camadas ocultas:
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   1. s ((0 x 0.351) + (1 x 1.076) + (1 x 1.116)) = 0.899
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   2. s ((0 x 0.097) + (1 x 0.165)+(1 x 0.542)) = 0.593
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   3. s ((0x 0.457) + (1 x 0.165)+(1 x 0.331)) = 0.378
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="rede3" class="aligncenter size-full wp-image-443" data-attachment-id="443" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?fit=833%2C648" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?fit=300%2C233" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?fit=833%2C648" data-orig-size="833,648" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/rede3/" data-recalc-dims="1" height="648" sizes="(max-width: 833px) 100vw, 833px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=833%2C648" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?w=833 833w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=300%2C233 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=768%2C597 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=200%2C156 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=690%2C537 690w" width="833"/>
  </span>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Observando os valores dos nós das camadas ocultas (camadas do meio), podemos ver que os nós foram atualizados para refletir nossa computação. Agora temos nossas entradas para os nós da camada oculta. Para calcular a previsão de saída, uma vez mais usamos o
   <em>
    dot product
   </em>
   seguido por uma ativação sigmóide:
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   s ((0.899 x 0.383) + (0.593 x – 0.327) + (0.378 x -0.329)) = 0.506
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   A saída da rede é, portanto, 0.506. Podemos aplicar uma função de etapa (step function) para determinar se a saída é a classificação correta ou não:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="saida" class="aligncenter size-full wp-image-445" data-attachment-id="445" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="saida" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?fit=560%2C170" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?fit=300%2C91" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?fit=560%2C170" data-orig-size="560,170" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/saida-2/" data-recalc-dims="1" height="170" sizes="(max-width: 560px) 100vw, 560px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?resize=560%2C170" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?w=560 560w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?resize=300%2C91 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?resize=200%2C61 200w" width="560"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aplicando a step function com saida = 0.506, vemos que nossa rede prevê 1 que é, de fato, o rótulo de classe correto. No entanto, a nossa rede não está muito confiante neste rótulo de classe. O valor previsto 0.506 está muito próximo do limite da etapa. Idealmente, esta previsão deve ser mais próxima de 0.98 ou 0.99., implicando que a nossa rede realmente aprendeu o padrão no conjunto de dados. Para que nossa rede realmente “aprenda”, precisamos aplicar o backpropagation.
  </span>
 </p>
 <p>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Backpropagation
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para qualquer problema de aprendizagem supervisionada, nós selecionamos pesos que fornecem a estimativa ótima de uma função que modela nossos dados de treinamento. Em outras palavras, queremos encontrar um conjunto de pesos W que minimize a saída de J(W), onde J(W) é a função de perda, ou o erro da rede. Nos capítulos anteriores, discutimos o algoritmo de gradiente descendente, em que atualizamos cada peso por alguma redução escalar negativa da derivada do erro em relação a esse peso. Se optarmos por usar gradiente descendente (ou quase qualquer outro algoritmo de otimização convexo), precisamos encontrar as derivadas na forma numérica.
  </span>
 </p>
 <blockquote>
  <p style="text-align: center;">
   <span style="color: #000000;">
    O objetivo do backpropagation é otimizar os pesos para que a rede neural possa aprender a mapear corretamente as entradas para as saídas.
   </span>
  </p>
 </blockquote>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para outros algoritmos de aprendizado de máquina, como regressão logística ou regressão linear, o cálculo das derivadas é uma aplicação elementar de diferenciação. Isso ocorre porque as saídas desses modelos são apenas as entradas multiplicadas por alguns pesos escolhidos e, no máximo, alimentados por uma única função de ativação (a função sigmóide na regressão logística). O mesmo, no entanto, não pode ser dito para redes neurais. Para demonstrar isso, aqui está um diagrama de uma rede neural de dupla camada:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="diam" class="aligncenter size-full wp-image-454" data-attachment-id="454" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="diam" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?fit=641%2C138" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?fit=300%2C65" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?fit=641%2C138" data-orig-size="641,138" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/diam/" data-recalc-dims="1" height="138" sizes="(max-width: 641px) 100vw, 641px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?resize=641%2C138" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?w=641 641w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?resize=300%2C65 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?resize=200%2C43 200w" width="641"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como você pode ver, cada neurônio é uma função do anterior conectado a ele. Em outras palavras, se alguém alterasse o valor de w1, os neurônios “hidden 1” e “hidden 2” (e, finalmente, a saída) mudariam. Devido a essa noção de dependências funcionais, podemos formular matematicamente a saída como uma função composta extensiva:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="func" class="aligncenter size-full wp-image-460" data-attachment-id="460" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?fit=341%2C124" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?fit=300%2C109" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?fit=341%2C124" data-orig-size="341,124" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func-2/" data-recalc-dims="1" height="124" sizes="(max-width: 341px) 100vw, 341px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?resize=341%2C124" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?w=341 341w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?resize=300%2C109 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?resize=200%2C73 200w" width="341"/>
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   ou simplesmente:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="func2" class="aligncenter size-full wp-image-461" data-attachment-id="461" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?fit=519%2C33" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?fit=300%2C19" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?fit=519%2C33" data-orig-size="519,33" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func2-3/" data-recalc-dims="1" height="33" sizes="(max-width: 519px) 100vw, 519px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?resize=519%2C33" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?w=519 519w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?resize=300%2C19 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?resize=200%2C13 200w" width="519"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para aplicar o algoritmo de backpropagation, nossa função de ativação deve ser diferenciável, de modo que possamos calcular a derivada parcial do erro em relação a um dado peso wi,j, loss(E), saída de nó oj e saída de rede j.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="derivada" class="aligncenter size-full wp-image-449" data-attachment-id="449" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="derivada" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?fit=232%2C68" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?fit=232%2C68" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?fit=232%2C68" data-orig-size="232,68" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/derivada-2/" data-recalc-dims="1" height="68" sizes="(max-width: 232px) 100vw, 232px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?resize=232%2C68" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?w=232 232w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?resize=200%2C59 200w" width="232"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui, a saída é uma função composta dos pesos, entradas e função (ou funções) de ativação. É importante perceber que as unidades / nós ocultos são simplesmente cálculos intermediários que, na realidade, podem ser reduzidos a cálculos da camada de entrada. Se fôssemos então tirar a derivada da função com relação a algum peso arbitrário (por exemplo, w1), aplicaríamos iterativamente a regra da cadeia (da qual eu tenho certeza que você se lembra do
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/" rel="noopener" style="color: #000000; text-decoration: underline;" target="_blank">
     capítulo anterior
    </a>
   </span>
   ). O resultado seria semelhante ao seguinte:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="func3" class="aligncenter size-full wp-image-462" data-attachment-id="462" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?fit=784%2C73" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?fit=300%2C28" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?fit=784%2C73" data-orig-size="784,73" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func3/" data-recalc-dims="1" height="73" sizes="(max-width: 784px) 100vw, 784px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=784%2C73" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?w=784 784w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=300%2C28 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=768%2C72 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=200%2C19 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=690%2C64 690w" width="784"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora, vamos anexar mais uma operação à cauda da nossa rede neural. Esta operação irá calcular e retornar o erro – usando a função de custo – da nossa saída:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="func4" class="aligncenter size-full wp-image-463" data-attachment-id="463" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?fit=752%2C144" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?fit=300%2C57" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?fit=752%2C144" data-orig-size="752,144" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func4/" data-recalc-dims="1" height="144" sizes="(max-width: 752px) 100vw, 752px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=752%2C144" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?w=752 752w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=300%2C57 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=200%2C38 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=690%2C132 690w" width="752"/>
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Tudo o que fizemos foi adicionar outra dependência funcional; nosso erro é agora uma função da saída e, portanto, uma função da entrada, pesos e função de ativação. Se fôssemos calcular a derivada do erro com qualquer peso arbitrário (novamente, escolheríamos w1), o resultado seria:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="func5" class="aligncenter size-full wp-image-464" data-attachment-id="464" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="func5" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?fit=649%2C79" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?fit=300%2C37" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?fit=649%2C79" data-orig-size="649,79" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func5/" data-recalc-dims="1" height="79" sizes="(max-width: 649px) 100vw, 649px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?resize=649%2C79" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?w=649 649w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?resize=300%2C37 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?resize=200%2C24 200w" width="649"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Cada uma dessas derivações pode ser simplificada, uma vez que escolhemos uma função de ativação e erro, de modo que todo o resultado represente um valor numérico. Nesse ponto, qualquer abstração foi removida e a derivada de erro pode ser usada na descida do gradiente (como discutido anteriormente aqui no livro) para melhorar iterativamente o peso. Calculamos as derivadas de erro w.r.t. para todos os outros pesos na rede e aplicamos gradiente descendente da mesma maneira. Isso é backpropagation – simplesmente o cálculo de derivadas que são alimentadas para um algoritmo de otimização convexa. Chamamos isso de “retropropagação” porque estamos usando o erro de saída para atualizar os pesos, tomando passos iterativos usando a regra da cadeia até que alcancemos o valor de peso ideal.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Depois de compreender o funcionamento do algoritmo backpropagation, você percebe sua simplicidade. Claro, a aritmética/cálculos reais podem ser difíceis, mas esse processo é tratado pelos nossos computadores. Na realidade, o backpropagation é apenas uma aplicação da regra da cadeia (chain rule). Como as redes neurais são estruturas de modelo de aprendizado de máquina multicamadas complicadas, cada peso “contribui” para o erro geral de uma maneira mais complexa e, portanto, as derivadas reais exigem muito esforço para serem produzidas. No entanto, uma vez que passamos pelo cálculo, o backpropagation das redes neurais é equivalente à descida de gradiente típica para regressão logística / linear.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim, como regra geral de atualizações de peso, podemos usar a Regra Delta (Delta Rule):
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   <strong>
    Novo Peso = Peso Antigo – Derivada * Taxa de Aprendizagem
   </strong>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A taxa de aprendizagem (learning rate) é introduzida como uma constante (geralmente muito pequena), a fim de forçar o peso a ser atualizado de forma suave e lenta (para evitar grandes passos e comportamento caótico).
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Para validar esta equação:
  </span>
 </p>
 <ul>
  <li>
   <span style="color: #000000;">
    Se a Derivada for positiva, isso significa que um aumento no peso aumentará o erro, portanto, o novo peso deverá ser menor.
   </span>
  </li>
  <li>
   <span style="color: #000000;">
    Se a Derivada é negativa, isso significa que um aumento no peso diminuirá o erro, portanto, precisamos aumentar os pesos.
   </span>
  </li>
  <li>
   <span style="color: #000000;">
    Se a Derivada é 0, significa que estamos em um mínimo estável. Assim, nenhuma atualização nos pesos é necessária -&gt; chegamos a um estado estável.
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Existem vários métodos de atualização de peso. Esses métodos são frequentemente chamados de
   <em>
    otimizadores
   </em>
   . A regra delta é a mais simples e intuitiva, no entanto, possui várias desvantagens. Confira nas referências ao final do capítulo, um excelente artigo sobre otimizadores.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como atualizamos os pesos com uma pequena etapa delta de cada vez, serão necessárias várias iterações para ocorrer o aprendizado. Na rede neural, após cada iteração, a força de descida do gradiente atualiza os pesos para um valor cada vez menor da função de perda global. A atualização de peso na rede neural é guiada pela força do gradiente descendente sobre o erro.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Quantas iterações são necessárias para convergir (ou seja, alcançar uma função de perda mínima global)? Isso vai depender de diversos fatores:
  </span>
 </p>
 <ul style="text-align: justify;">
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Depende de quão forte é a taxa de aprendizado que estamos aplicando. Alta taxa de aprendizado significa aprendizado mais rápido, mas com maior chance de instabilidade.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Depende também dos hyperparâmetros da rede (quantas camadas, quão complexas são as funções não-lineares, etc..). Quanto mais variáveis, mais leva tempo para convergir, mas a precisão tende a ser maior.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Depende do uso do método de otimização, pois algumas regras de atualização de peso são comprovadamente mais rápidas do que outras.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Depende da inicialização aleatória da rede. Talvez com alguma sorte você inicie a rede com pesos quase ideais e esteja a apenas um passo da solução ideal. Mas o contrário também pode ocorrer.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Depende da qualidade do conjunto de treinamento. Se a entrada e a saída não tiverem correlação entre si, a rede neural não fará mágica e não poderá aprender uma correlação aleatória.
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ou seja, treinar uma rede neural não é tarefa simples. Imagine agora treinar uma rede profunda, com várias camadas intermediárias e milhões ou mesmo bilhões de pontos de dados e você compreende o quão trabalhoso isso pode ser e quantas decisões devem ser tomadas pelo Cientista de Dados ou Engenheiro de IA. E aprender a trabalhar de forma profissional, requer tempo, dedicação e preparo e melhor ainda se isso puder ser 100% em português para acelerar seu aprendizado. Construir aplicações de IA é uma habilidade com demanda cada vez maior no mercado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Pensando nisso, a Data Science Academy oferece um programa completo, onde esses e vários outros conceitos são estudados em detalhes e com várias aplicações práticas, usando TensorFlow. A Formação Inteligência Artificial é composta de 9 cursos, tudo 100% online e 100% em português, que aliam teoria e prática na medida certa, com aplicações reais de Inteligência Artificial. Confira o programa completo dos cursos:
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial.
    </a>
   </span>
   Várias empresas em todo Brasil já estão treinando seus profissionais conosco! Venha fazer parte da revolução da IA.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora que você já compreende como funciona o backpropagation, podemos retornar ao código Python e ver tudo isso funcionando na prática. Mas isso é assunto para o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener" target="_blank">
    Dot Product
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener" target="_blank">
    How the backpropagation algorithm works
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/3-DeltaRule.pdf" rel="noopener" target="_blank">
    Delta Rule
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ruder.io/optimizing-gradient-descent/" rel="noopener" target="_blank">
    An overview of gradient descent optimization algorithms
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Derivada" rel="noopener" target="_blank">
    Derivada
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ruder.io/optimizing-gradient-descent/" rel="noopener" target="_blank">
    An overview of gradient descent optimization algorithms
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/" rel="noopener" target="_blank">
    Optimization: Stochastic Gradient Descent
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html" rel="noopener" target="_blank">
    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-436" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-436" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-436" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-436" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-436-5e0dd059b429e" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=436&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-436-5e0dd059b429e" id="like-post-wrapper-140353593-436-5e0dd059b429e">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-16">
 Capítulo 16 – Algoritmo Backpropagation em Python
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Depois de compreender como funciona o backpropagation, podemos agora entender o código usado em alguns capítulos anteriores para implementar o algoritmo (o qual vamos reproduzir aqui). O arquivo com o código completo pode ser encontrado no repositório do livro no
  <span style="text-decoration: underline;">
   <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener" target="_blank">
    Github
   </a>
  </span>
  .
 </p>
 <p style="text-align: justify;">
  Em nosso código nós temos os métodos
  <strong>
   update_mini_batch
  </strong>
  e
  <strong>
   backprop
  </strong>
  da classe Network. Em particular, o método
  <strong>
   update_mini_batch
  </strong>
  atualiza os pesos e bias da rede calculando o gradiente para o mini_batch atual de exemplos (dados) de treinamento:
 </p>
 <p>
 </p>
 <p>
  <img alt="metodo1" class="aligncenter wp-image-489 size-large" data-attachment-id="489" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="metodo1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?fit=1024%2C378" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?fit=300%2C111" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?fit=1318%2C486" data-orig-size="1318,486" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/metodo1/" data-recalc-dims="1" height="378" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=1024%2C378" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=1024%2C378 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=300%2C111 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=768%2C283 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=200%2C74 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=690%2C254 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?w=1318 1318w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A maior parte do trabalho é feita pela linha:
 </p>
 <p style="text-align: center;">
  <strong>
   delta_nabla_b, delta_nabla_w = self.backprop (x, y)
  </strong>
 </p>
 <p style="text-align: justify;">
  que usa o método backprop para descobrir as derivadas parciais ∂Cx / ∂blj e ∂Cx / ∂wljk. Isso invoca o algoritmo de backpropagation, que é uma maneira rápida de calcular o gradiente da função de custo. Portanto, update_mini_batch funciona simplesmente calculando esses gradientes para cada exemplo de treinamento no mini_batch e, em seguida, atualizando self.weights e self.biases adequadamente. Há uma pequena mudança – usamos uma abordagem ligeiramente diferente para indexar as camadas. Essa alteração é feita para aproveitar um recurso do Python, ou seja, o uso de índices de lista negativa para contar para trás a partir do final de uma lista, por exemplo, lst[-3] é a terceira última entrada em uma lista chamada lst. O código para backprop está abaixo, junto com algumas funções auxiliares, que são usadas para calcular a função σ, a derivada σ′ e a derivada da função de custo. Com essas inclusões, você deve ser capaz de entender o código de maneira independente:
 </p>
 <p>
 </p>
 <p>
  <img alt="backprop" class="aligncenter size-large wp-image-490" data-attachment-id="490" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="backprop" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?fit=1024%2C916" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?fit=300%2C268" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?fit=1292%2C1156" data-orig-size="1292,1156" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/backprop/" data-recalc-dims="1" height="916" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=1024%2C916" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=1024%2C916 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=300%2C268 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=768%2C687 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=200%2C179 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=690%2C617 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?w=1292 1292w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Observe o método backprop. Começamos inicalizando as matrizes de pesos (nabla_w) e bias (nabla_b) com zeros. Essas  matrizes serão alimentadas com valores durante o processo de treinamento. Isso é o que a rede neural artificial efetivamente aprende. Depois de inicializar alguns objetos, temos um loop for para cada valor de b e w (que a esta altura você já sabe se trata de bias e pesos, respectivamente). Neste loop, usamos a função np.dot do Numpy para a multiplicação entre matrizes e adição do bias, colocamos o resultado na lista z e fazemos uma chamada à função de ativação Sigmóide. Ao final deste loop, teremos a lista com todas as ativações e finalizamos a passada para a frente.
 </p>
 <p style="text-align: justify;">
  Na passada para trás (Backward Pass) calculamos as derivadas e fazemos as multiplicações de matrizes mais uma vez (o funcionamento de redes neurais artificiais é baseado em um conceito elementar da Álgebra Linear, a multiplicação de matrizes). Repare que chamamos o método Transpose() para gerar a transposta da matriz e assim ajustar as dimensões antes de efetuar os cálculo. Por fim, retornamos bias e pesos.
 </p>
 <p>
 </p>
 <h3 style="text-align: justify;">
  Em que sentido backpropagation é um algoritmo rápido?
 </h3>
 <p style="text-align: justify;">
  Para responder a essa pergunta, vamos considerar outra abordagem para calcular o gradiente. Imagine que é o início da pesquisa de redes neurais. Talvez seja a década de 1950 ou 1960, e você é a primeira pessoa no mundo a pensar em usar gradiente descendente para o aprendizado! Mas, para que a ideia funcione, você precisa de uma maneira de calcular o gradiente da função de custo. Você volta ao seu conhecimento de cálculo e decide se pode usar a regra da cadeia (chain rule) para calcular o gradiente. Mas depois de brincar um pouco, a álgebra parece complicada e você fica desanimado. Então você tenta encontrar outra abordagem. Você decide considerar o custo como uma função apenas dos pesos C = C(w) (voltaremos ao bias em um momento). Você numera os pesos w1, w2,… e deseja computar ∂C / ∂wj para um peso específico wj. Uma maneira óbvia de fazer isso é usar a aproximação
 </p>
 <p style="text-align: justify;">
  <img alt="form" class="aligncenter size-full wp-image-484" data-attachment-id="484" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?fit=488%2C154" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?fit=300%2C95" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?fit=488%2C154" data-orig-size="488,154" data-permalink="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/form-4/" data-recalc-dims="1" height="154" sizes="(max-width: 488px) 100vw, 488px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?resize=488%2C154" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?w=488 488w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?resize=300%2C95 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?resize=200%2C63 200w" width="488"/>
 </p>
 <p style="text-align: justify;">
  onde ϵ&gt; 0 é um pequeno número positivo e ej é o vetor unitário na direção j. Em outras palavras, podemos estimar ∂C / ∂wj calculando o custo C para dois valores ligeiramente diferentes de wj e, em seguida, aplicando a equação. A mesma ideia nos permitirá calcular as derivadas parciais ∂C / ∂b em relação aos vieses (bias).
 </p>
 <p style="text-align: justify;">
  Essa abordagem parece muito promissora. É simples conceitualmente e extremamente fácil de implementar, usando apenas algumas linhas de código. Certamente, parece muito mais promissor do que a ideia de usar a regra da cadeia para calcular o gradiente!
 </p>
 <p style="text-align: justify;">
  Infelizmente, embora essa abordagem pareça promissora, quando você implementa o código, ele fica extremamente lento. Para entender porque, imagine que temos um milhão de pesos em nossa rede. Então, para cada peso distinto wj, precisamos computar C (w + ϵej) para calcular ∂C / ∂wj. Isso significa que, para calcular o gradiente, precisamos computar a função de custo um milhão de vezes diferentes, exigindo um milhão de passos para frente pela rede (por exemplo, treinamento). Precisamos calcular C(w) também, em um total de um milhão de vezes e em uma única passada pela rede.
 </p>
 <p style="text-align: justify;">
  O que há de inteligente no backpropagation é que ele nos permite calcular simultaneamente todas as derivadas parciais ∂C / ∂wj usando apenas uma passagem direta pela rede, seguida por uma passagem para trás pela rede. Grosso modo, o custo computacional do passe para trás é quase o mesmo que o do forward. Isso deve ser plausível, mas requer algumas análises para fazer uma declaração cuidadosa. É plausível porque o custo computacional dominante no passe para frente é multiplicado pelas matrizes de peso, enquanto no passo para trás é multiplicado pelas transpostas das matrizes de peso. Obviamente, essas operações têm um custo computacional similar. E assim, o custo total da retropropagação (backpropagation) é aproximadamente o mesmo que fazer apenas duas passagens pela rede. Compare isso com o milhão e um passe para frente que precisávamos para a abordagem que descrevi anteriormente. E assim, embora a retropropagação pareça superficialmente mais complexa do que a abordagem anterior, é na verdade muito, muito mais rápida.
 </p>
 <p style="text-align: justify;">
  Essa aceleração foi amplamente apreciada em 1986 e expandiu enormemente a gama de problemas que as redes neurais poderiam resolver. Isso, por sua vez, causou uma onda de pessoas usando redes neurais. Claro, a retropropagação não é uma panacéia. Mesmo no final da década de 1980, as pessoas enfrentavam limites, especialmente quando tentavam usar a retropropagação para treinar redes neurais profundas, ou seja, redes com muitas camadas ocultas. Mais adiante, no livro, veremos como os computadores modernos e algumas novas ideias inteligentes tornam possível usar a retropropagação para treinar redes neurais bem profundas.
 </p>
 <p style="text-align: justify;">
  Seu trabalho agora é estudar e compreender cada linha de código usada em nossa rede de amostra. Esse código é bem simples e o objetivo é mostrar a você como as coisas funcionam programaticamente. Ainda vamos treinar nossa rede, avaliar seu desempenho, otimizar algumas operações e compreender outros conceitos básicos. Temos muito mais vindo por aí! Até o próximo capítulo!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener" target="_blank">
    Dot Product
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener" target="_blank">
    How the backpropagation algorithm works
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/3-DeltaRule.pdf" rel="noopener" target="_blank">
    Delta Rule
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ruder.io/optimizing-gradient-descent/" rel="noopener" target="_blank">
    An overview of gradient descent optimization algorithms
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Derivada" rel="noopener" target="_blank">
    Derivada
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ruder.io/optimizing-gradient-descent/" rel="noopener" target="_blank">
    An overview of gradient descent optimization algorithms
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/" rel="noopener" target="_blank">
    Optimization: Stochastic Gradient Descent
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html" rel="noopener" target="_blank">
    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning
   </a>
  </span>
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-480" href="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-480" href="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-480" href="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-480" href="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-480-5e0dd05cd603d" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=480&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-480-5e0dd05cd603d" id="like-post-wrapper-140353593-480-5e0dd05cd603d">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-17">
 Capítulo 17 – Cross-Entropy Cost Function
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Quando um jogador de tênis está aprendendo a praticar o esporte, ele geralmente passa a maior parte do tempo desenvolvendo o movimento do corpo. Apenas gradualmente ele desenvolve as tacadas, aprende a movimentar a bola com precisão para a quadra adversária e com isso vai construindo sua técnica, que se aprimora à medida que ele pratica. De maneira semelhante, até agora nos concentramos em entender o algoritmo de retropropagação (backpropagation), a base para aprender a maioria das atividades em redes neurais. A partir de agora, estudaremos um conjunto de técnicas que podem ser usadas para melhorar nossa implementação do backpropagation e, assim, melhorar a maneira como nossas redes aprendem.
 </p>
 <p style="text-align: justify;">
  As técnicas que desenvolveremos incluem: uma melhor escolha de função de custo, conhecida como função de custo de entropia cruzada (ou Cross-Entropy Cost Function); quatro métodos de “regularização” (regularização de L1 e L2, dropout e expansão artificial dos dados de treinamento), que melhoram nossas redes para generalizar além dos dados de treinamento; um método melhor para inicializar os pesos na rede; e um conjunto de heurísticas para ajudar a escolher bons hyperparâmetros para a rede. Também vamos analisar várias outras técnicas com menos profundidade. As discussões são em grande parte independentes umas das outras e, portanto, você pode avançar se quiser. Também implementaremos muitas das técnicas em nosso código e usaremos para melhorar os resultados obtidos no problema de classificação de dígitos manuscritos estudado nos
  <a href="http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/" rel="noopener" target="_blank">
   capítulos anteriores
  </a>
  .
 </p>
 <p style="text-align: justify;">
  Naturalmente, estamos cobrindo apenas algumas das muitas técnicas que foram desenvolvidas para uso em redes neurais. A filosofia é que o melhor acesso à multiplicidade de técnicas disponíveis é o estudo aprofundado de algumas das mais importantes. Dominar essas técnicas importantes não é apenas útil por si só, mas também irá aprofundar sua compreensão sobre quais problemas podem surgir quando você usa redes neurais. Isso deixará você bem preparado para aprender rapidamente outras técnicas, conforme necessário.
 </p>
 <p>
 </p>
 <h3>
  A Função de Custo
 </h3>
 <p style="text-align: justify;">
  A maioria de nós acha desagradável estar errado. Logo depois de começar a aprender piano, minha filha fez sua primeira apresentação diante de uma platéia. Ela estava nervosa e começou a tocar a peça com uma oitava muito baixa. Ela ficou confusa e não pôde continuar até que alguém apontasse o erro. Ela ficou muito envergonhada. Ainda que desagradável, também aprendemos rapidamente quando estamos decididamente errados. Você pode apostar que a próxima vez que ela se apresentou diante de uma platéia, ela começou na oitava correta! Em contraste, aprendemos mais lentamente quando nossos erros são menos bem definidos.
 </p>
 <p style="text-align: justify;">
  Idealmente, esperamos que nossas redes neurais aprendam rapidamente com seus erros. Mas é isso que acontece na prática? Para responder a essa pergunta, vamos dar uma olhada em um exemplo simples. O exemplo envolve um neurônio com apenas uma entrada:
 </p>
 <p>
 </p>
 <p>
  <img alt="neuron" class="aligncenter size-full wp-image-509" data-attachment-id="509" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="neuron" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?fit=297%2C85" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?fit=297%2C85" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?fit=297%2C85" data-orig-size="297,85" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/neuron/" data-recalc-dims="1" height="85" sizes="(max-width: 297px) 100vw, 297px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?resize=297%2C85" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?w=297 297w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?resize=200%2C57 200w" width="297"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Nós vamos treinar esse neurônio para fazer algo ridiculamente fácil: obter a entrada 1 e gerar a saída 0. Claro, essa é uma tarefa tão trivial que poderíamos facilmente descobrir um peso apropriado e um viés (bias) de forma manual, sem usar um algoritmo de aprendizado. No entanto, vai nos ajudar a compreender melhor o processo de usar gradiente descendente para tentar aprender um peso e viés. Então, vamos dar uma olhada em como o neurônio aprende.
 </p>
 <p style="text-align: justify;">
  Para tornar as coisas definitivas, escolhemos o peso inicial como 0.6 e o ​​viés inicial como 0.9. Estas são escolhas genéricas usadas como um lugar para começar a aprender, eu não as escolhi para serem especiais de alguma forma. A saída inicial do neurônio é 0.82, então um pouco de aprendizado será necessário antes que nosso neurônio se aproxime da saída desejada 0,0.
 </p>
 <p style="text-align: justify;">
  No gráfico abaixo, podemos ver como o neurônio aprende uma saída muito mais próxima de 0.0. Durante o treinamento, o modelo está realmente computando o gradiente, e usando o gradiente para atualizar o peso e o viés, e exibir o resultado. A taxa de aprendizado é η = 0.15, o que acaba sendo lento o suficiente para que possamos acompanhar o que está acontecendo, mas rápido o suficiente para que possamos obter um aprendizado substancial em apenas alguns segundos. O custo é a função de custo quadrático, C, apresentada nos capítulos anteriores. Vou lembrá-lo da forma exata da função de custo em breve.
 </p>
 <p>
 </p>
 <p>
  <img alt="train" class="aligncenter size-full wp-image-510" data-attachment-id="510" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="train" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?fit=524%2C292" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?fit=300%2C167" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?fit=524%2C292" data-orig-size="524,292" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/train/" data-recalc-dims="1" height="292" sizes="(max-width: 524px) 100vw, 524px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?resize=524%2C292" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?w=524 524w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?resize=300%2C167 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?resize=200%2C111 200w" width="524"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Como você pode ver, o neurônio aprende um peso e um viés que diminui o custo e dá uma saída do neurônio de cerca de 0.09 (Epoch, ou Época em português, é o número de passadas que nosso modelo faz pelos dados. A cada passada, os pesos são atualizados, o aprendizado ocorre e o custo, ou a taxa de erros, diminui). Isso não é exatamente o resultado desejado, 0.0, mas é muito bom.
 </p>
 <p style="text-align: justify;">
  Suponha, no entanto, que, em vez disso, escolhamos o peso inicial e o viés inicial como 2.0. Nesse caso, a saída inicial é 0.98, o que é muito ruim. Vamos ver como o neurônio aprende a gerar 0 neste caso:
 </p>
 <p>
 </p>
 <p>
  <img alt="train2" class="aligncenter size-full wp-image-511" data-attachment-id="511" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="train2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?fit=517%2C288" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?fit=300%2C167" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?fit=517%2C288" data-orig-size="517,288" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/train2/" data-recalc-dims="1" height="288" sizes="(max-width: 517px) 100vw, 517px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?resize=517%2C288" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?w=517 517w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?resize=300%2C167 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?resize=200%2C111 200w" width="517"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Embora este exemplo use a mesma taxa de aprendizado (η = 0.15), podemos ver que a aprendizagem começa muito mais devagar. De fato, nas primeiras 150 épocas de aprendizado, os pesos e vieses não mudam muito. Então o aprendizado entra em ação e, como em nosso primeiro exemplo, a saída do neurônio se aproxima rapidamente de 0.0.
 </p>
 <p style="text-align: justify;">
  Esse comportamento é estranho quando comparado ao aprendizado humano. Como eu disse no começo deste capítulo, muitas vezes aprendemos mais rápido quando estamos muito errados sobre algo. Mas acabamos de ver que nosso neurônio artificial tem muita dificuldade em aprender quando está muito errado – muito mais dificuldade do que quando está apenas um pouco errado. Além do mais, verifica-se que esse comportamento ocorre não apenas neste exemplo, mas em redes mais gerais. Por que aprender tão devagar? E podemos encontrar uma maneira de evitar essa desaceleração?
 </p>
 <p style="text-align: justify;">
  Para entender a origem do problema, considere que nosso neurônio aprende mudando o peso e o viés a uma taxa determinada pelas derivadas parciais da função custo, ∂C/∂w e ∂C/∂b. Então, dizer “aprender é lento” é realmente o mesmo que dizer que essas derivadas parciais são pequenas. O desafio é entender por que eles são pequenas. Para entender isso, vamos calcular as derivadas parciais. Lembre-se de que estamos usando a função de custo quadrático, que é dada por:
 </p>
 <p>
  <img alt="cost" class="aligncenter size-full wp-image-512" data-attachment-id="512" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="cost" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?fit=123%2C69" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?fit=123%2C69" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?fit=123%2C69" data-orig-size="123,69" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/cost/" data-recalc-dims="1" height="69" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?resize=123%2C69" width="123"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  onde
  <strong>
   a
  </strong>
  é a saída do neurônio quando a entrada de treinamento x = 1 é usada, e y = 0 é a saída desejada correspondente. Para escrever isso mais explicitamente em termos de peso e viés, lembre-se que a = σ(z), onde z = wx + b. Usando a regra da cadeia para diferenciar em relação ao peso e viés, obtemos:
 </p>
 <p>
 </p>
 <p>
  <img alt="cost2" class="aligncenter size-full wp-image-513" data-attachment-id="513" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="cost2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?fit=268%2C122" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?fit=268%2C122" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?fit=268%2C122" data-orig-size="268,122" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/cost2/" data-recalc-dims="1" height="122" sizes="(max-width: 268px) 100vw, 268px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?resize=268%2C122" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?w=268 268w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?resize=200%2C91 200w" width="268"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  onde substitui x = 1 e y = 0. Para entender o comportamento dessas expressões, vamos olhar mais de perto o termo σ ′ (z) no lado direito. Lembre-se da forma da função σ:
 </p>
 <p>
 </p>
 <p>
  <img alt="sig" class="aligncenter size-full wp-image-514" data-attachment-id="514" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="sig" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?fit=436%2C300" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?fit=300%2C206" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?fit=436%2C300" data-orig-size="436,300" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/sig/" data-recalc-dims="1" height="300" sizes="(max-width: 436px) 100vw, 436px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?resize=436%2C300" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?w=436 436w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?resize=300%2C206 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?resize=200%2C138 200w" width="436"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Podemos ver neste gráfico que quando a saída do neurônio é próxima de 1, a curva fica muito plana, e então σ ′ (z) fica muito pequeno. As equações acima então nos dizem que ∂C/∂w e ∂C/∂b ficam muito pequenos. Esta é a origem da desaceleração da aprendizagem. Além do mais, como veremos mais adiante, a desaceleração do aprendizado ocorre basicamente pelo mesmo motivo em redes neurais mais genéricas, não apenas neste exemplo simples.
 </p>
 <p>
 </p>
 <h3>
  A Função de Custo de Entropia Cruzada
 </h3>
 <p style="text-align: justify;">
  Como podemos abordar a desaceleração da aprendizagem? Acontece que podemos resolver o problema substituindo o custo quadrático por uma função de custo diferente, conhecida como entropia cruzada. Para entender a entropia cruzada, vamos nos afastar um pouco do nosso modelo super-simples. Vamos supor que estamos tentando treinar um neurônio com diversas variáveis de entrada, x1, x2,…, pesos correspondentes w1, w2,… e um viés, b:
 </p>
 <p>
 </p>
 <p>
  <img alt="neuron2" class="aligncenter size-full wp-image-515" data-attachment-id="515" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="neuron2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?fit=288%2C138" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?fit=288%2C138" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?fit=288%2C138" data-orig-size="288,138" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/neuron2/" data-recalc-dims="1" height="138" sizes="(max-width: 288px) 100vw, 288px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?resize=288%2C138" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?w=288 288w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?resize=200%2C96 200w" width="288"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A saída do neurônio é, naturalmente,
  <strong>
   a = σ(z)
  </strong>
  , onde
  <strong>
   z = ∑jwjxj + b
  </strong>
  é a soma ponderada das entradas. Nós definimos a função de custo de entropia cruzada para este neurônio assim:
 </p>
 <p>
 </p>
 <p>
  <img alt="entropy" class="aligncenter size-full wp-image-516" data-attachment-id="516" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="entropy" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?fit=327%2C74" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?fit=300%2C68" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?fit=327%2C74" data-orig-size="327,74" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/entropy/" data-recalc-dims="1" height="74" sizes="(max-width: 327px) 100vw, 327px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?resize=327%2C74" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?w=327 327w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?resize=300%2C68 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?resize=200%2C45 200w" width="327"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  onde
  <strong>
   n
  </strong>
  é o número total de itens de dados de treinamento, a soma é sobre todas as entradas de treinamento
  <strong>
   x,
  </strong>
  e
  <strong>
   y
  </strong>
  é a saída desejada correspondente. Não é óbvio que a expressão anterior resolva o problema de desaceleração do aprendizado. De fato, francamente, nem é óbvio que faz sentido chamar isso de uma função de custo! Antes de abordar a desaceleração da aprendizagem, vamos ver em que sentido a entropia cruzada pode ser interpretada como uma função de custo.
 </p>
 <p style="text-align: justify;">
  Duas propriedades em particular tornam razoável interpretar a entropia cruzada como uma função de custo. Primeiro, não é negativo, isto é, C &gt; 0. Para visualizar isso, observe na fórmula anterior que: (a) todos os termos individuais na soma são negativos, já que ambos os logaritmos são de números no intervalo de 0 a 1; e (b) há um sinal de menos na frente da soma.
 </p>
 <p style="text-align: justify;">
  Segundo, se a saída real do neurônio estiver próxima da saída desejada para todas as entradas de treinamento x, então a entropia cruzada será próxima de zero. Para ver isso, suponha, por exemplo, que y = 0 e a ≈ 0 para alguma entrada x. Este é um caso quando o neurônio está fazendo um bom trabalho nessa entrada. Vemos que o primeiro termo (na fórmula acima) para o custo, desaparece, desde que y = 0, enquanto o segundo termo é apenas −ln (1 − a) ≈ 0. Uma análise semelhante é válida quando y = 1 e a ≈ 1. E assim, a contribuição para o custo será baixa, desde que a saída real esteja próxima da saída desejada.
 </p>
 <p style="text-align: justify;">
  <strong>
   Em suma, a entropia cruzada é positiva e tende a zero, à medida que o neurônio melhora a computação da saída desejada, y, para todas as entradas de treinamento, x
  </strong>
  .
 </p>
 <p style="text-align: justify;">
  Essas são as duas propriedades que esperamos intuitivamente para uma função de custo. De fato, ambas as propriedades também são satisfeitas pelo custo quadrático. Portanto, isso é uma boa notícia para a entropia cruzada. Mas a função custo de entropia cruzada tem o benefício de que, ao contrário do custo quadrático, evita o problema de desaceleração do aprendizado. Para ver isso, vamos calcular a derivada parcial do custo de entropia cruzada em relação aos pesos. Substituímos a = σ (z) na fórmula acima e aplicamos a regra da cadeia duas vezes, obtendo:
 </p>
 <p>
 </p>
 <p>
  <img alt="form1" class="aligncenter size-full wp-image-518" data-attachment-id="518" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?fit=406%2C134" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?fit=300%2C99" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?fit=406%2C134" data-orig-size="406,134" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/form1/" data-recalc-dims="1" height="134" sizes="(max-width: 406px) 100vw, 406px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?resize=406%2C134" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?w=406 406w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?resize=300%2C99 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?resize=200%2C66 200w" width="406"/>
 </p>
 <p>
 </p>
 <p>
  Colocando tudo em um denominador comum e simplificando, isso se torna:
 </p>
 <p>
 </p>
 <p>
  <img alt="form2" class="aligncenter size-full wp-image-519" data-attachment-id="519" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?fit=328%2C91" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?fit=300%2C83" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?fit=328%2C91" data-orig-size="328,91" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/form2-4/" data-recalc-dims="1" height="91" sizes="(max-width: 328px) 100vw, 328px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?resize=328%2C91" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?w=328 328w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?resize=300%2C83 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?resize=200%2C55 200w" width="328"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Usando a definição da função sigmóide, σ (z) = 1 / (1 + ez), e um pouco de álgebra, podemos mostrar que σ (z) = σ (z) (1 − σ (z)). Vemos que os termos σ′ (z) e σ (z) (1 − σ (z)) se cancelam na equação acima, e simplificando torna-se:
 </p>
 <p>
 </p>
 <p>
  <img alt="form3" class="aligncenter size-full wp-image-520" data-attachment-id="520" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?fit=217%2C79" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?fit=217%2C79" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?fit=217%2C79" data-orig-size="217,79" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/form3-3/" data-recalc-dims="1" height="79" sizes="(max-width: 217px) 100vw, 217px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?resize=217%2C79" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?w=217 217w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?resize=200%2C73 200w" width="217"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Esta é uma bela expressão. Ela nos diz que a taxa na qual o peso aprende é controlada por σ (z) −y, ou seja, pelo erro na saída. Quanto maior o erro, mais rápido o neurônio aprenderá. Isso é exatamente o que nós esperamos intuitivamente. Em particular, evita a lentidão de aprendizado causada pelo termo σ′ (z) na equação análoga para o custo quadrático. Quando usamos a entropia cruzada, o termo σ′ (z) é cancelado e não precisamos mais nos preocupar em ser pequeno. Este cancelamento é o milagre especial assegurado pela função de custo de entropia cruzada. Na verdade, não é realmente um milagre. Como veremos mais adiante, a entropia cruzada foi especialmente escolhida por ter apenas essa propriedade.
 </p>
 <p style="text-align: justify;">
  De maneira semelhante, podemos calcular a derivada parcial para o viés. Eu não vou passar por todos os detalhes novamente, mas você pode facilmente verificar que:
 </p>
 <p>
  <img alt="form4" class="aligncenter size-full wp-image-521" data-attachment-id="521" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?fit=197%2C75" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?fit=197%2C75" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?fit=197%2C75" data-orig-size="197,75" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/form4-2/" data-recalc-dims="1" height="75" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?resize=197%2C75" width="197"/>
 </p>
 <p>
 </p>
 <p>
  Novamente, isso evita a lentidão de aprendizado causada pelo termo σ′ (z) na equação análoga para o custo quadrático.
 </p>
 <p>
  Agora vamos retornar ao exemplo do início deste capítulo, e explorar o que acontece quando usamos a entropia cruzada em vez do custo quadrático. Para nos reorientarmos, começaremos com o caso em que o custo quadrático foi bom, com peso inicial de 0.6 e viés inicial de 0.9. Veja o que acontece quando substituímos o custo quadrático pela entropia cruzada:
 </p>
 <p>
 </p>
 <p>
  <img alt="train3" class="aligncenter size-full wp-image-522" data-attachment-id="522" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="train3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?fit=518%2C286" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?fit=300%2C166" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?fit=518%2C286" data-orig-size="518,286" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/train3/" data-recalc-dims="1" height="286" sizes="(max-width: 518px) 100vw, 518px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?resize=518%2C286" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?w=518 518w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?resize=300%2C166 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?resize=200%2C110 200w" width="518"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Como era de se esperar, o neurônio aprende perfeitamente bem neste caso, assim como fez anteriormente. E agora vamos olhar para o caso em que nosso neurônio ficou preso antes, com o peso e o viés ambos começando em 2.0:
 </p>
 <p>
 </p>
 <p>
  <img alt="train4" class="aligncenter size-full wp-image-523" data-attachment-id="523" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="train4" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?fit=533%2C284" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?fit=300%2C160" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?fit=533%2C284" data-orig-size="533,284" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/train4/" data-recalc-dims="1" height="284" sizes="(max-width: 533px) 100vw, 533px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?resize=533%2C284" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?w=533 533w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?resize=300%2C160 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?resize=200%2C107 200w" width="533"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <strong>
   Sucesso!
  </strong>
  Desta vez, o neurônio aprendeu rapidamente, exatamente como esperávamos. Se você observar atentamente, pode ver que a inclinação da curva de custo era muito mais íngreme inicialmente do que a região plana inicial na curva correspondente para o custo quadrático. É essa inclinação que a entropia cruzada nos ajuda a resolver, impedindo-nos de ficar presos exatamente quando esperamos que nosso neurônio aprenda mais depressa, ou seja, quando o neurônio começa errado.
 </p>
 <p style="text-align: justify;">
  Eu não disse qual taxa de aprendizado foi usada nos exemplos que acabei de ilustrar. Anteriormente, com o custo quadrático, usamos η = 0.15. Deveríamos ter usado a mesma taxa de aprendizado nos novos exemplos? De fato, com a mudança na função de custo, não é possível dizer precisamente o que significa usar a “mesma” taxa de aprendizado; é uma comparação de maçãs e laranjas. Para ambas as funções de custo, simplesmente experimentei encontrar uma taxa de aprendizado que possibilitasse ver o que está acontecendo. Se você ainda estiver curioso, aqui está o resumo: usei η = 0.005 nos exemplos que acabei de fornecer.
 </p>
 <p style="text-align: justify;">
  Você pode contestar que a mudança na taxa de aprendizado torna os gráficos acima sem sentido. Quem se importa com a rapidez com que o neurônio aprende, quando a nossa escolha de taxa de aprendizado foi arbitrária, para começar ?! Mas essa objeção não procede. O ponto dos gráficos não é sobre a velocidade absoluta de aprendizagem. É sobre como a velocidade do aprendizado muda. Em particular, quando usamos o custo quadrático, a aprendizagem é mais lenta quando o neurônio está inequivocamente errado do que é mais tarde durante o treinamento, à medida que o neurônio se aproxima da saída correta; enquanto o aprendizado de entropia cruzada é mais rápido quando o neurônio está inequivocamente errado. Essas declarações não dependem de como a taxa de aprendizado é definida.
 </p>
 <p style="text-align: justify;">
  Estamos estudando a entropia cruzada para um único neurônio. No entanto, é fácil generalizar a entropia cruzada para redes multicamadas de muitos neurônios. Em particular, suponha que y = y1, y2,… são os valores desejados nos neurônios de saída, ou seja, os neurônios na camada final, enquanto aL1, aL2,… são os valores reais de saída. Então nós definimos a entropia cruzada por:
 </p>
 <p>
 </p>
 <p>
  <img alt="cost" class="aligncenter size-full wp-image-524" data-attachment-id="524" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="cost" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?fit=399%2C68" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?fit=300%2C51" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?fit=399%2C68" data-orig-size="399,68" data-permalink="http://deeplearningbook.com.br/cross-entropy-cost-function/cost-2/" data-recalc-dims="1" height="68" sizes="(max-width: 399px) 100vw, 399px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?resize=399%2C68" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?w=399 399w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?resize=300%2C51 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?resize=200%2C34 200w" width="399"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Isso é o mesmo que nossa expressão anterior, exceto que agora nós temos o ∑j somando todos os neurônios de saída. Não vou explicitamente trabalhar com uma derivação, mas deve ser plausível que o uso da expressão anterior evite uma desaceleração na aprendizagem em muitas redes de neurônios.
 </p>
 <p style="text-align: justify;">
  A propósito, estou usando o termo “entropia cruzada” de uma maneira que confundiu alguns dos primeiros leitores, já que parece superficialmente entrar em conflito com outras fontes. Em particular, é comum definir a entropia cruzada para duas distribuições de probabilidade, pj e qj, como ∑jpjlnqj. Esta definição pode ser conectada a fórmula da entropia para um neurônio mostrada anteriormente, se tratarmos um único neurônio sigmóide como saída de uma distribuição de probabilidade que consiste na ativação a do neurônio ae seu complemento 1 − a.
 </p>
 <p style="text-align: justify;">
  No entanto, quando temos muitos neurônios sigmoides na camada final, o vetor aLj de ativações não costuma formar uma distribuição de probabilidade. Como resultado, uma definição como ∑jpjlnqj não faz sentido, já que não estamos trabalhando com distribuições de probabilidade. Em vez disso, você pode pensar na fórmula da entropia para múltiplos neurônios como um conjunto somado de entropias cruzadas por neurônio, com a ativação de cada neurônio sendo interpretada como parte de uma distribuição de probabilidade de dois elementos. Sim, eu sei que isso não é simples.
 </p>
 <p style="text-align: justify;">
  Nesse sentido, a fórmula da entropia para múltiplos neurônios é uma generalização da entropia cruzada para distribuições de probabilidade.
 </p>
 <p style="text-align: justify;">
  Quando devemos usar a entropia cruzada em vez do custo quadrático? De fato, a entropia cruzada é quase sempre a melhor escolha, desde que os neurônios de saída sejam neurônios sigmóides. Para entender por que, considere que, quando estamos configurando a rede, normalmente inicializamos os pesos e vieses usando algum tipo de aleatoriedade. Pode acontecer que essas escolhas iniciais resultem na rede sendo decisivamente errada para alguma entrada de treinamento – isto é, um neurônio de saída terá saturado próximo de 1, quando deveria ser 0, ou vice-versa. Se estamos usando o custo quadrático que irá desacelerar a aprendizagem, ele não vai parar de aprender completamente, já que os pesos continuarão aprendendo com outras entradas de treinamento, mas é obviamente indesejável.
 </p>
 <p style="text-align: justify;">
  Construir aplicações de IA é uma habilidade com demanda cada vez maior no mercado.
 </p>
 <p style="text-align: justify;">
  Pensando nisso, a Data Science Academy oferece um programa completo, onde esses e vários outros conceitos são estudados em detalhes e com várias aplicações práticas, usando TensorFlow. A
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
  é composta de 9 cursos, tudo 100% online e 100% em português, que aliam teoria e prática na medida certa, com aplicações reais de Inteligência Artificial. Confira o programa completo dos cursos:
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
  . Várias empresas em todo Brasil já estão treinando seus profissionais conosco! Venha fazer parte da revolução da IA.
 </p>
 <p style="text-align: justify;">
  Até o próximo capítulo!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener" target="_blank">
    Dot Product
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Derivada" rel="noopener" target="_blank">
    Derivada
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-506" href="http://deeplearningbook.com.br/cross-entropy-cost-function/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-506" href="http://deeplearningbook.com.br/cross-entropy-cost-function/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-506" href="http://deeplearningbook.com.br/cross-entropy-cost-function/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-506" href="http://deeplearningbook.com.br/cross-entropy-cost-function/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/cross-entropy-cost-function/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/cross-entropy-cost-function/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-506-5e0dd114eaaf9" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=506&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-506-5e0dd114eaaf9" id="like-post-wrapper-140353593-506-5e0dd114eaaf9">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-18">
 Capítulo 18 – Entropia Cruzada Para Quantificar a Diferença Entre Duas Distribuições de Probabilidade
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  A Cross-Entropy (ou entropia cruzada, se você preferir o termo em português) é fácil de implementar como parte de um programa que aprende usando gradiente descendente e backpropagation. Faremos isso nos próximos capítulos quando treinarmos uma rede completa, desenvolvendo uma versão melhorada do nosso programa anterior para classificar os dígitos manuscritos do dataset MNIST. O novo programa é chamado de network2.py e incorpora não apenas a entropia cruzada, mas também várias outras técnicas que estudaremos mais adiante. Agora, vejamos como usar a Entropia Cruzada Para Quantificar a Diferença Entre Duas Distribuições de Probabilidade.
 </p>
 <p style="text-align: justify;">
  Por enquanto, vamos ver como nosso novo programa classifica os dígitos MNIST. Usaremos uma rede com 30 neurônios ocultos, e usaremos um tamanho de mini-lote de 10. Definimos a taxa de aprendizado para η = 0,5 e nós treinamos por 30 épocas. A interface para o network2.py será um pouco diferente do network.py, mas ainda deve estar claro o que está acontecendo. Nos próximos capítulos apresentamos o código completo no repositório do livro no
  <span style="text-decoration: underline;">
   <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener" target="_blank">
    Github
   </a>
  </span>
  .
 </p>
 <p>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
   <img alt="corss-entropy" class="aligncenter wp-image-560 size-full" data-attachment-id="560" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="corss-entropy" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?fit=553%2C154" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?fit=300%2C84" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?fit=553%2C154" data-orig-size="553,154" data-permalink="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/corss-entropy/" data-recalc-dims="1" height="154" sizes="(max-width: 553px) 100vw, 553px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?resize=553%2C154" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?w=553 553w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?resize=300%2C84 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?resize=200%2C56 200w" title="Entropia Curzada" width="553"/>
  </a>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Perceba que o comando net.large_weight_initializer() é usado para inicializar os pesos e vieses da mesma maneira que já descrevemos anteriormente. Precisamos executar este comando porque mais adiante vamos alterar o peso padrão para inicialização em nossas redes. O resultado da execução da sequência de comandos acima é uma rede com 95,49% de precisão.
 </p>
 <p style="text-align: justify;">
  Vejamos também o caso em que usamos 100 neurônios ocultos, a entropia cruzada, e mantemos os parâmetros da mesma forma. Neste caso, obtemos uma precisão de 96,82%. Essa é uma melhoria substancial em relação aos resultados que obtivemos nos
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/" rel="noopener" target="_blank">
    capítulos anteriores
   </a>
  </span>
  , onde a precisão de classificação foi de 96,59%, usando o custo quadrático. Isso pode parecer uma pequena mudança, mas considere que a taxa de erro caiu de 3,41% para 3,18%. Ou seja, eliminamos cerca de um em quatorze dos erros originais. Isso é uma melhoria bastante útil.
 </p>
 <p style="text-align: justify;">
  É encorajador que o custo de entropia cruzada nos dê resultados semelhantes ou melhores do que o custo quadrático. No entanto, esses resultados não provam conclusivamente que a entropia cruzada é uma escolha melhor. A razão é que nós colocamos apenas um pequeno esforço na escolha de hyperparâmetros como taxa de aprendizado, tamanho de mini-lote e assim por diante. Para que a melhoria seja realmente convincente, precisaríamos fazer um trabalho completo de otimização desses hyperparâmetros. Ainda assim, os resultados são encorajadores e reforçam nosso argumento teórico anterior de que a entropia cruzada é uma escolha melhor do que o custo quadrático.
 </p>
 <p style="text-align: justify;">
  Isso, a propósito, é parte de um padrão geral que veremos nos próximos capítulos e, na verdade, em grande parte do restante do livro. Vamos desenvolver uma nova técnica, vamos experimentá-la e obteremos resultados “aprimorados”. É claro que é bom vermos essas melhorias, mas a interpretação de tais melhorias é sempre problemática. Elas só são verdadeiramente convincentes se virmos uma melhoria depois de nos esforçarmos para otimizar todos os outros hyperparâmetros. Isso é uma grande quantidade de trabalho, exigindo muito poder de computação, e normalmente não vamos fazer uma investigação tão exaustiva. Em vez disso, procederemos com base em testes informais como os realizados até aqui.
 </p>
 <p style="text-align: justify;">
  Até agora, discutimos a entropia cruzada de forma bem detalhada. Por que tanto esforço quando a entropia cruzada nos dá apenas uma pequena melhora em nossos resultados com o dataset MNIST? Mais adiante veremos outras técnicas, notadamente a regularização, que trazem melhorias muito maiores. Então, por que tanto foco na entropia cruzada? Parte da razão é que a entropia cruzada é uma função de custo amplamente utilizada e, portanto, vale a pena compreendê-la bem. Mas a razão mais importante é que a saturação dos neurônios é um problema importante nas redes neurais, um problema ao qual voltaremos repetidamente ao longo do livro. Por isso discutimos a entropia cruzada em extensão pois é um bom laboratório para começar a entender a saturação dos neurônios e como ela pode ser abordada.
 </p>
 <p>
 </p>
 <h3 style="text-align: justify;">
  O que significa a entropia cruzada? De onde isso vem?
 </h3>
 <p style="text-align: justify;">
  Nossa discussão sobre a entropia cruzada se concentrou na análise algébrica e na implementação prática. Isso é útil, mas deixa questões conceituais mais amplas não respondidas, como: o que significa a entropia cruzada? Existe alguma maneira intuitiva de pensar sobre a entropia cruzada? E quanto ao significado intuitivo da entropia cruzada? Como devemos pensar sobre isso?
 </p>
 <p style="text-align: justify;">
  Explicar isso em profundidade nos levaria mais longe do que queremos ir neste livro. No entanto, vale ressaltar que existe uma maneira padrão de interpretar a entropia cruzada que vem do campo da teoria da informação. Vejamos.
 </p>
 <p style="text-align: justify;">
  Já sabemos que para treinar uma rede neural, você precisa encontrar o erro entre as saídas calculadas e as saídas alvo desejadas. A medida de erro mais comum é chamada de erro quadrático médio (ou Mean Square Error). No entanto, existem alguns resultados de pesquisa que sugerem o uso de uma medida diferente, denominada erro de entropia cruzada, como método preferível em relação ao erro quadrático médio.
 </p>
 <p style="text-align: justify;">
  A medida de entropia cruzada tem sido utilizada como alternativa ao erro quadrático médio. A entropia cruzada pode ser usada como uma medida de erro quando as saídas de uma rede podem ser pensadas como representando hipóteses independentes (por exemplo, cada nó significa um conceito diferente) e as ativações dos nós podem ser entendidas como representando a probabilidade (ou a confiança) que cada uma das hipóteses pode ser verdadeira. Nesse caso, o vetor de saída representa uma distribuição de probabilidade, e nossa medida de erro – entropia cruzada – indica a distância entre o que a rede acredita que essa distribuição deve ser e o que realmente deveria ser. Existe também uma razão prática para usar a entropia cruzada. Pode ser mais útil em problemas nos quais os alvos são 0 e 1. A entropia cruzada tende a permitir que erros alterem pesos mesmo quando houver nós saturados (o que significa que suas derivadas são próximas de 0). Vamos compreender melhor isso:
 </p>
 <p style="text-align: justify;">
  <strong>
   A entropia cruzada é comumente usada para quantificar a diferença entre duas distribuições de probabilidade.
  </strong>
  Geralmente, a distribuição “verdadeira” (dos dados usados para treinamento) é expressa em termos de uma distribuição One-Hot.
 </p>
 <p style="text-align: justify;">
  Por exemplo, suponha que para uma instância de treinamento específica (uma única linha no seu dataset), a classe seja B (de 3 possíveis possibilidades: A, B e C). A distribuição única para esta instância de treinamento é, portanto:
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   Pr(Class A)  Pr(Class B)  Pr(Class C)
  </strong>
 </p>
 <p style="text-align: center;">
  <strong>
   0.0          1.0          0.0
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Você pode interpretar a distribuição acima da seguinte forma: a instância de treinamento tem 0% de probabilidade de ser classe A, 100% de probabilidade de ser classe B e 0% de probabilidade de ser a classe C.
 </p>
 <p style="text-align: justify;">
  Agora, suponha que seu algoritmo de aprendizado de máquina tenha previsto a seguinte distribuição de probabilidade:
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   Pr(Class A)  Pr(Class B)  Pr(Class C)
  </strong>
 </p>
 <p style="text-align: center;">
  <strong>
   0.228          0.619           0.153
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Quão próxima é a distribuição prevista da distribuição verdadeira?
  <span style="text-decoration: underline;">
   <strong>
    É isso que determina o erro de entropia cruzada
   </strong>
  </span>
  . A entropia cruzada é representada por esta fórmula:
 </p>
 <p>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
   <img alt="cross-entropy" class="aligncenter wp-image-561 size-full" data-attachment-id="561" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="cross-entropy" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?fit=625%2C116" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?fit=300%2C56" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?fit=625%2C116" data-orig-size="625,116" data-permalink="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/cross-entropy/" data-recalc-dims="1" height="116" sizes="(max-width: 625px) 100vw, 625px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?resize=625%2C116" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?w=625 625w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?resize=300%2C56 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?resize=200%2C37 200w" title="Entropia Cruzada" width="625"/>
  </a>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A soma é sobre as três classes A, B e C. Se você completar o cálculo, você achará que a perda é 0.479. Então, é assim que “longe” está a sua previsão da distribuição verdadeira.
 </p>
 <p style="text-align: justify;">
  A entropia cruzada é uma das muitas funções de perda possíveis. Essas funções de perda são tipicamente escritas como J(theta) e podem ser usadas dentro da descida do gradiente, que é uma estrutura iterativa para mover os parâmetros (ou coeficientes) para os valores ótimos.
  <strong>
   A entropia cruzada descreve a perda entre duas distribuições de probabilidade.
  </strong>
 </p>
 <p style="text-align: justify;">
  Ao usar uma rede neural para realizar classificação e predição, geralmente é melhor usar o erro de entropia cruzada do que o erro de classificação e um pouco melhor usar o erro de entropia cruzada do que o erro quadrático médio para avaliar a qualidade da rede neural. É importante deixar claro que estamos lidando apenas com uma rede neural que é usada para classificar os dados, como a previsão da concessão de crédito (sim ou não), ou ainda outras classificações como idade, sexo ou dígitos no dataset MNIST e assim por diante. Não estamos lidando com uma rede neural que faz regressão, onde o valor a ser previsto é numérico.
 </p>
 <p style="text-align: justify;">
  Até o próximo capítulo!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener" target="_blank">
    Dot Product
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://pt.wikipedia.org/wiki/Derivada" rel="noopener" target="_blank">
    Derivada
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-558" href="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-558" href="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-558" href="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-558" href="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-558-5e0dd116e5454" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=558&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-558-5e0dd116e5454" id="like-post-wrapper-140353593-558-5e0dd116e5454">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-19">
 Capítulo 19 – Overfitting e Regularização – Parte 1
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  O físico
  <span style="text-decoration: underline;">
   <a href="https://www.nobelprize.org/nobel_prizes/physics/laureates/1938/fermi-bio.html" rel="noopener" target="_blank">
    Enrico Fermi
   </a>
  </span>
  , ganhador do Prêmio Nobel de Física em 1938, foi questionado sobre sua opinião em relação a um modelo matemático que alguns colegas haviam proposto como a solução para um importante problema de física não resolvido. O modelo teve excelente performance no experimento, mas Fermi estava cético. Ele perguntou quantos parâmetros livres poderiam ser definidos no modelo. “Quatro” foi a resposta. Fermi respondeu: “Eu lembro que meu amigo Johnny Von Neumann costumava dizer: com quatro parâmetros eu posso encaixar um elefante, e com cinco eu posso fazê-lo mexer seu tronco” *. Com isso, ele quis dizer que não se deve ficar impressionado quando um modelo complexo se ajusta bem a um conjunto de dados. Com parâmetros suficientes, você pode ajustar qualquer conjunto de dados.
 </p>
 <p style="text-align: justify;">
  (* A citação vem de um artigo de Freeman Dyson, que é uma das pessoas que propôs o modelo. O artigo “Um elefante de quatro parâmetros” ou “A four-parameter elephant” pode ser encontrado
  <span style="text-decoration: underline;">
   <a href="https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/" rel="noopener" target="_blank">
    aqui
   </a>
  </span>
  .)
 </p>
 <p style="text-align: justify;">
  O ponto, claro, é que modelos com um grande número de parâmetros podem descrever uma variedade incrivelmente ampla de fenômenos. Mesmo que tal modelo esteja de acordo com os dados disponíveis, isso não o torna um bom modelo. Isso pode significar apenas que há liberdade suficiente no modelo que pode descrever quase qualquer conjunto de dados de tamanho determinado, sem capturar nenhuma percepção genuína do fenômeno em questão. Quando isso acontece, o modelo funcionará bem para os dados existentes, mas não conseguirá generalizar para novas situações. O verdadeiro teste de um modelo é sua capacidade de fazer previsões em situações que não foram expostas antes.
 </p>
 <p style="text-align: justify;">
  Fermi e von Neumann suspeitavam de modelos com quatro parâmetros. Nossa rede de 30 neurônios ocultos para classificação de dígitos MNIST possui quase 24.000 parâmetros! Nossa rede de 100 neurônios ocultos tem cerca de 80.000 parâmetros e redes neurais profundas de última geração às vezes contêm milhões ou até bilhões de parâmetros. Devemos confiar nos resultados?
 </p>
 <p style="text-align: justify;">
  Vamos aguçar este problema construindo uma situação em que a nossa rede faz um mau trabalho ao generalizar para novas situações. Usaremos
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/" rel="noopener" target="_blank">
    nossa rede de 30 neurônios ocultos
   </a>
  </span>
  , com seus 23.860 parâmetros. Mas não treinamos a rede usando todas as imagens de treinamento de 50.000 dígitos MNIST. Em vez disso, usaremos apenas as primeiras 1.000 imagens de treinamento. Usar esse conjunto restrito tornará o problema com a generalização muito mais evidente. Vamos treinar usando a função de custo de entropia cruzada, com uma taxa de aprendizado de η = 0,5 e um tamanho de mini-lote de 10. No entanto, vamos treinar por 400 épocas, pois não estamos usando muitos exemplos de treinamento. Vamos usar network2 para ver como a função de custo muda (o código você encontra no repositório do curso no
  <span style="text-decoration: underline;">
   <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener" target="_blank">
    Github)
   </a>
  </span>
  :
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="network" class="aligncenter size-full wp-image-580" data-attachment-id="580" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="network" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?fit=574%2C172" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?fit=300%2C90" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?fit=574%2C172" data-orig-size="574,172" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/network/" data-recalc-dims="1" height="172" sizes="(max-width: 574px) 100vw, 574px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?resize=574%2C172" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?w=574 574w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?resize=300%2C90 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?resize=200%2C60 200w" width="574"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Usando os resultados, podemos traçar a maneira como o custo muda à medida que a rede aprende (o script overfitting.py contém o código que gera esse resultado):
 </p>
 <p style="text-align: justify;">
  <img alt="overfitting1" class="aligncenter size-full wp-image-581" data-attachment-id="581" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="overfitting1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?fit=815%2C615" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?fit=300%2C226" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting1/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=815%2C615" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?w=815 815w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=300%2C226 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=768%2C580 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=200%2C151 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p style="text-align: justify;">
  Isso parece encorajador, mostrando uma redução suave no custo, exatamente como esperamos. Note que eu só mostrei as épocas de treinamento de 200 a 399. Isso nos dá uma boa visão dos últimos estágios do aprendizado, que, como veremos, é onde está a ação interessante.
 </p>
 <p style="text-align: justify;">
  Vamos agora ver como a precisão da classificação nos dados de teste muda com o tempo:
 </p>
 <p style="text-align: justify;">
  <img alt="overfitting2" class="aligncenter size-full wp-image-582" data-attachment-id="582" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="overfitting2" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?fit=815%2C615" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?fit=300%2C226" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting2/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=815%2C615" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?w=815 815w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=300%2C226 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=768%2C580 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=200%2C151 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p style="text-align: justify;">
  Mais uma vez, eu ampliei um pouco. Nas primeiras 200 épocas (não mostradas), a precisão sobe para pouco menos de 82%. O aprendizado então diminui gradualmente. Finalmente, por volta da época 280, a precisão da classificação praticamente pára de melhorar. As épocas posteriores meramente vêem pequenas flutuações estocásticas perto do valor da precisão na época 280. Compare isso com o gráfico anterior, em que o custo associado aos dados de treinamento continua a cair suavemente. Se olharmos apenas para esse custo, parece que nosso modelo ainda está ficando “melhor”. Mas os resultados da precisão do teste mostram que a melhoria é uma ilusão. Assim como o modelo que Fermi não gostava, o que nossa rede aprende após a época 280 não mais se generaliza para os dados de teste. E assim não é um aprendizado útil. Dizemos que a rede está super adaptando ou com sobreajuste ou ainda com overfitting, a partir da época 280.
 </p>
 <p style="text-align: justify;">
  Você pode se perguntar se o problema aqui é que eu estou olhando para o custo dos dados de treinamento, ao contrário da precisão da classificação nos dados de teste. Em outras palavras, talvez o problema seja que estamos fazendo uma comparação de maçãs e laranjas. O que aconteceria se comparássemos o custo dos dados de treinamento com o custo dos dados de teste, estaríamos comparando medidas semelhantes? Ou talvez pudéssemos comparar a precisão da classificação tanto nos dados de treinamento quanto nos dados de teste? Na verdade, essencialmente o mesmo fenômeno aparece, não importa como fazemos a comparação. Os detalhes mudam, no entanto. Por exemplo, vamos analisar o custo nos dados de teste:
 </p>
 <p style="text-align: justify;">
  <img alt="overfitting3" class="aligncenter size-full wp-image-583" data-attachment-id="583" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="overfitting3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting3/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p style="text-align: justify;">
  Podemos ver que o custo nos dados de teste melhora até a época 15, mas depois disso ele realmente começa a piorar, mesmo que o custo nos dados de treinamento continue melhorando. Este é outro sinal de que nosso modelo está super adaptando (overfitting). No entanto, coloca um enigma, que é se devemos considerar a época 15 ou a época 280 como o ponto em que o overfitting está dominando a aprendizagem? Do ponto de vista prático, o que realmente nos importa é melhorar a precisão da classificação nos dados de teste, enquanto o custo dos dados de teste não é mais do que um proxy para a precisão da classificação. E assim faz mais sentido considerar a época 280 como o ponto além do qual o overfitting está dominando o aprendizado em nossa rede neural.
 </p>
 <p style="text-align: justify;">
  Outro sinal de overfitting pode ser visto na precisão da classificação nos dados de treinamento:
 </p>
 <p style="text-align: justify;">
  <img alt="overfitting4" class="aligncenter size-full wp-image-584" data-attachment-id="584" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="overfitting4" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting4/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p style="text-align: justify;">
  A precisão aumenta até 100%. Ou seja, nossa rede classifica corretamente todas as 1.000 imagens de treinamento! Enquanto isso, nossa precisão de teste atinge apenas 82,27%. Portanto, nossa rede realmente está aprendendo sobre as peculiaridades do conjunto de treinamento, não apenas reconhecendo os dígitos em geral. É quase como se nossa rede estivesse apenas memorizando o conjunto de treinamento, sem entender os dígitos suficientemente bem para generalizar o conjunto de testes.
 </p>
 <p style="text-align: justify;">
  Overfitting é um grande problema em redes neurais. Isso é especialmente verdadeiro em redes modernas, que geralmente têm um grande número de pesos e vieses. Para treinar de forma eficaz, precisamos de uma maneira de detectar quando o overfitting está acontecendo. E precisamos aplicar técnicas para reduzir os efeitos do overfitting (por todo esse trabalho e conhecimento necessário,
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Cientistas de Dados
   </a>
  </span>
  devem ser muito bem remunerados).
 </p>
 <p style="text-align: justify;">
  A maneira óbvia de detectar overfitting é usar a abordagem acima, mantendo o controle da precisão nos dados de teste conforme nossos treinos da rede. Se percebermos que a precisão nos dados de teste não está mais melhorando, devemos parar de treinar. É claro que, estritamente falando, isso não é necessariamente um sinal de overfitting. Pode ser que a precisão nos dados de teste e os dados de treinamento parem de melhorar ao mesmo tempo. Ainda assim, a adoção dessa estratégia impedirá o overfitting.
 </p>
 <p style="text-align: justify;">
  Na verdade, usaremos uma variação dessa estratégia. Lembre-se de que, quando carregamos os dados MNIST, carregamos em três conjuntos de dados:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="code" class="aligncenter size-full wp-image-585" data-attachment-id="585" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="code" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?fit=459%2C69" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?fit=300%2C45" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?fit=459%2C69" data-orig-size="459,69" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/code/" data-recalc-dims="1" height="69" sizes="(max-width: 459px) 100vw, 459px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?resize=459%2C69" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?w=459 459w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?resize=300%2C45 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?resize=200%2C30 200w" width="459"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Até agora, usamos o
  <strong>
   training_data
  </strong>
  e
  <strong>
   test_data
  </strong>
  e ignoramos o
  <strong>
   validation_data
  </strong>
  . O validation_data contém 10.000 imagens de dígitos, imagens que são diferentes das 50.000 imagens no conjunto de treinamento MNIST e das 10.000 imagens no conjunto de teste MNIST. Em vez de usar o test_data para evitar overfitting, usaremos o validation_data. Para fazer isso, usaremos praticamente a mesma estratégia descrita acima para o test_data. Ou seja, calcularemos a precisão da classificação nos dados de validação no final de cada época. Quando a precisão da classificação nos dados de validação estiver saturada, paramos de treinar. Essa estratégia é chamada de parada antecipada (Early-Stopping). É claro que, na prática, não sabemos imediatamente quando a precisão está saturada. Em vez disso, continuamos treinando até termos certeza de que a precisão está saturada.
 </p>
 <p style="text-align: justify;">
  Por que usar o validation_data para evitar overfitting, em vez de test_data? Na verdade, isso faz parte de uma estratégia mais geral, que é usar o validation_data para avaliar diferentes opções de avaliação de hiperparâmetros, como o número de épocas para treinamento, a taxa de aprendizado, a melhor arquitetura de rede e assim por diante. Usamos essas avaliações para encontrar e definir bons valores para os hiperparâmetros. De fato, embora eu não tenha mencionado isso até agora, isto é, em parte, como chegamos às escolhas de hiperparâmetros feitas anteriormente neste livro. (Mais sobre isso depois.)
 </p>
 <p style="text-align: justify;">
  Claro, isso não responde de forma alguma à pergunta de por que estamos usando o validation_data para evitar overfitting, em vez de test_data. Para entender o porquê, considere que, ao definir os hiperparâmetros, é provável que tentemos muitas opções diferentes para os hiperparâmetros. Se definirmos os hiperparâmetros com base nas avaliações do test_data, será possível acabarmos super adequando nossos hiperparâmetros ao test_data. Ou seja, podemos acabar encontrando hiperparâmetros que se encaixam em peculiaridades particulares dos dados de teste, mas onde o desempenho da rede não se generalizará para outros conjuntos de dados. Protegemos contra isso descobrindo os hiperparâmetros usando o validation_data. Então, uma vez que tenhamos os hiperparâmetros que queremos, fazemos uma avaliação final da precisão usando o test_data. Isso nos dá confiança de que nossos resultados nos dados de teste são uma medida real de quão bem nossa rede neural se generaliza. Para colocar de outra forma, você pode pensar nos dados de validação como um tipo de dados de treinamento que nos ajuda a aprender bons parâmetros. Essa abordagem para encontrar bons hiperparâmetros é às vezes conhecida como o método “hold out”, uma vez que os dados de validação são mantidos separados ou “mantidos” a partir dos dados de treinamento.
 </p>
 <p style="text-align: justify;">
  Agora, na prática, mesmo depois de avaliar o desempenho nos dados de teste, podemos mudar nossa opinião e tentar outra abordagem – talvez uma arquitetura de rede diferente – que envolva a descoberta de um novo conjunto de hiperparâmetros. Se fizermos isso, não há perigo de acabarmos com o test_data também? Precisamos de uma regressão potencialmente infinita de conjuntos de dados, para que possamos ter certeza de que nossos resultados serão generalizados? Abordar essa preocupação é um problema profundo e difícil. Mas para nossos objetivos práticos, não vamos nos preocupar muito com essa questão. Em vez disso, vamos nos concentrar no método básico de retenção, com base nos dados training_data, validation_data e test_data, conforme descrito acima.
 </p>
 <p style="text-align: justify;">
  Vimos que o overfitting ocorre quando estamos usando apenas 1.000 imagens de treinamento. O que acontece quando usamos o conjunto completo de treinamento de 50.000 imagens? Manteremos todos os outros parâmetros iguais (30 neurônios ocultos, taxa de aprendizado de 0,5, tamanho de mini-lote de 10), mas treinamos usando todas as 50.000 imagens por 30 épocas. Aqui está um gráfico mostrando os resultados da precisão de classificação nos dados de treinamento e nos dados de teste. Observe que usei os dados de teste aqui, em vez dos dados de validação, para tornar os resultados mais diretamente comparáveis aos gráficos anteriores.
 </p>
 <p style="text-align: justify;">
  <img alt="overfitting_full" class="aligncenter size-full wp-image-586" data-attachment-id="586" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="overfitting_full" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?fit=815%2C615" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?fit=300%2C226" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting_full/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=815%2C615" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?w=815 815w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=300%2C226 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=768%2C580 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=200%2C151 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p style="text-align: justify;">
  Como você pode ver, a precisão nos dados de teste e treinamento permanece muito mais próxima do que quando estávamos usando 1.000 exemplos de treinamento. Em particular, a melhor precisão de classificação de 97,86% nos dados de treinamento é apenas 2,53% maior do que os 95,33% nos dados de teste. Isso é comparado com a diferença de 17,73% que tivemos anteriormente! Overfitting ainda está acontecendo, mas foi bastante reduzido. Nossa rede está se generalizando muito melhor dos dados de treinamento para os dados de teste. Em geral, uma das melhores maneiras de reduzir o overfitting é aumentar o volume (tamanho) dos dados de treinamento (fica claro agora porque
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=big-data-fundamentos" rel="noopener" target="_blank">
    Big Data
   </a>
  </span>
  está revolucionando a Ciência de Dados?). Com dados de treinamento suficientes, é difícil até mesmo uma rede muito grande sofrer de overfitting. Infelizmente, os dados de treinamento podem ser caros ou difíceis de adquirir, por isso nem sempre é uma opção prática.
 </p>
 <p style="text-align: justify;">
  Aumentar a quantidade de dados de treinamento é uma maneira de reduzir o overfitting. Mas existem outras maneiras de reduzir a extensão do overfitting? Uma abordagem possível é reduzir o tamanho da nossa rede. No entanto, redes grandes têm o potencial de serem mais poderosas do que redes pequenas e, portanto, essa é uma opção que só adotamos em último caso.
 </p>
 <p style="text-align: justify;">
  Felizmente, existem outras técnicas que podem reduzir o overfitting, mesmo quando temos uma rede fixa e dados de treinamento fixos. Estas técnicas são conhecidas como técnicas de regularização e serão assunto do próximo capítulo.
 </p>
 <p style="text-align: justify;">
  Até lá!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener" target="_blank">
    Dot Product
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-579" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-579" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-579" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-579" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-579-5e0dd119035da" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=579&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-579-5e0dd119035da" id="like-post-wrapper-140353593-579-5e0dd119035da">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-20">
 Capítulo 20 – Overfitting e Regularização – Parte 2
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Aumentar a quantidade de dados de treinamento é uma maneira de reduzir o overfitting. Mas existem outras maneiras de reduzir a extensão de ocorrência do overfitting? Uma abordagem possível é reduzir o tamanho da nossa rede. No entanto, redes grandes têm o potencial de serem mais poderosas do que redes pequenas e essa é uma opção que só adotaríamos com relutância.
 </p>
 <p style="text-align: justify;">
  Felizmente, existem outras técnicas que podem reduzir o overfitting, mesmo quando temos uma rede de tamanho fixo e dados de treinamento em quantidade limitada. Essas técnicas são conhecidos como
  <strong>
   técnicas de regularização
  </strong>
  . Neste capítulo descrevemos uma das técnicas de regularização mais comumente usadas, uma técnica às vezes conhecida como decaimento de peso (weight decay) ou Regularização L2. A ideia da Regularização L2 é adicionar um termo extra à função de custo, um termo chamado termo de regularização. Aqui está a entropia cruzada regularizada:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form" class="aligncenter size-full wp-image-596" data-attachment-id="596" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?fit=952%2C154" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?fit=300%2C49" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?fit=952%2C154" data-orig-size="952,154" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form-5/" data-recalc-dims="1" height="154" sizes="(max-width: 952px) 100vw, 952px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=952%2C154" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?w=952 952w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=300%2C49 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=768%2C124 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=200%2C32 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=690%2C112 690w" width="952"/>
 </p>
 <p style="text-align: center;">
  Equação 1
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  O primeiro termo é apenas a expressão usual para a entropia cruzada. Mas adicionamos um segundo termo, a soma dos quadrados de todos os pesos da rede. Isto é escalonado por um fator λ / 2n, onde λ &gt; 0 é conhecido como o parâmetro de regularização e
  <strong>
   n
  </strong>
  é, como de costume, o tamanho do nosso conjunto de treinamento. Vou discutir mais tarde como λ é escolhido. É importante notar também que o termo de regularização não inclui os vieses. Eu também voltarei a isso mais frente.
 </p>
 <p style="text-align: justify;">
  Claro, é possível regularizar outras funções de custo, como o custo quadrático. Isso pode ser feito de maneira semelhante:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form2" class="aligncenter size-full wp-image-597" data-attachment-id="597" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?fit=622%2C160" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?fit=300%2C77" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?fit=622%2C160" data-orig-size="622,160" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form2-5/" data-recalc-dims="1" height="160" sizes="(max-width: 622px) 100vw, 622px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?resize=622%2C160" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?w=622 622w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?resize=300%2C77 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?resize=200%2C51 200w" width="622"/>
 </p>
 <p style="text-align: center;">
  Equação 2
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Em ambos os casos, podemos escrever a função de custo regularizada como:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form3" class="aligncenter size-full wp-image-598" data-attachment-id="598" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?fit=366%2C138" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?fit=300%2C113" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?fit=366%2C138" data-orig-size="366,138" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form3-4/" data-recalc-dims="1" height="138" sizes="(max-width: 366px) 100vw, 366px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?resize=366%2C138" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?w=366 366w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?resize=300%2C113 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?resize=200%2C75 200w" width="366"/>
 </p>
 <p style="text-align: center;">
  Equação 3
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  onde C0 é a função de custo original e não regularizada.
 </p>
 <p style="text-align: justify;">
  Intuitivamente, o efeito da regularização é fazer com que a rede prefira aprender pequenos pesos, sendo todas as outras coisas iguais. Pesos grandes só serão permitidos se melhorarem consideravelmente a primeira parte da função de custo. Dito de outra forma, a regularização pode ser vista como uma forma de se comprometer entre encontrar pequenos pesos e minimizar a função de custo original. A importância relativa dos dois elementos do compromisso depende do valor de λ: quando λ é pequeno, preferimos minimizar a função de custo original, mas quando λ é grande, preferimos pesos pequenos.
 </p>
 <p style="text-align: justify;">
  Agora, não é de todo óbvio porque fazer este tipo de compromisso deve ajudar a reduzir o overfitting! Mas acontece que sim, reduz. Abordaremos a questão de porque isso ajuda na redução do overfitting no próximo capítulo, mas primeiro vamos trabalhar em um exemplo mostrando como a regularização reduz o overfitting.
 </p>
 <p style="text-align: justify;">
  Para construir um exemplo, primeiro precisamos descobrir como aplicar nosso algoritmo de aprendizado de
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/" rel="noopener" target="_blank">
    descida de gradiente estocástico
   </a>
  </span>
  em uma rede neural regularizada. Em particular, precisamos saber como calcular as derivadas parciais ∂C/∂w e ∂C/∂b para todos os pesos e vieses na rede. Tomando as derivadas parciais da Equação 3 acima, temos:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form4" class="aligncenter size-full wp-image-599" data-attachment-id="599" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?fit=352%2C244" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?fit=300%2C208" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?fit=352%2C244" data-orig-size="352,244" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form4-3/" data-recalc-dims="1" height="244" sizes="(max-width: 352px) 100vw, 352px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?resize=352%2C244" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?w=352 352w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?resize=300%2C208 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?resize=200%2C139 200w" width="352"/>
 </p>
 <p style="text-align: center;">
  Equação 4
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Os termos ∂C0/∂w e ∂C0/∂b podem ser calculados usando backpropagation, conforme descrito nos capítulos anteriores. E assim vemos que é fácil calcular o gradiente da função de custo regularizada, pois basta usar backpropagation, como de costume, e depois adicionar
  <strong>
   (λ/n).w
  </strong>
  à derivada parcial de todos os termos de peso. As derivadas parciais em relação aos vieses são inalteradas e, portanto, a regra de aprendizado de descida de gradiente para os vieses não muda da regra usual:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form5" class="aligncenter size-full wp-image-600" data-attachment-id="600" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?fit=276%2C166" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?fit=276%2C166" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?fit=276%2C166" data-orig-size="276,166" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form5-2/" data-recalc-dims="1" height="166" sizes="(max-width: 276px) 100vw, 276px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?resize=276%2C166" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?w=276 276w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?resize=200%2C120 200w" width="276"/>
 </p>
 <p style="text-align: center;">
  Equação 5
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  A regra de aprendizado para os pesos se torna:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form6" class="aligncenter size-full wp-image-601" data-attachment-id="601" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form6" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?fit=478%2C234" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?fit=300%2C147" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?fit=478%2C234" data-orig-size="478,234" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form6/" data-recalc-dims="1" height="234" sizes="(max-width: 478px) 100vw, 478px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?resize=478%2C234" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?w=478 478w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?resize=300%2C147 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?resize=200%2C98 200w" width="478"/>
 </p>
 <p style="text-align: center;">
  Equação 6
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Isto é exatamente o mesmo que a regra usual de aprendizado de descida de gradiente, exceto pelo fato de primeiro redimensionarmos o peso w por um fator 1 − (ηλ/n). Esse
  <em>
   reescalonamento
  </em>
  é, às vezes, chamado de redução de peso, uma vez que diminui os pesos. À primeira vista, parece que isso significa que os pesos estão sendo direcionados para zero, mas isso não é bem isso, uma vez que o outro termo pode levar os pesos a aumentar, se isso causar uma diminuição na função de custo não regularizada.
 </p>
 <p style="text-align: justify;">
  Ok, é assim que a descida de gradiente funciona. E quanto à descida de gradiente estocástica? Bem, assim como na descida de gradiente estocástica não-regularizada, podemos estimar ∂C0/∂w pela média de um mini-lote de m exemplos de treinamento. Assim, a regra de aprendizagem regularizada para a descida de gradiente estocástica torna-se:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form7" class="aligncenter size-full wp-image-602" data-attachment-id="602" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form7" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?fit=556%2C164" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?fit=300%2C88" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?fit=556%2C164" data-orig-size="556,164" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form7/" data-recalc-dims="1" height="164" sizes="(max-width: 556px) 100vw, 556px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?resize=556%2C164" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?w=556 556w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?resize=300%2C88 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?resize=200%2C59 200w" width="556"/>
 </p>
 <p style="text-align: center;">
  Equação 7
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  onde a soma é sobre exemplos de treinamento x no mini-lote, e Cx é o custo (não-regularizado) para cada exemplo de treinamento. Isto é exatamente o mesmo que a regra usual para descida de gradiente estocástico, exceto pelo fator de decaimento de peso de 1 − (ηλ/n). Finalmente, e por completo, deixe-me declarar a regra de aprendizagem regularizada para os vieses. Isto é, naturalmente, exatamente o mesmo que no caso não regularizado:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form8" class="aligncenter size-full wp-image-603" data-attachment-id="603" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form8" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?fit=370%2C134" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?fit=300%2C109" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?fit=370%2C134" data-orig-size="370,134" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form8/" data-recalc-dims="1" height="134" sizes="(max-width: 370px) 100vw, 370px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?resize=370%2C134" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?w=370 370w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?resize=300%2C109 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?resize=200%2C72 200w" width="370"/>
 </p>
 <p style="text-align: center;">
  Equação 8
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  onde a soma é sobre exemplos de treinamento x no mini-lote.
 </p>
 <p style="text-align: justify;">
  Vamos ver como a regularização altera o desempenho da nossa rede neural. Usaremos uma rede com 30 neurônios ocultos, um tamanho de mini-lote de 10, uma taxa de aprendizado de 0,5 e a função de custo de entropia cruzada. No entanto, desta vez vamos usar um parâmetro de regularização de λ = 0,1. Note que no código, usamos o nome da variável
  <strong>
   lmbda
  </strong>
  , porque
  <strong>
   lambda
  </strong>
  é uma palavra reservada em Python, com um significado não relacionado ao que estamos fazendo aqui (caso tenha dúvidas sobre as palavras reservadas em Python, acesse o curso gratuito
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener" target="_blank">
    Python Fundamentos Para Análise de Dados – Capítulo 2
   </a>
  </span>
  ).
 </p>
 <p style="text-align: justify;">
  Eu também usei o test_data novamente, não o validation_data. Estritamente falando, devemos usar o validation_data, por todas as razões que discutimos anteriormente. Mas decidi usar o test_data porque ele torna os resultados mais diretamente comparáveis com nossos resultados anteriores e não regularizados. Você pode facilmente alterar o código para usar o validation_data e você verá que ele terá resultados semelhantes.
 </p>
 <p style="text-align: justify;">
  <img alt="python" class="aligncenter wp-image-604 size-large" data-attachment-id="604" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="python" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?fit=1024%2C397" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?fit=300%2C116" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?fit=1228%2C476" data-orig-size="1228,476" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/python/" data-recalc-dims="1" height="397" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=1024%2C397" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=1024%2C397 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=300%2C116 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=768%2C298 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=200%2C78 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=690%2C267 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?w=1228 1228w" width="1024"/>
 </p>
 <p style="text-align: justify;">
  O custo com os dados de treinamento diminui durante todo o tempo, da mesma forma que no caso anterior, não regularizado no
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/" rel="noopener" target="_blank">
    capítulo anterior
   </a>
  </span>
  :
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="regularized1" class="aligncenter size-full wp-image-605" data-attachment-id="605" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="regularized1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?fit=815%2C615" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?fit=300%2C226" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/regularized1/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=815%2C615" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?w=815 815w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=300%2C226 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=768%2C580 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=200%2C151 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Mas desta vez a precisão no test_data continua a aumentar durante as 400 épocas:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="regularized2" class="aligncenter size-full wp-image-606" data-attachment-id="606" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="regularized2" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?fit=815%2C615" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?fit=300%2C226" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/regularized2/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=815%2C615" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?w=815 815w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=300%2C226 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=768%2C580 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=200%2C151 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Claramente, o uso da regularização suprimiu o overfitting. Além do mais, a precisão é consideravelmente maior, com uma precisão de classificação de pico de 87.1%, em comparação com o pico de 82.27% obtido no caso não regularizado. De fato, quase certamente poderíamos obter resultados consideravelmente melhores, continuando a treinar mais de 400 épocas. Parece que, empiricamente, a regularização está fazendo com que nossa rede generalize melhor e reduza consideravelmente os efeitos do overfitting.
 </p>
 <p style="text-align: justify;">
  O que acontece se sairmos do ambiente artificial de ter apenas 1.000 imagens de treinamento e retornar ao conjunto completo de treinamento de 50.000 imagens? É claro, já vimos que o overfitting é muito menos problemático com as 50.000 imagens. A regularização ajuda ainda mais? Vamos manter os hiperparâmetros iguais ao exemplo anterior – 30 épocas, taxa de aprendizado de 0,5, tamanho de mini-lote de 10. No entanto, precisamos modificar o parâmetro de regularização. A razão é porque o tamanho n do conjunto de treinamento mudou de n = 1.000 para n = 50.000, e isso muda o fator de decaimento de peso 1 − (ηλ/n). Se continuássemos a usar λ = 0,1, isso significaria muito menos perda de peso e, portanto, muito menos efeito de regularização. Nós compensamos mudando para λ = 5.0.
 </p>
 <p style="text-align: justify;">
  Ok, vamos treinar nossa rede, parando primeiro para reinicializar os pesos:
 </p>
 <p style="text-align: justify;">
  <img alt="python2" class="aligncenter size-large wp-image-607" data-attachment-id="607" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="python2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?fit=1024%2C185" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?fit=300%2C54" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?fit=1226%2C222" data-orig-size="1226,222" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/python2/" data-recalc-dims="1" height="185" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=1024%2C185" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=1024%2C185 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=300%2C54 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=768%2C139 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=200%2C36 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=690%2C125 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?w=1226 1226w" width="1024"/>
 </p>
 <p style="text-align: justify;">
  Obtemos os resultados:
 </p>
 <p style="text-align: justify;">
  <img alt="regularized_full" class="aligncenter size-full wp-image-608" data-attachment-id="608" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="regularized_full" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/regularized_full/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=690%2C521 690w" width="815"/>
 </p>
 <p style="text-align: justify;">
  Há muitas boas notícias aqui. Primeiro, nossa precisão de classificação nos dados de teste aumentou de 95.49%, quando não foi regularizada, para 96.49%. Isso é uma grande melhoria. Em segundo lugar, podemos ver que a diferença entre os resultados nos dados de treinamento e teste é muito menor do que antes, com um percentual abaixo de zero. Essa ainda é uma lacuna significativa, mas obviamente fizemos um progresso substancial para reduzir o overfitting.
 </p>
 <p style="text-align: justify;">
  Finalmente, vamos ver qual a precisão da classificação de teste que obtemos quando usamos 100 neurônios ocultos e um parâmetro de regularização de λ = 5.0. Eu não vou passar por uma análise detalhada de overfitting aqui, isso é puramente por diversão, só para ver a precisão que podemos obter quando usamos nossos novos truques: a função de custo de entropia cruzada e a Regularização L2.
 </p>
 <p style="text-align: justify;">
  <img alt="python3" class="aligncenter size-large wp-image-609" data-attachment-id="609" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="python3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?fit=1024%2C232" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?fit=300%2C68" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?fit=1236%2C280" data-orig-size="1236,280" data-permalink="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/python3/" data-recalc-dims="1" height="232" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=1024%2C232" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=1024%2C232 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=300%2C68 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=768%2C174 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=200%2C45 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=690%2C156 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?w=1236 1236w" width="1024"/>
 </p>
 <p style="text-align: justify;">
  O resultado final é uma precisão de classificação de 97.92% nos dados de validação. É um grande salto do caso dos 30 neurônios ocultos. Na verdade, ajustando um pouco mais, para executar por 60 épocas com η = 0.1 e λ = 5.0, quebramos a barreira de 98%, alcançando uma precisão de classificação de 98.04% nos dados de validação. Nada mal para o que acaba sendo 152 linhas de código!
 </p>
 <p style="text-align: justify;">
  Descrevi a regularização como uma forma de reduzir o overfitting e aumentar as precisões de classificação. Na verdade, esse não é o único benefício. Empiricamente, ao executar várias execuções de nossas redes com o dataset MNIST, mas com diferentes inicializações de peso (aleatórias), descobrimos que as execuções não-regularizadas ocasionalmente ficarão “presas”, aparentemente capturadas em mínimos locais da função de custo. O resultado é que diferentes execuções às vezes fornecem resultados bastante diferentes. Por outro lado, as execuções regularizadas forneceram resultados muito mais facilmente replicáveis.
 </p>
 <p style="text-align: justify;">
  Por que isso está acontecendo? Heuristicamente, se a função de custo for
  <em>
   desregularizada
  </em>
  , o comprimento do vetor de peso provavelmente crescerá, todas as outras coisas sendo iguais. Com o tempo, isso pode levar o vetor de peso a ser realmente muito grande. Isso pode fazer com que o vetor de peso fique preso apontando mais ou menos na mesma direção, já que as mudanças devido a descida do gradiente fazem apenas pequenas alterações na direção, quando o comprimento é longo. Acredito que esse fenômeno esteja dificultando o nosso algoritmo de aprendizado para explorar adequadamente o espaço de pesos e, consequentemente, mais difícil encontrar bons mínimos da função de custo.
 </p>
 <p>
  Ainda não acabamos sobre regularização. Mais sobre isso no próximo capítulo! Até lá!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener" target="_blank">
    Dot Product
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-594" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-594" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-594" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-594" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-594-5e0dd11b5ace2" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=594&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-594-5e0dd11b5ace2" id="like-post-wrapper-140353593-594-5e0dd11b5ace2">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-21">
 Capítulo 21 – Afinal, Por Que a Regularização Ajuda a Reduzir o Overfitting?
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  Vimos no
  <span style="text-decoration: underline;">
   <a href="http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/" rel="noopener" target="_blank">
    capítulo anterior
   </a>
  </span>
  que a regularização ajuda a reduzir o overfitting. Isso é encorajador, mas, infelizmente, não é óbvio porque a regularização ajuda a resolver o overfitting! Uma história padrão que as pessoas contam para explicar o que está acontecendo segue mais ou menos esse raciocínio: pesos menores são, em certo sentido, de menor complexidade e, portanto, fornecem uma explicação mais simples e mais poderosa para os dados e devem, normalmente, ser preferidos. É uma história bastante concisa e contém vários elementos que talvez pareçam dúbios ou mistificadores. Vamos descompactar essa explicação e examiná-la criticamente. Afinal, Por Que a Regularização Ajuda a Reduzir o Overfitting?
 </p>
 <p style="text-align: justify;">
  Para fazer isso, vamos supor que temos um conjunto de dados simples para o qual desejamos construir um modelo:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="graph" class="aligncenter size-large wp-image-612" data-attachment-id="612" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graph" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?fit=1024%2C672" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?fit=300%2C197" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?fit=1118%2C734" data-orig-size="1118,734" data-permalink="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/graph/" data-recalc-dims="1" height="672" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=1024%2C672" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=1024%2C672 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=300%2C197 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=768%2C504 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=200%2C131 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=690%2C453 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?w=1118 1118w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Implicitamente, estamos estudando algum fenômeno do mundo real aqui, com x e y representando dados desse fenômeno. Nosso objetivo é construir um modelo que nos permita prever y como uma função de x (isso é o que fazemos em
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
  ). Poderíamos tentar usar redes neurais para construir esse modelo, mas vou fazer algo ainda mais simples: vou tentar modelar y como um polinômio em x. Estou fazendo isso em vez de usar redes neurais porque usar polinômios tornará as coisas particularmente transparentes. Uma vez que tenhamos entendido o caso polinomial, vamos traduzir para redes neurais.
 </p>
 <p style="text-align: justify;">
  Há dez pontos no gráfico acima, o que significa que podemos encontrar um único polinômio de 9ª ordem y = a0x9 + a1x8 +… + a9 que se ajusta exatamente aos dados. Aqui está o gráfico desse polinômio:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="graph2" class="aligncenter size-large wp-image-613" data-attachment-id="613" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graph2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?fit=1024%2C698" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?fit=300%2C204" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?fit=1124%2C766" data-orig-size="1124,766" data-permalink="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/graph2/" data-recalc-dims="1" height="698" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=1024%2C698" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=1024%2C698 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=300%2C204 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=768%2C523 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=200%2C136 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=690%2C470 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?w=1124 1124w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Isso fornece um ajuste exato. Mas também podemos obter um bom ajuste usando o modelo linear y = 2x:
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="graph3" class="aligncenter size-large wp-image-614" data-attachment-id="614" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graph3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?fit=1024%2C648" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?fit=300%2C190" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?fit=1128%2C714" data-orig-size="1128,714" data-permalink="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/graph3/" data-recalc-dims="1" height="648" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=1024%2C648" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=1024%2C648 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=300%2C190 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=768%2C486 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=200%2C127 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=690%2C437 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?w=1128 1128w" width="1024"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Qual destes é o melhor modelo? E qual modelo é mais provável de generalizar bem a outros exemplos do mesmo fenômeno do mundo real?
 </p>
 <p style="text-align: justify;">
  Essas são questões difíceis. De fato, não podemos determinar com certeza a resposta para qualquer uma das perguntas acima, sem muito mais informações sobre o fenômeno do mundo real que estamos analisando (é onde entra a experiência do
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Cientista de Dados
   </a>
  </span>
  sobre áreas de negócio). Mas vamos considerar duas possibilidades: (1) o polinômio de 9ª ordem é, de fato, o modelo que realmente descreve o fenômeno do mundo real, e o modelo, portanto, generalizará perfeitamente; (2) o modelo correto é y = 2x, mas há um pequeno ruído adicional devido a, digamos, erros de medição, e é por isso que o modelo não é um ajuste exato.
 </p>
 <p style="text-align: justify;">
  Não é possível a priori dizer qual dessas duas possibilidades está correta (ou, na verdade, se alguma terceira possibilidade é válida). Logicamente, qualquer uma poderia ser verdade. E não é uma diferença trivial. É verdade que nos dados fornecidos há apenas uma pequena diferença entre os dois modelos. Mas suponha que queremos predizer o valor de y correspondendo a um grande valor de x, muito maior do que qualquer um mostrado nos gráficos acima. Se tentarmos fazer isso, haverá uma diferença dramática entre as previsões dos dois modelos, já que o modelo polinomial de 9ª ordem passa a ser dominado pelo termo x9, enquanto o modelo linear permanece, bem, linear.
 </p>
 <p style="text-align: justify;">
  Um ponto de vista é dizer que, na ciência, devemos seguir a explicação mais simples, a menos que sejamos obrigados a fazer o contrário. Quando encontramos um modelo simples que parece explicar muitos dados, somos tentados a gritar “Eureka!” Afinal, parece improvável que uma explicação simples ocorra apenas por coincidência. Em vez disso, suspeitamos que o modelo deve estar expressando alguma verdade subjacente sobre o fenômeno. No caso em questão, o modelo y = 2x + ruído parece muito mais simples que y = a0x9 + a1x8 +…. Seria surpreendente se essa simplicidade tivesse ocorrido por acaso, e então suspeitamos que y = 2x + ruído expressa alguma verdade subjacente. Nesse ponto de vista, o modelo de 9ª ordem está realmente aprendendo apenas os efeitos do ruído local. E assim, enquanto o modelo de 9ª ordem funciona perfeitamente para esses pontos de dados particulares, o modelo não conseguirá generalizar para outros pontos de dados, e o modelo linear terá maior poder preditivo.
 </p>
 <p style="text-align: justify;">
  Vamos ver o que esse ponto de vista significa para redes neurais. Suponha que nossa rede tenha, na maioria das vezes, pequenos pesos, como tenderá a acontecer em uma rede regularizada. O tamanho menor dos pesos significa que o comportamento da rede não mudará muito se alterarmos algumas entradas aleatórias aqui e ali. Isso dificulta que uma rede regularizada aprenda os efeitos do ruído local nos dados. Pense nisso como uma maneira de fazer com que as evidências não importem muito para a saída da rede. Em vez disso, uma rede regularizada aprende a responder a tipos de evidências que são vistas com frequência em todo o conjunto de treinamento. Por outro lado, uma rede com grandes pesos pode alterar bastante seu comportamento em resposta a pequenas alterações na entrada. Assim, uma rede não regularizada pode usar grandes pesos para aprender um modelo complexo que contém muitas informações sobre o ruído nos dados de treinamento. Em suma, as redes regularizadas são levadas a construir modelos relativamente simples baseados em padrões vistos frequentemente nos dados de treinamento e são resistentes às peculiaridades de aprendizagem do ruído nos dados de treinamento. A esperança é que isso forçará nossas redes a aprender de verdade sobre o fenômeno em questão e a generalizar melhor o que aprendem.
 </p>
 <p style="text-align: justify;">
  Com isso dito, e mantendo a necessidade de cautela em mente, é um fato empírico que as redes neurais regularizadas geralmente generalizam melhor do que as redes não regularizadas. A verdade é que ninguém ainda desenvolveu uma explicação teórica inteiramente convincente para explicar porque a regularização ajuda a generalizar as redes. De fato, os pesquisadores continuam a escrever artigos nos quais tentam abordagens diferentes à regularização, comparam-nas para ver qual funciona melhor e tentam entender por que diferentes abordagens funcionam melhor ou pior. Embora muitas vezes ajude, não temos uma compreensão sistemática inteiramente satisfatória do que está acontecendo, apenas heurísticas incompletas e regras gerais.
 </p>
 <p style="text-align: justify;">
  Há um conjunto mais profundo de questões aqui, questões que vão para o coração da ciência. É a questão de como generalizamos. A regularização pode nos dar uma varinha mágica computacional que ajuda nossas redes a generalizar melhor, mas não nos dá uma compreensão baseada em princípios de como a generalização funciona, nem de qual é a melhor abordagem.
 </p>
 <p style="text-align: justify;">
  Isso é particularmente irritante porque na vida cotidiana, nós humanos generalizamos bem. Mostradas apenas algumas imagens de um elefante, uma criança aprenderá rapidamente a reconhecer outros elefantes. É claro que eles podem ocasionalmente cometer erros, talvez confundindo um rinoceronte com um elefante, mas em geral esse processo funciona notavelmente com precisão. Então nós temos um sistema – o cérebro humano – com um grande número de parâmetros livres. E depois de ser mostrado apenas uma ou algumas imagens de treinamento, o sistema aprende a generalizar para outras imagens. Nossos cérebros estão, em certo sentido, se regularizando incrivelmente bem! Como fazemos isso? Neste ponto não sabemos. Espero que nos próximos anos desenvolvamos técnicas mais poderosas de regularização em redes neurais artificiais, técnicas que permitirão que as redes neurais generalizem bem, mesmo a partir de pequenos conjuntos de dados.
 </p>
 <p style="text-align: justify;">
  De fato, nossas redes já generalizam melhor do que se poderia esperar a priori. Uma rede com 100 neurônios ocultos tem quase 80.000 parâmetros. Temos apenas 50.000 imagens em nossos dados de treinamento. É como tentar encaixar um polinômio de grau 80.000 em 50.000 pontos de dados. Consequentemente, nossa rede deve se ajustar muito bem. E, no entanto, como vimos anteriormente, essa rede realmente faz um ótimo trabalho generalizando. Por que esse é o caso? Não é bem entendido. Foi conjecturado que “a dinâmica do aprendizado de gradiente descendente em redes multicamadas tem um efeito de ‘autorregulação'”. Isso é excepcionalmente bom, mas também é um tanto inquietante que não entendemos porque exatamente isso ocorre e por isso muitas vezes modelos de redes neurais profundas são chamados de “caixa preta”. Enquanto isso, adotaremos a abordagem pragmática e usaremos a regularização sempre que pudermos. Nossas redes neurais serão melhores assim.
 </p>
 <p style="text-align: justify;">
  Deixe-me concluir esta seção voltando a um detalhe que deixei inexplicado antes: o fato de que a regularização L2 não restringe os vieses. É claro que seria fácil modificar o procedimento de regularização para regularizar os vieses. Empiricamente, fazendo isso muitas vezes não muda muito os resultados, então, em certa medida, é apenas uma convenção se regularizar os vieses ou não. No entanto, vale a pena notar que ter um grande viés não torna um neurônio sensível às suas entradas da mesma maneira que ter pesos grandes. Portanto, não precisamos nos preocupar com grandes vieses que permitem que nossa rede aprenda o ruído em nossos dados de treinamento. Ao mesmo tempo, permitir grandes vieses dá às nossas redes mais flexibilidade no comportamento – em particular, grandes vieses facilitam a saturação dos neurônios, o que às vezes é desejável. Por essas razões, geralmente não incluímos termos de viés quando regularizamos a rede neural.
 </p>
 <p style="text-align: justify;">
  Você já percebeu que regularização é um assunto importante quando tratamos de redes neurais. Nos próximos capítulos estudaremos mais duas técnicas de regularização: Regularização L1 e Dropout! Não perca!
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  Referências:
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-611" href="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-611" href="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-611" href="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-611" href="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-611-5e0dd11de5af2" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=611&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-611-5e0dd11de5af2" id="like-post-wrapper-140353593-611-5e0dd11de5af2">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-22">
 Capítulo 22 – Regularização L1
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Existem muitas técnicas de regularização além da Regularização L2 que vimos no
  </span>
  <a href="http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    capítulo anterior
   </span>
  </a>
  .
  <span style="color: #000000;">
   De fato, tantas técnicas foram desenvolvidas que é difícil resumir todas elas. Neste e nos próximos dois capítulos, vamos descrever brevemente três outras abordagens para reduzir o overfitting: Regularização L1, Dropout e aumento artificial do tamanho do conjunto de treinamento. Não aprofundaremos tanto nessas técnicas como fizemos com a Regularização L2. Em vez disso, o objetivo é familiarizar você com as ideias principais e apreciar a diversidade de técnicas de regularização disponíveis.
  </span>
 </p>
 <p>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Regularização L1
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nesta abordagem, modificamos a função de custo não regularizada, adicionando a soma dos valores absolutos dos pesos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form1" class="aligncenter size-full wp-image-649" data-attachment-id="649" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?fit=350%2C166" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?fit=300%2C142" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?fit=350%2C166" data-orig-size="350,166" data-permalink="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form1-2/" data-recalc-dims="1" height="166" sizes="(max-width: 350px) 100vw, 350px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?resize=350%2C166" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?w=350 350w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?resize=300%2C142 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?resize=200%2C95 200w" width="350"/>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Equação 1
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Intuitivamente, isso é semelhante à Regularização L2, penalizando grandes pesos e tendendo a fazer com que a rede prefira pequenos pesos. Naturalmente, o termo de Regularização L1 não é o mesmo que o termo de Regularização L2 e, portanto, não devemos esperar obter exatamente o mesmo comportamento. Vamos tentar entender como o comportamento de uma rede treinada usando a Regularização L1 difere de uma rede treinada usando a Regularização L2.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para fazer isso, vejamos as derivadas parciais da função de custo. A partir da fórmula anterior obtemos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form2" class="aligncenter size-full wp-image-650" data-attachment-id="650" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?fit=404%2C154" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?fit=300%2C114" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?fit=404%2C154" data-orig-size="404,154" data-permalink="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form2-6/" data-recalc-dims="1" height="154" sizes="(max-width: 404px) 100vw, 404px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?resize=404%2C154" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?w=404 404w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?resize=300%2C114 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?resize=200%2C76 200w" width="404"/>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Equação 2
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   onde sgn(w) é o sinal de w, isto é, +1 se w é positivo e −1 se w é negativo. Usando essa expressão, podemos facilmente modificar a retropropagação (backpropagation) para fazer a descida de gradiente estocástica usando a Regularização L1. A regra de atualização resultante para uma rede regularizada L1 é:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form3" class="aligncenter size-full wp-image-651" data-attachment-id="651" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?fit=580%2C158" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?fit=300%2C82" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?fit=580%2C158" data-orig-size="580,158" data-permalink="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form3-5/" data-recalc-dims="1" height="158" sizes="(max-width: 580px) 100vw, 580px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?resize=580%2C158" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?w=580 580w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?resize=300%2C82 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?resize=200%2C54 200w" width="580"/>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Equação 3 – Regra de atualização L1
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   onde, como de costume, podemos estimar ∂C0/∂w usando uma média de mini-lote, se desejarmos. Compare isso com a regra de atualização para a Regularização L2:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="form4" class="aligncenter size-full wp-image-652" data-attachment-id="652" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?fit=554%2C148" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?fit=300%2C80" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?fit=554%2C148" data-orig-size="554,148" data-permalink="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form4-4/" data-recalc-dims="1" height="148" sizes="(max-width: 554px) 100vw, 554px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?resize=554%2C148" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?w=554 554w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?resize=300%2C80 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?resize=200%2C53 200w" width="554"/>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Equação 4 – Regra de atualização L2
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em ambas as expressões, o efeito da regularização é diminuir os pesos. Isso está de acordo com a nossa intuição de que ambos os tipos de regularização penalizam grandes pesos. Mas a maneira como os pesos diminuem é diferente. Na Regularização L1, os pesos diminuem em uma quantidade constante para 0. Na Regularização L2, os pesos diminuem em um valor proporcional a w. E assim, quando um peso específico tem uma grande magnitude, a Regularização L1 reduz o peso muito menos do que a Regularização L2. Em contraste, quando |w| é pequena, a Regularização L1 reduz o peso muito mais do que a Regularização L2. O resultado é que a Regularização L1 tende a concentrar o peso da rede em um número relativamente pequeno de conexões de alta importância, enquanto os outros pesos são direcionados para zero.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas há ainda um pequeno detalhe na discussão acima. A derivada parcial ∂C/∂w não é definida quando w = 0. A razão é que a função |w| tem um “canto” agudo em w = 0 e, portanto, não é diferenciável nesse ponto. Tudo bem, no entanto. O que faremos é aplicar a regra usual (não regularizada) para descida de gradiente estocástica quando w = 0. Isso ajuda a resolver a questão – intuitivamente, o efeito da regularização é diminuir os pesos e, obviamente, não pode reduzir um peso que já é 0. Para colocá-lo com mais precisão, usaremos as Equações (2) e (3) com a convenção que sgn(0) = 0. Isso dá uma regra legal e compacta para se fazer uma descida gradiente estocástica com Regularização L1.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Agora vamos para o Dropout, no próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Neural Networks and Deep Learning
   </span>
  </a>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-647" href="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-647" href="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-647" href="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-647" href="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-647-5e0dd11fed5ec" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=647&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-647-5e0dd11fed5ec" id="like-post-wrapper-140353593-647-5e0dd11fed5ec">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-23">
 Capítulo 23 – Como Funciona o Dropout?
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Dropout é uma técnica radicalmente diferente para regularização. Ao contrário da Regularização L1 e L2, o Dropout não depende da modificação da função de custo. Em vez disso, no Dropout, modificamos a própria rede. Deixe-me descrever a mecânica básica de
   <strong>
    Como Funciona o Dropout?
   </strong>
   antes de entender porque ele funciona e quais são os resultados. Suponha que estamos tentando treinar uma rede neural:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="rede" class="aligncenter size-full wp-image-655" data-attachment-id="655" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?fit=310%2C324" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?fit=287%2C300" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?fit=310%2C324" data-orig-size="310,324" data-permalink="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/rede-3/" data-recalc-dims="1" height="324" sizes="(max-width: 310px) 100vw, 310px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?resize=310%2C324" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?w=310 310w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?resize=287%2C300 287w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?resize=200%2C209 200w" width="310"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em particular, suponha que tenhamos uma entrada de treinamento x e a saída desejada correspondente y. Normalmente, nós treinamos pela propagação direta de x através da rede, e depois retrocedemos (retropropagação) para determinar a contribuição do erro para o gradiente. Com o Dropout, esse processo é modificado. Começamos por eliminar aleatoriamente (e temporariamente) alguns dos neurônios ocultos na rede, deixando os neurônios de entrada e saída intocados. Depois de fazer isso, terminaremos com uma rede da seguinte forma (observe as linhas tracejadas na figura abaixo). Note os neurônios que foram temporariamente eliminados:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <img alt="rede2" class="aligncenter size-full wp-image-656" data-attachment-id="656" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?fit=310%2C324" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?fit=287%2C300" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?fit=310%2C324" data-orig-size="310,324" data-permalink="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/rede2-3/" data-recalc-dims="1" height="324" sizes="(max-width: 310px) 100vw, 310px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?resize=310%2C324" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?w=310 310w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?resize=287%2C300 287w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?resize=200%2C209 200w" width="310"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nós encaminhamos para frente a entrada x através da rede modificada, e depois retropropagamos o resultado, também através da rede modificada. Depois de fazer isso em um mini-lote de exemplos, atualizamos os pesos e vieses apropriados. Em seguida, repetimos o processo, primeiro restaurando os neurônios removidos, depois escolhendo um novo subconjunto aleatório de neurônios ocultos para excluir, estimando o gradiente para um mini-lote diferente e atualizando os pesos e vieses na rede.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao repetir esse processo várias vezes, nossa rede aprenderá um conjunto de pesos e vieses. Naturalmente, esses pesos e vieses terão sido aprendidos sob condições em que parte dos neurônios ocultos foram descartados. Quando realmente executamos a rede completa, isso significa que mais neurônios ocultos estarão ativos. Para compensar isso, reduzimos pela metade os pesos que saem dos neurônios ocultos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esse procedimento de desistência pode parecer estranho e ad-hoc. Por que esperamos que ajude com a regularização? Para explicar o que está acontecendo, gostaria que você parasse brevemente de pensar sobre o Dropout e, em vez disso, imagine o treinamento de redes neurais no modo padrão (sem Dropout). Em particular, imagine que treinamos várias redes neurais diferentes, todas usando os mesmos dados de treinamento. É claro que as redes podem não começar idênticas e, como resultado, após o treinamento, elas podem, às vezes, dar resultados diferentes. Quando isso acontece, podemos usar algum tipo de esquema de média ou votação para decidir qual saída aceitar. Por exemplo, se nós treinamos cinco redes, e três delas estão classificando um dígito como um “3”, então provavelmente é um “3”. As outras duas redes provavelmente estão cometendo um erro. Este tipo de esquema de média é frequentemente encontrado como uma maneira poderosa (embora requeira mais capacidade computacional) de reduzir o overfitting. A razão é que as diferentes redes podem se sobrepor de diferentes maneiras e a média pode ajudar a eliminar esse tipo de overfitting.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O que isso tem a ver com o Dropout? Heuristicamente, quando abandonamos diferentes conjuntos de neurônios, é como se estivéssemos treinando redes neurais diferentes. E assim, o procedimento de eliminação é como calcular a média dos efeitos de um grande número de redes diferentes. As diferentes redes se adaptarão de diferentes maneiras, e assim, esperançosamente, o efeito líquido do Dropout será reduzir o overfitting.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma explicação heurística relacionada ao Dropout é dada em um dos primeiros artigos a usar a técnica: “Esta técnica reduz co-adaptações complexas de neurônios, já que um neurônio não pode confiar na presença de outros neurônios em particular. É, portanto, forçado a aprenda recursos mais robustos que são úteis em conjunto com muitos subconjuntos aleatórios diferentes dos outros neurônios”. Em outras palavras, se pensarmos em nossa rede como um modelo que está fazendo previsões, então podemos pensar no Dropout como uma forma de garantir que o modelo seja robusto para a perda de qualquer evidência individual. Nesse ponto, é um pouco semelhante à Regularização L1 e L2, que tendem a reduzir os pesos e, assim, tornar a rede mais robusta para perder qualquer conexão individual na rede.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Naturalmente, a verdadeira medida do Dropout é que ele foi muito bem sucedido em melhorar o desempenho das redes neurais.
  </span>
  <span style="color: #000000;">
   O
  </span>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener" target="_blank">
    artigo original
   </a>
  </span>
  ,
  <span style="color: #000000;">
   introduzindo a técnica, aplicou-a a muitas tarefas diferentes. Para nós, é de particular interesse que eles aplicaram o Dropout na classificação de dígitos MNIST, usando uma rede neural feedforward “vanilla” ao longo de linhas similares àquelas que estamos considerando. O documento observou que o melhor resultado que alguém alcançou até aquele ponto usando tal arquitetura foi a precisão de classificação de 98,4% no conjunto de testes. Eles melhoraram isso para 98,7% de precisão usando uma combinação de Dropout e uma forma modificada de Regularização L2. Da mesma forma, resultados impressionantes foram obtidos para muitas outras tarefas, incluindo problemas de reconhecimento de imagem e fala e processamento de linguagem natural. O Dropout tem sido especialmente útil no treinamento de redes grandes e profundas, nas quais o problema do overfitting é frequentemente agudo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O Dropout é estudado em detalhes e na prática no curso
  </span>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
    Deep Learning II
   </a>
  </span>
  <span style="color: #000000;">
   da Data Science Academy.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener" target="_blank">
    ImageNet Classification with Deep Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener" target="_blank">
    Improving neural networks by preventing co-adaptation of feature detectors
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-654" href="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-654" href="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-654" href="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-654" href="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-654-5e0dd1220aed3" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=654&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-654-5e0dd1220aed3" id="like-post-wrapper-140353593-654-5e0dd1220aed3">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-24">
 Capítulo 24 – Expandir Artificialmente os Dados de Treinamento
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vimos
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/" rel="noopener" target="_blank">
     anteriormente
    </a>
   </span>
   que a precisão da classificação com o dataset MNIST caiu para porcentagens em torno de 80%, quando usamos apenas 1.000 imagens de treinamento. Não é de surpreender que isso aconteça, uma vez que menos dados de treinamento significam que nossa rede será exposta a menos variações na forma como os seres humanos escrevem dígitos. Vamos tentar treinar nossa rede de 30 neurônios ocultos com uma variedade de diferentes tamanhos de conjuntos de dados de treinamento, para ver como o desempenho varia. Nós treinaremos usando um tamanho de mini-lote de 10, uma taxa de aprendizado η = 0,5, um parâmetro de regularização λ = 5.0 e a função de custo de entropia cruzada. Treinaremos por 30 épocas quando o conjunto completo de dados de treinamento for usado e aumentaremos o número de épocas proporcionalmente quando conjuntos de treinamento menores forem usados. Para garantir que o fator de decaimento do peso (weight decay factor) permaneça o mesmo nos conjuntos de treinamento, usaremos um parâmetro de regularização de λ = 5.0 quando o conjunto de dados de treinamento completo for usado, e reduziremos proporcionalmente quando conjuntos de treinamento menores forem usados. Observe esse gráfico:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="more_data" class="aligncenter size-full wp-image-670" data-attachment-id="670" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="more_data" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=690%2C521 690w" width="815"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como você pode ver analisando o gráfico acima, as precisões de classificação melhoram consideravelmente à medida que usamos mais dados de treinamento. Presumivelmente, essa melhoria continuaria se houvesse mais dados disponíveis. É claro que, olhando para o gráfico acima, parece que estamos chegando perto da saturação. Suponha, no entanto, que refizemos o gráfico com o tamanho do conjunto de treinamento plotado logaritmicamente:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="more_data_log" class="aligncenter size-full wp-image-671" data-attachment-id="671" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="more_data_log" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_log/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=690%2C521 690w" width="815"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Parece claro que o gráfico ainda está subindo em direção aos 100% de precisão. Isso sugere que, se usássemos muito mais dados de treinamento – digamos, milhões ou até bilhões de amostras de dígitos manuscritos, em vez de apenas 50.000, provavelmente teríamos um desempenho consideravelmente melhor, mesmo nessa rede muito pequena.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Obter mais dados de treinamento é uma ótima ideia. Infelizmente, pode ser caro e nem sempre é possível na prática. No entanto, há outra técnica que pode funcionar quase tão bem, que é expandir artificialmente os dados de treinamento. Suponha, por exemplo, que tomemos uma imagem de treinamento MNIST, o dígito 5:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="more_data_5" class="aligncenter wp-image-672 size-thumbnail" data-attachment-id="672" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="more_data_5" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?fit=815%2C615" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?fit=300%2C226" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_5/" data-recalc-dims="1" height="150" sizes="(max-width: 150px) 100vw, 150px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?resize=150%2C150" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?resize=150%2C150 150w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?resize=280%2C280 280w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?zoom=3&amp;resize=150%2C150 450w" width="150"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   e rotacionamos por um pequeno ângulo, digamos 15 graus:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="more_data_rotated_5" class="aligncenter size-thumbnail wp-image-673" data-attachment-id="673" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="more_data_rotated_5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_rotated_5/" data-recalc-dims="1" height="150" sizes="(max-width: 150px) 100vw, 150px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?resize=150%2C150" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?resize=150%2C150 150w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?resize=280%2C280 280w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?zoom=3&amp;resize=150%2C150 450w" width="150"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ainda é reconhecivelmente o mesmo dígito. E ainda no nível do pixel é bem diferente de qualquer imagem atualmente nos dados de treinamento MNIST. É possível que adicionar essa imagem aos dados de treinamento possa ajudar nossa rede a aprender mais sobre como classificar os dígitos. Além do mais, obviamente, não estamos limitados a adicionar apenas uma imagem. Podemos expandir nossos dados de treinamento fazendo muitas rotações pequenas de todas as imagens de treinamento MNIST e, em seguida, usando os dados de treinamento expandidos para melhorar o desempenho de nossa rede.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa técnica é muito poderosa e tem sido amplamente usada. Vejamos alguns dos resultados de um
   <span style="text-decoration: underline;">
    <a href="https://ieeexplore.ieee.org/document/1227801/" rel="noopener" target="_blank">
     artigo
    </a>
   </span>
   que aplicou diversas variações da técnica ao MNIST. Uma das arquiteturas de redes neurais que eles consideraram foi similar às que estamos usando, uma rede feedforward com 800 neurônios ocultos e usando a função de custo de entropia cruzada. Executando a rede com os dados de treinamento MNIST padrão, eles obtiveram uma precisão de classificação de 98,4% em seu conjunto de testes. Eles então expandiram os dados de treinamento, usando não apenas rotações, como descrevi acima, mas também traduzindo e distorcendo as imagens. Ao treinar no conjunto de dados expandido, aumentaram a precisão de sua rede para 98,9%. Eles também experimentaram o que chamaram de “distorções elásticas”, um tipo especial de distorção de imagem destinada a emular as oscilações aleatórias encontradas nos músculos da mão. Usando as distorções elásticas para expandir os dados, eles alcançaram uma precisão ainda maior, 99,3%. Efetivamente, eles estavam ampliando a experiência de sua rede, expondo-a ao tipo de variações encontradas na caligrafia real. Caso queira aprender sobre estas técnicas, elas são estudadas em detalhes em
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/curso-visao-computacional-e-reconhecimento-de-imagens" rel="noopener" target="_blank">
     Visão Computacional e Reconhecimento de Imagens
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Variações sobre essa técnica podem ser usadas para melhorar o desempenho em muitas tarefas de aprendizado, não apenas no reconhecimento de manuscrito. O princípio geral é expandir os dados de treinamento aplicando operações que reflitam a variação do mundo real. Não é difícil pensar em maneiras de fazer isso. Suponha, por exemplo, que você esteja construindo uma rede neural para fazer o reconhecimento de fala. Nós humanos podemos reconhecer a fala mesmo na presença de distorções como ruído de fundo e assim você pode expandir seus dados adicionando ruído de fundo. Também podemos reconhecer a fala se ela estiver acelerada ou desacelerada. Então, essa é outra maneira de expandir os dados de treinamento. Essas técnicas nem sempre são usadas – por exemplo, em vez de expandir os dados de treinamento adicionando ruído, pode ser mais eficiente limpar a entrada para a rede aplicando primeiro um filtro de redução de ruído. Ainda assim, vale a pena manter a ideia de expandir os dados de treinamento e buscar oportunidades para aplicar a técnica.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora você compreende melhor o poder do
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=big-data-fundamentos" rel="noopener" target="_blank">
     Big Data
    </a>
   </span>
   , pois com mais dados, em maior variedade e gerados em alta velocidade, conseguimos chegar a resultados nunca antes vistos em Inteligência Artificial. Vamos ver novamente como a precisão da nossa rede neural varia com o tamanho do conjunto de treinamento:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="more_data_log2" class="aligncenter wp-image-674 size-full" data-attachment-id="674" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="more_data_log2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_log2/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=690%2C521 690w" width="815"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Suponha que, em vez de usar uma rede neural, usemos alguma outra técnica de aprendizado de máquina para classificar os dígitos. Por exemplo, vamos tentar usar as máquinas de vetores de suporte (SVMs). Não se preocupe se você não estiver familiarizado com SVMs, não precisamos entender seus detalhes (caso queira aprender sobre SVMs, elas são estudadas em detalhes em
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/curso-machine-learning" rel="noopener" target="_blank">
     Machine Learning
    </a>
   </span>
   ). Vamos usar o SVM fornecido pela biblioteca scikit-learn. Veja como o desempenho do SVM varia em função do tamanho do conjunto de treinamento. Eu tracei os resultados da rede neural também, para facilitar a comparação:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="more_data_comparison" class="aligncenter wp-image-675 size-full" data-attachment-id="675" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="more_data_comparison" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_comparison/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=690%2C521 690w" width="815"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Provavelmente, a primeira coisa que chama a atenção sobre esse gráfico é que nossa rede neural supera o SVM para cada tamanho de conjunto de treinamento. Isso é bom, embora tenhamos usado as configurações prontas do SVM do scikit-learn, enquanto fizemos um bom trabalho customizando nossa rede neural. Um fato sutil, porém interessante, sobre o gráfico é que, se treinarmos o SVM usando 50.000 imagens, ele terá melhor desempenho (94,48% de precisão) do que a nossa rede neural quando treinado usando 5.000 imagens (precisão de 93,24%).
   <strong>
    Em outras palavras, mais dados de treinamento podem, às vezes, compensar diferenças no algoritmo de aprendizado de máquina usado
   </strong>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Algo ainda mais interessante pode ocorrer. Suponha que estamos tentando resolver um problema usando dois algoritmos de aprendizado de máquina, algoritmo A e algoritmo B. Às vezes acontece que o algoritmo A superará o algoritmo B com um conjunto de dados de treinamento, enquanto o algoritmo B superará o algoritmo A com um conjunto diferente de dados de treinamento. Não vemos isso acima – seria necessário que os dois gráficos se cruzassem – mas a resposta correta à pergunta “O algoritmo A é melhor que o algoritmo B?” seria: “Qual o tamanho do conjunto de dados de treinamento que você está usando?”
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Tudo isso é uma precaução a ter em mente, tanto ao fazer o desenvolvimento quanto ao ler artigos de pesquisa. Muitos artigos concentram-se em encontrar novos truques para obter melhor desempenho em conjuntos de dados de referência padrão. “Nossa técnica XPTO nos deu uma melhoria de X por cento no benchmark padrão Y” é uma forma canônica de alegação de pesquisa. Tais alegações são, com frequência, genuinamente interessantes, mas devem ser entendidas como aplicáveis ​​apenas no contexto do conjunto de dados de treinamento específico usado. Imagine uma história alternativa na qual as pessoas que originalmente criaram o conjunto de dados de referência tinham uma concessão de pesquisa maior. Eles podem ter usado o dinheiro extra para coletar mais dados de treinamento. É perfeitamente possível que o “aprimoramento” devido à técnica de “XPTO” desapareça em um conjunto maior de dados. Em outras palavras, a suposta melhoria pode ser apenas um acidente da história. A mensagem a ser retirada, especialmente em aplicações práticas, é que o que queremos é melhores algoritmos e melhores dados de treinamento. Não há problema em procurar algoritmos melhores, mas certifique-se de não estar se concentrando apenas em melhores algoritmos, excluindo a busca por mais ou melhores dados de treinamento.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com isso concluímos nosso mergulho no overfitting e na regularização. Claro, voltaremos novamente ao assunto. Como já mencionamos várias vezes, o overfitting é um grande problema nas redes neurais, especialmente à medida que os computadores se tornam mais poderosos e temos a capacidade de treinar redes maiores. Como resultado, há uma necessidade premente de desenvolver técnicas poderosas de regularização para reduzir o overfitting, e esta é uma área extremamente ativa de pesquisa.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   No próximo capítulo vamos tratar de um outro importante assunto: a inicialização de pesos. Até lá.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ieeexplore.ieee.org/document/1227801/" rel="noopener" target="_blank">
    Best practices for convolutional neural networks applied to visual document analysis
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://dl.acm.org/citation.cfm?doid=1073012.1073017" rel="noopener" target="_blank">
    Scaling to very very large corpora for natural language disambiguation
   </a>
  </span>
 </p>
 <p>
  <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
   Gradient-Based Learning Applied to Document Recognition
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener" target="_blank">
    Understanding Activation Functions in Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1" rel="noopener" target="_blank">
    Redes Neurais, princípios e práticas
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener" target="_blank">
    ImageNet Classification with Deep Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener" target="_blank">
    Improving neural networks by preventing co-adaptation of feature detectors
   </a>
  </span>
 </p>
 <p>
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-669" href="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-669" href="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-669" href="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-669" href="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-669-5e0dd1243870c" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=669&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-669-5e0dd1243870c" id="like-post-wrapper-140353593-669-5e0dd1243870c">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-25">
 Capítulo 25 – Inicialização de Pesos em Redes Neurais Artificiais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Quando criamos nossas redes neurais, temos que fazer escolhas para os valores iniciais de pesos e vieses (bias). Até agora, nós os escolhemos de acordo com uma prescrição que discutimos nos
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/" rel="noopener" target="_blank">
     capítulos anteriores
    </a>
   </span>
   . Só para lembrar, a prescrição era escolher tanto os pesos quanto os vieses usando variáveis aleatórias Gaussianas independentes, normalizadas para ter a média 0 e desvio padrão 1 (esse é um conceito fundamental em Estatística e caso queira adquirir conhecimento em Estatística, confira nossa mais nova Formação:
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
   ).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Embora esta abordagem tenha funcionado bem, foi bastante
   <em>
    ad-hoc
   </em>
   , e vale a pena revisitar para ver se podemos encontrar uma maneira melhor de definir nossos pesos e vieses iniciais, e talvez ajudar nossas redes neurais a aprender mais rápido. É o que iremos estudar neste capítulo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para começar, vamos compreender porque podemos fazer um pouco melhor do que inicializar pesos e vieses com valores Gaussianos normalizados. Para ver porque, suponha que estamos trabalhando com uma rede com um grande número – digamos 1.000 – de neurônios de entrada. E vamos supor que usamos valores Gaussianos normalizados para inicializar os pesos conectados à primeira camada oculta. Por enquanto, vou me concentrar especificamente nos pesos que conectam os neurônios de entrada ao primeiro neurônio na camada oculta e ignorar o restante da rede:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede" class="aligncenter wp-image-692 size-full" data-attachment-id="692" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?fit=268%2C294" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?fit=268%2C294" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?fit=268%2C294" data-orig-size="268,294" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede-5/" data-recalc-dims="1" height="294" sizes="(max-width: 268px) 100vw, 268px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?resize=268%2C294" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?w=268 268w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?resize=200%2C219 200w" width="268"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos supor, por simplicidade, que estamos tentando treinar usando uma entrada de treinamento x na qual metade dos neurônios de entrada estão ativados, isto é, configurados para 1, e metade dos neurônios de entrada estão desligados, ou seja, ajustados para 0. O argumento a seguir aplica-se de forma mais geral, mas você obterá a essência deste caso especial. Vamos considerar a soma ponderada
   <strong>
    z = ∑jwjxj + b
   </strong>
   de entradas para nosso neurônio oculto. Ocorre que 500 termos nesta soma desaparecem, porque a entrada correspondente xj é zero e, assim, z é uma soma sobre um total de 501 variáveis aleatórias Gaussianas normalizadas, representando os 500 termos de peso e o termo extra de viés (bias). Logo, z é ele próprio uma distribuição Gaussiana com média zero e desvio padrão ≈ 22.4 (raiz quadrada de 501). Ou seja, z tem uma distribuição Gaussiana muito ampla, sem um pico agudo, conforme a figura abaixo:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede2" class="aligncenter wp-image-693 size-full" data-attachment-id="693" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?fit=593%2C144" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?fit=300%2C73" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?fit=593%2C144" data-orig-size="593,144" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede2-4/" data-recalc-dims="1" height="144" sizes="(max-width: 593px) 100vw, 593px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?resize=593%2C144" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?w=593 593w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?resize=300%2C73 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?resize=200%2C49 200w" width="593"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em particular, podemos ver neste gráfico que é bem provável que | z | será bastante grande, isto é, z &gt; 1 ou z &lt; -1. Se for esse o caso, a saída σ(z) do neurônio oculto estará muito próxima de 1 ou 0. Isso significa que nosso neurônio oculto terá saturado. E quando isso acontece, como sabemos, fazer pequenas mudanças nos pesos fará apenas mudanças absolutamente minúsculas na ativação de nosso neurônio oculto. Essa mudança minúscula na ativação do neurônio oculto, por sua vez, dificilmente afetará o resto dos neurônios na rede, e veremos uma mudança minúscula correspondente na função de custo. Como resultado, esses pesos só aprenderão muito lentamente quando usarmos o algoritmo de descida do gradiente. É semelhante ao problema que discutimos anteriormente em outros capítulos, no qual os neurônios de saída que saturaram o valor errado fizeram com que o aprendizado diminuísse. Abordamos esse problema anterior com uma escolha inteligente de função de custo. Infelizmente, enquanto isso ajudou com os neurônios de
   <strong>
    saída
   </strong>
   saturados, ele não faz nada pelo problema dos neurônios
   <strong>
    ocultos
   </strong>
   saturados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Temos falado sobre a entrada de pesos para a primeira camada oculta. Naturalmente, argumentos semelhantes aplicam-se também a camadas ocultas posteriores: se os pesos em camadas ocultas posteriores forem inicializados usando Gaussianos normalizados, então as ativações estarão frequentemente muito próximas de 0 ou 1, e o aprendizado prosseguirá muito lentamente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Existe alguma maneira de escolhermos melhores inicializações para os pesos e vieses, para que não tenhamos esse tipo de saturação e, assim, evitar uma desaceleração na aprendizagem? Suponha que tenhamos um neurônio com pesos de entrada
   <strong>
    nin
   </strong>
   . Então, inicializaremos esses pesos como variáveis ​​aleatórias gaussianas com média 0 e desvio padrão:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form" class="aligncenter wp-image-704 size-full" data-recalc-dims="1" height="31" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?resize=62%2C31" width="62"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isto é, vamos “esmagar os gaussianos”, tornando menos provável que nosso neurônio seja saturado. Continuaremos a escolher o viés como um Gaussiano com média 0 e desvio padrão 1, por motivos pelos quais voltaremos daqui a pouco. Com essas escolhas, a soma ponderada
   <strong>
    z = ∑jwjxj + b
   </strong>
   será novamente uma variável aleatória Gaussiana com média 0, mas será muito mais aguda que antes. Suponha, como fizemos anteriormente, que 500 das entradas são zero e 500 são 1. Então é fácil mostrar (veja o gráfico abaixo) que z tem uma distribuição Gaussiana com média 0 e desvio padrão igual a 1,22…(raiz quadrada de 3/2). Isso é muito mais agudo do que antes, tanto que até o gráfico abaixo subestima a situação, já que precisamos redimensionar o eixo vertical, quando comparado ao gráfico anterior:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede3" class="aligncenter size-full wp-image-694" data-attachment-id="694" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?fit=592%2C381" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?fit=300%2C193" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?fit=592%2C381" data-orig-size="592,381" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede3-2/" data-recalc-dims="1" height="381" sizes="(max-width: 592px) 100vw, 592px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?resize=592%2C381" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?w=592 592w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?resize=300%2C193 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?resize=200%2C129 200w" width="592"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   É muito menos provável que tal neurônio sature e, correspondentemente, é muito menos provável que tenha problemas com a lentidão do aprendizado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Eu afirmei acima que nós continuaremos a inicializar os vieses como antes, como variáveis ​​aleatórias Gaussianas com uma média de 0 e um desvio padrão de 1. Isto não tem problema, pois é pouco provável que nossos neurônios vão saturar. Na verdade, não importa muito como inicializamos os vieses, desde que evitemos o problema com a saturação dos neurônios. Algumas pessoas vão tão longe a ponto de inicializar todos os vieses com 0, e dependem da descida de gradiente para aprender vieses apropriados. Mas como é improvável que faça muita diferença, continuaremos com o mesmo procedimento de inicialização de antes.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos comparar os resultados para as nossas abordagens antiga e nova para inicialização de peso, usando a tarefa de classificação de dígitos MNIST. Como antes, usaremos 30 neurônios ocultos, um tamanho de mini-lote de 10, um parâmetro de regularização λ = 5.0 e a função de custo de entropia cruzada. Diminuiremos ligeiramente a taxa de aprendizado de η = 0,5 para 0,1, pois isso torna os resultados um pouco mais visíveis nos gráficos. Podemos treinar usando o antigo método de inicialização de peso (o código pode ser encontrado no repositório deste livro no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr" rel="noopener" target="_blank">
     Github
    </a>
   </span>
   ):
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede4" class="aligncenter size-full wp-image-695" data-attachment-id="695" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?fit=607%2C185" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?fit=300%2C91" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?fit=607%2C185" data-orig-size="607,185" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede4/" data-recalc-dims="1" height="185" sizes="(max-width: 607px) 100vw, 607px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?resize=607%2C185" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?w=607 607w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?resize=300%2C91 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?resize=200%2C61 200w" width="607"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Também podemos treinar usando a nova abordagem para inicializar o peso. Na verdade, isso é ainda mais fácil, já que a maneira padrão de inicializar os pesos da rede2 é usar essa nova abordagem. Isso significa que podemos omitir a chamada net.large_weight_initializer () acima:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede5" class="aligncenter size-full wp-image-696" data-attachment-id="696" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?fit=609%2C86" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?fit=300%2C42" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?fit=609%2C86" data-orig-size="609,86" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede5/" data-recalc-dims="1" height="86" sizes="(max-width: 609px) 100vw, 609px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?resize=609%2C86" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?w=609 609w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?resize=300%2C42 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?resize=200%2C28 200w" width="609"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Plotando os resultados, obtemos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede6" class="aligncenter size-full wp-image-697" data-attachment-id="697" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede6" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?fit=454%2C379" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?fit=300%2C250" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?fit=454%2C379" data-orig-size="454,379" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede6/" data-recalc-dims="1" height="379" sizes="(max-width: 454px) 100vw, 454px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?resize=454%2C379" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?w=454 454w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?resize=300%2C250 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?resize=200%2C167 200w" width="454"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em ambos os casos, acabamos com uma precisão de classificação um pouco acima de 96%. A precisão final da classificação é quase exatamente a mesma nos dois casos, mas a nova técnica de inicialização é muito, muito mais rápida. No final da primeira época de treinamento, a antiga abordagem de inicialização de peso tem uma precisão de classificação abaixo de 87%, enquanto a nova abordagem já chega a quase 93%. O que parece estar acontecendo é que nossa nova abordagem para a inicialização do peso nos leva a um processo muito melhor, o que nos permite obter bons resultados muito mais rapidamente. O mesmo fenômeno também é visto se traçarmos resultados com 100 neurônios ocultos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede7" class="aligncenter size-full wp-image-698" data-attachment-id="698" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede7" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?fit=461%2C388" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?fit=300%2C252" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?fit=461%2C388" data-orig-size="461,388" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede7/" data-recalc-dims="1" height="388" sizes="(max-width: 461px) 100vw, 461px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?resize=461%2C388" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?w=461 461w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?resize=300%2C252 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?resize=200%2C168 200w" width="461"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste caso, as duas curvas não se encontram. No entanto, nossas experiências sugerem que, com apenas mais algumas épocas de treinamento (não mostradas), as precisões se tornam quase exatamente as mesmas. Portanto, com base nesses experimentos, parece que a inicialização do peso aprimorado apenas acelera o aprendizado, não altera o desempenho final de nossas redes. No entanto, veremos mais a frente alguns exemplos de redes neurais em que o comportamento de longo prazo é significativamente melhor com a inicialização de peso usando:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form" class="aligncenter size-full wp-image-704" data-attachment-id="704" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?fit=62%2C31" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?fit=62%2C31" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?fit=62%2C31" data-orig-size="62,31" data-permalink="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/form-6/" data-recalc-dims="1" height="31" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?resize=62%2C31" width="62"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim, não é apenas a velocidade de aprendizado que é melhorada, mas também o desempenho final.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A abordagem acima para a inicialização do peso ajuda a melhorar a maneira como nossas redes neurais aprendem. Outras técnicas para inicialização de peso também foram propostas, muitas baseadas nessa ideia básica. Não vamos rever as outras abordagens aqui, já que a descrita anteriormente funciona bem o suficiente para nossos propósitos. Se você estiver interessado em pesquisar mais, recomendamos a leitura das páginas 14 e 15 de um artigo de 2012 de Yoshua Bengio (um dos padrinhos do Deep Learning), bem como as referências nele contidas:
   <span style="text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
   . Nos cursos
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-i" rel="noopener" target="_blank">
     Deep Learning I
    </a>
   </span>
   e
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
     Deep Learning II
    </a>
   </span>
   esse tema também é estudado em detalhes.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Até o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   <span style="text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-690" href="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-690" href="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-690" href="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-690" href="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-690-5e0dd1264b84b" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=690&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-690-5e0dd1264b84b" id="like-post-wrapper-140353593-690-5e0dd1264b84b">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-26">
 Capítulo 26 – Como Escolher os Hiperparâmetros de Uma Rede Neural
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até agora não explicamos como foram escolhidos os valores dos hiperparâmetros como a taxa de aprendizado, η, o parâmetro de regularização, λ e assim por diante. Fornecemos valores que funcionaram muito bem, mas, na prática, quando você está usando redes neurais para resolver um problema, pode ser difícil encontrar bons parâmetros. Neste capítulo, começamos nosso estudo sobre Como Escolher os Hiperparâmetros de Uma Rede Neural. Vamos começar?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Imagine, por exemplo, que acabamos de ser apresentados ao dataset MNIST e começamos a trabalhar nele, sem saber nada sobre quais hiperparâmetros usar. Vamos supor que, por sorte, em nossos primeiros experimentos, escolhemos muitos dos hiperparâmetros da mesma forma como foi feito nos
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/" rel="noopener" target="_blank">
     capítulos anteriores
    </a>
   </span>
   : 30 neurônios ocultos, um tamanho de mini-lote de 10, treinando por 30 épocas usando a entropia cruzada. Mas escolhemos uma taxa de aprendizado η = 10.0 e o parâmetro de regularização λ = 1000.0. Aqui está um exemplo de execução da rede (o script está disponível no repositório do livro no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr" rel="noopener" target="_blank">
     Github
    </a>
   </span>
   )
  </span>
  <span style="color: #000000;">
   :
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="net1" class="aligncenter size-full wp-image-720" data-attachment-id="720" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="net1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?fit=611%2C531" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?fit=300%2C261" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?fit=611%2C531" data-orig-size="611,531" data-permalink="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net1/" data-recalc-dims="1" height="531" sizes="(max-width: 611px) 100vw, 611px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?resize=611%2C531" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?w=611 611w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?resize=300%2C261 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?resize=200%2C174 200w" width="611"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nossas precisões de classificação não são melhores do que o acaso! Nossa rede está agindo como um gerador de ruído aleatório!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   “Bem, isso é fácil de consertar”, você pode dizer, “apenas diminua a taxa de aprendizado e os hiperparâmetros de regularização”. Infelizmente, você não sabe a priori quais são os hiperparâmetros que você precisa ajustar. Talvez o verdadeiro problema seja que nossa rede de neurônios ocultos nunca funcionará bem, não importa como os outros hiperparâmetros sejam escolhidos? Talvez realmente precisemos de pelo menos 100 neurônios ocultos? Ou 300 neurônios ocultos? Ou várias camadas ocultas? Ou uma abordagem diferente para codificar a saída? Talvez nossa rede esteja aprendendo, mas precisamos treinar em mais épocas? Talvez os mini-lotes sejam pequenos demais? Talvez seja melhor voltarmos para a função de custo quadrático? Talvez precisemos tentar uma abordagem diferente para inicializar o peso? E assim por diante. Se fosse fácil, não precisaríamos de um
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
     Cientista de Dados
    </a>
   </span>
   , não é verdade?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   É fácil sentir-se perdido com tantas escolhas e combinações possíveis para os hiperparâmetros. Isso pode ser particularmente frustrante se sua rede for muito grande ou usar muitos dados de treinamento, pois você pode treinar por horas, dias ou semanas, apenas para não obter resultados. Se a situação persistir, prejudicará sua confiança. Talvez as redes neurais sejam a abordagem errada para o seu problema? Talvez você devesse largar o emprego e trabalhar com a apicultura?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nos próximos capítulos, explicaremos algumas heurísticas que podem ser usadas para definir os hiperparâmetros em uma rede neural. O objetivo é ajudá-lo a desenvolver um fluxo que permita que você faça um bom trabalho definindo hiperparâmetros. Claro, não vamos cobrir tudo sobre otimização de hiperparâmetros. Esse é um assunto enorme, e não é, de qualquer forma, um problema que já está completamente resolvido, nem existe um acordo universal entre os profissionais sobre as estratégias corretas a serem usadas. Há sempre mais um truque que você pode tentar para obter um pouco mais de desempenho da sua rede. Mas temos algumas heurísticas com as quais podemos começar.
  </span>
 </p>
 <p>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Compreendendo a Situação – Estratégia Geral
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao usar redes neurais para atacar um novo problema, o primeiro desafio é obter qualquer aprendizado não-trivial, ou seja, para que a rede obtenha resultados melhores que o acaso. Isso pode ser surpreendentemente difícil, especialmente ao confrontar uma nova classe de problemas. Vejamos algumas estratégias que você pode usar se tiver esse tipo de problema.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Suponha, por exemplo, que você esteja atacando o MNIST pela primeira vez. Você começa entusiasmado, mas fica um pouco desanimado quando sua primeira rede falha completamente, como no exemplo acima. O caminho a percorrer é reduzir o tamanho do problema. Livre-se de todas as imagens de treinamento e validação, exceto imagens de 0s ou 1s. Em seguida, tente treinar uma rede para distinguir 0s de 1s. Não só isso é um problema inerentemente mais fácil do que distinguir todos os dez dígitos, como também reduz a quantidade de dados de treinamento em 80%, acelerando o treinamento por um fator de 5. Isso permite experimentações muito mais rápidas e, portanto, fornece uma visão mais rápida sobre como construir uma boa rede.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você pode acelerar ainda mais a experimentação, desmembrando sua rede na rede mais simples, provavelmente fazendo aprendizado significativo. Se você acredita que uma rede [784, 10] provavelmente faz uma classificação melhor que o acaso com o dataset de dígitos MNIST, então comece sua experimentação com essa rede. Vai ser muito mais rápido do que treinar uma rede [784, 30, 10], e você pode “falhar” mais rápido (este é um conceito muito comum nos EUA: “fail fast”, ou seja, cometa falhas o mais rápido possível e aprenda com elas. Não se preocupe em tentar atingir a perfeição, pois você não vai conseguir de qualquer forma).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você pode acelerar mais na experimentação aumentando a frequência de monitoramento. No network2.py, monitoramos o desempenho no final de cada época de treinamento. Com 50.000 imagens por época, isso significa esperar um pouco – cerca de dez segundos por época, no meu laptop, ao treinar uma rede [784, 30, 10] – antes de obter feedback sobre o quanto a rede está aprendendo. É claro que dez segundos não são muito longos, mas se você quiser testar dezenas de opções de hiperparâmetros, é irritante, e se você quiser testar centenas ou milhares de opções, isso começa a ficar debilitante. Podemos obter feedback mais rapidamente, monitorando a precisão da validação com mais frequência, digamos, a cada 1.000 imagens de treinamento. Além disso, em vez de usar o conjunto completo de 10.000 imagens de validação para monitorar o desempenho, podemos obter uma estimativa muito mais rápida usando apenas 100 imagens de validação. Tudo o que importa é que a rede veja imagens suficientes para aprender de verdade e obter uma boa estimativa aproximada de desempenho. Claro, nosso programa network2.py atualmente não faz esse tipo de monitoramento. Mas, como um clímax para obter um efeito semelhante para fins de ilustração, vamos reduzir nossos dados de treinamento para apenas as primeiras 1.000 imagens de treinamento MNIST. Vamos tentar e ver o que acontece. (Para manter o código abaixo simples, não implementei a ideia de usar apenas imagens 0 e 1. Claro, isso pode ser feito com um pouco mais de trabalho).
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="net2" class="aligncenter size-full wp-image-721" data-attachment-id="721" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="net2" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?fit=615%2C268" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?fit=300%2C131" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?fit=615%2C268" data-orig-size="615,268" data-permalink="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net2/" data-recalc-dims="1" height="268" sizes="(max-width: 615px) 100vw, 615px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?resize=615%2C268" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?w=615 615w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?resize=300%2C131 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?resize=200%2C87 200w" width="615"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ainda estamos recebendo puro ruído! Mas há uma grande vitória: agora estamos obtendo feedback em uma fração de segundo, em vez de uma vez a cada dez segundos. Isso significa que você pode experimentar mais rapidamente outras opções de hiperparâmetros, ou até mesmo conduzir experimentos testando muitas opções diferentes de hiperparâmetros quase simultaneamente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No exemplo acima, eu deixamos λ como λ = 1000.0, como usamos anteriormente. Mas como mudamos o número de exemplos de treinamento, deveríamos realmente mudar λ para manter
   <em>
    weight decay
   </em>
   o mesmo. Isso significa mudar λ para 20.0. Se fizermos isso, então é o que acontece:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="net3" class="aligncenter size-full wp-image-722" data-attachment-id="722" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="net3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?fit=618%2C335" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?fit=300%2C163" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?fit=618%2C335" data-orig-size="618,335" data-permalink="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net3/" data-recalc-dims="1" height="335" sizes="(max-width: 618px) 100vw, 618px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?resize=618%2C335" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?w=618 618w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?resize=300%2C163 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?resize=200%2C108 200w" width="618"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ah! Nós temos um sinal. Não é um sinal muito bom, mas um sinal, no entanto. Isso é algo que podemos construir, modificando os hiperparâmetros para tentar melhorar ainda mais. Talvez nós achemos que nossa taxa de aprendizado precisa ser maior. (Como você talvez perceba, é um palpite bobo, por razões que discutiremos em breve, mas chegaremos lá. Não existe atalho para o aprendizado). Então, para testar nosso palpite, tentamos alterar η até 100.0:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="net4" class="aligncenter size-full wp-image-723" data-attachment-id="723" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="net4" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?fit=611%2C349" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?fit=300%2C171" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?fit=611%2C349" data-orig-size="611,349" data-permalink="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net4/" data-recalc-dims="1" height="349" sizes="(max-width: 611px) 100vw, 611px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?resize=611%2C349" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?w=611 611w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?resize=300%2C171 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?resize=200%2C114 200w" width="611"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso não é bom, pois sugere que nosso palpite estava errado e o problema não era que a taxa de aprendizado fosse muito baixa. Então, em vez disso, tentamos alterar η para η = 1.0:
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="net5" class="aligncenter size-full wp-image-724" data-attachment-id="724" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="net5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?fit=611%2C349" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?fit=300%2C171" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?fit=611%2C349" data-orig-size="611,349" data-permalink="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net5/" data-recalc-dims="1" height="349" sizes="(max-width: 611px) 100vw, 611px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?resize=611%2C349" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?w=611 611w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?resize=300%2C171 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?resize=200%2C114 200w" width="611"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora ficou melhor! E assim podemos continuar, ajustando individualmente cada hiperparâmetro, melhorando gradualmente o desempenho. Uma vez feita a exploração para encontrar um valor melhor para η, seguimos para encontrar um bom valor para λ. Em seguida, experimente uma arquitetura mais complexa, digamos uma rede com 10 neurônios ocultos e ajuste os valores para η e λ novamente. Depois, aumente para 20 neurônios ocultos e então, ajuste outros hiperparâmetros um pouco mais e assim por diante, em cada estágio avaliando o desempenho usando nossos dados de validação e usando essas avaliações para encontrar melhores hiperparâmetros. Ao fazer isso, normalmente leva mais tempo para testemunhar o impacto devido a modificações dos hiperparâmetros, e assim podemos diminuir gradualmente a frequência de monitoramento.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Tudo isso parece muito promissor como uma estratégia ampla. No entanto, quero voltar a esse estágio inicial de encontrar hiperparâmetros que permitem que uma rede aprenda qualquer coisa. De fato, mesmo a discussão acima transmite uma perspectiva muito positiva. Pode ser extremamente frustrante trabalhar com uma rede que não está aprendendo nada. Você pode ajustar os hiperparâmetros por dias e ainda não obter uma resposta significativa. Por isso, gostaria de enfatizar novamente que, durante os primeiros estágios, você deve se certificar de que pode obter um feedback rápido dos experimentos. Intuitivamente, pode parecer que simplificar o problema e a arquitetura apenas irá atrasá-lo. Na verdade, isso acelera as coisas, pois você encontra muito mais rapidamente uma rede com um sinal significativo. Uma vez que você tenha recebido tal sinal, muitas vezes você pode obter melhorias rápidas aprimorando os hiperparâmetros. Assim como em tudo na vida, começar pode ser a coisa mais difícil a se fazer.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ok, essa é a estratégia geral. Vamos agora olhar algumas recomendações específicas para definir hiperparâmetros. Vou me concentrar na taxa de aprendizado, η, no parâmetro de regularização L2, λ e no tamanho do mini-lote. No entanto, muitas das observações também se aplicam a outros hiperparâmetros, incluindo aqueles associados à arquitetura de rede, outras formas de regularização e alguns hiperparâmetros que encontraremos mais adiante aqui no Deep Learning Book, como o coeficiente momentum.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Oh não! O capítulo acabou! Fique tranquilo, continuamos no próximo. Até lá!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
 </div>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-719" href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-719" href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-719" href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-719" href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-719-5e0dd12850e2c" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=719&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-719-5e0dd12850e2c" id="like-post-wrapper-140353593-719-5e0dd12850e2c">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-27">
 Capítulo 27 – A Taxa de Aprendizado de Uma Rede Neural
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos continuar a discussão do
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/" rel="noopener" target="_blank">
     capítulo anterior
    </a>
   </span>
   sobre a escolha dos hiperparâmetros de um modelo de rede neural, estudando um dos mais importantes, a taxa de aprendizado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Suponha que executemos três redes neurais artificiais sendo treinadas com o dataset MNIST com três taxas de aprendizado diferentes, η = 0.025, η = 0.25 e η = 2.5, respectivamente. Vamos definir os outros hiperparâmetros de acordo com as experiências nos capítulos anteriores, executando mais de 30 epochs, com um tamanho de mini-lote de 10 e com λ = 5.0. Também voltaremos a usar todas as 50.000 imagens de treinamento. Aqui está um gráfico mostrando o comportamento do custo de treinamento enquanto treinamos:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="multiple_eta" class="aligncenter size-full wp-image-742" data-attachment-id="742" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="multiple_eta" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?fit=815%2C615" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?fit=300%2C226" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?fit=815%2C615" data-orig-size="815,615" data-permalink="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/multiple_eta/" data-recalc-dims="1" height="615" sizes="(max-width: 815px) 100vw, 815px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=815%2C615" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=690%2C521 690w" width="815"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com η = 0.025, o custo diminui suavemente até a época final. Com η = 0.25 o custo inicialmente diminui, mas após cerca de 20 épocas ele está próximo da saturação, e daí em diante a maioria das mudanças são meramente pequenas e aparentemente oscilações aleatórias. Finalmente, com η = 2.5, o custo faz grandes oscilações desde o início. Para entender o motivo das oscilações, lembre-se de que a descida estocástica do gradiente supostamente nos levará gradualmente a um vale da função de custo (conforme explicado
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/" rel="noopener" target="_blank">
     aqui
    </a>
   </span>
   ):
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="taxa de aprendizado" class="aligncenter size-full wp-image-743" data-attachment-id="743" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="taxa de aprendizado" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?fit=555%2C409" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?fit=300%2C221" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?fit=555%2C409" data-orig-size="555,409" data-permalink="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/tikz33/" data-recalc-dims="1" height="409" sizes="(max-width: 555px) 100vw, 555px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?resize=555%2C409" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?w=555 555w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?resize=300%2C221 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?resize=200%2C147 200w" width="555"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No entanto, se η for muito grande, os passos serão tão grandes que poderão, na verdade, ultrapassar o mínimo, fazendo com que o algoritmo simplesmente fique perdido durante o treinamento. Isso é provavelmente o que está causando a oscilação do custo quando η = 2.5. Quando escolhemos η = 0.25, os passos iniciais nos levam a um mínimo da função de custo, e é só quando chegamos perto desse mínimo que começamos a sofrer com o problema de
   <em>
    overshooting
   </em>
   . E quando escolhemos η = 0.025, não sofremos este problema durante as primeiras 30 épocas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Claro, escolher η tão pequeno cria outro problema, que reduz a velocidade da descida estocástica do gradiente, aumentando o tempo total de treinamento. Uma abordagem ainda melhor seria começar com η = 0.25, treinar por 20 épocas e então mudar para η = 0.025. Discutiremos essas tabelas de taxas de aprendizado variáveis posteriormente. Por enquanto, porém, vamos nos ater a descobrir como encontrar um único valor bom para a taxa de aprendizado, η.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com esta imagem em mente, podemos definir η da seguinte maneira. Primeiro, estimamos o valor limite para η no qual o custo nos dados de treinamento começa imediatamente a diminuir, em vez de oscilar ou aumentar. Essa estimativa não tem que ser muito precisa. Você pode estimar a ordem de magnitude começando com η = 0.01. Se o custo diminuir durante as primeiras épocas, então você deve sucessivamente tentar η = 0.1, 1.0,… até encontrar um valor para η onde o custo oscile ou aumente durante as primeiras poucas épocas (isso faz parte do trabalho de um
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
     Cientista de Dados
    </a>
   </span>
   ).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Alternativamente, se o custo oscilar ou aumentar durante as primeiras épocas, quando η = 0.01, então tente η = 0.001 ,0.0001,… até encontrar um valor para η onde o custo diminui durante as primeiras poucas épocas. Seguindo este procedimento, obteremos uma estimativa da ordem de magnitude para o valor limite de η. Você pode, opcionalmente, refinar sua estimativa, para escolher o maior valor de η no qual o custo diminui durante as primeiras poucas épocas, digamos η = 0.5 ou η = 0.2 (não há necessidade de que isso seja super-preciso). Isso nos dá uma estimativa para o valor limite de η. E claro, documente tudo!!!!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Obviamente, o valor real de η que você usa não deve ser maior que o valor limite. De fato, se o valor de η permanecer utilizável ao longo de muitas épocas, então você provavelmente desejará usar um valor para η que seja menor, digamos, um fator de dois abaixo do limite. Essa escolha normalmente permitirá que você treine por muitas épocas, sem causar muita lentidão no aprendizado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No caso dos dados MNIST, seguir esta estratégia leva a uma estimativa de 0.1 para a ordem de magnitude do valor limite de η. Depois de um pouco mais de refinamento, obtemos um valor limite η = 0.5. Seguindo a prescrição acima, isso sugere usar η = 0.25 como nosso valor para a taxa de aprendizado. De fato, eu descobri que usar η = 0.5 funcionava bem o suficiente em 30 épocas que, na maioria das vezes, eu não me preocupava em usar um valor menor de η.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Tudo isso parece bastante simples. No entanto, usar o custo de treinamento para escolher η parece contradizer o que dissemos anteriormente, que escolheríamos os hiperparâmetros avaliando o desempenho usando nossos dados de validação. Na verdade, usaremos a precisão de validação para escolher o hiperparâmetro de regularização, o tamanho do mini-lote e os parâmetros de rede, como o número de camadas e neurônios ocultos, e assim por diante (estudaremos isso nos próximos capítulos).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por que as coisas diferem para a taxa de aprendizado? Francamente, essa escolha é uma preferência estética pessoal e talvez seja um tanto idiossincrática. O raciocínio é que os outros hiperparâmetros são destinados a melhorar a precisão final da classificação no conjunto de testes, e por isso faz sentido selecioná-los com base na precisão da validação. No entanto, a taxa de aprendizado é apenas para influenciar a precisão final da classificação. Sua finalidade principal é realmente controlar o tamanho da etapa na descida do gradiente e monitorar o custo do treinamento é a melhor maneira de detectar se o tamanho da etapa é muito grande. Com isso dito, essa é uma preferência pessoal. No início, durante o aprendizado, o custo do treinamento geralmente diminui apenas se a precisão da validação melhorar e assim, na prática, é improvável que faça muita diferença em qual critério você usa.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   No próximo capítulo tem mais. Até lá!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-741" href="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-741" href="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-741" href="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-741" href="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-741-5e0dd12a595c6" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=741&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-741-5e0dd12a595c6" id="like-post-wrapper-140353593-741-5e0dd12a595c6">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-28">
 Capítulo 28 – Usando Early Stopping Para Definir o Número de Épocas de Treinamento
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao treinar redes neurais, várias decisões precisam ser tomadas em relação às configurações (
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/" rel="noopener" target="_blank">
     hiperparâmetros
    </a>
   </span>
   ) usadas, a fim de obter um bom desempenho. Um desses hiperparâmetros é o número de épocas de treinamento: ou seja, quantas passagens completas do conjunto de dados (épocas) devem ser usadas? Se usarmos poucas épocas, poderemos ter problemas de underfitting (ou seja, não aprender tudo o que pudermos com os dados de treinamento); se usarmos muitas épocas, podemos ter o problema oposto, overfitting (“aprender demais”, ou seja, ajustar o “ruído” nos dados de treinamento, e não o sinal).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Usamos o Early Stopping (“Parada Antecipada” ou “Parada Precoce”) exatamente para tentar definir manualmente esse valor. Também pode ser considerado um tipo de método de regularização (como L1/L2 weight decay e dropout estudados
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/" rel="noopener" target="_blank">
     anteriormente
    </a>
   </span>
   aqui no livro), pois pode impedir o overfitting da rede neural. A imagem abaixo ajuda a definir claramente o que é o Early Stopping:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="earlystopping" class="aligncenter size-full wp-image-759" data-attachment-id="759" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="earlystopping" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?fit=800%2C450" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?fit=300%2C169" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?fit=800%2C450" data-orig-size="800,450" data-permalink="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/earlystopping/" data-recalc-dims="1" height="450" sizes="(max-width: 800px) 100vw, 800px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=800%2C450" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=300%2C169 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=768%2C432 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=200%2C113 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=690%2C388 690w" width="800"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao treinar uma rede neural, geralmente se está interessado em obter uma rede com desempenho ideal de generalização. No entanto, todas as arquiteturas de rede neural padrão, como o perceptron multicamada totalmente conectado, são propensas a overfitting. Enquanto a rede parece melhorar, isto é, o erro no conjunto de treinamento diminui, em algum momento durante o treinamento na verdade começa a piorar novamente, ou seja, o erro em exemplos invisíveis aumenta.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Normalmente, o erro de generalização é estimado pelo erro de validação, isto é, o erro médio em um conjunto de validação, um conjunto fixo de exemplos que não são do conjunto de treino. Existem basicamente duas maneiras de combater o overfitting: reduzindo o número de dimensões do espaço de parâmetros ou reduzindo o tamanho efetivo de cada dimensão. Técnicas para reduzir o número de parâmetros são aprendizagem construtiva gananciosa, poda ou compartilhamento de peso. Técnicas para reduzir o tamanho de cada dimensão de parâmetro são a regularização, como weight decay ou dropout, ou a Parada Precoce (Early Stopping). A parada precoce é amplamente usada porque é simples de entender e implementar e foi relatada como sendo superior aos métodos de regularização em muitos casos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Usar Early Stopping significa que, no final de cada época, devemos calcular a precisão da classificação nos dados de validação. Quando a precisão parar de melhorar, terminamos o treinamento. Isso torna a configuração do número de épocas muito simples. Em particular, isso significa que não precisamos nos preocupar em descobrir explicitamente como o número de épocas depende dos outros hiperparâmetros, pois isso é feito automaticamente. Além disso, a Parada Antecipada também impede automaticamente o overfitting. Isto é, obviamente, uma coisa boa, embora nos estágios iniciais da experimentação possa ser útil desligar a Parada Antecipada, para que você possa ver quaisquer sinais de overfitting e usá-los para definir sua abordagem de regularização.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para implementar a Parada Antecipada, precisamos dizer com mais precisão o que significa que a precisão da classificação parou de melhorar. Como já vimos, a precisão pode se mover um pouco, mesmo quando a tendência geral é melhorar. Se pararmos pela primeira vez, a precisão diminui, então quase certamente pararemos quando houver mais melhorias a serem feitas. Uma regra melhor é terminar se a melhor precisão de classificação não melhorar por algum tempo. Suponha, por exemplo, que estamos trabalhando com o dataset MNIST. Poderíamos optar por terminar se a precisão da classificação não melhorou durante as últimas dez épocas. Isso garante que não paremos cedo demais, em resposta à má sorte no treinamento, mas também que não estamos esperando para sempre uma melhoria que nunca acontece.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esta regra de “parar o treinamento se não melhorar em dez épocas” é boa para a exploração inicial do MNIST. No entanto, as redes podem às vezes estabilizar-se perto de uma determinada precisão de classificação por algum tempo, apenas para começar a melhorar novamente. Se você está tentando obter um desempenho realmente bom, a regra de “parar o treinamento se não melhorar em dez épocas” pode ser muito agressiva. Nesse caso, sugerimos usar essa regra para a experimentação inicial e, gradualmente, adotar regras mais brandas, conforme entender melhor a maneira como sua rede treina: sem melhoria em vinte épocas, sem melhoria em cinquenta épocas e assim por diante. Claro, isso introduz um novo hiperparâmetro para otimizar! Na prática, no entanto, geralmente é fácil definir esse hiperparâmetro para obter bons resultados. Da mesma forma, para problemas diferentes do MNIST, a regra de não-melhoria-em-dez pode ser agressiva demais ou não ser agressiva o suficiente, dependendo dos detalhes do problema. No entanto, com um pouco de experimentação, geralmente é fácil encontrar uma boa estratégia para o Early Stopping. Isso faz parte do trabalho do
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
     Cientista de Dados
    </a>
   </span>
   ou do
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
     Engenheiro de Inteligência Artificial
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até aqui, nós não usamos o Early Stopping em nossos experimentos MNIST. A razão é que temos feito muitas comparações entre diferentes abordagens de aprendizado. Para tais comparações, é útil usar o mesmo número de épocas em cada caso. No entanto, vale a pena modificar o network2.py (disponível no repositório do curso no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener" target="_blank">
     Github
    </a>
   </span>
   ) para implementar o Early Stopping, e deixaremos isso como tarefa para você. Se precisar de ajuda, o Early Stopping é estudado em detalhes e com atividades práticas em Deep Learning na DSA, no curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
     Deep Learning II
    </a>
   </span>
   .
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Até o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Formação Inteligência Artificial
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-758" href="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-758" href="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-758" href="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-758" href="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-758-5e0dd12c51434" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=758&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-758-5e0dd12c51434" id="like-post-wrapper-140353593-758-5e0dd12c51434">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-29">
 Capítulo 29 – Definindo o Tamanho do Mini-Batch
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Quando os dados de treinamento são divididos em pequenos lotes, cada lote recebe o nome de Mini-Batch (ou Mini-Lote). Suponha que os dados de treinamento tenham 32.000 instâncias e que o tamanho de um Mini-Batch esteja definido como 32. Então, haverá 1.000 Mini-Batches. Mas qual deve ser o tamanho do Mini-Batch? Isso é o que veremos neste capítulo
  </span>
  <span style="color: #000000;">
   : Definindo o Tamanho do Mini-Batch.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas porque usamos Mini-Batches? Digamos que você tenha cerca de 1 bilhão de dados de treinamento. Se você decidir usar o conjunto completo de treinamento em cada época, você precisará de muita memória RAM e armazenamento para processar esses dados, sendo bem provável que sua máquina (ou mesmo um
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-engenheiro-de-dados" rel="noopener" target="_blank">
     cluster de computadores
    </a>
   </span>
   ) não tenha memória suficiente. Se você decidir usar um exemplo de treinamento em cada época, de um bilhão de dados, de uma só vez, você está ignorando a filosofia de vetorização e isso tornará o processo de treinamento muito mais lento.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Portanto, usamos um subconjunto de dados de treinamento (chamamos de “Mini-Batch”) de cada vez em cada época. Isso nos permitirá manter os dois objetivos: ajustar dados suficientes na memória do computador e manter a filosofia de vetorização ao mesmo tempo. Uma coisa importante sobre o Mini-Batch é que, é melhor escolher o tamanho do Mini-Batch como múltiplo de 2 e os valores comuns são: 64, 128, 256 e 512. Sinta-se à vontade para usar outros valores e discutiremos mais sobre isso mais a frente aqui mesmo neste capítulo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Veja um exemplo: digamos que você tenha 1 bilhão de dados de treinamento. Você define seu tamanho de Mini-Batch para, digamos, 512. Portanto, em cada época você tem 512 dados de treinamento para processar. Esta configuração levará aproximadamente: (1.000.000.000 / 512) = 1.953.125 épocas para ser concluída. Portanto, o tamanho do Mini-Batch é a quantidade de dados que você deseja processar em cada época.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Se atualizarmos os parâmetros do modelo após o processamento de todos os dados de treinamento (ou seja, época), levaria muito tempo para obter uma atualização do modelo no treinamento, e os dados de treinamento inteiros provavelmente não caberiam na memória. Se atualizarmos os parâmetros do modelo após o processamento de cada instância (por exemplo, descida de gradiente estocástico), as atualizações do modelo seriam demasiado ruidosas e o processo não seria computacionalmente eficiente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Portanto, a utilização do Mini-Batch (principalmente na descida do gradiente) é introduzida como um trade-off entre {atualizações rápidas do modelo, eficiência de memória} e {atualizações precisas do modelo, eficiência computacional}. É trabalho do
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
     Cientista de Dados
    </a>
   </span>
   ajustar mais esse parâmetro no processo de treinamento.
  </span>
 </p>
 <h2>
 </h2>
 <h2>
 </h2>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Mas Como Devemos Definir o Tamanho do Mini-Batch?
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para responder a essa pergunta, vamos primeiro supor que estamos fazendo aprendizado on-line, ou seja, que estamos usando um tamanho de Mini-Batch igual a 1.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A preocupação óbvia sobre o aprendizado online é que o uso de Mini-Lotes que contêm apenas um único exemplo de treinamento causará erros significativos em nossa estimativa do gradiente. A razão é que as estimativas graduais individuais não tem que ser super precisas. Tudo o que precisamos é de uma estimativa precisa o suficiente para que nossa função de custo continue diminuindo. É como se você estivesse tentando chegar ao Pólo Norte, mas tivesse uma bússola informando 10 a 20 graus cada vez que você olhasse para ela. Desde que você pare para checar a bússola com frequência, e a bússola acerte na direção, você acabará chegando ao Pólo Norte.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com base nesse argumento, parece que devemos usar o aprendizado on-line. De fato, a situação acaba sendo mais complicada do que isso. Em um problema do
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/" rel="noopener" target="_blank">
     último
    </a>
   </span>
   capítulo, mostramos que é possível usar técnicas de matriz para calcular a atualização de gradiente para todos os exemplos em um Mini-Lote simultaneamente, em vez de fazer um loop sobre eles. Dependendo dos detalhes de seu hardware e da biblioteca de
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener" target="_blank">
     álgebra linear
    </a>
   </span>
   , pode ser um pouco mais rápido calcular a estimativa de gradiente para um Mini-Lote de (por exemplo) tamanho 100, em vez de computar a estimativa de gradiente Mini-Lote fazendo um loop sobre os 100 exemplos de treinamento separadamente. Pode levar (digamos) apenas 50 vezes mais tempo, em vez de 100 vezes mais tempo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora, a princípio, parece que isso não nos ajuda muito. Com nosso Mini-Lote de tamanho 100, a regra de aprendizado para os pesos se parece com:
  </span>
 </p>
 <p style="text-align: justify;">
  <img alt="form1" class="aligncenter size-full wp-image-779" data-attachment-id="779" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?fit=504%2C154" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?fit=300%2C92" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?fit=504%2C154" data-orig-size="504,154" data-permalink="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/form1-3/" data-recalc-dims="1" height="154" sizes="(max-width: 504px) 100vw, 504px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?resize=504%2C154" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?w=504 504w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?resize=300%2C92 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?resize=200%2C61 200w" width="504"/>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   onde a soma é sobre exemplos de treinamento no Mini-Lote. Isso é equivalente a:
  </span>
 </p>
 <p style="text-align: justify;">
  <img alt="form2" class="aligncenter size-full wp-image-780" data-attachment-id="780" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?fit=386%2C104" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?fit=300%2C81" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?fit=386%2C104" data-orig-size="386,104" data-permalink="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/form2-7/" data-recalc-dims="1" height="104" sizes="(max-width: 386px) 100vw, 386px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?resize=386%2C104" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?w=386 386w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?resize=300%2C81 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?resize=200%2C54 200w" width="386"/>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   para aprendizagem online. Mesmo que demore 50 vezes mais para fazer a atualização do Mini-Batch, ainda parece ser melhor fazer o aprendizado online, porque estaríamos atualizando com muito mais frequência. Suponha, no entanto, que no caso do Mini-Lote nós aumentemos a taxa de aprendizado por um fator 100, então a regra de atualização se torna:
  </span>
 </p>
 <p style="text-align: justify;">
  <img alt="form3" class="aligncenter size-full wp-image-781" data-attachment-id="781" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?fit=432%2C140" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?fit=300%2C97" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?fit=432%2C140" data-orig-size="432,140" data-permalink="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/form3-6/" data-recalc-dims="1" height="140" sizes="(max-width: 432px) 100vw, 432px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?resize=432%2C140" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?w=432 432w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?resize=300%2C97 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?resize=200%2C65 200w" width="432"/>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso é muito parecido com 100 instâncias separadas de aprendizado online com uma taxa de aprendizado de η. Mas leva apenas 50 vezes mais tempo do que fazer uma única instância de aprendizado online. Naturalmente, não é exatamente o mesmo que 100 instâncias de aprendizado online, já que no Mini-Lote os ∇Cxs são todos avaliados para o mesmo conjunto de pesos, ao contrário do aprendizado cumulativo que ocorre no caso online. Ainda assim, parece claramente possível que o uso do Mini-Lote maior acelere as coisas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com esses fatores em mente, escolher o melhor tamanho de Mini-Lote é um trade-off (escolha). Muito pequeno, e você não consegue aproveitar ao máximo os benefícios de boas bibliotecas de matrizes otimizadas para hardware veloz. Demasiado grande e você simplesmente não está atualizando seus pesos com frequência suficiente. O que você precisa é escolher um valor que maximize a velocidade de aprendizado. Felizmente, a escolha do tamanho do Mini-Lote no qual a velocidade é maximizada é relativamente independente dos outros hiperparâmetros (além da arquitetura geral), portanto, você não precisa ter otimizado esses hiperparâmetros para encontrar um bom tamanho Mini-Lote. O caminho a percorrer é, portanto, usar alguns valores aceitáveis ​​(mas não necessariamente ideais) para os outros hiperparâmetros, e então testar vários tamanhos diferentes de Mini-Lotes, escalando η como fizemos no exemplo acima. Plote a precisão da validação em relação ao tempo (como em tempo real decorrido, não em época!) e escolha o tamanho do Mini-Lote que forneça a melhoria mais rápida no desempenho. Com o tamanho do Mini-Lote escolhido, você pode continuar a otimizar os outros hiperparâmetros. Entendeu agora porque
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
     Cientistas de Dados
    </a>
   </span>
   devem ser muito bem remunerados?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Claro, como você, sem dúvida, percebeu, não fizemos essa otimização em nossa rede de exemplo (que você encontra no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr" rel="noopener" target="_blank">
     Github
    </a>
   </span>
   ). De fato, nossa implementação não usa a abordagem mais rápida para atualizações de Mini-Batch. Nós simplesmente usamos um tamanho de Mini-Lote de 10 sem comentários ou explicações em quase todos os exemplos. Por causa disso, poderíamos ter acelerado o aprendizado reduzindo o tamanho do Mini-Lote. Não fizemos isso, em parte porque queríamos ilustrar o uso de Mini-Lotes além do tamanho 1, e em parte porque nossos experimentos preliminares sugeriam que a aceleração seria bastante modesta, uma vez que nossa rede de exemplo é bem simples. Em implementações práticas, no entanto, certamente implementaríamos a abordagem mais rápida para atualizações de Mini-Batch e, em seguida, faríamos um esforço para otimizar o tamanho do Mini-Lote, a fim de maximizar nossa velocidade geral.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nos cursos da
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
     Formação Inteligência Artificial
    </a>
    ,
   </span>
   os alunos trabalho com Mini-Batches pois os datasets usados são muito grandes e precisamos otimizar o tempo de treinamento. Todos os alunos da Formação tem acesso remoto gratuito ao super servidor da DSA com duas GPUs e ensinamos como otimizar o treinamento e usar os recursos computacionais de forma eficiente. A definição dos Mini-Batches é uma das atividades principais. Acesse o programa completo dos cursos aqui:
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
   e comece sua capacitação hoje mesmo.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Matemática Para Machine Learning
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-772" href="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-772" href="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-772" href="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-772" href="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-772-5e0dd12e47708" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=772&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-772-5e0dd12e47708" id="like-post-wrapper-140353593-772-5e0dd12e47708">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-30">
 Capítulo 30 – Variações do Stochastic Gradient Descent – Hessian Optimization e Momentum
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Cada técnica mostrada até
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/" rel="noopener" target="_blank">
     aqui
    </a>
   </span>
   é valiosa e deve ser dominada por aqueles que pretendem trabalhar com redes neurais artificiais e aplicações de Inteligência Artificial, mas essa não é a única razão pela qual nós as explicamos. O ponto principal é familiarizar você com alguns dos problemas que podem ocorrer nas redes neurais e com um estilo de análise que pode ajudar a superar esses problemas. De certo modo, aprendemos a pensar sobre redes neurais. Agora neste capítulo, esquematizamos brevemente algumas outras técnicas. Esses esboços são menos aprofundados do que as discussões anteriores, mas devem transmitir algum sentimento pela diversidade de técnicas disponíveis para uso em redes neurais. Lembrando que você sempre pode estudar todas essas técnicas em detalhes nos cursos da
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
   .
  </span>
 </p>
 <h2>
 </h2>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Variações do Stochastic Gradient Descent
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A descida de gradiente estocástico pela retropropagação tem nos servido bem no ataque ao problema de classificação de dígitos do dataset MNIST. No entanto, existem muitas outras abordagens para otimizar a função de custo e, às vezes, essas outras abordagens oferecem desempenho superior ao gradiente estocástico em mini-lote. Neste capítulo discutiremos duas dessas abordagens, Hessian Optimization e Momentum.
  </span>
 </p>
 <h3>
 </h3>
 <h3 style="text-align: justify;">
  Hessian Optimization
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para iniciar nossa discussão, ajuda a colocar as redes neurais de lado por um tempo. Em vez disso, vamos apenas considerar o problema abstrato de minimizar uma função de custo C que é uma função de muitas variáveis, w = w1, w2,…, então C = C(w). Pelo teorema de Taylor, a função custo pode ser aproximada perto de um ponto w por:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter size-full wp-image-799" data-attachment-id="799" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?fit=794%2C290" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?fit=300%2C110" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?fit=794%2C290" data-orig-size="794,290" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form1-4/" data-recalc-dims="1" height="290" sizes="(max-width: 794px) 100vw, 794px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=794%2C290" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?w=794 794w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=300%2C110 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=768%2C281 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=200%2C73 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=690%2C252 690w" width="794"/>
  </span>
 </p>
 <p style="text-align: center;">
  Fórmula 1
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos reescrever isso de forma mais compacta:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-800" data-attachment-id="800" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?fit=876%2C122" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?fit=300%2C42" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?fit=876%2C122" data-orig-size="876,122" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form2-8/" data-recalc-dims="1" height="122" sizes="(max-width: 876px) 100vw, 876px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=876%2C122" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?w=876 876w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=300%2C42 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=768%2C107 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=200%2C28 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=690%2C96 690w" width="876"/>
  </span>
 </p>
 <p style="text-align: center;">
  Fórmula 2
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   onde ∇C é o vetor gradiente usual e H é uma matriz conhecida como Matriz Hessiana. Suponha que nós aproximemos C descartando os termos de ordem superior representados por … acima:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="" class="aligncenter size-full wp-image-801" data-attachment-id="801" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?fit=778%2C124" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?fit=300%2C48" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?fit=778%2C124" data-orig-size="778,124" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form3-7/" data-recalc-dims="1" height="124" sizes="(max-width: 778px) 100vw, 778px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=778%2C124" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?w=778 778w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=300%2C48 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=768%2C122 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=200%2C32 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=690%2C110 690w" width="778"/>
  </span>
 </p>
 <p style="text-align: center;">
  Fórmula 3
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Usando o cálculo, podemos mostrar que a expressão do lado direito pode ser minimizada escolhendo:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form4" class="aligncenter size-full wp-image-802" data-attachment-id="802" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?fit=282%2C72" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?fit=282%2C72" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?fit=282%2C72" data-orig-size="282,72" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form4-5/" data-recalc-dims="1" height="72" sizes="(max-width: 282px) 100vw, 282px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?resize=282%2C72" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?w=282 282w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?resize=200%2C51 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?resize=280%2C72 280w" width="282"/>
  </span>
 </p>
 <p style="text-align: center;">
  Fórmula 4
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Considerando que a Fórmula 3 é uma boa expressão aproximada para a função custo, então esperamos que a mudança do ponto w para
   <img alt="form6" class="aligncenter size-full wp-image-812" data-attachment-id="812" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form6" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?fit=400%2C64" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?fit=300%2C48" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?fit=400%2C64" data-orig-size="400,64" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form6-2/" data-recalc-dims="1" height="64" sizes="(max-width: 400px) 100vw, 400px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?resize=400%2C64" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?w=400 400w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?resize=300%2C48 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?resize=200%2C32 200w" width="400"/>
   deva diminuir significativamente a função custo. Isso sugere um algoritmo possível para minimizar o custo:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Escolha um ponto de partida, w.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Atualize w para um novo ponto w ′ = w − H ^ − 1 ∇C, onde o Hessian H e ∇C são calculados em w.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Atualize w′ para um novo ponto w′′ = w′ − H′ ^ − 1 ∇′C, onde o Hessian H′ e ∇′C são calculados em w′.
   </span>
   <br/>
   <span style="color: #000000;">
    …
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na prática, a Fórmula 3 é apenas uma aproximação e é melhor dar passos menores. Fazemos isso alterando repetidamente w por uma quantidade
   <img alt="form8" class="aligncenter size-full wp-image-814" data-attachment-id="814" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form8" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?fit=310%2C70" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?fit=300%2C68" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?fit=310%2C70" data-orig-size="310,70" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form8-3/" data-recalc-dims="1" height="70" sizes="(max-width: 310px) 100vw, 310px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?resize=310%2C70" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?w=310 310w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?resize=300%2C68 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?resize=200%2C45 200w" width="310"/>
   onde η é conhecido como taxa de aprendizado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa abordagem para minimizar uma função de custo é conhecida como Hessian Technique ou Hessian Optimization. Existem resultados teóricos e empíricos mostrando que os métodos de Hessian convergem em um mínimo em menos etapas do que a descida de gradiente padrão. Em particular, ao incorporar informações sobre mudanças de segunda ordem na função de custo, é possível que a abordagem Hessiana evite muitas patologias que podem ocorrer na descida de gradiente. Além disso, há versões do algoritmo de retropropagação que podem ser usadas para computar o Hessian.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Se a Hessian Optimization é tão bom, por que não a estamos usando em nossas redes neurais? Infelizmente, embora tenha muitas propriedades desejáveis, tem uma propriedade muito indesejável: é muito difícil de aplicar na prática. Parte do problema é o tamanho da matriz Hessiana. Suponha que você tenha uma rede neural com 107 pesos e vieses. Em seguida, a matriz Hessiana correspondente conterá 107 × 107 = 1014 entradas. Isso é um número grande de entradas! E isso torna a computação H ^ − 1 ∇C extremamente difícil na prática. No entanto, isso não significa que não seja útil entender. De fato, há muitas variações na descida de gradiente que são inspiradas pela Hessian Optimization, mas que evitam o problema com matrizes excessivamente grandes. Vamos dar uma olhada em uma dessas técnicas, a descida do gradiente baseada em Momentum.
  </span>
 </p>
 <h3>
 </h3>
 <h3 style="text-align: justify;">
 </h3>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Momentum
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Intuitivamente, a vantagem da Hessian Optimization é que ela incorpora não apenas informações sobre o gradiente, mas também informações sobre como o gradiente está mudando. A descida do gradiente baseada no Momentum baseia-se em uma intuição similar, mas evita grandes matrizes de derivadas secundárias. Para entender a técnica de Momentum, pense em nossa imagem original de descida do gradiente, na qual consideramos uma bola rolando em um vale (veja figura abaixo). Observamos que a descida do gradiente é, apesar de seu nome, apenas vagamente semelhante a uma bola caindo no fundo de um vale.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A técnica de Momentum modifica a descida do gradiente de duas maneiras que a tornam mais semelhante à imagem física. Primeiro, introduz uma noção de “velocidade” para os parâmetros que estamos tentando otimizar. O gradiente atua para alterar a velocidade, não (diretamente) a “posição”, da mesma maneira que as forças físicas alteram a velocidade, afetando apenas indiretamente a posição. Em segundo lugar, o método Momentum introduz um tipo de termo de fricção, que tende a reduzir gradualmente a velocidade.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos dar uma descrição matemática mais precisa. Introduzimos variáveis de velocidade v = v1, v2,…, uma para cada variável wj correspondente. Então nós substituímos a regra de atualização de descida de gradiente w → w′ = w − η∇C por:
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="form5" class="aligncenter size-full wp-image-808" data-attachment-id="808" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?fit=374%2C152" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?fit=300%2C122" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?fit=374%2C152" data-orig-size="374,152" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form5-3/" data-recalc-dims="1" height="152" sizes="(max-width: 374px) 100vw, 374px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?resize=374%2C152" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?w=374 374w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?resize=300%2C122 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?resize=200%2C81 200w" width="374"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nessas equações, μ é um hiperparâmetro que controla a quantidade de amortecimento ou atrito no sistema. Para entender o significado das equações, é útil considerar primeiro o caso onde μ = 1, o que corresponde a nenhum atrito. Quando esse é o caso, a inspeção das equações mostra que a “força” ∇C está agora modificando a velocidade, v, e a velocidade está controlando a taxa de variação de w. Intuitivamente, nós aumentamos a velocidade adicionando repetidamente termos de gradiente a ela. Isso significa que se o gradiente estiver na (aproximadamente) mesma direção através de várias rodadas de aprendizado, poderemos desenvolver um pouco de vapor movendo-se nessa direção. Pense, por exemplo, no que acontece se estivermos nos movendo diretamente por um declive:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="gradient" class="aligncenter size-full wp-image-803" data-attachment-id="803" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gradient" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?fit=555%2C409" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?fit=300%2C221" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?fit=555%2C409" data-orig-size="555,409" data-permalink="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/gradient/" data-recalc-dims="1" height="409" sizes="(max-width: 555px) 100vw, 555px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?resize=555%2C409" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?w=555 555w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?resize=300%2C221 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?resize=200%2C147 200w" width="555"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A cada passo a velocidade se torna maior no declive, então nos movemos mais e mais rapidamente para o fundo do vale. Isso pode permitir que a técnica de Momentum funcione muito mais rapidamente do que a descida de gradiente padrão. Claro, um problema é que, uma vez que chegarmos ao fundo do vale, vamos ultrapassar. Ou, se o gradiente deve mudar rapidamente, então podemos nos encontrar indo na direção errada. Essa é a razão para o hiperparâmetro µ nas equações acima.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Eu disse anteriormente que μ controla a quantidade de atrito no sistema; para ser um pouco mais preciso, você deve pensar em 1 − μ como a quantidade de atrito no sistema. Quando μ = 1, como vimos, não há atrito e a velocidade é completamente controlada pelo gradiente ∇C. Em contraste, quando μ = 0 há muito atrito, a velocidade não pode se acumular e as equações acima reduzem à equação usual para o gradiente descendente, w → w ′ = w − η∇C. Na prática, usar um valor intermediário entre 0 e 1 pode nos dar muito do benefício de ser capaz de aumentar a velocidade, mas sem causar overshooting. Podemos escolher um valor para μ usando os dados de validação retidos, da mesma maneira que selecionamos η e λ. Essa técnica é estudada em detalhes
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii" rel="noopener" target="_blank">
     aqui
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Evitei nomear o hiperparâmetro μ até agora. A razão é que o nome padrão para μ é mal escolhido: é chamado de coeficiente de momentum. Isso é potencialmente confuso, já que μ não é de maneira alguma a noção de momento da física. Pelo contrário, está muito mais relacionado ao atrito. No entanto, o termo coeficiente de momentum é amplamente utilizado, por isso continuaremos a usá-lo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma coisa boa sobre a técnica do Momentum é que não é preciso quase nenhum trabalho para modificar uma implementação de descida de gradiente para incorporar o Momentum. Ainda podemos usar a retropropagação para calcular os gradientes, assim como antes, e usar ideias como a amostragem de mini-lotes estocasticamente escolhidos. Desta forma, podemos obter algumas das vantagens da Hessian Optimization, usando informações sobre como o gradiente está mudando, mas sem as desvantagens e com apenas pequenas modificações no nosso código. Na prática, a técnica do Momentum é comumente usada e, muitas vezes, acelera o aprendizado.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Vejo você no próximo capítulo!
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Formação Inteligência Artificial
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-794" href="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-794" href="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-794" href="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-794" href="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-794-5e0dd1306aea2" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=794&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-794-5e0dd1306aea2" id="like-post-wrapper-140353593-794-5e0dd1306aea2">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-31">
 Capítulo 31 – As Redes Neurais Artificiais Podem Computar Qualquer Função?
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Um dos fatos mais impressionantes sobre redes neurais é que elas podem computar qualquer função. Isto é, suponha que alguém lhe dê alguma função complicada, f(x):
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="image1" class="aligncenter size-full wp-image-822" data-attachment-id="822" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="image1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?fit=300%2C300" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?fit=300%2C300" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?fit=300%2C300" data-orig-size="300,300" data-permalink="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/image1/" data-recalc-dims="1" height="300" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=300%2C300" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?w=300 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=150%2C150 150w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=200%2C200 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=280%2C280 280w" width="300"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Não importa qual seja a função, é garantido que existe uma rede neural de modo que, para cada entrada possível, x, o valor f(x) (ou alguma aproximação) seja transmitido da rede, por exemplo:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="image2" class="aligncenter size-full wp-image-823" data-attachment-id="823" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="image2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?fit=350%2C220" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?fit=300%2C189" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?fit=350%2C220" data-orig-size="350,220" data-permalink="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/image2/" data-recalc-dims="1" height="220" sizes="(max-width: 350px) 100vw, 350px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?resize=350%2C220" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?w=350 350w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?resize=300%2C189 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?resize=200%2C126 200w" width="350"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Este resultado é válido mesmo se a função tiver muitas entradas, f = f(x1,…, xm) e muitas saídas. Por exemplo, aqui está uma rede computando uma função com m = 3 entradas e n = 2 saídas:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="image3" class="aligncenter size-full wp-image-824" data-attachment-id="824" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="image3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?fit=450%2C370" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?fit=300%2C247" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?fit=450%2C370" data-orig-size="450,370" data-permalink="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/image3/" data-recalc-dims="1" height="370" sizes="(max-width: 450px) 100vw, 450px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?resize=450%2C370" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?w=450 450w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?resize=300%2C247 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?resize=200%2C164 200w" width="450"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Este resultado nos diz que as redes neurais têm um tipo de universalidade. Não importa qual função queremos computar, sabemos que existe uma rede neural que pode fazer o trabalho.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Além do mais, esse teorema da universalidade é válido mesmo se restringirmos nossas redes a ter apenas uma única camada intermediária entre os neurônios de entrada e de saída – uma chamada camada oculta única. Portanto, mesmo arquiteturas de rede muito simples podem ser extremamente poderosas e isso ajuda a explicar porque as redes neurais vem sendo usadas em aplicações avançadas de Inteligência Artificial.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O teorema da universalidade é bem conhecido por pessoas que usam redes neurais. Mas porque é verdade não é tão amplamente compreendido. A maioria das explicações disponíveis é bastante técnica. Por exemplo, um dos artigos originais que comprovou o resultado utilizou o teorema de Hahn-Banach, o teorema da representação de Riesz e alguma análise de Fourier. Se você é um matemático, o argumento não é difícil de seguir, mas não é tão fácil para a maioria das pessoas. É uma pena, já que as razões subjacentes à universalidade são simples e belas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nos próximos capítulos, faremos uma explicação simples e principalmente visual do teorema da universalidade. Nós vamos passo a passo através das idéias principais. Você entenderá porque é verdade que as redes neurais podem computar qualquer função. Você entenderá algumas das limitações do resultado. E você entenderá como o resultado se relaciona com redes neurais profundas (Deep Learning).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os capítulos serão estruturados para ser agradáveis e objetivos. Desde que você tenha apenas um pouco de familiaridade básica com redes neurais, você deve ser capaz de seguir a explicação. No entanto, iremos fornecer links ocasionais para materiais anteriores, para ajudar a preencher quaisquer lacunas em seu conhecimento.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os teoremas da universalidade são um lugar comum na ciência da computação, tanto que às vezes nos esquecemos do quão surpreendentes eles são. Mas vale a pena lembrar-nos: a capacidade de calcular uma função arbitrária é verdadeiramente notável. Quase qualquer processo que você possa imaginar pode ser considerado como computação de função. Considere o problema de nomear uma peça musical com base em uma pequena amostra da peça. Isso pode ser pensado como computação de uma função. Ou considere o problema de traduzir um texto chinês para o inglês. Mais uma vez, isso pode ser pensado como computação de uma função. Ou considere o problema de analisar um arquivo de filme mp4 e gerar uma descrição do enredo do filme e uma discussão sobre a qualidade da atuação dos atores. Novamente, isso pode ser pensado como um tipo de computação de função. Universalidade significa que, em princípio, as redes neurais podem fazer tudo isso e muito mais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   É claro, só porque sabemos que existe uma rede neural que pode (por exemplo) traduzir o texto chinês para o inglês, isso não significa que temos boas técnicas para construir ou mesmo reconhecer tal rede. Essa limitação se aplica também aos teoremas da universalidade tradicionais para modelos como circuitos booleanos. Mas, como vimos anteriormente no livro, as redes neurais possuem algoritmos poderosos para funções de aprendizado. Essa combinação de algoritmos de aprendizado + universalidade é uma mistura atraente. Até agora, o livro se concentrou nos algoritmos de aprendizado. Nos próximos capítulos, nos concentramos na universalidade e no que ela significa.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   A compreensão desse conceito é a chave para as arquiteturas mais avançadas de Deep Learning, que estão por vir mais a frente, neste livro!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências
  </span>
  :
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-821" href="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-821" href="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-821" href="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-821" href="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-821-5e0dd1327788d" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=821&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-821-5e0dd1327788d" id="like-post-wrapper-140353593-821-5e0dd1327788d">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-32">
 Capítulo 32 – Como Uma Rede Neural Artificial Encontra a Aproximação de Uma Função
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Este é um capítulo muito importante para compreender como as redes neurais realmente funcionam e
   <strong>
    Como Uma Rede Neural Artificial Encontra a Aproximação de Uma Função
   </strong>
   . Acompanhe a explicação passo a passo analisando cada um dos gráficos apresentados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas antes de explicar porque o teorema da universalidade é verdadeiro, quero mencionar duas advertências a esta declaração informal: “uma rede neural pode computar qualquer função”, que vimos no capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/" rel="noopener" target="_blank">
     anterior
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Primeiro, isso não significa que uma rede possa ser usada para calcular exatamente qualquer função. Em vez disso, podemos obter uma aproximação que seja tão boa quanto desejamos. Aumentando o número de neurônios ocultos, podemos melhorar a aproximação. Por exemplo, anteriormente ilustramos uma rede computando alguma função f(x) usando três neurônios ocultos. Para a maioria das funções, apenas uma aproximação de baixa qualidade será possível usando três neurônios ocultos. Ao aumentar o número de neurônios ocultos (digamos, para cinco), podemos obter uma melhor aproximação:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede" class="aligncenter size-full wp-image-834" data-attachment-id="834" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?fit=350%2C380" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?fit=276%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?fit=350%2C380" data-orig-size="350,380" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-6/" data-recalc-dims="1" height="380" sizes="(max-width: 350px) 100vw, 350px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?resize=350%2C380" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?w=350 350w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?resize=276%2C300 276w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?resize=200%2C217 200w" width="350"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   E podemos melhorar ainda mais aumentando o número de neurônios ocultos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para tornar esta afirmação mais precisa, suponha que tenhamos uma função f(x) que gostaríamos de computar com alguma precisão desejada ϵ &gt; 0. A garantia é que usando neurônios ocultos suficientes sempre podemos encontrar uma rede neural cuja saída g(x) satisfaça | g(x) − f(x) | &lt; ϵ, para todas as entradas x. Em outras palavras, a aproximação será boa dentro da precisão desejada para cada entrada possível.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A segunda ressalva é que a classe de funções que podem ser aproximadas da maneira descrita são as funções contínuas. Se uma função é descontínua, isto é, faz saltos bruscos e repentinos, então, em geral, não será possível aproximar usando uma rede neural. Isso não é surpreendente, já que nossas redes neurais calculam funções contínuas de sua entrada. No entanto, mesmo que a função que realmente gostaríamos de computar fosse descontínua, muitas vezes a aproximação contínua é boa o suficiente. Se é assim, então podemos usar uma rede neural. Na prática, isso geralmente não é uma limitação importante.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em suma, uma afirmação mais precisa do teorema da universalidade é que redes neurais com uma única camada oculta podem ser usadas para aproximar qualquer função contínua a qualquer precisão desejada. Neste e no próximo capítulo, vamos provar uma versão desse resultado.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Universalidade Com Uma Entrada e Uma Saída
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para entender por que o teorema da universalidade é verdadeiro, vamos começar entendendo como construir uma rede neural que se aproxima de uma função com apenas uma entrada e uma saída:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede2" class="aligncenter size-full wp-image-835" data-attachment-id="835" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?fit=300%2C300" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?fit=300%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?fit=300%2C300" data-orig-size="300,300" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede2-5/" data-recalc-dims="1" height="300" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=300%2C300" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?w=300 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=150%2C150 150w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=200%2C200 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=280%2C280 280w" width="300"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Este é o cerne do problema da universalidade. Uma vez que entendemos esse caso especial, é realmente fácil estender para funções com muitas entradas e muitas saídas (tema do próximo capítulo).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para construir um insight sobre como construir uma rede para calcular f, vamos começar com uma rede contendo apenas uma camada oculta, com dois neurônios ocultos e uma camada de saída contendo um único neurônio de saída:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede" class="aligncenter size-full wp-image-837" data-attachment-id="837" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?fit=350%2C220" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?fit=300%2C189" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?fit=350%2C220" data-orig-size="350,220" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-7/" data-recalc-dims="1" height="220" sizes="(max-width: 350px) 100vw, 350px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?resize=350%2C220" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?w=350 350w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?resize=300%2C189 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?resize=200%2C126 200w" width="350"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para ter uma ideia de como funcionam os componentes da rede, vamos nos concentrar no neurônio oculto superior. No diagrama abaixo, aumentando o valor de w, podemos ver imediatamente como a função computada pelo neurônio oculto superior muda:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede" class="aligncenter wp-image-857 size-large" data-attachment-id="857" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?fit=1024%2C517" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?fit=300%2C152" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?fit=1164%2C588" data-orig-size="1164,588" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-9/" data-recalc-dims="1" height="517" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=1024%2C517" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=1024%2C517 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=300%2C152 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=768%2C388 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=200%2C101 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=690%2C349 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?w=1164 1164w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como aprendemos anteriormente no livro, o que está sendo computado pelo neurônio oculto é σ(wx + b), onde σ(z) ≡ 1 / (1 + e^-z) é a função sigmóide. Até agora, fizemos uso frequente dessa forma algébrica. Mas, para a prova da universalidade, obteremos mais discernimento ignorando inteiramente a álgebra e, em vez disso, manipulando e observando a forma mostrada no gráfico. Isso não apenas nos dará uma ideia melhor do que está acontecendo, mas também nos dará uma prova de universalidade que se aplica a outras funções de ativação que não a função sigmóide.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para começar esta prova, podemos aumentar o bias, b, no diagrama acima. Você verá que, conforme o bias aumenta, o gráfico se move para a esquerda, mas sua forma não muda.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em seguida, podemos diminuir o viés (bias). Você verá que conforme o viés diminui, o gráfico se move para a direita, mas, novamente, sua forma não muda. Em seguida, diminuímos o peso para cerca de 2 ou 3. Você verá que à medida que diminui o peso, a curva se alarga. Talvez seja necessário alterar o bias também, para manter a curva no quadro.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Finalmente, aumentamos o peso acima de w = 100. A curva fica mais íngreme, até que, eventualmente, ela começa a parecer uma função de passo (Step Function). A imagem a seguir mostra como deve ser resultado:
  </span>
 </p>
 <p>
  <img alt="rede3" class="aligncenter size-full wp-image-851" data-attachment-id="851" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?fit=1024%2C523" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?fit=300%2C153" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?fit=1136%2C580" data-orig-size="1136,580" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede3-4/" data-recalc-dims="1" height="580" sizes="(max-width: 1136px) 100vw, 1136px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=1136%2C580" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?w=1136 1136w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=300%2C153 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=768%2C392 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=1024%2C523 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=200%2C102 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=690%2C352 690w" width="1136"/>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos simplificar um pouco nossa análise aumentando o peso para que a saída realmente seja uma Step Function, para uma aproximação muito boa. Abaixo eu plotei a saída do neurônio oculto superior quando o peso é w = 999.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede4" class="aligncenter size-full wp-image-840" data-attachment-id="840" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?fit=1024%2C509" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?fit=300%2C149" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?fit=1154%2C574" data-orig-size="1154,574" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede4-2/" data-recalc-dims="1" height="574" sizes="(max-width: 1154px) 100vw, 1154px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=1154%2C574" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?w=1154 1154w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=300%2C149 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=768%2C382 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=1024%2C509 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=200%2C99 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=690%2C343 690w" width="1154"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na verdade, é um pouco mais fácil trabalhar com funções step do que com funções gerais sigmóides. A razão é que, na camada de saída, somamos contribuições de todos os neurônios ocultos. É fácil analisar a soma de várias funções step, mas é mais difícil pensar sobre o que acontece quando você adiciona um monte de curvas em forma de sigmóide. E assim torna as coisas muito mais fáceis de assumir que nossos neurônios ocultos estão emitindo funções step. Mais concretamente, fazemos isso fixando o peso w como sendo um valor muito grande e, em seguida, definindo a posição da etapa modificando o bias. É claro que tratar a saída como uma função step é uma aproximação, mas é uma aproximação muito boa e, por enquanto, vamos tratá-la como exata. Voltarei mais tarde para discutir o impacto dos desvios dessa aproximação.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em que valor de x a etapa ocorre? Em outras palavras, como a posição da etapa depende do peso e do viés?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para responder a essa pergunta, podemos modificar o peso e o viés no diagrama acima. Você consegue descobrir como a posição da etapa depende de w e b. Com um pouco de trabalho, você deve ser capaz de se convencer de que a posição da etapa é proporcional a b e inversamente proporcional a w.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na verdade, a etapa está na posição s = −b / w, como você pode ver modificando o peso e o bias no diagrama a seguir:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede5" class="aligncenter size-full wp-image-841" data-attachment-id="841" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?fit=1024%2C510" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?fit=300%2C149" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?fit=1168%2C582" data-orig-size="1168,582" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede5-2/" data-recalc-dims="1" height="582" sizes="(max-width: 1168px) 100vw, 1168px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=1168%2C582" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?w=1168 1168w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=300%2C149 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=768%2C383 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=1024%2C510 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=200%2C100 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=690%2C344 690w" width="1168"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso simplificará muito nossas vidas para descrever os neurônios ocultos usando apenas um único parâmetro, s, que é a posição do passo, s = −b / w.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede6" class="aligncenter size-full wp-image-842" data-attachment-id="842" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede6" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?fit=1024%2C533" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?fit=300%2C156" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?fit=1156%2C602" data-orig-size="1156,602" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/redd6/" data-recalc-dims="1" height="602" sizes="(max-width: 1156px) 100vw, 1156px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=1156%2C602" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?w=1156 1156w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=300%2C156 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=768%2C400 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=1024%2C533 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=200%2C104 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=690%2C359 690w" width="1156"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como mencionado acima, nós implicitamente definimos o peso w na entrada como um valor grande – grande o suficiente para que a função de passo seja uma boa aproximação. Podemos facilmente converter um neurônio parametrizado dessa maneira de volta ao modelo convencional, escolhendo o viés b = −ws.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até agora, nos concentramos na saída apenas do neurônio oculto superior. Vamos dar uma olhada no comportamento de toda a rede. Em particular, vamos supor que os neurônios ocultos estejam computando funções de passos parametrizadas pelos pontos de degrau s1 (neurônio superior) e s2 (neurônio de baixo). E eles terão os respectivos pesos de saída w1 e w2. Aqui está a rede:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede7" class="aligncenter size-full wp-image-843" data-attachment-id="843" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede7" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?fit=1024%2C550" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?fit=300%2C161" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?fit=1154%2C620" data-orig-size="1154,620" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede7-2/" data-recalc-dims="1" height="620" sizes="(max-width: 1154px) 100vw, 1154px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=1154%2C620" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?w=1154 1154w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=300%2C161 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=768%2C413 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=1024%2C550 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=200%2C107 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=690%2C371 690w" width="1154"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O que está sendo plotado à direita é a saída ponderada w1a1 + w2a2 da camada oculta. Aqui, a1 e a2 são as saídas dos neurônios ocultos superior e inferior, respectivamente. Essas saídas são frequentemente conhecidas como ativações dos neurônios.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos aumentar ou diminuir o ponto de passo s1 do neurônio oculto superior e isso nos dá uma ideia de como isso altera a saída ponderada da camada oculta. Vale a pena entender o que acontece quando o s1 passa do s2. Você verá que o gráfico muda de forma quando isso acontece, já que nos movemos de uma situação em que o neurônio oculto superior é o primeiro a ser ativado para uma situação em que o neurônio oculto na parte inferior é o primeiro a ser ativado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Da mesma forma, podemos manipular o ponto de passo s2 do neurônio oculto na parte inferior e ter uma ideia de como isso altera a saída combinada dos neurônios ocultos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Finalmente, podemos definir w1 como 0.8 e w2 como −0.8. Você recebe uma função “bump”, que começa no ponto s1, termina no ponto s2 e tem a altura 0.8. Por exemplo, a saída ponderada pode ser assim:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede8" class="aligncenter size-full wp-image-844" data-attachment-id="844" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede8" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?fit=1024%2C582" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?fit=300%2C170" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?fit=1148%2C652" data-orig-size="1148,652" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede8/" data-recalc-dims="1" height="652" sizes="(max-width: 1148px) 100vw, 1148px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=1148%2C652" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?w=1148 1148w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=300%2C170 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=768%2C436 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=1024%2C582 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=200%2C114 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=690%2C392 690w" width="1148"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Claro, podemos redimensionar o bump para ter qualquer altura. Vamos usar um único parâmetro, h, para indicar a altura. Para reduzir a confusão, também removerei as notações “s1 = …” e “w1 = …”.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede9" class="aligncenter size-full wp-image-845" data-attachment-id="845" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede9" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?fit=1024%2C552" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?fit=300%2C162" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?fit=1154%2C622" data-orig-size="1154,622" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede9/" data-recalc-dims="1" height="622" sizes="(max-width: 1154px) 100vw, 1154px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=1154%2C622" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?w=1154 1154w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=300%2C162 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=768%2C414 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=1024%2C552 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=200%2C108 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=690%2C372 690w" width="1154"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos alterar o valor de h para cima e para baixo, para ver como a altura do bump muda.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você notará, a propósito, que estamos usando nossos neurônios de uma forma que pode ser pensada não apenas em termos gráficos, mas em termos de programação mais convencionais, como uma espécie de declaração if-then-else, por exemplo:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="code" class="aligncenter size-full wp-image-846" data-attachment-id="846" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="code" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?fit=754%2C158" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?fit=300%2C63" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?fit=754%2C158" data-orig-size="754,158" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/code-2/" data-recalc-dims="1" height="158" sizes="(max-width: 754px) 100vw, 754px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=754%2C158" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?w=754 754w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=300%2C63 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=200%2C42 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=690%2C145 690w" width="754"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na maior parte eu vou ficar com o ponto de vista gráfico. Mas, no que se segue, às vezes você pode achar útil trocar pontos de vista e pensar sobre as coisas em termos de se-então-senão (uma das bases da programação convencional).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos usar o nosso truque de fazer bump para obter dois solavancos, colando dois pares de neurônios ocultos na mesma rede:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede11" class="aligncenter size-full wp-image-847" data-attachment-id="847" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede11" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?fit=1024%2C528" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?fit=300%2C155" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?fit=1168%2C602" data-orig-size="1168,602" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede11/" data-recalc-dims="1" height="602" sizes="(max-width: 1168px) 100vw, 1168px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=1168%2C602" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?w=1168 1168w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=300%2C155 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=768%2C396 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=1024%2C528 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=200%2C103 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=690%2C356 690w" width="1168"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Eu suprimi os pesos aqui, simplesmente escrevendo os valores h para cada par de neurônios ocultos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   De maneira mais geral, podemos usar essa ideia para obter o máximo de picos que quisermos, de qualquer altura. Em particular, podemos dividir o intervalo [0,1] em um número grande, N, de subintervalos, e usar N pares de neurônios ocultos para configurar picos de qualquer altura desejada. Vamos ver como isso funciona para N = 5. Desculpa pela a complexidade do diagrama abaixo (eu poderia esconder a complexidade abstraindo mais, mas acho que vale a pena colocar um pouco de complexidade, para obter uma ideia mais concreta de como essas redes funciona):
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede" class="aligncenter size-full wp-image-848" data-attachment-id="848" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?fit=939%2C1024" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?fit=275%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?fit=1168%2C1274" data-orig-size="1168,1274" data-permalink="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-8/" data-recalc-dims="1" height="1274" sizes="(max-width: 1168px) 100vw, 1168px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=1168%2C1274" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?w=1168 1168w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=768%2C838 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=939%2C1024 939w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=200%2C218 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=690%2C753 690w" width="1168"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você pode ver que existem cinco pares de neurônios ocultos. Os pontos escalonados para os respectivos pares de neurônios são 0,1 / 5, depois 1 / 5,2 / 5 e assim por diante, para 4 / 5,5 / 5. Esses valores são fixos – eles fazem com que tenhamos cinco saliências uniformemente espaçadas no gráfico.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Cada par de neurônios tem um valor de h associado a ele. Lembre-se, as conexões saídas dos neurônios têm pesos h e −h (não marcados). Ao alterar os pesos de saída, estamos realmente projetando a função!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Conforme alteramos as alturas, é possível ver a mudança correspondente nos valores h. E há também uma mudança nos pesos de saída correspondentes, que são + h e −h.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em outras palavras, podemos manipular diretamente a função que aparece no gráfico à direita e ver isso refletido nos valores h à esquerda.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas aqui consideramos uma entrada e uma saída, o que é bem simples. Com múltiplas entradas o conceito é basicamente o mesmo, mas iremos discutir as particularidades nos próximos capítulos, quando mergulharmos nas redes neurais profundas. Até lá.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning (material utilizado com autorização do autor)
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
 </div>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-833" href="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-833" href="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-833" href="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-833" href="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-833-5e0dd134c0ab2" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=833&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-833-5e0dd134c0ab2" id="like-post-wrapper-140353593-833-5e0dd134c0ab2">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-33">
 Capítulo 33 – Por que as Redes Neurais Profundas São Difíceis de Treinar?
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Iniciamos agora a terceira e última parte deste livro, em que estudaremos como funciona Deep Learning e os principais modelos e arquiteturas de redes neurais profundas, com diversos exemplos e aplicações. Mas primeiro temos que responder a seguinte pergunta: Por que as Redes Neurais Profundas São Difíceis de Treinar?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Imagine que você é um engenheiro que foi solicitado a projetar um computador do zero. Um dia, você está trabalhando em seu escritório, projetando circuitos lógicos, estabelecendo portas AND e OU, e assim por diante, quando seu chefe chega com más notícias. O cliente acaba de adicionar um requisito de design surpreendente: o circuito para o computador inteiro deve ter apenas duas camadas de profundidade:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="shallow_circuit" class="aligncenter size-full wp-image-894" data-attachment-id="894" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="shallow_circuit" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?fit=541%2C305" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?fit=300%2C169" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?fit=541%2C305" data-orig-size="541,305" data-permalink="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/shallow_circuit/" data-recalc-dims="1" height="305" sizes="(max-width: 541px) 100vw, 541px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?resize=541%2C305" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?w=541 541w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?resize=300%2C169 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?resize=200%2C113 200w" width="541"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você fica estupefato e diz ao seu chefe: “O cliente está louco!”
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Seu chefe responde: “Eu acho que eles são loucos também. Mas precisamos atender este requisito.”
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na verdade, há um sentido limitado em que o cliente não é louco. Suponha que você tenha permissão para usar uma porta lógica especial que permite a você aplicar o AND (o “e” da lógica) e juntar quantas entradas desejar. E você também tem permissão para uma porta NAND com muitas entradas, ou seja, uma porta que pode aplicar o AND a várias entradas e depois nega a saída. Com essas portas especiais, é possível calcular qualquer função usando um circuito com apenas duas camadas de profundidade.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas só porque algo é possível, não é uma boa ideia. Na prática, quando resolvemos problemas de projeto de circuitos (ou quase todos os tipos de problemas algorítmicos), geralmente começamos descobrindo como resolver sub-problemas, e então gradualmente integramos as soluções. Em outras palavras, criamos uma solução através de várias camadas de abstração.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por exemplo, suponha que estamos projetando um circuito lógico para multiplicar dois números. Provavelmente, queremos construí-lo a partir de sub-circuitos, fazendo operações como adicionar dois números. Os sub-circuitos para adicionar dois números serão, por sua vez, construídos a partir de sub-sub-circuitos para adicionar dois bits. Muito grosso modo, nosso circuito será parecido com:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="circuit_multiplication" class="aligncenter size-full wp-image-895" data-attachment-id="895" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="circuit_multiplication" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?fit=541%2C351" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?fit=300%2C195" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?fit=541%2C351" data-orig-size="541,351" data-permalink="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/circuit_multiplication/" data-recalc-dims="1" height="351" sizes="(max-width: 541px) 100vw, 541px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?resize=541%2C351" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?w=541 541w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?resize=300%2C195 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?resize=200%2C130 200w" width="541"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ou seja, nosso circuito final contém pelo menos três camadas de elementos de circuito. Na verdade, provavelmente conterá mais de três camadas, pois dividimos as sub-tarefas em unidades menores do que as descritas anteriormente. Mas você compreendeu a ideia geral.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Então circuitos profundos facilitam o processo de design. Mas eles não são apenas úteis para o design. Existem, de fato, provas matemáticas mostrando que, para algumas funções, circuitos muito superficiais requerem exponencialmente mais elementos de circuitos para serem computados do que circuitos profundos. Por exemplo, uma famosa série de
   <span style="text-decoration: underline;">
    <a href="https://eccc.weizmann.ac.il//report/2012/137/" rel="noopener" target="_blank">
     artigos
    </a>
   </span>
   no início dos anos 1980 mostrou que calcular a paridade de um conjunto de bits requer muitos portões exponencialmente, se feito com um circuito superficial. Por outro lado, se você usa circuitos mais profundos, é fácil calcular a paridade usando um pequeno circuito: basta calcular a paridade de pares de bits, depois usar esses resultados para calcular a paridade de pares de pares de bits e assim por diante. construindo rapidamente a paridade geral. Os circuitos profundos, portanto, podem ser intrinsecamente muito mais poderosos que os circuitos superficiais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até agora, este livro abordou redes neurais como o cliente louco. Quase todas as redes com as quais trabalhamos têm apenas uma camada oculta de neurônios (mais as camadas de entrada e saída):
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede neural profunda" class="aligncenter size-full wp-image-896" data-attachment-id="896" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede neural profunda" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?fit=333%2C274" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?fit=300%2C247" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?fit=333%2C274" data-orig-size="333,274" data-permalink="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/tikz35/" data-recalc-dims="1" height="274" sizes="(max-width: 333px) 100vw, 333px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?resize=333%2C274" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?w=333 333w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?resize=300%2C247 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?resize=200%2C165 200w" width="333"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essas redes simples têm sido extraordinariamente úteis: nos capítulos anteriores, usamos redes como essa para classificar dígitos manuscritos com precisão superior a 98%! No entanto, intuitivamente, esperamos que as redes com muito mais camadas ocultas sejam mais poderosas:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede neural profunda" class="aligncenter size-full wp-image-897" data-attachment-id="897" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede neural profunda" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?fit=560%2C279" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?fit=300%2C149" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?fit=560%2C279" data-orig-size="560,279" data-permalink="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/tikz36/" data-recalc-dims="1" height="279" sizes="(max-width: 560px) 100vw, 560px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?resize=560%2C279" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?w=560 560w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?resize=300%2C149 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?resize=200%2C100 200w" width="560"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Tais redes poderiam usar as camadas intermediárias para construir múltiplas camadas de abstração, assim como fazemos em circuitos booleanos. Por exemplo, se estamos fazendo reconhecimento de padrões visuais, então os neurônios da primeira camada podem aprender a reconhecer bordas, os neurônios da segunda camada podem aprender a reconhecer formas mais complexas, digamos, triângulo ou retângulos, construídos a partir de bordas. A terceira camada reconheceria formas ainda mais complexas. E assim por diante.
   <strong>
    Essas múltiplas camadas de abstração parecem propiciar às redes profundas uma vantagem convincente em aprender a resolver problemas complexos de reconhecimento de padrões.
   </strong>
   Além disso, assim como no caso dos circuitos, existem resultados teóricos sugerindo que as redes profundas são intrinsecamente mais poderosas do que as redes superficiais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como podemos treinar essas redes profundas? Nos próximos capítulos, tentaremos treinar redes profundas usando nosso algoritmo de aprendizado: descendente de gradiente estocástico por retropropagação (que já estudamos em detalhes nos capítulos anteriores, mas que agora aplicaremos em redes neurais profundas). Mas vamos nos deparar com problemas, com nossas redes profundas não realizando muito (se for o caso) melhor do que redes rasas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa falha parece surpreendente à luz da discussão acima. Em vez de desistir de redes profundas, vamos nos aprofundar e tentar entender o que está dificultando o treinamento de nossas redes profundas. Quando olharmos de perto, descobriremos que as diferentes camadas da nossa rede profunda estão aprendendo em velocidades muito diferentes. Em particular, quando as camadas posteriores da rede estão aprendendo bem, as camadas iniciais geralmente ficam presas durante o treinamento, aprendendo quase nada. Este empecilho não é simplesmente devido à má sorte. Em vez disso, descobriremos que existem razões fundamentais para a lentidão do aprendizado, conectadas ao nosso uso de técnicas de aprendizado baseadas em gradientes.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   À medida que nos aprofundamos no problema, aprenderemos que o fenômeno oposto também pode ocorrer: as primeiras camadas podem estar aprendendo bem, mas as camadas posteriores podem ficar presas. Na verdade, descobriremos que existe uma instabilidade intrínseca associada ao aprendizado por gradiente descendente em redes neurais profundas de muitas camadas. Essa instabilidade tende a resultar em camadas anteriores ou posteriores ficando presas durante o treinamento.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas, ao nos debruçarmos sobre essas dificuldades, podemos começar a entender o que é necessário para treinar redes profundas de maneira eficaz.
  </span>
  <span style="color: #000000;">
   E isso é exatamente o que faremos nos próximos capítulos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora é que começa a diversão. Até lá.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
 </div>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-884" href="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-884" href="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-884" href="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-884" href="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-884-5e0dd136d859d" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=884&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-884-5e0dd136d859d" id="like-post-wrapper-140353593-884-5e0dd136d859d">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-34">
 Capítulo 34 – O Problema da Dissipação do Gradiente
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Então,
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/" rel="noopener" target="_blank">
     por que as redes neurais profundas são difíceis de treinar
    </a>
   </span>
   ?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para responder a essa pergunta, primeiro revisitemos o caso de uma rede com apenas uma camada oculta. Como de costume, usaremos o problema de classificação de dígitos MNIST o mesmo já estudado nos capítulos anteriores e que você encontra no repositório deste livro no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr" rel="noopener" target="_blank">
     Github
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A partir de um shell do Python, nós carregamos os dados MNIST:
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="mnist" class="aligncenter wp-image-919 size-full" data-attachment-id="919" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="mnist" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?fit=1024%2C101" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?fit=300%2C29" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?fit=1302%2C128" data-orig-size="1302,128" data-permalink="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/image1-4/" data-recalc-dims="1" height="115" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=1170%2C115" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?w=1302 1302w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=300%2C29 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=768%2C76 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=1024%2C101 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=200%2C20 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=690%2C68 690w" width="1170"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Montamos nossa rede:
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="mnist" class="aligncenter wp-image-920 size-full" data-attachment-id="920" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="mnist" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?fit=672%2C128" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?fit=300%2C57" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?fit=672%2C128" data-orig-size="672,128" data-permalink="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/image2-4/" data-recalc-dims="1" height="128" sizes="(max-width: 672px) 100vw, 672px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?resize=672%2C128" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?w=672 672w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?resize=300%2C57 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?resize=200%2C38 200w" width="672"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esta rede possui 784 neurônios na camada de entrada, correspondendo a 28 × 28 = 784 pixels na imagem de entrada. Utilizamos 30 neurônios ocultos, assim como 10 neurônios de saída, correspondentes às 10 classificações possíveis para os dígitos MNIST (‘0’, ‘1’, ‘2’,…, ‘9’).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos tentar treinar nossa rede por 30 épocas completas, usando mini-lotes de 10 exemplos de treinamento por vez, uma taxa de aprendizado η = 0.1 e um parâmetro de regularização λ = 5.0. À medida que treinarmos, monitoramos a precisão da classificação no conjunto de dados validation_data. Podemos executar o script test.py com todos os comandos. Via prompt de comando ou terminal, digitamos:
   <strong>
    python test.py
   </strong>
   (o treinamento pode levar muitos minutos dependendo da velocidade do computador).
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="treinamento" class="aligncenter size-full wp-image-923" data-attachment-id="923" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="treinamento" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?fit=982%2C228" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?fit=300%2C70" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?fit=982%2C228" data-orig-size="982,228" data-permalink="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/image3-3/" data-recalc-dims="1" height="228" sizes="(max-width: 982px) 100vw, 982px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=982%2C228" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?w=982 982w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=300%2C70 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=768%2C178 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=200%2C46 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=690%2C160 690w" width="982"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao final do treinamento, obtemos uma precisão de classificação de 96,48% (aproximadamente), comparável a nossos resultados anteriores com uma configuração semelhante.
  </span>
  <span style="color: #000000;">
   Agora, vamos adicionar outra camada oculta, também com 30 neurônios, e tentar treinar com os mesmos hiperparâmetros. Usamos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   <span style="color: #000000;">
    net = network2.Network([784, 30, 30, 10])
   </span>
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isto dá uma melhor precisão de classificação 96,90%. Isso é encorajador: um pouco mais de profundidade está ajudando. Vamos adicionar outra camada oculta de 30 neurônios.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   <span style="color: #000000;">
    net = network2.Network([784, 30, 30, 30, 10])
   </span>
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso não ajuda em nada. Na verdade, o resultado cai para 96,57%, próximo à nossa rede original. E suponha que inserimos mais uma camada oculta.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: center;">
  <strong>
   <span style="color: #000000;">
    net = network2.Network([784, 30, 30, 30, 30, 10])
   </span>
  </strong>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esse comportamento parece estranho. Intuitivamente, camadas ocultas extras devem tornar a rede capaz de aprender funções de classificação mais complexas e, assim, fazer uma melhor classificação. Certamente, as coisas não devem piorar, já que as camadas extras podem, no pior dos casos, simplesmente não fazer nada. Mas não é isso que está acontecendo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Então, o que está acontecendo? Vamos supor que as camadas ocultas extras realmente possam ajudar em princípio e o problema é que nosso algoritmo de aprendizado não está encontrando os pesos e vieses corretos. Gostaríamos de descobrir o que está errado em nosso algoritmo de aprendizado e como fazer melhor.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para entender melhor o que está errado, vamos visualizar como a rede aprende. Abaixo, traçamos parte de uma rede [784,30,30,10], ou seja, uma rede com duas camadas ocultas, cada uma contendo 30 neurônios ocultos. Cada neurônio no diagrama tem uma pequena barra nele, representando a rapidez com que o neurônio está mudando à medida que a rede aprende. Uma barra grande significa que o peso e o viés do neurônio estão mudando rapidamente, enquanto uma barra pequena significa que os pesos e o viés estão mudando lentamente. Mais precisamente, as barras indicam o gradiente ∂C / ∂b para cada neurônio, ou seja, a taxa de mudança do custo em relação ao viés do neurônio. Nos capítulos anteriores, vimos que essa quantidade de gradiente controlava não apenas a rapidez com que o viés muda durante o aprendizado, mas também a rapidez com que os pesos inseridos no neurônio também mudam. Não se preocupe se você não se lembrar dos detalhes: a única coisa a ter em mente é simplesmente que essas barras mostram a rapidez com que os pesos e os vieses de cada neurônio mudam conforme a rede aprende.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para manter o diagrama simples, mostrei apenas os seis principais neurônios nas duas camadas ocultas. Eu omiti os neurônios de entrada, pois eles não têm pesos nem viés para aprender. Eu também omiti os neurônios de saída, já que estamos fazendo comparações por camadas, e faz mais sentido comparar camadas com o mesmo número de neurônios. Os resultados foram plotados no início do treinamento, ou seja, imediatamente após a inicialização da rede. Aqui estão eles:
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="rede" class="aligncenter size-full wp-image-924" data-attachment-id="924" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?fit=470%2C620" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?fit=227%2C300" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?fit=470%2C620" data-orig-size="470,620" data-permalink="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/download-2/" data-recalc-dims="1" height="620" sizes="(max-width: 470px) 100vw, 470px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?resize=470%2C620" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?w=470 470w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?resize=227%2C300 227w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?resize=200%2C264 200w" width="470"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A rede foi inicializada aleatoriamente e, portanto, não é surpreendente que haja muita variação na rapidez com que os neurônios aprendem. Ainda assim, uma coisa que vale ressaltar é que as barras na segunda camada oculta são em sua maioria muito maiores que as barras na primeira camada oculta. Como resultado, os neurônios da segunda camada oculta aprendem um pouco mais rápido que os neurônios da primeira camada oculta. Isso é meramente uma coincidência, ou os neurônios da segunda camada oculta provavelmente aprenderão mais rápido do que os neurônios na primeira camada oculta em geral?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para determinar se esse é o caso, é útil ter uma maneira global de comparar a velocidade de aprendizado na primeira e segunda camadas ocultas. Para fazer isso, vamos indicar o gradiente como δlj = ∂C / ∂blj, ou seja, o gradiente para o neurônio jth na camada lth.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos pensar no gradiente δ1 como um vetor cujas entradas determinam a rapidez com que a primeira camada oculta aprende, e δ2 como um vetor cujas entradas determinam a rapidez com que a segunda camada oculta aprende. Em seguida, usaremos os comprimentos desses vetores como medidas globais da velocidade na qual as camadas estão aprendendo. Assim, por exemplo, o comprimento “δ1” mede a velocidade na qual a primeira camada oculta está aprendendo, enquanto o comprimento “δ2” mede a velocidade na qual a segunda camada oculta está aprendendo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com essas definições, e na mesma configuração que foi plotada acima, encontramos δδ1 = 0.07… e δδ2 = 0.31…. Isso confirma nossa suspeita anterior: os neurônios na segunda camada oculta realmente estão aprendendo muito mais rápido que os neurônios da primeira camada oculta.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O que acontece se adicionarmos mais camadas ocultas? Se tivermos três camadas ocultas, em uma rede [784,30,30,30,10], então as respectivas velocidades de aprendizado serão 0,012, 0,060 e 0,283. Novamente, as camadas ocultas anteriores estão aprendendo muito mais lentamente que as camadas ocultas posteriores. Suponha que adicionemos mais uma camada com 30 neurônios ocultos. Nesse caso, as respectivas velocidades de aprendizado são 0,003, 0,017, 0,070 e 0,285. O padrão é válido: as camadas iniciais aprendem mais lentamente que as camadas posteriores.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Temos observado a velocidade de aprendizado no início do treinamento, ou seja, logo após as redes serem inicializadas. Como a velocidade do aprendizado muda à medida que treinamos nossas redes? Vamos voltar para ver a rede com apenas duas camadas ocultas. A velocidade de aprendizado muda da seguinte forma:
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="training_speed_2_layers" class="aligncenter size-full wp-image-926" data-attachment-id="926" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="training_speed_2_layers" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?fit=800%2C600" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?fit=300%2C225" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?fit=800%2C600" data-orig-size="800,600" data-permalink="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/training_speed_2_layers/" data-recalc-dims="1" height="600" sizes="(max-width: 800px) 100vw, 800px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=800%2C600" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?w=800 800w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=300%2C225 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=768%2C576 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=200%2C150 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=690%2C518 690w" width="800"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para gerar esses resultados, usamos a descida do gradiente em lote com apenas 1.000 imagens de treinamento, treinadas em mais de 500 épocas. Isso é um pouco diferente do que normalmente treinamos nos capítulos anteriores, mas acontece que o uso de gradiente estocástico em mini-lote dá resultados muito mais ruidosos (embora muito similares, quando você mede o ruído). Usar os parâmetros que escolhemos é uma maneira fácil de suavizar os resultados, para que possamos ver o que está acontecendo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em qualquer caso, como você pode ver, as duas camadas começam a aprender em velocidades muito diferentes (como já sabemos). A velocidade em ambas as camadas cai muito rapidamente, antes de se recuperar. Mas, apesar de tudo, a primeira camada oculta aprende muito mais lentamente do que a segunda camada oculta.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   E quanto a redes mais complexas? Aqui estão os resultados de uma experiência semelhante, mas desta vez com três camadas ocultas (uma rede [784,30,30,30,10]):
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="training_speed_3_layers" class="aligncenter size-full wp-image-927" data-attachment-id="927" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="training_speed_3_layers" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?fit=800%2C600" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?fit=300%2C225" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?fit=800%2C600" data-orig-size="800,600" data-permalink="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/training_speed_3_layers/" data-recalc-dims="1" height="600" sizes="(max-width: 800px) 100vw, 800px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=800%2C600" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=768%2C576 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=200%2C150 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=690%2C518 690w" width="800"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mais uma vez, as primeiras camadas ocultas aprendem muito mais lentamente do que as camadas ocultas posteriores. Finalmente, vamos adicionar uma quarta camada oculta (uma rede [784,30,30,30,30,10]) e ver o que acontece quando treinamos:
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="training_speed_4_layers" class="aligncenter size-full wp-image-928" data-attachment-id="928" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="training_speed_4_layers" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?fit=800%2C600" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?fit=300%2C225" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?fit=800%2C600" data-orig-size="800,600" data-permalink="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/training_speed_4_layers/" data-recalc-dims="1" height="600" sizes="(max-width: 800px) 100vw, 800px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=800%2C600" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?w=800 800w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=300%2C225 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=768%2C576 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=200%2C150 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=690%2C518 690w" width="800"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mais uma vez, as primeiras camadas ocultas aprendem muito mais lentamente do que as camadas ocultas posteriores. Nesse caso, a primeira camada oculta está aprendendo aproximadamente 100 vezes mais lenta que a camada oculta final. Natural que estivéssemos tendo problemas para treinar essas redes antes!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Temos aqui uma observação importante: em pelo menos algumas redes neurais profundas, o gradiente tende a diminuir à medida que nos movemos para trás através das camadas ocultas. Isso significa que os neurônios nas camadas anteriores aprendem muito mais lentamente que os neurônios nas camadas posteriores. E, embora tenhamos visto isso em apenas uma única rede, há razões fundamentais pelas quais isso acontece em muitas redes neurais. O fenômeno é conhecido como
   <strong>
    O Problema da Dissipação do Gradiente
   </strong>
   ou
   <strong>
    The Vanishing Gradient Problem
   </strong>
   . Esse é um problema muito comum e ainda mais evidente em Redes Neurais Recorrentes, usadas em aplicações de Processamento de Linguagem Natural.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por que o problema de dissipação do gradiente ocorre? Existem maneiras de evitar isso? E como devemos lidar com isso no treinamento de redes neurais profundas? Na verdade, aprenderemos rapidamente que não é inevitável, embora a alternativa também não seja muito atraente: às vezes, o gradiente fica muito maior nas camadas anteriores! Este problema é chamado de explosão do gradiente, e não é uma notícia muito melhor do que o problema da dissipação do gradiente. Geralmente, verifica-se que o gradiente em redes neurais profundas é instável, tendendo a explodir ou a desaparecer nas camadas anteriores. Essa instabilidade é um problema fundamental para o aprendizado baseado em gradiente em redes neurais profundas. É algo que precisamos entender e, se possível, tomar medidas para resolver.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Momentaneamente se afastando das redes neurais, imagine que estamos tentando minimizar numericamente uma função f(x) de uma única variável. Não seria uma boa notícia se a derivada f′(x) fosse pequena? Isso não significaria que já estávamos perto de um extremo? De forma semelhante, o pequeno gradiente nas primeiras camadas de uma rede profunda pode significar que não precisamos fazer muito ajuste dos pesos e vieses?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Claro, isso não é o caso. Lembre-se de que inicializamos aleatoriamente o peso e os vieses na rede. É extremamente improvável que nossos pesos e vieses iniciais façam um bom trabalho em qualquer coisa que desejamos que nossa rede faça. Para ser concreto, considere a primeira camada de pesos em uma rede [784,30,30,30,10] para o problema MNIST. A inicialização aleatória significa que a primeira camada elimina a maior parte das informações sobre a imagem de entrada. Mesmo que as camadas posteriores tenham sido extensivamente treinadas, elas ainda acharão extremamente difícil identificar a imagem de entrada, simplesmente porque elas não possuem informações suficientes. E assim, não é possível que não seja preciso aprender muito na primeira camada. Se vamos treinar redes profundas, precisamos descobrir como resolver o problema da dissipação do gradiente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Se eu fosse você, não perderia o próximo capítulo com uma explicação matemática para esse importante fenômeno no treinamento de redes neurais profundas (Deep Learning).
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
   (material usado com autorização do autor)
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
 </div>
 <div class="sharedaddy sd-sharing-enabled">
 </div>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-914" href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-914" href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-914" href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-914" href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-914-5e0dd138d62f0" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=914&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-914-5e0dd138d62f0" id="like-post-wrapper-140353593-914-5e0dd138d62f0">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-35">
 Capítulo 35 – A Matemática do Problema de Dissipação do Gradiente em Deep Learning
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos continuar a discussão iniciada no capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/" rel="noopener" target="_blank">
     anterior
    </a>
   </span>
   . Para entender porque o problema da dissipação do gradiente ocorre, vamos considerar a rede neural profunda mais simples: uma com apenas um único neurônio em cada camada. Aqui está uma rede com três camadas ocultas:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="tikz37" class="aligncenter size-full wp-image-956" data-attachment-id="956" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tikz37" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?fit=545%2C47" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?fit=300%2C26" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?fit=545%2C47" data-orig-size="545,47" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/tikz37/" data-recalc-dims="1" height="47" sizes="(max-width: 545px) 100vw, 545px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?resize=545%2C47" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?w=545 545w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?resize=300%2C26 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?resize=200%2C17 200w" width="545"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui, w1, w2,… são os pesos, b1, b2,… são os vieses e C é alguma função de custo. Apenas para lembrar como isso funciona, a saída aj do neurônio j é σ(zj), onde σ é a função de ativação sigmóide usual, e zj = wjaj − 1 + bj é a entrada ponderada para o neurônio. Eu desenhei o custo C no final para enfatizar que o custo é uma função da saída da rede, a4: se a saída real da rede estiver próxima da saída desejada, então o custo será baixo, enquanto se estiver longe, o custo será alto.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos estudar o gradiente ∂C / ∂b1 associado ao primeiro neurônio oculto. Definiremos uma expressão para ∂C / ∂b1 e, estudando essa expressão, entenderemos porque o problema da dissipação do gradiente ocorre.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vou começar simplesmente mostrando a expressão para ∂C / ∂b1. Parece assustador, mas na verdade tem uma estrutura simples, que descreverei em breve. Aqui está a expressão (ignore a rede, por enquanto, e note que σ ′ é apenas a derivada da função σ):
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="tikz38" class="aligncenter size-full wp-image-957" data-attachment-id="957" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tikz38" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?fit=552%2C96" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?fit=300%2C52" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?fit=552%2C96" data-orig-size="552,96" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/tikz38/" data-recalc-dims="1" height="96" sizes="(max-width: 552px) 100vw, 552px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?resize=552%2C96" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?w=552 552w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?resize=300%2C52 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?resize=200%2C35 200w" width="552"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A estrutura na expressão é a seguinte: existe um termo σ ′ (zj) no produto para cada neurônio na rede; um peso wj para cada peso na rede; e um termo final ∂C / ∂a4, correspondente à função de custo no final. Observe que coloquei cada termo na expressão acima da parte correspondente da rede. Então a rede em si é um mnemônico para a expressão.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Há também uma explicação simples de porque a expressão acima é verdadeira e, portanto, é divertido (e talvez esclarecedor) dar uma olhada nessa explicação.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Imagine que fazemos uma pequena mudança Δb1 no viés b1. Isso irá desencadear uma série de mudanças em cascata no resto da rede. Primeiro, causa uma mudança Δa1 na saída do primeiro neurônio oculto. Isso, por sua vez, causará uma mudança Δz2 na entrada ponderada para o segundo neurônio oculto. Então, uma mudança Δa2 na saída do segundo neurônio oculto. E assim por diante, até chegar a uma mudança de C no custo na saída. Nós temos
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter size-full wp-image-958" data-attachment-id="958" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?fit=288%2C168" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?fit=288%2C168" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?fit=288%2C168" data-orig-size="288,168" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form1-5/" data-recalc-dims="1" height="168" sizes="(max-width: 288px) 100vw, 288px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?resize=288%2C168" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?w=288 288w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?resize=200%2C117 200w" width="288"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso sugere que podemos descobrir uma expressão para o gradiente ∂C / ∂b1, acompanhando cuidadosamente o efeito de cada etapa dessa cascata.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para fazer isso, vamos pensar em como Δb1 faz com que a saída a1 do primeiro neurônio oculto mude. Nós temos a1 = σ (z1) = σ (w1a0 + b1), então:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-959" data-attachment-id="959" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?fit=566%2C250" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?fit=300%2C133" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?fit=566%2C250" data-orig-size="566,250" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form2-9/" data-recalc-dims="1" height="250" sizes="(max-width: 566px) 100vw, 566px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?resize=566%2C250" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?w=566 566w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?resize=300%2C133 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?resize=200%2C88 200w" width="566"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esse termo σ ′ (z1) deve parecer familiar: é o primeiro termo em nossa expressão reivindicada para o gradiente ∂C / ∂b1. Intuitivamente, esse termo converte uma mudança Δb1 no viés em uma mudança Δa1 na ativação de saída. Essa mudança Δa1 por sua vez causa uma mudança na entrada ponderada z2 = w2a1 + b2 para o segundo neurônio oculto:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form3" class="aligncenter size-full wp-image-960" data-attachment-id="960" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?fit=380%2C242" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?fit=300%2C191" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?fit=380%2C242" data-orig-size="380,242" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form3-8/" data-recalc-dims="1" height="242" sizes="(max-width: 380px) 100vw, 380px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?resize=380%2C242" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?w=380 380w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?resize=300%2C191 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?resize=200%2C127 200w" width="380"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Combinando nossas expressões para Δz2 e Δa1, vemos como a mudança no viés b1 se propaga ao longo da rede para afetar z2:
  </span>
 </p>
 <p>
  <img alt="form" class="aligncenter size-full wp-image-974" data-attachment-id="974" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?fit=450%2C134" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?fit=300%2C89" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?fit=450%2C134" data-orig-size="450,134" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form-7/" data-recalc-dims="1" height="134" sizes="(max-width: 450px) 100vw, 450px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?resize=450%2C134" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?w=450 450w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?resize=300%2C89 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?resize=200%2C60 200w" width="450"/>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Novamente, isso deve parecer familiar: agora temos os dois primeiros termos em nossa expressão reivindicada para o gradiente ∂C / ∂b1.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos continuar dessa maneira, rastreando a maneira como as alterações se propagam pelo resto da rede. Em cada neurônio, pegamos um termo σ ′ (zj) e, em cada peso, escolhemos um termo wj. O resultado final é uma expressão que relaciona a mudança final ΔC no custo para a mudança inicial Δb1 no viés:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form5" class="aligncenter size-full wp-image-961" data-attachment-id="961" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?fit=814%2C160" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?fit=300%2C59" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?fit=814%2C160" data-orig-size="814,160" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form5-4/" data-recalc-dims="1" height="160" sizes="(max-width: 814px) 100vw, 814px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=814%2C160" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?w=814 814w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=300%2C59 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=768%2C151 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=200%2C39 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=690%2C136 690w" width="814"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Dividindo por Δb1, de fato, obtemos a expressão desejada para o gradiente:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form6" class="aligncenter size-full wp-image-962" data-attachment-id="962" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form6" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?fit=750%2C178" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?fit=300%2C71" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?fit=750%2C178" data-orig-size="750,178" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form6-3/" data-recalc-dims="1" height="178" sizes="(max-width: 750px) 100vw, 750px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=750%2C178" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?w=750 750w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=300%2C71 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=200%2C47 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=690%2C164 690w" width="750"/>
  </span>
 </p>
 <h2>
 </h2>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Por que o problema da dissipação do gradiente ocorre afinal?
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para entender porque o problema da dissipação do gradiente ocorre, vamos escrever explicitamente a expressão inteira para o gradiente:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form7" class="aligncenter size-full wp-image-963" data-attachment-id="963" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form7" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?fit=938%2C168" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?fit=300%2C54" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?fit=938%2C168" data-orig-size="938,168" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form7-2/" data-recalc-dims="1" height="168" sizes="(max-width: 938px) 100vw, 938px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=938%2C168" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?w=938 938w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=300%2C54 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=768%2C138 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=200%2C36 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=690%2C124 690w" width="938"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com exceção do último termo, essa expressão é um produto de termos da forma wjσ ′ (zj). Para entender como cada um desses termos se comporta, vamos ver um gráfico da função σ ′:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form9" class="aligncenter wp-image-965 size-full" data-attachment-id="965" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form9" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?fit=1022%2C696" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?fit=300%2C204" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?fit=1022%2C696" data-orig-size="1022,696" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form9/" data-recalc-dims="1" height="696" sizes="(max-width: 1022px) 100vw, 1022px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=1022%2C696" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?w=1022 1022w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=300%2C204 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=768%2C523 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=200%2C136 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=690%2C470 690w" width="1022"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A derivada atinge um máximo em σ ′ (0) = 1/4. Agora, se usarmos nossa abordagem padrão para inicializar os pesos na rede, escolheremos os pesos usando uma distribuição normal (Gaussiana) com média 0 e desvio padrão 1. Assim, os pesos geralmente satisfazem | wj | &lt; 1. Reunindo essas observações, vemos que os termos wjσ ′ (zj) geralmente satisfazem | wjσ ′ (zj) | &lt; 1/4. E quando tomamos um produto de muitos desses termos, o produto tenderá a diminuir exponencialmente: quanto mais termos, menor será o produto. Isso está começando a cheirar como uma possível explicação para o problema da dissipação do gradiente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para tornar tudo isso um pouco mais explícito, vamos comparar a expressão para ∂C / ∂b1 com uma expressão para o gradiente em relação a um viés posterior, digamos ∂C / ∂b3. Naturalmente, não explicamos explicitamente uma expressão para ∂C / ∂b3, mas segue o mesmo padrão descrito acima para ∂C / ∂b1. Aqui está a comparação das duas expressões:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form10" class="aligncenter wp-image-966 size-large" data-attachment-id="966" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form10" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?fit=1024%2C542" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?fit=300%2C159" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?fit=1180%2C624" data-orig-size="1180,624" data-permalink="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form10/" data-recalc-dims="1" height="542" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=1024%2C542" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=1024%2C542 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=300%2C159 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=768%2C406 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=200%2C106 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=690%2C365 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?w=1180 1180w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As duas expressões compartilham muitos termos. Mas o gradiente ∂C / ∂b1 inclui dois termos extras, cada um da forma wjσ ′ (zj). Como vimos, esses termos são tipicamente menores que 1/4 de magnitude. E assim o gradiente ∂C / ∂b1 normalmente será um fator de 16 (ou mais) menor que ∂C / ∂b3. Esta é a origem essencial do problema da dissipação do gradiente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   É claro que este é um argumento informal, não uma prova rigorosa de que o problema da dissipação do gradiente ocorrerá. Existem várias cláusulas de escape possíveis. Em particular, podemos nos perguntar se os pesos wj poderiam crescer durante o treinamento. Se o fizerem, é possível que os termos wjσ ′ (zj) no produto deixem de satisfazer | wjσ ′ (zj) | &lt; 1/4. De fato, se os termos se tornarem grandes o suficiente – maiores que 1 – então não teremos mais um problema de dissipação do gradiente. Em vez disso, o gradiente crescerá exponencialmente à medida que nos movemos para trás pelas camadas. Em vez de um problema de dissipação do gradiente, teremos um problema de explosão do gradiente. Mas isso é assunto para o próximo capítulo!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para aprender todos os detalhes matemáticos por trás desse processo, confira nosso curso único e exclusivo no Brasil:
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener" target="_blank">
     Matemática Para Machine Learning
    </a>
   </span>
   . Até o próximo capítulo.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
   <span style="text-decoration: underline;">
    Formação Inteligência Artificial
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
    Neural Networks and Deep Learning
   </a>
   (material usado com autorização do autor)
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-955" href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-955" href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-955" href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-955" href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-955-5e0dd13b939f6" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=955&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-955-5e0dd13b939f6" id="like-post-wrapper-140353593-955-5e0dd13b939f6">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-36">
 Capítulo 36 – Outros Problemas com o Gradiente em Redes Neurais Artificiais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/" rel="noopener" target="_blank">
     anterior
    </a>
   </span>
   descrevemos para você a Matemática que ajuda a explicar a causa do problema da dissipação do gradiente. Mas a dissipação não é o único problema que pode ocorrer. Neste capítulo vamos descrever outros possíveis problemas com o gradiente em redes neurais artificiais.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Explosão do Gradiente
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos ver um exemplo explícito em que ocorre a explosão dos gradientes. O exemplo é bem simples e vamos alterar alguns parâmetros na rede de maneira a garantir que tenhamos a explosão do gradiente. Mesmo fazendo essa alteração para forçar o problema, a explosão do gradiente não é apenas uma possibilidade hipotética e realmente pode acontecer.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Há duas etapas para obter a explosão do gradiente. Primeiro, escolhemos todos os pesos na rede como grandes, digamos w1 = w2 = w3 = w4 = 100. Segundo, vamos escolher os vieses para que os termos σ ′ (zj) não sejam muito pequenos. Isso é realmente muito fácil de fazer: tudo o que precisamos é escolher os vieses para garantir que a entrada ponderada para cada neurônio seja zj = 0 (e então σ ′ (zj) = 1/4). Então, por exemplo, queremos z1 = w1a0 + b1 = 0. Podemos conseguir isso ajustando b1 = −100 ∗ a0. Podemos usar a mesma ideia para selecionar os outros vieses. Quando fazemos isso, vemos que todos os termos wjσ ′ (zj) são iguais a 100 ∗ 1/4 = 25. Com estas escolhas ocorre o problema da explosão do gradiente.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Instabilidade do Gradiente
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O problema fundamental aqui não é tanto o problema do gradiente que desaparece ou o problema do gradiente que explode. É que o gradiente nas camadas iniciais é o produto dos termos de todas as camadas posteriores. Quando há muitas camadas, essa é uma situação intrinsecamente instável. A única maneira que todas as camadas podem aprender perto da mesma velocidade é se todos esses produtos de termos estiverem próximos de se equilibrar. Sem algum mecanismo ou razão subjacente para que o equilíbrio ocorra, é altamente improvável que aconteça simplesmente por acaso. Em suma, o problema real aqui é que as redes neurais sofrem de um problema de instabilidade do gradiente. Como resultado, se usarmos técnicas de aprendizado baseadas em gradiente padrão, camadas diferentes na rede tenderão a aprender em velocidades totalmente diferentes.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   O Problema Mais Comum é Mesmo a Dissipação do Gradiente
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vimos que o gradiente pode desaparecer ou explodir nas camadas iniciais de uma rede profunda. De fato, ao usar neurônios sigmóides, o gradiente geralmente desaparece. Para ver porque, considere novamente a expressão | wσ ′ (z) |. Para evitar o problema da dissipação do gradiente, precisamos | wσ ′ (z) | ≥1. Você pode pensar que isso pode acontecer facilmente se w for muito grande. No entanto, é mais difícil do que parece. A razão é que o termo σ ′ (z) também depende de w: σ ′ (z) = σ ′ (wa + b), onde a é a ativação da entrada. Então, quando fazemos w grande, precisamos ter cuidado para que não tornemos simultaneamente σ ′ (wa + b) pequeno. Isso acaba sendo uma restrição considerável. A razão é que quando fazemos w grande, tendemos a tornar o wa + b muito grande. Olhando para um gráfico de σ ′ você pode ver que isso nos coloca fora das “asas” da função σ ′, onde é preciso valores muito pequenos. A única maneira de evitar isso é se a ativação de entrada estiver dentro de um intervalo bastante estreito de valores. Às vezes isso vai acontecer, mas frequentemente, porém, isso não acontece. E assim, no caso genérico, temos a dissipação dos gradientes.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Gradientes Instáveis em Redes Mais Complexas
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Temos estudado redes simples como exemplo, com apenas um neurônio em cada camada oculta. E quanto a redes profundas mais complexas, com muitos neurônios em cada camada oculta (tipicamente Deep Learning)?
  </span>
 </p>
 <p style="text-align: justify;">
  <p style="text-align: justify;">
   <span style="color: #000000;">
    <img alt="network" class="aligncenter size-full wp-image-986" data-attachment-id="986" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="network" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?fit=560%2C279" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?fit=300%2C149" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?fit=560%2C279" data-orig-size="560,279" data-permalink="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/network-2/" data-recalc-dims="1" height="279" sizes="(max-width: 560px) 100vw, 560px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?resize=560%2C279" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?w=560 560w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?resize=300%2C149 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?resize=200%2C100 200w" width="560"/>
   </span>
  </p>
  <p>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    De fato, o mesmo comportamento ocorre em tais redes. Nos capítulos anteriores onde estudamos retropropagação, vimos que o gradiente na camada l de uma rede de camada L é dado por:
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    <img alt="form" class="aligncenter size-full wp-image-987" data-attachment-id="987" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?fit=406%2C48" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?fit=300%2C35" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?fit=406%2C48" data-orig-size="406,48" data-permalink="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/form-8/" data-recalc-dims="1" height="48" sizes="(max-width: 406px) 100vw, 406px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?resize=406%2C48" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?w=406 406w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?resize=300%2C35 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?resize=200%2C24 200w" width="406"/>
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Aqui, Σ ′ (zl) é uma matriz diagonal cujas entradas são os valores σ ′ (z) para as entradas ponderadas para a camada l. As wl são as matrizes de peso para as diferentes camadas. E ∇aC é o vetor de derivadas parciais de C em relação às ativações de saída.
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Essa é uma expressão muito mais complicada do que no caso de um único neurônio. Ainda assim, se você olhar de perto, a forma essencial é muito semelhante, com muitos pares da forma:
   </span>
  </p>
  <p>
   <img alt="form2" class="aligncenter size-full wp-image-991" data-attachment-id="991" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?fit=91%2C31" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?fit=91%2C31" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?fit=91%2C31" data-orig-size="91,31" data-permalink="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/form2-10/" data-recalc-dims="1" height="31" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?resize=91%2C31" width="91"/>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Além disso, as matrizes Σ′ (zj) possuem pequenas entradas na diagonal, nenhuma maior que 1/4. Desde que as matrizes de peso wj não sejam muito grandes, cada termo adicional:
   </span>
   <span style="color: #000000;">
    <img alt="form3" class="aligncenter size-full wp-image-992" data-attachment-id="992" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?fit=92%2C34" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?fit=92%2C34" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?fit=92%2C34" data-orig-size="92,34" data-permalink="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/form3-9/" data-recalc-dims="1" height="34" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?resize=92%2C34" width="92"/>
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    tende a fazer o vetor gradiente menor, levando a uma dissipação do gradiente. Mais genericamente, o grande número de termos no produto tende a levar a um gradiente instável, assim como no nosso exemplo anterior. Na prática isso é tipicamente encontrado em redes sigmóides que os gradientes desaparecem exponencialmente de forma rápida nas camadas anteriores. Como resultado, o aprendizado diminui nessas camadas. Essa desaceleração não é apenas um acidente ou uma inconveniência: é uma consequência fundamental da abordagem que estamos adotando para o aprendizado da rede.
   </span>
  </p>
  <h3 style="text-align: justify;">
   <span style="color: #000000;">
    Outros Obstáculos Para a Aprendizagem Profunda
   </span>
  </h3>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Nestes últimos capítulos, nos concentramos na dissipação de gradientes – e, em geral, gradientes instáveis – como um obstáculo à aprendizagem profunda. De fato, gradientes instáveis são apenas um obstáculo para o aprendizado profundo, embora seja um importante obstáculo fundamental. Muitas pesquisas em andamento têm como objetivo entender melhor os desafios que podem ocorrer quando se treinam redes profundas. Vamos mencionar brevemente alguns artigos, para dar a você o sabor de algumas das perguntas que as pessoas estão fazendo.
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Como primeiro exemplo, em 2010
    <span style="text-decoration: underline;">
     <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener" target="_blank">
      Glorot e Bengio
     </a>
    </span>
    encontraram evidências sugerindo que o uso de funções de ativação sigmóide pode causar problemas ao treinamento de redes profundas. Em particular, eles encontraram evidências de que o uso de sigmóides fará com que as ativações na camada oculta final saturem perto de 0 no início do treinamento, diminuindo substancialmente o aprendizado. Eles sugeriram algumas funções de ativação alternativas, que parecem não sofrer tanto com esse problema de saturação.
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Como um segundo exemplo, em 2013,
    <span style="text-decoration: underline;">
     <a href="http://www.cs.toronto.edu/~hinton/absps/momentum.pdf" rel="noopener" target="_blank">
      Sutskever, Martens, Dahl e Hinton
     </a>
    </span>
    estudaram o impacto na aprendizagem profunda tanto da inicialização de peso aleatório quanto do cronograma de momentum na descida de gradiente estocástica baseada no momento. Em ambos os casos, fazer boas escolhas fez uma diferença substancial na capacidade de treinar redes profundas.
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Esses exemplos sugerem que “O que dificulta o treinamento de redes profundas?” é uma questão complexa. Nestes últimos capítulos, nos concentramos nas instabilidades associadas ao aprendizado baseado em gradiente em redes profundas. Os resultados dos dois últimos parágrafos sugerem que há também um papel desempenhado pela escolha da função de ativação, a forma como os pesos são inicializados e até mesmo detalhes de como a aprendizagem por gradiente descendente é implementada. E, claro, a escolha da arquitetura de rede e outros hiperparâmetros também é importante. Assim, muitos fatores podem desempenhar um papel em dificultar a formação de redes profundas, e a compreensão de todos esses fatores ainda é objeto de pesquisas em andamento. A boa notícia é que, a partir do próximo capítulo, vamos mudar isso e desenvolver várias abordagens para o aprendizado profundo que, até certo ponto, conseguem superar ou direcionar todos esses desafios.
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Não é incrível o que estamos vivenciando neste exato momento da história humana? Tudo isso que estudamos até aqui forma a base de aplicações de Inteligência Artificial que já são encontradas no mercado, em diversas aplicações e até mesmo em nossos smartphones. E ainda estamos apenas no começo.
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    E você, quer ou não fazer parte desta incrível revolução trazida pela Inteligência Artificial? Se a resposta for sim, o que está esperando?
   </span>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    Referências:
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">
     Neural Networks and Deep Learning
    </a>
    (material usado com autorização do autor)
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" target="_blank">
     Machine Learning
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </p>
  <p>
  </p>
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-984" href="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-984" href="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-984" href="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-984" href="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-984-5e0dd13dd9917" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=984&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-984-5e0dd13dd9917" id="like-post-wrapper-140353593-984-5e0dd13dd9917">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-37">
 Capítulo 37 – O Efeito do Batch Size no Treinamento de Redes Neurais Artificiais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A partir deste capítulo você vai compreender em mais detalhes a arquitetura dos principais modelos de
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-i" rel="noopener" target="_blank">
     Deep Learning
    </a>
   </span>
   , com ênfase nas escolhas dos hiperparâmetros e abordagens de treinamento. Vamos começar com O Efeito do Batch Size no Treinamento de Redes Neurais Artificiais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste experimento, vamos investigar o efeito do tamanho do lote (Batch Size) na dinâmica de treinamento. Tamanho do lote (Batch Size) é um termo usado em aprendizado de máquina e refere-se ao número de exemplos de treinamento usados em uma iteração. O Batch Size pode ser uma das três opções:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    <strong>
     batch mode
    </strong>
    : onde o tamanho do lote é igual ao conjunto de dados total, tornando os valores de iteração e épocas equivalentes.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    <strong>
     mini-batch mode
    </strong>
    : onde o tamanho do lote é maior que um, mas menor que o tamanho total do conjunto de dados. Geralmente, um número que pode ser dividido no tamanho total do conjunto de dados.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    <strong>
     stochastic mode
    </strong>
    : onde o tamanho do lote é igual a um. Portanto, o gradiente e os parâmetros da rede neural são atualizados após cada amostra.
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A métrica em que nos concentraremos é o
   <em>
    gap de generalização
   </em>
   , que é definido como a diferença entre o valor do tempo de treinamento e o valor do tempo de teste. Vamos investigar o tamanho do lote no contexto da classificação de imagens (o mesmo usado em diversos capítulos anteriores). Especificamente, usaremos o conjunto de dados MNIST.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No nosso caso, a diferença de generalização é simplesmente a diferença entre a precisão da classificação no tempo de teste e o tempo de treinamento. Estas experiências foram destinadas a fornecer alguma intuição básica sobre os efeitos do tamanho do lote. É bem conhecido na comunidade de aprendizado de máquina que a dificuldade de fazer afirmações gerais sobre os efeitos de hiperparâmetros geralmente varia de conjunto de dados a conjunto de dados e modelo a modelo. Portanto, as conclusões que fazemos aqui só podem servir como indicações em vez de declarações gerais sobre o tamanho do lote.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O tamanho do lote é um dos hiperparâmetros mais importantes para sintonizar os modernos sistemas de aprendizagem profunda. Os
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" target="_blank">
     Cientistas de Dados
    </a>
   </span>
   muitas vezes querem usar um tamanho de lote maior para treinar seu modelo, uma vez que permite acelerações computacionais do paralelismo das GPUs. No entanto, é bem conhecido que um tamanho de lote muito grande levará a uma generalização deficiente (embora atualmente não se saiba exatamente porque isso acontece). Para as funções convexas que estamos tentando otimizar, há uma disputa inerente entre os benefícios de tamanhos de lotes menores e maiores.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por um lado, usar um lote igual a todo o conjunto de dados garante a convergência para o ótimo global da função objetivo. No entanto, isso é à custa de uma convergência empírica mais lenta para esse ótimo. Por outro lado, o uso de tamanhos menores de lotes mostrou empiricamente uma convergência mais rápida para soluções “boas”. Isso é intuitivamente explicado pelo fato de que tamanhos de lote menores permitem que o modelo “inicie o aprendizado antes de ver todos os dados”. A desvantagem de usar um tamanho de lote menor é que não há garantia que o modelo vai convergir para o ótimo global. Ele irá saltar em torno do ótimo global, dependendo da relação entre o tamanho do lote e o tamanho do conjunto de dados. Portanto, sob nenhuma restrição computacional, muitas vezes é aconselhável que se comece com um pequeno tamanho de lote, colhendo os benefícios de uma dinâmica de treinamento mais rápida, e aumente o tamanho do lote por meio de treinamento, aproveitando também os benefícios da convergência garantida.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Observou-se empiricamente que tamanhos de lote menores não só têm uma dinâmica de treinamento mais rápida, mas também generalização para o conjunto de dados de teste versus tamanhos de lote maiores. Mas esta afirmação tem seus limites. Sabemos que um tamanho de lote de 1 geralmente funciona muito mal. É geralmente aceito que existe um “ponto ideal” para o tamanho do lote entre 1 e todo o conjunto de dados de treinamento que fornecerá a melhor generalização. Esse “ponto ideal” geralmente depende do conjunto de dados e do modelo em questão. A razão para uma melhor generalização é vagamente atribuída à existência de “ruído” no treinamento de pequeno tamanho de lote. Como os sistemas de redes neurais são extremamente propensos a ajustes excessivos (overfitting), a ideia é que a visualização de vários tamanhos de lote pequenos, cada lote sendo uma representação “ruidosa” de todo o conjunto de dados, causará uma espécie de dinâmica de “tug-and-pull”. Essa dinâmica evita que a rede neural se ajuste excessivamente no conjunto de treinamento e, portanto, tenha um desempenho ruim no conjunto de testes.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Definição do Problema
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O problema exato que será investigado é o da classificação. Dada uma imagem X, o objetivo é prever o rótulo da imagem y. No caso do conjunto de dados MNIST, X são imagens em preto-e-branco dos dígitos 0 a 9 e y são as etiquetas de dígitos correspondentes “0” a “9”. Nosso modelo de escolha é uma rede neural. Especificamente, usaremos um Perceptron Multicamada (MLP).
  </span>
  <span style="color: #000000;">
   Salvo disposição em contrário, este é o modelo padrão foi usado no experimento:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    2 camadas ocultas totalmente conectadas (FC), 1024 unidades cada
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Função de ativação ReLU
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Função de perda: logaritmo negativo
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Otimizador: SGD (Stochastic Gradient Descent)
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Taxa de aprendizagem: 0,01
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Épocas: 30
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em última análise, a pergunta que queremos responder é “qual tamanho de lote devo usar ao treinar uma rede neural?”
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Efeito do Tamanho do Lote
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A primeira coisa que devemos fazer para confirmar o problema que estamos tentando investigar é mostrar a dependência entre o intervalo de generalização e o tamanho do lote. Eu tenho me referido à métrica que estamos considerando como “gap de generalização”. Essa é tipicamente a medida de autores sobre o tema usada em artigos e papers, mas para simplicidade em nosso estudo nós apenas nos preocuparemos com a precisão do teste sendo a mais alta possível. Como veremos, a precisão de treinamento e teste dependerá do tamanho do lote, por isso é mais significativo falar sobre a precisão de teste em vez do gap de generalização. Mais especificamente, queremos que a precisão do teste depois de um grande número de épocas de treinamento, seja alta. Quantas épocas é um “grande número de épocas”? Idealmente, isso é definido como o número de épocas de treinamento necessárias, de modo que qualquer treinamento adicional forneça pouco ou nenhum aumento na precisão de teste. Na prática, isso é difícil de determinar e teremos que adivinhar quantas épocas são apropriadas para alcançar um comportamento ideal. Apresento as precisões de teste do nosso modelo de rede neural treinado usando diferentes tamanhos de lote abaixo (para aprender a fazer tudo isso na prática, clique
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" target="_blank">
     aqui
    </a>
   </span>
   ).
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="grafico" class="aligncenter size-full wp-image-1005" data-attachment-id="1005" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="grafico" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?fit=646%2C232" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?fit=300%2C108" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?fit=646%2C232" data-orig-size="646,232" data-permalink="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/grafico/" data-recalc-dims="1" height="232" sizes="(max-width: 646px) 100vw, 646px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?resize=646%2C232" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?w=646 646w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?resize=300%2C108 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?resize=200%2C72 200w" width="646"/>
 </p>
 <p>
  <img alt="grafico2" class="aligncenter size-full wp-image-1006" data-attachment-id="1006" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="grafico2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?fit=646%2C235" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?fit=300%2C109" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?fit=646%2C235" data-orig-size="646,235" data-permalink="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/grafico2/" data-recalc-dims="1" height="235" sizes="(max-width: 646px) 100vw, 646px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?resize=646%2C235" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?w=646 646w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?resize=300%2C109 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?resize=200%2C73 200w" width="646"/>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Curvas em laranja: tamanho do lote 64
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em verde: tamanho do lote 256
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em lilás: tamanho do lote 1024
  </span>
 </p>
 <p style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Descoberta: tamanhos maiores de lotes levam a uma precisão menor nos dados de teste.
   </span>
  </strong>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O eixo x mostra o número de épocas de treinamento. O eixo y é rotulado para cada plotagem. MNIST é obviamente um conjunto de dados fácil de treinar; podemos alcançar 100% de precisão em treino e 98% em teste com apenas nosso modelo MLP base no tamanho de lote 64. Além disso, vemos uma clara tendência entre o tamanho do lote e a precisão do teste (e treinamento!). A nossa primeira conclusão é a seguinte: maiores tamanhos de lotes levam a uma menor precisão nos dados de teste. Esses padrões parecem existir em seu extremo para o conjunto de dados MNIST. Eu tentei tamanho de lote igual a 2 e alcançou uma precisão de teste ainda melhor de 99% (versus 98% para tamanho de lote 64)! Como aviso prévio, não espere que tamanhos de lote muito baixos, como 2, funcionem bem em conjuntos de dados mais complexos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas neste experimento não consideramos alterações na taxa de aprendizagem. Poderíamos aumentar a acurácia em teste com tamanhos de lote maiores, ajustando a taxa de aprendizagem (learning rate)? Não perca o próximo capítulo para descobrir! Até lá.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="color: #0000ff;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener" style="color: #0000ff;" target="_blank">
    <span style="text-decoration: underline;">
     Formação Inteligência Artificial
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Don’t Decay the Learning Rate, Increase the Batch Size
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Train longer, generalize better: closing the generalization gap in large batch training of neural networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Effect of batch size on training dynamics
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Neural Networks and Deep Learning
    </a>
    (material usado com autorização do autor)
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #0000ff; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener" style="color: #0000ff; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-995" href="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-995" href="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-995" href="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-995" href="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-995-5e0dd14016664" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=995&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-995-5e0dd14016664" id="like-post-wrapper-140353593-995-5e0dd14016664">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-38">
 Capítulo 38 – O Efeito da Taxa de Aprendizagem no Treinamento de Redes Neurais Artificiais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos retomar a discussão do capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/" rel="noopener noreferrer" target="_blank">
     anterior
    </a>
   </span>
   e tentar melhorar a precisão do modelo nos dados de teste a partir de um tamanho de lote maior, aumentando a taxa de aprendizado (learning rate). Vamos estudar O Efeito da Taxa de Aprendizagem no Treinamento de Redes Neurais Artificiais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Algumas pesquisas na literatura sobre otimização em Machine Learning mostraram que aumentar a taxa de aprendizado pode compensar tamanhos maiores de lotes. Com isso em mente, aumentamos a taxa de aprendizado do nosso modelo para ver se podemos recuperar a precisão nos dados de teste (que havia sido reduzida quando aumentando o tamanho do lote). Os gráficos abaixo mostram as conclusões:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="graf1" class="aligncenter size-full wp-image-1022" data-attachment-id="1022" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graf1" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?fit=645%2C237" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?fit=300%2C110" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?fit=645%2C237" data-orig-size="645,237" data-permalink="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf1/" data-recalc-dims="1" height="237" sizes="(max-width: 645px) 100vw, 645px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?resize=645%2C237" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?w=645 645w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?resize=300%2C110 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?resize=200%2C73 200w" width="645"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="graf2" class="aligncenter size-full wp-image-1023" data-attachment-id="1023" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graf2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?fit=647%2C244" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?fit=300%2C113" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?fit=647%2C244" data-orig-size="647,244" data-permalink="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf2/" data-recalc-dims="1" height="244" sizes="(max-width: 647px) 100vw, 647px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?resize=647%2C244" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?w=647 647w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?resize=300%2C113 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?resize=200%2C75 200w" width="647"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Curvas em laranja: tamanho do lote 64, taxa de aprendizagem 0.01 (referência)
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em lilás: tamanho do lote 1024, taxa de aprendizado de 0,01 (referência)
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em azul: tamanho do lote 1024, taxa de aprendizagem 0,1
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As curvas em laranja e lilás são para referência e são as mesmas do conjunto de gráficos do capítulo anterior. Como a curva lilás, a curva azul treina com um tamanho de lote grande de 1024. No entanto, a curva azul tem uma taxa de aprendizado aumentada em 10 vezes.
   <strong>
    Curiosamente, podemos recuperar a precisão nos dados de teste a partir de um tamanho de lote maior, aumentando a taxa de aprendizado
   </strong>
   . Usando um tamanho de lote de 64 (laranja) atinge uma precisão de teste de 98%, enquanto o uso de um tamanho de lote de 1024 atinge apenas cerca de 96%. Mas aumentando a taxa de aprendizado, usando um tamanho de lote de 1024 também alcança uma precisão de teste de 98%. Assim como com nossa conclusão anterior, tenha cautela ao analisar esses resultados. Sabe-se que o simples aumento da taxa de aprendizado não compensa totalmente grandes tamanhos de lotes em conjuntos de dados mais complexos do que o MNIST.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   E qual o efeito ao reduzir o tamanho do lote durante o treinamento do modelo?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A próxima pergunta interessante a ser feita é se o treinamento com lotes grandes “inicia você em um caminho ruim do qual você não pode se recuperar”. Ou seja, se começarmos a treinar com tamanho de lote 1024, então mudamos para tamanho de lote 64, podemos ainda alcançar a maior precisão em teste de 98%?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Investigamos três casos: treinar usando um tamanho de lote pequeno para uma única época, mudar para um tamanho de lote grande, treinar usando um tamanho de lote pequeno para muitas épocas e mudar para um tamanho maior de lote e treinar usando um tamanho grande de lote e então usar uma taxa de aprendizado mais alta com o mesmo tamanho de lote. Resultados abaixo:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="graf3" class="aligncenter size-full wp-image-1024" data-attachment-id="1024" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graf3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?fit=656%2C239" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?fit=300%2C109" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?fit=656%2C239" data-orig-size="656,239" data-permalink="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf3/" data-recalc-dims="1" height="239" sizes="(max-width: 656px) 100vw, 656px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?resize=656%2C239" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?w=656 656w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?resize=300%2C109 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?resize=200%2C73 200w" width="656"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="graf4" class="aligncenter size-full wp-image-1025" data-attachment-id="1025" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graf4" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?fit=653%2C248" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?fit=300%2C114" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?fit=653%2C248" data-orig-size="653,248" data-permalink="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf4/" data-recalc-dims="1" height="248" sizes="(max-width: 653px) 100vw, 653px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?resize=653%2C248" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?w=653 653w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?resize=300%2C114 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?resize=200%2C76 200w" width="653"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Curvas em laranja: treinar em tamanho de lote 64 por 30 épocas (referência)
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em amarelo neon: treinar em tamanho de lote 1024 por 60 épocas (referência)
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em verde: treinar em tamanho de lote 1024 para 1 época e depois mudar para tamanho de lote 64 por 30 épocas (total de 31 épocas)
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em amarelo escuro: treinar em tamanho de lote 1024 por 30 épocas e depois mudar para tamanho de lote 64 por 30 épocas (total de 60 épocas)
  </span>
  <br/>
  <span style="color: #000000;">
   Curvas em lilás: treinamento em tamanho de lote 1024 e aumento da taxa de aprendizado em 10x na época 31 (total de 60 épocas)
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como antes, as curvas em laranja são para um tamanho de lote pequeno. As curvas em amarelo neon servem como um controle para garantir que não estamos melhorando a precisão do teste porque estamos simplesmente treinando mais. Se você prestar muita atenção ao eixo x, as épocas são enumeradas de 0 a 30. Isso ocorre porque somente as últimas 30 épocas de treinamento são mostradas. Para experimentos com mais de 30 épocas de treinamento no total, as primeiras x – 30 épocas foram omitidas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   É difícil ver as outras 3 linhas porque elas estão sobrepostas, mas não importa, porque nos três casos recuperamos a precisão em teste de 98%! Em conclusão, começar com um tamanho grande de lote não “pega o modelo preso” em alguma vizinhança de ótimos locais ideais. O modelo pode alternar para um tamanho de lote menor ou uma taxa de aprendizado mais alta a qualquer momento para obter uma precisão de teste melhor.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para compreender o comportamento matemático por trás de todo esse processo, precisamos trazer o gradiente para esta discussão. É o que faremos no próximo capítulo. Até lá.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
   <span style="text-decoration: underline;">
    Formação Inteligência Artificial
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e" rel="noopener noreferrer" target="_blank">
    Effect of batch size on training dynamics
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1021" href="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1021" href="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1021" href="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1021" href="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1021-5e0dd14a71fc1" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1021&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1021-5e0dd14a71fc1" id="like-post-wrapper-140353593-1021-5e0dd14a71fc1">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-39">
 Capítulo 39 – Relação Entre o Tamanho do Lote e o Cálculo do Gradiente
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos continuar com a discussão dos dois
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/" rel="noopener noreferrer" target="_blank">
     capítulos anteriores
    </a>
   </span>
   e investigar a Relação Entre o Tamanho do Lote e o Cálculo do Gradiente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como explicar porque o treinamento com lotes maiores leva a uma precisão menor nos testes? Uma hipótese pode ser que as amostras de treinamento no mesmo lote interfiram (competem) com o gradiente um do outro. Uma amostra deseja mover os pesos do modelo em uma direção, enquanto outra amostra deseja mover os pesos na direção oposta. Portanto, seus gradientes tendem a ser cancelados e você obtém um pequeno gradiente geral. Talvez, se as amostras forem divididas em dois lotes, a concorrência será reduzida, pois o modelo poderá encontrar pesos que satisfarão as duas amostras, se forem feitas em sequência. Em outras palavras, a otimização sequencial de amostras é mais fácil do que a otimização simultânea em espaços de parâmetros complexos e de alta dimensão.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A hipótese é representada graficamente abaixo. A seta lilás mostra um único degrau de gradiente descendente usando um tamanho de lote de 2. As setas azul e vermelha mostram duas etapas sucessivas de descida do gradiente usando um tamanho de lote 1. A seta preta é a soma vetorial das setas azul e vermelha e representa o progresso geral que o modelo faz em duas etapas de tamanho de lote 1. Ambos os experimentos começam com os mesmos pesos no espaço de pesos. Embora não seja explicitamente mostrado na imagem, a hipótese é que a linha lilás é muito mais curta que a linha preta, devido à concorrência dos gradientes. Em outras palavras, o gradiente de uma única etapa de tamanho de lote grande é menor que a soma de gradientes de várias etapas de tamanho de lote pequeno.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="grad" class="aligncenter size-full wp-image-1035" data-attachment-id="1035" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="grad" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?fit=200%2C188" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?fit=200%2C188" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?fit=200%2C188" data-orig-size="200,188" data-permalink="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/grad/" data-recalc-dims="1" height="188" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?resize=200%2C188" width="200"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O experimento envolve a replicação da imagem mostrada acima. Nós treinamos o modelo para um determinado estado. Em seguida, o grupo de controle (seta lilás) é calculado encontrando o gradiente de etapa única com tamanho de lote 1024. O grupo experimental (seta preta) é calculado fazendo várias etapas de gradiente e encontrando a soma vetorial desses gradientes usando um tamanho de lote menor. O produto do número de etapas e do tamanho do lote é constante em 1024. Isso representa modelos diferentes que veem um número fixo de amostras.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por exemplo, para um tamanho de lote de 64, fazemos 1024/64 = 16 etapas, somando os 16 gradientes para encontrar o gradiente geral. Para tamanho de lote 1024, fazemos 1024/1024 = 1 passo. Observe que, para os tamanhos de lote menores, amostras diferentes são desenhadas para cada lote. A ideia é comparar os gradientes do modelo para diferentes tamanhos de lotes após os modelos terem visto o mesmo número de amostras. Como última advertência, para simplificar, apenas medimos o gradiente da última camada do nosso modelo MLP (Multilayer Perceptron), que possui 1024 ⋅ 10 = 10240 pesos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Investigamos os seguintes tamanhos de lote: 1, 2, 3, 4, 5, 6, 7, 8, 16, 32, 64, 128, 256, 512, 1024.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Um teste significou:
  </span>
 </p>
 <ol>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Carregamento / redefinição dos pesos do modelo para um ponto treinado fixo (usamos os pesos do modelo após o treinamento para 2/30 epochs em tamanho de lote 1024).
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Amostragem aleatória de 1024 amostras de dados do conjunto de treinamento.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Treinamento do modelo através de todas as 1024 amostras de dados uma vez, com diferentes tamanhos de lote.
   </span>
  </li>
 </ol>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para cada tamanho de lote, repetimos o experimento mil vezes. Não coletamos mais dados porque armazenar os tensores de gradiente é realmente muito caro (mantivemos os tensores de cada tentativa para computar estatísticas de ordem mais alta mais tarde). O tamanho total do arquivo com tensores de gradiente foi de 600 MB.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para cada um dos 1000 ensaios, computamos a norma euclidiana do tensor de gradiente somado (seta preta na nossa imagem). Então, calculamos a média e o desvio padrão dessas normas ao longo dos 1000 testes. Isso é feito para cada tamanho de lote.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Eu queria investigar dois regimes de peso diferentes: no início do treinamento, quando os pesos não convergiam e muito aprendizado ocorria e, mais tarde, durante o treinamento, quando os pesos quase convergiam e o aprendizado mínimo estava ocorrendo. Para o regime inicial, eu treinei o modelo MLP para 2 épocas com tamanho de lote 1024 e para o regime posterior, eu treinei o modelo por 30 épocas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="graph" class="aligncenter size-full wp-image-1036" data-attachment-id="1036" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="graph" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?fit=578%2C434" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?fit=300%2C225" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?fit=578%2C434" data-orig-size="578,434" data-permalink="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/graph-2/" data-recalc-dims="1" height="434" sizes="(max-width: 578px) 100vw, 578px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?resize=578%2C434" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?w=578 578w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?resize=200%2C150 200w" width="578"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Descoberta: tamanhos de lotes maiores produzem etapas de gradiente maiores do que tamanhos de lotes menores para o mesmo número de amostras vistas.
   </span>
  </strong>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O eixo x mostra o tamanho do lote. O eixo y mostra a norma euclidiana média de tensores de gradiente em 1000 tentativas. As barras de erro indicam a variação da norma euclidiana em 1000 tentativas. Os pontos azuis é o experimento realizado no regime inicial, onde o modelo foi treinado por 2 épocas. Os pontos verdes é o regime posterior em que o modelo foi treinado por 30 épocas. Como esperado, o gradiente é maior no início do treinamento (os pontos azuis são maiores que os pontos verdes).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao contrário da nossa hipótese, a norma gradiente média aumenta com o tamanho do lote! Esperávamos que os gradientes fossem menores para um tamanho de lote maior devido à competição entre as amostras de dados. Em vez disso, o que encontramos é que tamanhos maiores de lotes fazem etapas de gradiente maiores do que tamanhos de lote menores para o mesmo número de amostras vistas. Observe que a norma euclidiana pode ser interpretada como a distância euclidiana entre o novo conjunto de pesos e o conjunto inicial de pesos. Portanto, o treinamento com lotes grandes tende a se afastar dos pesos iniciais depois de ver um número fixo de amostras do que o treinamento com tamanhos de lote menores. A relação entre o tamanho do lote e a norma de gradiente é √x. Em outras palavras, a relação entre o tamanho do lote e a norma do gradiente quadrado é linear.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Além disso, a variação é muito menor para tamanhos menores de lotes. No entanto, o que podemos nos interessar é a magnitude da variância em relação à magnitude da média. Portanto, para fazer uma comparação mais perspicaz, dimensiono a média e o desvio padrão de cada tamanho de lote para a média do tamanho do lote 1024. Em outras palavras,
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form" class="aligncenter size-full wp-image-1037" data-attachment-id="1037" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?fit=468%2C328" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?fit=300%2C210" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?fit=468%2C328" data-orig-size="468,328" data-permalink="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/form-9/" data-recalc-dims="1" height="328" sizes="(max-width: 468px) 100vw, 468px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?resize=468%2C328" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?w=468 468w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?resize=300%2C210 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?resize=200%2C140 200w" width="468"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   onde as barras representam valores normalizados e i indica um determinado tamanho de lote.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Justificamos o dimensionamento da média e do desvio padrão da norma de gradiente porque isso é equivalente a aumentar a taxa de aprendizado para o experimento com tamanhos de lote menores. Essencialmente, queremos saber “pela mesma distância afastada dos pesos iniciais, qual é a variação nas normas de gradiente para diferentes tamanhos de lote”? Tenha em mente que estamos medindo a variação nas normas de gradiente e não a variação nos gradientes em si, que é uma métrica muito mais precisa.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-1038" data-attachment-id="1038" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?fit=583%2C420" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?fit=300%2C216" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?fit=583%2C420" data-orig-size="583,420" data-permalink="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/form2-11/" data-recalc-dims="1" height="420" sizes="(max-width: 583px) 100vw, 583px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?resize=583%2C420" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?w=583 583w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?resize=300%2C216 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?resize=200%2C144 200w" width="583"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Descoberta: Para a mesma distância média da norma euclidiana dos pesos iniciais do modelo, tamanhos de lote maiores têm maior variância na distância.
   </span>
  </strong>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nós vemos um resultado muito surpreendente acima. Para a mesma distância média da norma euclidiana dos pesos iniciais do modelo, tamanhos de lote maiores têm maior variância na distância. Isso é bastante! Em suma, dados dois modelos treinados com tamanhos de lotes diferentes, em qualquer gradiente em particular, se a taxa de aprendizado for ajustada de modo que ambos os modelos se movam em média na mesma distância, o modelo com o tamanho maior do lote variará mais em relação a sua movimentação. Isso é um pouco contra-intuitivo, pois é bem conhecido que tamanhos menores de lotes são “ruidosos” e, portanto, você pode esperar que a variação da norma de gradiente seja maior. Observe que, para cada teste, estamos usando 1024 amostras diferentes, em vez de usar as mesmas 1024 amostras em todos os testes. Observe também que a variação entre os testes pode ser causada por duas coisas: as diferentes amostras que são extraídas do conjunto de dados entre os diferentes testes e a semente aleatória para cada teste (que não foi controlada, mas deve realmente ser). Seguindo em frente, vou supor que a variação é causada pelo primeiro fator: amostras diferentes. Este é um resultado interessante e poderíamos seguir esta investigação por muitos capítulos. Mas por hora, você já recebeu informação suficiente para compreender a Relação Entre o Tamanho do Lote e o Cálculo do Gradiente. Esse tipo de experimento é muito importante em trabalhos de pesquisa e desenvolvimento de novos modelos!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos assim concluir mais uma etapa deste livro e começar nossa última e mais emocionante jornada de aprendizagem, estudando as principais arquiteturas de Deep Learning. Eu não perderia se fosse você! Até o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000;" target="_blank">
    <span style="text-decoration: underline;">
     Formação Inteligência Artificial
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Effect of batch size on training dynamics
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1034" href="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1034" href="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1034" href="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1034" href="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1034-5e0dd14c9510a" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1034&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1034-5e0dd14c9510a" id="like-post-wrapper-140353593-1034-5e0dd14c9510a">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-40">
 Capítulo 40 – Introdução as Redes Neurais Convolucionais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nos primeiros capítulos deste livro ensinamos nossas redes neurais a fazer um bom trabalho reconhecendo imagens de dígitos manuscritos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="digits" class="aligncenter wp-image-1063 size-medium" data-attachment-id="1063" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="digits" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?fit=623%2C128" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?fit=300%2C62" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?fit=623%2C128" data-orig-size="623,128" data-permalink="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/digits-3/" data-recalc-dims="1" height="62" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?resize=300%2C62" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?resize=300%2C62 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?resize=200%2C41 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?w=623 623w" width="300"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Fizemos isso usando redes nas quais camadas adjacentes são totalmente conectadas umas às outras. Ou seja, todos os neurônios da rede estão conectados a todos os neurônios em camadas adjacentes:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede" class="aligncenter size-full wp-image-1064" data-attachment-id="1064" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?fit=560%2C279" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?fit=300%2C149" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?fit=560%2C279" data-orig-size="560,279" data-permalink="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/rede-10/" data-recalc-dims="1" height="279" sizes="(max-width: 560px) 100vw, 560px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?resize=560%2C279" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?w=560 560w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?resize=300%2C149 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?resize=200%2C100 200w" width="560"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em particular, para cada pixel na imagem de entrada, codificamos a intensidade do pixel como o valor de um neurônio correspondente na camada de entrada. Para as imagens de 28 × 28 pixels que estamos usando, isso significa que nossa rede tem 784 (= 28 × 28) neurônios de entrada. Em seguida, treinamos os pesos e vieses da rede para que a saída da rede identificasse corretamente a imagem de entrada: ‘0’, ‘1’, ‘2’, …, ‘8’ ou ‘9’.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nossas redes anteriores funcionam muito bem: obtivemos uma precisão de classificação melhor que 98%, usando dados de treinamento e teste do conjunto de dados de dígitos manuscritos MNIST. Mas, após reflexão, é estranho usar redes com camadas totalmente conectadas para classificar imagens. A razão é que tal arquitetura de rede não leva em conta a estrutura espacial das imagens. Por exemplo, ela trata os pixels de entrada que estão distantes e próximos exatamente no mesmo nível. Tais conceitos de estrutura espacial devem ser inferidos dos dados de treinamento. Mas e se, em vez de começarmos com uma arquitetura de rede que é rasa, utilizássemos uma arquitetura que tenta tirar proveito da estrutura espacial? É onde entram as redes neurais convolucionais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essas redes usam uma arquitetura especial que é particularmente bem adaptada para classificar imagens. O uso dessa arquitetura torna as redes convolucionais rápidas de treinar. Isso, por sua vez, nos ajuda a treinar redes profundas de muitas camadas, que são muito boas na classificação de imagens. Hoje, redes neurais convolucionais ou alguma variante próxima são usadas na maioria das redes neurais para reconhecimento de imagem.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As origens das redes neurais convolucionais remontam aos anos 70. Mas o artigo seminal que estabeleceu o tema moderno das redes convolucionais foi um artigo de 1998, “
   <span style="text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-based learning applied to document recognition
    </a>
   </span>
   “, de Yann LeCun, Léon Bottou, Yoshua Bengio e Patrick Haffner. Desde então, LeCun fez uma observação interessante sobre a terminologia para redes convolucionais: “A inspiração neural [biológica] em modelos como redes convolucionais é muito tênue. É por isso que eu os chamo de ‘redes convolucionais’ e não ‘redes neurais convolucionais’, e por isso os nós eu chamo de ‘unidades’ e não ‘neurônios’ “. Apesar desta observação, as redes convolucionais usam muitas das mesmas ideias que as redes neurais que estudamos até agora: ideias como retropropagação, gradiente descendente, regularização, funções de ativação não lineares e assim por diante. E assim, vamos seguir a prática comum e considerá-las um tipo de rede neural. Usarei os termos “rede neural convolucional” e “rede convolucional” alternadamente. Também usarei os termos “neurônio [artificial]” e “unidade” alternadamente.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Definição
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma Rede Neural Convolucional (ConvNet / Convolutional Neural Network / CNN) é um algoritmo de Aprendizado Profundo que pode captar uma imagem de entrada, atribuir importância (pesos e vieses que podem ser aprendidos) a vários aspectos / objetos da imagem e ser capaz de diferenciar um do outro. O pré-processamento exigido em uma ConvNet é muito menor em comparação com outros algoritmos de classificação. Enquanto nos métodos primitivos os filtros são feitos à mão, com treinamento suficiente, as ConvNets têm a capacidade de aprender esses filtros / características.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A arquitetura de uma ConvNet é análoga àquela do padrão de conectividade de neurônios no cérebro humano e foi inspirada na organização do Visual Cortex. Os neurônios individuais respondem a estímulos apenas em uma região restrita do campo visual conhecida como Campo Receptivo. Uma coleção desses campos se sobrepõe para cobrir toda a área visual. Veremos isso em detalhes mais a frente!
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Por que usar ConvNets e não rede feed-forward?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma imagem não é nada além de uma matriz de valores de pixels, certo? Então, por que não apenas achatar a imagem (por exemplo, converter uma matriz 3×3 em um vetor 9×1. Se a image é uma matriz, nenhum problema em converter em uma vetor) e alimentá-lo para um Perceptron Multi-Layer para fins de classificação? Na verdade não.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em casos de imagens binárias extremamente básicas, o método pode mostrar uma pontuação de precisão média durante a previsão de classes, mas teria pouca ou nenhuma precisão quando se trata de imagens complexas com dependências de pixel por toda parte.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma ConvNet é capaz de capturar com sucesso as dependências espaciais e temporais em uma imagem através da aplicação de filtros relevantes. A arquitetura executa um melhor ajuste ao conjunto de dados da imagem devido à redução no número de parâmetros envolvidos e à capacidade de reutilização dos pesos. Em outras palavras, a rede pode ser treinada para entender melhor a sofisticação da imagem.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede2" class="aligncenter size-full wp-image-1065" data-attachment-id="1065" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?fit=550%2C400" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?fit=300%2C218" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?fit=550%2C400" data-orig-size="550,400" data-permalink="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/rede2-7/" data-recalc-dims="1" height="400" sizes="(max-width: 550px) 100vw, 550px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?resize=550%2C400" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?w=550 550w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?resize=300%2C218 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?resize=200%2C145 200w" width="550"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na figura acima, temos uma imagem RGB (Red – Green – Blue) que foi separada por seus três planos coloridos – Vermelho, Verde e Azul. Existem vários desses espaços de cores nos quais existem imagens – Escala de cinza, RGB, HSV, CMYK etc.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você pode imaginar como a computação ficaria intensiva assim que as imagens atingissem dimensões, digamos, 8K (7680 × 4320). A função da ConvNet é reduzir as imagens para uma forma mais fácil de processar, sem perder recursos que são críticos para obter uma boa previsão. Isso é importante quando queremos projetar uma arquitetura que não seja apenas boa em recursos de aprendizado, mas que também seja escalável para conjuntos de dados massivos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa é uma das arquiteturas de Deep Learning mais incríveis e com mais aplicações práticas e dedicaremos alguns capítulos a este arquitetura.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As redes neurais convolucionais usam três ideias básicas: campos receptivos locais, pesos compartilhados e pooling. Vamos dar uma olhada em cada uma dessas ideias? Então não perca os próximos capítulos!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Obs: caso esteja muito ansioso, você já pode começar a estudar essa arquitetura agora mesmo. Em nosso curso gratuito
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=microsoft-power-bi-para-data-science" rel="noopener noreferrer" target="_blank">
     Microsoft Power BI Para Data Science
    </a>
   </span>
   , o bônus do curso no último capítulo é um exemplo completo passo a passo em linguagem Python mostrando essa arquitetura. Se quiser ir mais a fundo e estudar a arquitetura em detalhes, então confira os cursos
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-i" rel="noopener noreferrer" target="_blank">
     Deep Learning I
    </a>
   </span>
   e
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=viso-computacional-e-reconhecimento-de-imagem" rel="noopener noreferrer" target="_blank">
     Visão Computacional
    </a>
   </span>
   , onde a arquitetura é explicada nos mínimos detalhes e usada em diversas aplicações práticas.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Don’t Decay the Learning Rate, Increase the Batch Size
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Train longer, generalize better: closing the generalization gap in large batch training of neural networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     A Comprehensive Guide to Convolutional Neural Networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks and Deep Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1062" href="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1062" href="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1062" href="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1062" href="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1062-5e0dd157391df" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1062&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1062-5e0dd157391df" id="like-post-wrapper-140353593-1062-5e0dd157391df">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-41">
 Capítulo 41 – Campos Receptivos Locais em Redes Neurais Convolucionais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos estudar em detalhes a partir de agora as Redes Neurais Convolucionais, uma das principais arquiteturas de Deep Learning, amplamente usada em Visão Computacional. E começaremos compreendendo o que são os Campos Receptivos Locais. Mas antes, afinal, o que é Visão Computacional, amplamente usada em aplicações de
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
     Inteligência Artificial
    </a>
   </span>
   ?
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   O Que é Visão Computacional?
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Imagine a seguinte situação: você está em uma sala com mais duas pessoas. Uma delas arremessa uma bola e você a pega com as mãos. Nada poderia ser mais simples, certo?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Errado. Na verdade, este é um dos processos mais complexos que já tentamos compreender, ou seja, como o cérebro processa a visão de modo que sabemos exatamente o que é uma bola e quando ela está vindo em nossa direção? E ensinar uma máquina que seja capaz de ver da mesma forma que nós seres humanos é uma tarefa realmente difícil, não só porque é difícil fazer computadores executarem um cálculo matemático que reproduza a visão humana, mas porque não estamos inteiramente certos de como o processo da visão realmente funciona.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Primeiro vamos descrever de forma sucinta e aproximada como ocorre o processo de visão no caso do arremesso da bola: a imagem da esfera passa através de seu olho e chega a sua retina, que faz alguma análise elementar e envia o resultado ao cérebro, onde o córtex visual analisa mais profundamente a imagem. Em seguida, ele envia para o resto do córtex, que compara a tudo o que já sabe, classifica os objetos e dimensões e, finalmente, decide sobre algo a fazer: levantar a mão e pegar a bola (tendo previsto o seu caminho). Isso ocorre em uma pequena fração de segundo, com quase nenhum esforço consciente e quase nunca falha. Assim, recriar a visão humana não é apenas um problema difícil, é um conjunto deles, cada um dos quais depende do outro.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=viso-computacional-e-reconhecimento-de-imagem" rel="noopener noreferrer" target="_blank">
     Visão Computacional
    </a>
   </span>
   é o processo de modelagem e replicação da visão humana usando software e hardware. A Visão Computacional é uma disciplina que estuda como reconstruir, interromper e compreender uma cena 3d a partir de suas imagens 2d em termos das propriedades da estrutura presente na cena.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Visão Computacional e reconhecimento de imagem são termos frequentemente usados como sinônimos, mas o primeiro abrange mais do que apenas analisar imagens. Isso porque, mesmo para os seres humanos, “ver” também envolve a percepção em muitas outras frentes, juntamente com uma série de análises. Cada ser humano usa cerca de dois terços do seu cérebro para o processamento visual, por isso não é nenhuma surpresa que os computadores precisariam usar mais do que apenas o reconhecimento de imagem para obter sua visão de forma correta.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O reconhecimento de imagens em si – a análise de pixel e padrão de imagens – é uma parte integrante do processo de Visão Computacional que envolve tudo, desde reconhecimento de objetos e caracteres até análise de texto e sentimento. O reconhecimento de imagem de hoje, ainda na maior parte, apenas identifica objetos básicos como “uma banana ou uma bicicleta em uma imagem.” Mesmo crianças podem fazer isso, mas o potencial da Visão Computacional é sobre-humano: ser capaz de ver claramente no escuro, através de paredes, em longas distâncias e processar todos esses dados rapidamente e em volume maciço.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Já a Visão Computacional em seu sentido mais pleno está sendo usada na vida cotidiana e nos negócios para conduzir todos os tipos de tarefas, incluindo identificar doenças médicas em raios-x, identificar produtos e onde comprá-los, anúncios dentro de imagens editoriais, entre outros. A Visão Computacional pode ser usada para digitalizar plataformas de mídia social a fim de encontrar imagens relevantes que não podem ser descobertas por meio de pesquisas tradicionais. A tecnologia é complexa e, assim como todas as tarefas acima mencionadas, requer mais do que apenas reconhecimento de imagem, mas também análise semântica de grandes conjuntos de dados. E a Visão Computacional é a principal técnica por trás dos veículos autônomos, que devem mudar completamente o mundo como o conhecemos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ninguém nunca disse que isso seria fácil. Exceto, talvez, Marvin Minsky, pioneiro da Inteligência Artificial, que, em 1966, instruía um estudante de pós-graduação a “conectar uma câmera a um computador e fazer com que descrevesse o que vê”. Mas a verdade é que 50 anos depois, ainda estamos trabalhando nisso, porém agora há uma diferença (ou duas talvez): temos Big Data e Processamento Paralelo em GPU’s. E acredite. Isso está realmente fazendo a diferença.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As Redes Neurais Convolucionais formam uma das arquiteturas de Deep Learning mais amplamente usada em tarefas de Visão Computacional e reconhecimento de imagens e a partir de agora vamos compreender porque.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Campos Receptivos Locais
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nas camadas totalmente conectadas mostradas no capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/" rel="noopener noreferrer" target="_blank">
     anterior
    </a>
   </span>
   , as entradas foram representadas como uma linha vertical de neurônios. Ou seja, convertemos uma imagem 28 x 28 (que é uma matriz) em um vetor de apenas uma dimensão (falaremos sobre isso mais adiante). Mas para compreender o que é um campo receptivo local, vamos considerar a imagem de seu formato padrão de 28x 28 (tamanho das imagens no dataset MNIST). Em uma imagem, cada pixel é um valor numérico que representa a intensidade de cor de acordo com a escala de cor utilizada, como RGB (Red – Green – Blue), por exemplo, ou apenas intensidade em escala de cinza para imagens em preto e branco.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede1" class="aligncenter size-full wp-image-1082" data-attachment-id="1082" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?fit=236%2C258" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?fit=236%2C258" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?fit=236%2C258" data-orig-size="236,258" data-permalink="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede1-2/" data-recalc-dims="1" height="258" sizes="(max-width: 236px) 100vw, 236px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?resize=236%2C258" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?w=236 236w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?resize=200%2C219 200w" width="236"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como de costume, vamos conectar os pixels de entrada a uma camada de neurônios ocultos. Mas não vamos conectar todos os pixels de entrada a cada neurônio oculto. Em vez disso, apenas fazemos conexões em regiões pequenas e localizadas da imagem de entrada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para ser mais preciso, cada neurônio na primeira camada oculta será conectado a uma pequena região dos neurônios de entrada, digamos, por exemplo, uma região de 5 × 5, correspondendo a 25 pixels de entrada. Assim, para um neurônio oculto em particular, podemos ter conexões que se parecem com isso:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede2" class="aligncenter size-full wp-image-1083" data-attachment-id="1083" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?fit=353%2C258" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?fit=300%2C219" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?fit=353%2C258" data-orig-size="353,258" data-permalink="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede2-8/" data-recalc-dims="1" height="258" sizes="(max-width: 353px) 100vw, 353px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?resize=353%2C258" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?w=353 353w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?resize=300%2C219 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?resize=200%2C146 200w" width="353"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa região na imagem de entrada é chamada de
   <strong>
    campo receptivo local
   </strong>
   para o neurônio oculto. É uma pequena janela nos pixels de entrada. Cada conexão aprende um peso e o neurônio oculto também aprende um viés (bias) geral. Você pode pensar nesse neurônio oculto particular como aprendendo a analisar seu campo receptivo local específico.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em seguida, deslizamos o campo receptivo local por toda a imagem de entrada. Para cada campo receptivo local, existe um neurônio oculto diferente na primeira camada oculta. Para ilustrar isso concretamente, vamos começar com um campo receptivo local no canto superior esquerdo:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede3" class="aligncenter size-full wp-image-1084" data-attachment-id="1084" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?fit=502%2C258" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?fit=300%2C154" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?fit=502%2C258" data-orig-size="502,258" data-permalink="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede3-5/" data-recalc-dims="1" height="258" sizes="(max-width: 502px) 100vw, 502px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?resize=502%2C258" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?w=502 502w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?resize=300%2C154 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?resize=200%2C103 200w" width="502"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Então, deslizamos o campo receptivo local por um pixel para a direita (ou seja, por um neurônio), para conectar a um segundo neurônio oculto:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rede4" class="aligncenter size-full wp-image-1085" data-attachment-id="1085" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rede4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?fit=502%2C258" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?fit=300%2C154" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?fit=502%2C258" data-orig-size="502,258" data-permalink="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede4-3/" data-recalc-dims="1" height="258" sizes="(max-width: 502px) 100vw, 502px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?resize=502%2C258" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?w=502 502w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?resize=300%2C154 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?resize=200%2C103 200w" width="502"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   E assim por diante, construindo a primeira camada oculta. Observe que, se tivermos uma imagem de entrada 28 × 28 e campos receptivos locais 5 × 5, haverá 24 × 24 neurônios na camada oculta. Isso ocorre porque só podemos mover o campo receptivo local 23 neurônios para o lado (ou 23 neurônios para baixo), antes de colidir com o lado direito (ou inferior) da imagem de entrada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mostramos o campo receptivo local sendo movido por um pixel por vez. Na verdade, às vezes, um comprimento de passada diferente é usado. Por exemplo, podemos mover o campo receptivo local 2 pixels para a direita (ou para baixo), caso em que diríamos que um comprimento de passada de 2 é usado. Esse é um dos hyperparâmetros de uma rede neural convolucional, chamado stride length. No exemplo acima usado um stride length de 1, mas vale a pena saber que as pessoas às vezes experimentam comprimentos de passada diferentes.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como foi feito nos capítulos anteriores, se estivermos interessados em testar comprimentos de passada diferentes, podemos usar os dados de validação para escolher o comprimento da passada que oferece o melhor desempenho. A mesma abordagem também pode ser usada para escolher o tamanho do campo receptivo local – não há, é claro, nada de especial sobre o uso de um campo receptivo local 5 × 5. Em geral, campos receptivos locais maiores tendem a ser úteis quando as imagens de entrada são significativamente maiores que as imagens MNIST de 28 × 28 pixels.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Eu já disse que cada neurônio oculto tem um viés e pesos 5 × 5 conectados ao seu campo receptivo local. O que eu ainda não mencionei é que vamos usar os mesmos pesos e vieses para cada um dos 24 × 24 neurônios ocultos. Quer saber como faremos isso matematicamente? Então não perca o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  Referências:
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     O Que é Visão Computacional?
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" target="_blank">
    A Comprehensive Guide to Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1081" href="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1081" href="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1081" href="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1081" href="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1081-5e0dd159ee015" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1081&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1081-5e0dd159ee015" id="like-post-wrapper-140353593-1081-5e0dd159ee015">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-42">
 Capítulo 42 – Compartilhamento de Pesos em Redes Neurais Convolucionais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos continuar estudando Deep Learning e investigar como funciona o Compartilhamento de Pesos em Redes Neurais Convolucionais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Já dissemos que cada neurônio tem um viés e pesos 5 × 5 conectados ao seu
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/" rel="noopener noreferrer" target="_blank">
     campo receptivo local
    </a>
   </span>
   . O que eu não mencionamos é que vamos usar os mesmos pesos e vieses para cada um dos 24 × 24 neurônios ocultos. Em outras palavras, para o neurônio oculto, a saída é:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form" class="aligncenter wp-image-1099 size-medium" data-attachment-id="1099" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?fit=492%2C174" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?fit=300%2C106" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?fit=492%2C174" data-orig-size="492,174" data-permalink="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/form-10/" data-recalc-dims="1" height="106" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?resize=300%2C106" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?resize=300%2C106 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?resize=200%2C71 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?w=492 492w" width="300"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso significa que todos os neurônios da primeira camada oculta detectam exatamente o mesmo recurso, apenas em locais diferentes na imagem de entrada. Para entender porque isso faz sentido, suponha que os pesos e os vieses sejam tais que o neurônio oculto possa escolher, digamos, uma borda vertical em um campo receptivo local específico. Essa habilidade também é útil em outros lugares da imagem. Por isso, é útil aplicar o mesmo detector de recursos em toda a imagem. Para colocar esse conceito em termos um pouco mais abstratos, as redes convolucionais são bem adaptadas à invariância da transalação das imagens: girar uma foto de um gato 90 graus, ainda faz dela a imagem de um gato, embora os pixels agora estejam organizados de forma diferente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por esse motivo, às vezes chamamos o mapa da camada de entrada para a camada oculta de um mapa de recursos. Chamamos os pesos que definem o mapa de recursos de pesos compartilhados. E nós chamamos o viés usado no mapa de recursos desta maneira de viés compartilhado. Os pesos e vieses compartilhados costumam definir um kernel ou filtro. Na literatura, as pessoas às vezes usam esses termos de maneiras ligeiramente diferentes e mais a frente veremos alguns exemplos concretos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A estrutura de rede que descrevemos até agora pode detectar apenas um único tipo de recurso localizado. Para fazer reconhecimento de imagem, precisaremos de mais de um mapa de recursos. E assim, uma camada convolucional completa consiste em vários mapas de recursos, diferentes:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="" class="aligncenter wp-image-1100 size-full" data-attachment-id="1100" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="image" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?fit=537%2C256" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?fit=300%2C143" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?fit=537%2C256" data-orig-size="537,256" data-permalink="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/image/" data-recalc-dims="1" height="256" sizes="(max-width: 537px) 100vw, 537px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?resize=537%2C256" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?w=537 537w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?resize=300%2C143 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?resize=200%2C95 200w" width="537"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No exemplo mostrado acima, existem 3 mapas de recursos. Cada mapa de recursos é definido por um conjunto de pesos compartilhados de 5 × 5 e um único viés compartilhado. O resultado é que a rede pode detectar três tipos diferentes de recursos, sendo cada recurso detectável em toda a imagem.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No exemplo temos apenas 3 mapas de recursos, para manter o diagrama acima simples. No entanto, na prática, as redes convolucionais podem usar mais (e talvez muito mais) mapas de recursos. Uma das primeiras redes convolucionais, a
   <span style="text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/lenet/" rel="noopener noreferrer" target="_blank">
     LeNet-5
    </a>
   </span>
   , usou 6 mapas de recursos, cada um associado a um campo receptivo local 5 × 5, para reconhecer dígitos MNIST. Portanto, o exemplo ilustrado acima está bem próximo do LeNet-5. Nos exemplos que desenvolveremos mais adiante neste livro, usaremos camadas convolucionais com 20 e 40 mapas de recursos. Vamos dar uma olhada rápida em alguns dos recursos que são aprendidos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="features" class="aligncenter wp-image-1101 size-medium" data-attachment-id="1101" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="features" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?fit=800%2C600" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?fit=300%2C225" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?fit=800%2C600" data-orig-size="800,600" data-permalink="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/features/" data-recalc-dims="1" height="225" sizes="(max-width: 300px) 100vw, 300px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=300%2C225" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=300%2C225 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=768%2C576 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=200%2C150 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=690%2C518 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?w=800 800w" width="300"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As 20 imagens correspondem a 20 diferentes mapas de recursos (ou filtros ou kernels). Cada mapa é representado como uma imagem 5 × 5, correspondendo aos pesos 5 × 5 no campo receptivo local. Blocos mais brancos significam um peso menor (normalmente, mais negativo), portanto, o mapa de recursos responde menos aos pixels de entrada correspondentes. Blocos mais escuros significam um peso maior, portanto, o mapa de recursos responde mais aos pixels de entrada correspondentes. Muito grosso modo, as imagens acima mostram o tipo de características que a camada convolucional responde.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Então, o que podemos concluir desses mapas de recursos? Está claro que há estrutura espacial aqui além do que esperamos ao acaso: muitos dos recursos têm claras sub-regiões de luz e escuridão. Isso mostra que nossa rede realmente está aprendendo coisas relacionadas à estrutura espacial. No entanto, além disso, é difícil ver o que esses detectores de recursos estão aprendendo. De fato, agora há muito trabalho para entender melhor os recursos aprendidos pelas redes convolucionais. Se você estiver interessado em acompanhar esse trabalho, sugiro começar com o artigo
   <span style="text-decoration: underline;">
    <a href="https://arxiv.org/abs/1311.2901" rel="noopener noreferrer" target="_blank">
     Visualizando e Compreendendo Redes Convolucionais
    </a>
   </span>
   de Matthew Zeiler e Rob Fergus (2013) ou então
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=viso-computacional-e-reconhecimento-de-imagem" rel="noopener noreferrer" target="_blank">
     Visão Computacional e Reconhecimento de Imagens
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma grande vantagem do compartilhamento de pesos e vieses é que ele reduz bastante o número de parâmetros envolvidos em uma rede convolucional. Para cada mapa de recursos, precisamos de 5 × 5 = 25 pesos compartilhados, além de um único viés compartilhado. Portanto, cada mapa de recursos requer 26 parâmetros. Se temos 20 mapas de recursos, um total de 20 × 26 = 520 parâmetros define a camada convolucional. Em comparação, suponhamos que tivéssemos uma primeira camada totalmente conectada, com 784 = 28 × 28 neurônios de entrada e relativamente modestos 30 neurônios ocultos, como usamos em muitos dos exemplos anteriores no livro. Isso é um total de 784 × 30 pesos, além de um extra de 30 vieses, para um total de 23.550 parâmetros. Em outras palavras, a camada totalmente conectada teria mais de 40 vezes mais parâmetros que a camada convolucional.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   É claro que não podemos fazer uma comparação direta entre o número de parâmetros, já que os dois modelos são diferentes em termos essenciais. Mas, intuitivamente, parece provável que o uso de invariância de tradução pela camada convolucional reduza o número de parâmetros necessários para obter o mesmo desempenho que o modelo totalmente conectado. Isso, por sua vez, resultará em um treinamento mais rápido para o modelo convolucional e, em última análise, nos ajudará a construir redes profundas usando camadas convolucionais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A propósito, o nome convolucional vem do fato de que a operação na equação mostrada no início deste capítulo é às vezes conhecida como uma convolução. Um pouco mais precisamente, as pessoas às vezes escrevem essa equação como a1 = σ (b + w ∗ a0), onde a1 denota o conjunto de ativações de saída de um mapa de recursos, a0 é o conjunto de ativações de entrada e ∗ é chamado de operação de convolução.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No próximo capítulo estudaremos as camas de Pooling, outro “segredo” por trás das Redes Neurais Convolucionais e então estaremos prontos para colocar tudo isso junto.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/" rel="noopener noreferrer" target="_blank">
    O Que é Visão Computacional?
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" target="_blank">
    A Comprehensive Guide to Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1098" href="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1098" href="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1098" href="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1098" href="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1098-5e0dd15c5bd3a" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1098&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1098-5e0dd15c5bd3a" id="like-post-wrapper-140353593-1098-5e0dd15c5bd3a">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-43">
 Capítulo 43 – Camadas de Pooling em Redes Neurais Convolucionais
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Além das camadas convolucionais que acabamos de descrever nos capítulos anteriores, as redes neurais convolucionais também contêm camadas de agrupamento (ou Pooling). Camadas de Pooling são geralmente usadas imediatamente após camadas convolucionais e o que fazem é simplificar as informações na saída da camada convolucional. Vejamos o que são e como funcionam as Camadas de Pooling em Redes Neurais Convolucionais e na sequência vamos colocar todas as camadas juntas para compreender como funciona todo o processo nesta importante arquitetura de Deep Learning.
  </span>
 </p>
 <h2>
  <span style="color: #000000;">
   A Camada de Pooling
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma camada de pooling recebe cada saída do mapa de características da camada convolucional e prepara um mapa de características condensadas. Por exemplo, cada unidade na camada de pooling pode resumir uma região de (digamos) 2 × 2 neurônios na camada anterior. Como um exemplo concreto, um procedimento comum para o pooling é conhecido como pool máximo (ou Max-Pooling). No Max-Pooling, uma unidade de pooling simplesmente gera a ativação máxima na região de entrada 2 × 2, conforme ilustrado no diagrama a seguir:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="pooling" class="aligncenter size-full wp-image-1114" data-attachment-id="1114" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="pooling" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?fit=441%2C236" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?fit=300%2C161" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?fit=441%2C236" data-orig-size="441,236" data-permalink="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/pooling/" data-recalc-dims="1" height="236" sizes="(max-width: 441px) 100vw, 441px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?resize=441%2C236" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?w=441 441w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?resize=300%2C161 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?resize=200%2C107 200w" width="441"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Note que, como temos 24 × 24 neurônios emitidos da camada convolucional, após o agrupamento, temos 12 × 12 neurônios.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como mencionado acima, a camada convolucional geralmente envolve mais do que um único mapa de características. Aplicamos o Max-Pooling para cada mapa de recursos separadamente. Portanto, se houvesse três mapas de recursos, as camadas combinadas, convolutional e Max-Pooling, se pareceriam com:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="pooling2" class="aligncenter size-full wp-image-1115" data-attachment-id="1115" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="pooling2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?fit=521%2C199" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?fit=300%2C115" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?fit=521%2C199" data-orig-size="521,199" data-permalink="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/pooling2/" data-recalc-dims="1" height="199" sizes="(max-width: 521px) 100vw, 521px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?resize=521%2C199" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?w=521 521w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?resize=300%2C115 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?resize=200%2C76 200w" width="521"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos pensar em Max-Pooling como uma forma de a rede perguntar se um determinado recurso é encontrado em qualquer lugar de uma região da imagem. Em seguida, elimina a informação posicional exata. A intuição é que, uma vez que um recurso tenha sido encontrado, sua localização exata não é tão importante quanto sua localização aproximada em relação a outros recursos. Um grande benefício é que há muito menos recursos agrupados e, portanto, isso ajuda a reduzir o número de parâmetros necessários nas camadas posteriores. Genial, não?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O Max-Pooling não é a única técnica usada para o pooling. Outra abordagem comum é conhecida como Pooling L2. Aqui, em vez de tomar a ativação máxima de uma região 2 × 2 de neurônios, tomamos a raiz quadrada da soma dos quadrados das ativações na região 2 × 2. Embora os detalhes sejam diferentes, a intuição é semelhante ao agrupamento máximo: o Pooling L2 é uma maneira de condensar informações da camada convolucional. Na prática, ambas as técnicas têm sido amplamente utilizadas. E às vezes as pessoas usam outros tipos de operação de Pooling. Se você estiver realmente tentando otimizar o desempenho, poderá usar dados de validação para comparar várias abordagens diferentes ao Pooling e escolher a abordagem que funciona melhor.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Juntando Tudo
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos agora juntar todas essas ideias para formar uma rede neural convolucional completa. É semelhante à arquitetura que estávamos estudando nos capítulos anteriores, mas tem a adição de uma camada de 10 neurônios de saída, correspondentes aos 10 valores possíveis para dígitos MNIST (‘0’, ‘1’, ‘2’, etc):
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="pooling3" class="aligncenter size-full wp-image-1116" data-attachment-id="1116" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="pooling3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?fit=582%2C224" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?fit=300%2C115" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?fit=582%2C224" data-orig-size="582,224" data-permalink="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/pooling3/" data-recalc-dims="1" height="224" sizes="(max-width: 582px) 100vw, 582px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?resize=582%2C224" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?w=582 582w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?resize=300%2C115 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?resize=200%2C77 200w" width="582"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A rede começa com 28 × 28 neurônios de entrada (cada image de cada dígito do dataset MNIST tem 28 x 28 pixels), que são usados ​​para codificar as intensidades de pixel para uma imagem no dataset MNIST. Este é então seguido por uma camada convolucional usando um campo receptivo local de 5 x 5 e três mapas de características. O resultado é uma camada de 3 × 24 × 24 neurônios ocultos. A próxima etapa é uma camada de Max-Pooling, aplicada a regiões 2 × 2, em cada um dos três mapas de recursos. O resultado é uma camada de 3 × 12 × 12 neurônios ocultos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A camada final de conexões na rede é uma camada totalmente conectada. Ou seja, essa camada conecta todos os neurônios da camada de max-pooling a cada um dos 10 neurônios de saída. Essa arquitetura totalmente conectada é a mesma que usamos nos capítulos anteriores. Note, no entanto, que no diagrama acima, usei uma única seta, por simplicidade, em vez de mostrar todas as conexões. Claro, você pode facilmente imaginar as conexões.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ou seja, temos uma rede composta de muitas unidades simples, cujos comportamentos são determinados por seus pesos e vieses. E o objetivo geral ainda é o mesmo: usar dados de treinamento para treinar os pesos e vieses da rede para que a rede faça um bom trabalho classificando os dígitos de entrada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em particular, assim como no início do livro, nós vamos treinar nossa rede usando descida estocástica do gradiente e retropropagação. Isso ocorre principalmente da mesma maneira que nos capítulos anteriores. No entanto, precisamos fazer algumas modificações no procedimento de retropropagação. A razão é que nossa derivação anterior da retropropagação foi para redes com camadas totalmente conectadas. Felizmente, é simples modificar a derivação para camadas convolucional e max-pooling.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Quer ver tudo isso funcionando em linguagem Python? Então não perca o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/" rel="noopener noreferrer" target="_blank">
    O Que é Visão Computacional?
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" target="_blank">
    A Comprehensive Guide to Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1113" href="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1113" href="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1113" href="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1113" href="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1113-5e0dd1afb2614" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1113&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1113-5e0dd1afb2614" id="like-post-wrapper-140353593-1113-5e0dd1afb2614">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-44">
 Capítulo 44 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python – Parte 1
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nossa tarefa é simples: vamos fornecer a um modelo de Deep Learning uma imagem e o modelo terá que classificar se a imagem é de um cachorro ou gato! Parece fácil, não? Na verdade, não! Para que isso funcione precisamos construir e treinar um modelo de Deep Learning, o que envolve conhecimentos de Matemática, Estatística, Programação, Visão Computacional, Pré-Processamento de imagens, entre outras áreas. Mas nós vamos fazer isso juntos. Nos próximos capítulos vamos construir e treinar um modelo de Deep Learning para Reconhecimento de Imagens com Redes Neurais Convolucionais em Python. Será uma excelente oportunidade de praticar tudo que estudamos no livro até aqui e preparar você para os capítulos mais avançados deste livro, quando estudaremos outras arquiteturas de Deep Learning. Vamos começar?
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Plano de Trabalho
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para começar, precisamos definir claramente o problema a ser resolvido e como vamos resolvê-lo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Problema
   </strong>
   : Dada uma imagem, é um cachorro ou um gato?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A primeira coisa que precisamos é de muitas imagens de cachorros e gatos, para poder treinar um algoritmo de Deep Learning. Usaremos, portanto, uma abordagem de aprendizagem supervisionada, onde apresentaremos ao algoritmo diversas imagens, devidamente marcadas como sendo imagens de cães e gatos e então treinaremos o algoritmo. Ao final do treinamento, teremos um modelo que poderá receber novas imagens (desta vez não marcadas previamente) e então o modelo deverá ser capaz de classificar como sendo imagem de cão ou gato.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para essa tarefa, usaremos uma arquitetura de Rede Neural Convolucional, a mesma que estudamos nos capítulos anteriores. Essa arquitetura usa métodos de convolução para poder prever características específicas de uma imagem de acordo com o que aprende em um conjunto de treinamento. Por exemplo, podemos dizer que é possível perceber a diferença ao procurar bigodes em um gato ou focinho comprido em um cachorro. Mas uma Rede Neural Convolucional procura muitos outros recursos baseados no que temos em um conjunto de treinamento.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Solução: Usar uma Rede Neural Convolucional para aprender recursos de imagens e assim prever se uma imagem contém um cachorro ou um gato.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Definição dos Dados
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Data Science, Deep Learning, Machine Learning, Inteligência Artificial. Nada disso faz sentido sem dados, muitos dados (por isso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=big-data-fundamentos" rel="noopener noreferrer" target="_blank">
     Big Data
    </a>
   </span>
   é cada vez mais importante nos dias de hoje). E para esta tarefa, teremos os seguintes dados:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Conjunto de dados de treino: Teremos 12.500 imagens de cães e 12.500 imagens de gatos para o conjunto de treinamento.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Conjunto de dados de validação: Teremos 12.500 imagens de cães e gatos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Conjunto de dados de teste: Teremos 1.000 imagens de cães e gatos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa á uma questão onde os iniciantes tem muitas dúvidas. Por que precisamos de dados de treino, validação e teste? Usamos os dados de treino para treinar o algoritmo e então criar o modelo preditivo. Usamos os dados de validação, para avaliar o modelo durante o treinamento. Usamos os dados de teste para validar a performance do modelo já treinado, ou seja, apresentamos ao modelo dados que ele não viu durante o treinamento, a fim de garantir que ele é capaz de fazer previsões.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Estrutura de Trabalho
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos realizar as seguintes atividades na construção do modelo de Reconhecimento de Imagens com Redes Neurais Convolucionais em Python:
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   1- Visualização de Dados
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Começaremos nosso trabalho com tarefas de visualização de dados e análise exploratória. Precisamos compreender os dados antes de qualquer outra coisa, como as dimensões das imagens, escala de cores, detalhes de sombras e se as imagens possuem mais detalhes não relacionados diretamente a um cachorro ou gato, como nesta imagem abaixo! Afinal, o modelo terá que aprender que um cachorro vestido de Hello Kitty ainda é um cachorro!
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="dog" class="aligncenter size-full wp-image-1177" data-attachment-id="1177" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="dog" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?fit=289%2C480" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?fit=181%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?fit=289%2C480" data-orig-size="289,480" data-permalink="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/dog/" data-recalc-dims="1" height="480" sizes="(max-width: 289px) 100vw, 289px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?resize=289%2C480" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?w=289 289w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?resize=181%2C300 181w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?resize=200%2C332 200w" width="289"/>
  </span>
 </p>
 <p>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   2- Construindo a Rede Neural Convolucional
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nosso próximo passo será construir a arquitetura de rede. Definiremos quantas camadas serão usadas, camadas de convolução e pooling, funções de ativação, métrica de avaliação, descida do gradiente com backpropagation e outros detalhes. Discutiremos porque estamos fazendo nossas escolhas.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   3- Treinamento
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com a arquitetura definida, podemos então treinar a nossa rede, o que consiste em apresentar os dados ao algoritmo para que o aprendizado ocorra. Definiremos os hiperparâmetros e por quanto tempo treinaremos a rede. Vamos aproveitar e discutir sobre técnicas para evitar o overfitting (quando o modelo se ajusta demais aos dados de treino).
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   4- Fazer Previsões
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por fim, usaremos o modelo treinado para fazer as classificações. Entregaremos ao modelo novas imagens e ele terá que classificar se a imagem é de um cachorro ou gato! Na prática, o modelo nunca faz a classificação com 100% de certeza. O que ele faz é uma previsão (para ser ainda mais preciso, uma inferência) e nosso trabalho é garantir que essa previsão tenha o mais alto nível de acurácia possível.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Ferramentas
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para este trabalho usaremos, claro, linguagem Python com Keras e TensorFlow. Vamos explicar a você quais são as opções de frameworks e cada linha de código será explicada em detalhes. Vamos considerar que você já conhece linguagem Python e sabe executar um Jupyter Notebook. Se não sabe, então acesse pelo menos o Capítulo 1 do nosso curso gratuito
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener noreferrer" target="_blank">
     Python Fundamentos Para Análise de Dados
    </a>
   </span>
   .
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Está pronto? Então nos encontramos no próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <a href="http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/" rel="noopener noreferrer" target="_blank">
   <span style="text-decoration: underline;">
    O Que é Visão Computacional?
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" target="_blank">
    A Comprehensive Guide to Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1174" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1174" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1174" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1174" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1174-5e0dd1b1c6918" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1174&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1174-5e0dd1b1c6918" id="like-post-wrapper-140353593-1174-5e0dd1b1c6918">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-45">
 Capítulo 45 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python – Parte 2
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos iniciar nosso trabalho de Reconhecimento de Imagens com Redes Neurais Convolucionais em Python cuidando da nossa matéria-prima: dados. Precisamos fazer o download das imagens e organizá-las para então iniciar o trabalho.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Usaremos como fonte de dados, o famoso dataset
   <span style="text-decoration: underline;">
    <a href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener noreferrer" target="_blank">
     Dogs and Cats
    </a>
   </span>
   oferecido pelo Kaggle, o portal sobre Competições de Data Science, onde inclusive a Data Science Academy promove entre os alunos matriculados nas Formações as Competições DSA de Machine Learning. O Kaggle oferece diversos datasets públicos que podem ser usados para você desenvolver seus projetos e incluir no seu portfólio, uma excelente forma de demonstrar suas habilidades em Data Science e Machine Learning. Mostramos como construir um portfólio de projetos de Data Science aqui:
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=preparao-para-carreira-de-cientista-de-dados" rel="noopener noreferrer" target="_blank">
     Preparação Para Carreira de Cientista de Dados
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você pode fazer o download das imagens neste endereço:
   <span style="text-decoration: underline;">
    <a href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener noreferrer" target="_blank">
     Dogs vs. Cats
    </a>
   </span>
   . Mas nós já fizemos o download e disponibilizamos para você junto com o Jupyter Notebook no repositório deste livro no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener noreferrer" target="_blank">
     Github
    </a>
   </span>
   . Como este dataset é bastante famoso, alternativamente, você pode fazer o download oferecido pela Microsoft Research neste endereço
   <span style="text-decoration: underline;">
    <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54765" rel="noopener noreferrer" target="_blank">
     Kaggle Cats and Dogs Dataset
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Feito o download das imagens (você vai precisar de aproximadamente 1 GB de espaço em disco para as imagens), precisamos organizar os arquivos em uma estrutura de diretórios da seguinte forma:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="diretorios" class="aligncenter size-full wp-image-1198" data-attachment-id="1198" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="diretorios" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?fit=570%2C222" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?fit=300%2C117" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?fit=570%2C222" data-orig-size="570,222" data-permalink="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/diretorios/" data-recalc-dims="1" height="222" sizes="(max-width: 570px) 100vw, 570px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?resize=570%2C222" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?w=570 570w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?resize=300%2C117 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?resize=200%2C78 200w" width="570"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Crie um diretório (por exemplo Cap45, mas pode ser o nome que você quiser). Dentro dele crie mais 3 pastas: dataset_treino, dataset_validation e dataset_teste. Não use espaços no nome e muito menos acentos nas palavras, pois isso causa diversos problemas em programação.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Dentro da pasta dataset_treino, crie mais duas pastas, cats (que vai receber as 12.500 imagens de gatos) e dogs (que vai receber as 12.500 imagens de cachorros).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nas pastas dataset_validation e dataset_teste não é necessário criar sub pastas e dentro delas colocaremos as 12.500 imagens de validação e 1.000 imagens de teste, respectivamente. As imagens de validação serão usadas para avaliar o modelo durante o treinamento e as imagens de teste serão usadas para avaliar o modelo depois do treinamento.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos agora visualizar algumas imagens usando o Jupyter Notebook, que você encontra no repositório deste livro no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr/DeepLearningBook" rel="noopener noreferrer" target="_blank">
     Github
    </a>
   </span>
   . Caso não tenha familiaridade com o Jupyter Notebook, acesse o Capítulo 1 do curso gratuito
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=python-fundamentos" rel="noopener noreferrer" target="_blank">
     Python Fundamentos Para Análise de Dados
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Se você criou a estrutura de diretórios de forma correta, então as seguintes células mostrarão algumas das imagens:
  </span>
 </p>
 <p>
  <img alt="image1" class="aligncenter wp-image-1203 size-large" data-attachment-id="1203" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="image1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?fit=1024%2C851" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?fit=300%2C249" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?fit=1056%2C878" data-orig-size="1056,878" data-permalink="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/image1-5/" data-recalc-dims="1" height="851" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=1024%2C851" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=1024%2C851 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=300%2C249 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=768%2C639 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=200%2C166 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=690%2C574 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?w=1056 1056w" width="1024"/>
 </p>
 <p>
 </p>
 <p>
  <img alt="image2" class="aligncenter wp-image-1204 size-large" data-attachment-id="1204" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="image2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?fit=951%2C1024" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?fit=279%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?fit=1012%2C1090" data-orig-size="1012,1090" data-permalink="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/image2-5/" data-recalc-dims="1" height="1024" sizes="(max-width: 951px) 100vw, 951px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=951%2C1024" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=951%2C1024 951w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=279%2C300 279w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=768%2C827 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=200%2C215 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=690%2C743 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?w=1012 1012w" width="951"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Se as imagens foram mostradas de forma correta, então os dados estão prontos para serem explorados. É o que faremos no próximo capítulo! Até lá.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/" rel="noopener noreferrer" target="_blank">
    O Que é Visão Computacional?
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" target="_blank">
    A Comprehensive Guide to Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1193" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1193" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1193" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1193" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1193-5e0dd1b449ab0" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1193&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1193-5e0dd1b449ab0" id="like-post-wrapper-140353593-1193-5e0dd1b449ab0">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-46">
 Capítulo 46 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python – Parte 3
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma das principais dúvidas de quem está iniciando em Machine Learning e se depara com as Redes Neurais Convolucionais, é sobre como ocorre o aprendizado dos parâmetros (aquilo que o algoritmo realmente aprende durante o treinamento). O que exatamente está sendo feito quando apresentamos uma imagem a um algoritmo de Rede Neural Convolucional?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="dogs_cats" class="aligncenter wp-image-1224 size-full" data-attachment-id="1224" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="dogs_cats" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?fit=1024%2C576" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?fit=300%2C169" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?fit=1600%2C900" data-orig-size="1600,900" data-permalink="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/dogs_cats/" data-recalc-dims="1" height="658" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?resize=1170%2C658" width="1170"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Sabemos que em cada camada de convolução, a rede tenta entender os padrões básicos. Por exemplo: Na primeira camada de convolução, a rede tenta aprender padrões e bordas das imagens. Na segunda camada, ela tenta entender a forma / cor e outras coisas. Uma camada final chamada camada de recurso / camada totalmente conectada tenta classificar a imagem.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Antes de prepararmos o script com nossa rede convolucional, vamos compreender como definimos a arquitetura da rede e em quais camadas os parâmetros são aprendidos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Camada de Entrada (Input Layer)
   </strong>
   : O que a camada de entrada faz é ler a imagem. Portanto, não há parâmetros a serem aprendidos aqui.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Camada Convolucional (Convolutional Layer)
   </strong>
   : considere uma camada convolucional que usa os mapas de recursos “l” como entrada e tem os mapas de recursos “k” como saída. O tamanho do filtro é “n * m”.
  </span>
 </p>
 <p>
  <img alt="conv_layer" class="aligncenter wp-image-1227 size-medium" data-attachment-id="1227" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="conv_layer" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?fit=400%2C352" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?fit=300%2C264" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?fit=400%2C352" data-orig-size="400,352" data-permalink="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/conv_layer/" data-recalc-dims="1" height="264" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?resize=300%2C264" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?resize=300%2C264 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?resize=200%2C176 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?w=400 400w" width="300"/>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui, a entrada tem l = 32 mapas de recursos como entradas, k = 64 mapas de recursos como saídas e o tamanho do filtro é n = 3 e m = 3. É importante entender que não temos apenas um filtro 3 * 3, mas, na verdade, temos um filtro 3 * 3 * 32, já que nossa entrada tem 32 dimensões. E como uma saída da primeira camada convolucional, aprendemos 64 diferentes filtros 3 * 3 * 32 cujo peso total é “n * m * k * l”. Depois, há um termo chamado bias para cada mapa de recursos. Portanto, o número total de parâmetros é “(n * m * l + 1) * k”.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Camada de Pooling (Pooling Layer)
   </strong>
   : Não há parâmetros a aprender na camada de pooling. Essa camada é usada apenas para reduzir o tamanho da dimensão da imagem.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Obs: Podemos ter várias combinações de camada convolucional / camada de pooling em nossa arquitetura, sendo esta decisão do
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
     Cientista de Dados
    </a>
   </span>
   ou
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
     Engenheiro de Inteligência Artificial
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Camada Totalmente Conectada (Fully Connected Layer)
   </strong>
   : Nesta camada, todas as unidades de entrada possuem um peso separável para cada unidade de saída. Para entradas “n” e saídas “m”, o número de pesos é “n * m”. Além disso, essa camada possui o bias para cada nó de saída, portanto, os parâmetros aprendidos são “(n + 1) * m”.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Camada de Saída (Output Layer)
   </strong>
   : Esta camada é totalmente conectada, portanto, os parâmetros aprendidos também são “(n + 1) m”, quando “n” é o número de entradas e “m” é o número de saídas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A principal dificuldade ao definir a arquitetura de um CNN é a primeira camada totalmente conectada. Não sabemos a dimensionalidade da camada totalmente conectada. Para calcular isso, temos que começar com o tamanho da imagem de entrada e calcular o tamanho de cada camada convolucional.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No caso simples, o tamanho da camada totalmente conectada é calculado como “input_size – (filter_size – 1)”. Por exemplo, se o tamanho da imagem de entrada for (50,50) e o filtro for (3,3), então (50- (3-1)) = 48. Mas o tamanho da imagem de entrada de uma rede convolucional não deve ser menor que a entrada, então precisamos definir o padding.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para calcular o padding usamos: input_size + 2 * padding_size – (filter_size-1). Para o caso acima, (50 + (2 * 1) – (3–1) = 52 – 2 = 50) que fornece o mesmo tamanho de entrada. Perfeito, pois assim não perdemos nenhum detalhe da imagem.
  </span>
  <span style="color: #000000;">
   Se quisermos explicitamente diminuir a imagem durante a convolução, podemos definir um passo (stride).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Finalmente, para calcular o número de parâmetros que a rede vai aprender usamos: (n * m * k + 1) * f. Veremos tudo isso em código Python no próximo capítulo!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Caso queira mais detalhes sobre como definir a arquitetura da rede, consulte o famoso e excelente paper da AlexNet:
   <span style="text-decoration: underline;">
    <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener noreferrer" target="_blank">
     ImageNet Classification with Deep Convolutional Neural Networks
    </a>
    .
   </span>
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências
  </span>
  :
 </p>
 <p>
  <a href="http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/" rel="noopener noreferrer" target="_blank">
   <span style="text-decoration: underline;">
    O Que é Visão Computacional?
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener noreferrer" target="_blank">
    ImageNet Classification with Deep Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Don’t Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca" rel="noopener noreferrer" target="_blank">
    How to calculate the number of parameters in the CNN?
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" target="_blank">
    A Comprehensive Guide to Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1221" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1221" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1221" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1221" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1221-5e0dd1bb4902c" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1221&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1221-5e0dd1bb4902c" id="like-post-wrapper-140353593-1221-5e0dd1bb4902c">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-47">
 CapÃ­tulo 47 – Reconhecimento de Imagens com Redes Neurais Convolucionais em Python â Parte 4
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vejamos como implementar nosso modelo de Rede Neural Convolucional que vai aprender a diferenÃ§a nas imagens de cÃ£es e gatos e quando apresentarmos novas imagens ao modelo, ele serÃ¡ capaz de prever se a imagem Ã© de um cÃ£o ou gato de forma automÃ¡tica.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Abaixo vocÃª encontra todos os detalhes de construÃ§Ã£o da rede, treinamento do modelo e avaliaÃ§Ã£o (use a barra de rolagem para visualizar todo o cÃ³digo). O Jupyter Notebook completo estÃ¡ disponÃ­vel no repositÃ³rio deste livro no
   <span style="text-decoration: underline;">
    <a href="https://github.com/dsacademybr" rel="noopener noreferrer" target="_blank">
     Github
    </a>
   </span>
   .
  </span>
 </p>
 <style type="text/css">
  .errordiv { padding:10px; margin:10px; border: 1px solid #555555;color: #000000;background-color: #f8f8f8; width:500px; }#advanced_iframe {visibility:visible;opacity:1;}#ai-layer-div-advanced_iframe p {height:100%;margin:0;padding:0}
 </style>
 <script type="text/javascript">
  var ai_iframe_width_advanced_iframe = 0;  var ai_iframe_height_advanced_iframe = 0;var aiIsIe8=false;var aiOnloadScrollTop="true";
if (typeof aiReadyCallbacks === 'undefined') {
    var aiReadyCallbacks = [];  
} else if (!(aiReadyCallbacks instanceof Array)) {
    var aiReadyCallbacks = [];
}    function aiShowIframeId(id_iframe) { jQuery("#"+id_iframe).css("visibility", "visible");    }    function aiResizeIframeHeight(height) { aiResizeIframeHeight(height,advanced_iframe); }    function aiResizeIframeHeightId(height,width,id) {aiResizeIframeHeightById(id,height);}
 </script>
 <iframe allowtransparency="true" frameborder="0" height="900" id="advanced_iframe" name="advanced_iframe" src="http://deeplearningbook.com.br/wp-content/uploads/2019/06/NotebookâV2.html" style=";width:100%;height:900px;" width="100%">
 </iframe>
 <script type="text/javascript">
  var ifrm_advanced_iframe = document.getElementById("advanced_iframe");var hiddenTabsDoneadvanced_iframe = false;
function resizeCallbackadvanced_iframe() {}function aiChangeUrl(loc) {}
 </script>
 <script type="text/javascript">
 </script>
 <hr/>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com isso concluÃ­mos esta introduÃ§Ã£o a uma das mais famosas arquiteturas de Deep Learning, as Redes neurais Convolucionais. A partir do prÃ³ximo capÃ­tulo estudaremos outras arquiteturas. AtÃ© lÃ¡.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Lembrando que a DSA oferece um programa completo para quem deseja aprender Deep Learning e InteligÃªncia Artificial na prÃ¡tica e de forma profissional, a
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
     FormaÃ§Ã£o InteligÃªncia Artificial
    </a>
   </span>
   . FaÃ§a como milhares de alunos no Brasil e no mundo e comece sua capacitaÃ§Ã£o agora mesmo. O curso Ã© 100% online e 100% em portuguÃªs. Seja um profissional em alta demanda!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   ReferÃªncias:
  </span>
 </p>
 <p>
  <a href="http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/" rel="noopener noreferrer" target="_blank">
   <span style="text-decoration: underline;">
    O Que Ã© VisÃ£o Computacional?
   </span>
  </a>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    FormaÃ§Ã£o InteligÃªncia Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    FormaÃ§Ã£o AnÃ¡lise EstatÃ­stica Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
    FormaÃ§Ã£o Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener noreferrer" target="_blank">
    ImageNet Classification with Deep Convolutional Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1711.00489.pdf" rel="noopener noreferrer" target="_blank">
    Donât Decay the Learning Rate, Increase the Batch Size
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca" rel="noopener noreferrer" target="_blank">
    How to calculate the number of parameters in the CNN?
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener noreferrer" target="_blank">
    A Comprehensive Guide to Convolutional Neural Networksâ
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
 </div>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1248" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1248" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1248" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1248" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1248-5e0b861ce6ec0" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1248&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1248-5e0b861ce6ec0" id="like-post-wrapper-140353593-1248-5e0b861ce6ec0">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-48">
 Capítulo 48 – Redes Neurais Recorrentes
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A partir deste capítulo estudaremos diversas outras arquiteturas de Deep Learning, que estão sendo usadas em aplicações de Inteligência Artificial de última geração. Todas essas arquiteturas são estudadas e utilizadas na construção de sistema de IA na
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
   . Continue conosco nessa incrível jornada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os humanos não começam a pensar do zero a cada segundo. Ao ler este texto, você entende cada palavra com base em sua compreensão das palavras anteriores. Você não joga tudo fora e começa a pensar de novo. Seus pensamentos têm persistência.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As redes neurais artificiais tradicionais não podem fazer isso, o que traz algumas limitações para esses tipos de modelos. Por exemplo, imagine que você queira classificar o tipo de evento que está acontecendo em todos os pontos de um filme. Não está claro como uma rede neural tradicional poderia usar seu raciocínio sobre eventos anteriores no filme para informar os posteriores.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Redes Neurais Recorrentes resolvem esse problema. São redes com loops, permitindo que as informações persistam.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As redes recorrentes são um tipo de rede neural artificial projetada para reconhecer padrões em sequências de dados, como texto, genomas, caligrafia, palavra falada ou dados de séries numéricas que emanam de sensores, bolsas de valores e agências governamentais. Esses algoritmos consideram tempo e sequência, eles têm uma dimensão temporal.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A pesquisa mostra que eles são um dos tipos mais poderosos e úteis de rede neural, juntamente com o mecanismo de atenção e as redes de memória. As RNNs são aplicáveis até mesmo a imagens, que podem ser decompostas em uma série de amostras e tratadas como uma sequência.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como as redes recorrentes possuem um certo tipo de memória, e a memória também faz parte da condição humana, faremos analogias com a memória do cérebro, para uma melhor compreensão. Continue a leitura.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Revisão de Redes Feedforward
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para entender as redes recorrentes, primeiro vamos revisar o básico das redes feedforward, que estudamos em capítulos anteriores aqui mesmo no Deep Learning Book.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ambas as redes recebem o nome da forma como canalizam informações através de uma série de operações matemáticas realizadas nos nós da rede. As redes feedforward alimentam informações diretamente (nunca tocando em um determinado nó duas vezes), enquanto as redes recorrentes percorrem através de um loop.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No caso de redes feedforward, exemplos de entrada são alimentados na rede e transformados em uma saída; com a aprendizagem supervisionada, a saída seria por exemplo um rótulo, um nome aplicado à entrada. Ou seja, elas mapeiam dados brutos para categorias, reconhecendo padrões que podem sinalizar, por exemplo, que uma imagem de entrada deve ser rotulada como “gato” ou “cachorro”. A imagem abaixo mostra um exemplo de rede feedforward.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="Rede Feed Forward" class="aligncenter size-full wp-image-1271" data-attachment-id="1271" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Rede Feed Forward" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?fit=783%2C851" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?fit=276%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?fit=783%2C851" data-orig-size="783,851" data-permalink="http://deeplearningbook.com.br/redes-neurais-recorrentes/feedf/" data-recalc-dims="1" height="851" sizes="(max-width: 783px) 100vw, 783px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=783%2C851" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?w=783 783w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=276%2C300 276w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=768%2C835 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=200%2C217 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=690%2C750 690w" width="783"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma rede feedforward pode então ser treinada em imagens rotuladas, por exemplo, até minimizar o erro ao classificar suas categorias. Com o conjunto treinado de parâmetros (ou pesos, conhecidos coletivamente como um modelo), a rede procura categorizar os dados que nunca viu. Uma rede feedforward treinada pode ser exposta a qualquer coleção aleatória de fotografias, e a primeira fotografia a que está exposta não alterará necessariamente como classifica a segunda. Ver a fotografia de um gato não levará a rede a perceber um cachorro em seguida.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ou seja, uma rede feedforward não tem noção de ordem no tempo, e a única entrada que considera é o exemplo atual a que foi exposta. As redes feedforward são amnésicas em relação ao seu passado recente; elas lembram nostalgicamente apenas os momentos formativos do treinamento.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Redes Neurais Recorrentes
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As redes recorrentes, por outro lado, tomam como entrada não apenas o exemplo de entrada atual que veem, mas também o que perceberam anteriormente no tempo. Aqui está um diagrama com a representação de uma rede neural recorrente e uma rede feedforward.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="Nets" class="aligncenter size-full wp-image-1272" data-attachment-id="1272" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="Nets" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?fit=1024%2C566" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?fit=300%2C166" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?fit=2600%2C1437" data-orig-size="2600,1437" data-permalink="http://deeplearningbook.com.br/redes-neurais-recorrentes/rnn-2/" data-recalc-dims="1" height="647" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=1170%2C647" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?w=2600 2600w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=300%2C166 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=768%2C424 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=1024%2C566 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=200%2C111 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=690%2C381 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?w=2340 2340w" width="1170"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A decisão de uma rede recorrente alcançada na etapa de tempo t-1 afeta a decisão que alcançará um momento mais tarde na etapa de tempo t. Assim, as redes recorrentes têm duas fontes de entrada, o presente e o passado recente, que se combinam para determinar como respondem a novos dados, da mesma forma que fazemos na vida. Nas redes neurais recorrentes isso é feito, claro, com a ajuda da nossa querida Matemática. E por isso o curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener noreferrer" target="_blank">
     Matemática Para Machine Learning
    </a>
   </span>
   é um dos cursos de maior sucesso na Data Science Academy.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As redes recorrentes são diferenciadas das redes feedforward pelo loop de feedback conectado às suas decisões anteriores, ingerindo suas próprias saídas momento após momento como entrada. Costuma-se dizer que as redes recorrentes têm memória. A adição de memória às redes neurais tem uma finalidade: há informações na própria sequência e as redes recorrentes a utilizam para executar tarefas que as redes de feedforward não conseguem.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa informação sequencial é preservada no estado oculto da rede recorrente, que consegue passar por muitas etapas de tempo à medida que ela avança em cascata para afetar o processamento de cada novo exemplo. Essas correlações entre eventos são separadas por muitos momentos, e essas correlações são chamadas de “
   <strong>
    dependências de longo prazo
   </strong>
   ”, porque um evento no tempo depende e é uma função de um ou mais eventos que vieram antes. Uma maneira objetiva de pensar sobre as RNNs é a seguinte: elas são uma forma de compartilhar pesos ao longo do tempo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim como a memória humana circula invisivelmente dentro de um corpo, afetando nosso comportamento sem revelar sua forma completa, a informação circula nos estados ocultos de redes recorrentes. A língua portuguesa está cheia de palavras que descrevem os ciclos de feedback da memória. Quando dizemos que uma pessoa é assombrada por seus atos, por exemplo, estamos simplesmente falando sobre as consequências que as produções passadas causam no tempo presente. Os franceses chamam isso de “Le passé qui ne passe pas” ou “O passado que não passa”.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Descreveremos o processo de levar a memória adiante matematicamente da seguinte forma:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="" class="aligncenter size-full wp-image-1273" data-attachment-id="1273" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="recurrent_equation" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?fit=484%2C66" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?fit=300%2C41" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?fit=484%2C66" data-orig-size="484,66" data-permalink="http://deeplearningbook.com.br/redes-neurais-recorrentes/recurrent_equation/" data-recalc-dims="1" height="66" sizes="(max-width: 484px) 100vw, 484px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?resize=484%2C66" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?w=484 484w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?resize=300%2C41 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?resize=200%2C27 200w" width="484"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O estado oculto na etapa de tempo t é h_t. É uma função da entrada na mesma etapa de tempo x_t, modificada por uma matriz de peso W (como a que usamos para redes feedforward) adicionada ao estado oculto do passo de tempo anterior h_t-1 multiplicado por seu próprio estado oculto – para a matriz de estado oculto U, também conhecida como matriz de transição e semelhante a uma cadeia de Markov. As matrizes de peso são filtros que determinam quanta importância deve ser dada tanto à entrada atual quanto ao estado oculto do passado. O erro que eles geram retornará por meio de retropropagação (backpropagation) e será usado para ajustar seus pesos até que o erro não diminua mais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A soma da entrada de peso e do estado oculto é comprimida pela função φ – ou uma função sigmoide logística ou tanh, dependendo – que é uma ferramenta padrão para condensar valores muito grandes ou muito pequenos em um espaço logístico, bem como tornar os gradientes viáveis para retropropagação.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como esse loop de feedback ocorre a cada etapa da série, cada estado oculto contém traços não apenas do estado oculto anterior, mas também de todos aqueles que precederam h_t-1 pelo tempo que a memória pode persistir.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Dada uma série de letras, uma rede recorrente usará o primeiro caractere para ajudar a determinar sua percepção do segundo caractere, de tal forma que um q inicial possa levá-lo a inferir que a próxima letra será u.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas para que as redes neurais recorrentes possam aprender de forma efetiva, precisamos de uma versão levemente modificada do Backpropagation, o BPTT (Backpropagation Through Time). Mas isso é assunto para o próximo capítulo! Estudaremos ainda duas importantes variações das RNNs, as Long Short-Term Memory Units (LSTMs) e as Gated Recurrent Units (GRUs).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Enquanto isso, caso queira começar a se divertir com as RNNs, acompanhe esse tutorial oficial do TensorFlow:
   <span style="text-decoration: underline;">
    <a href="https://www.tensorflow.org/tutorials/sequences/recurrent" rel="noopener noreferrer" target="_blank">
     Recurrent Neural Networks
    </a>
   </span>
   . Para aulas práticas em português com linguagem Python, acesse
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
     aqui
    </a>
   </span>
   .
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent Neural Networks Cheatsheet
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://skymind.ai/wiki/lstm" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     A Beginner’s Guide to LSTMs and Recurrent Neural Networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Train longer, generalize better: closing the generalization gap in large batch training of neural networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks and Deep Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1269" href="http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1269" href="http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1269" href="http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1269" href="http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1269-5e0dd1be127f0" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1269&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1269-5e0dd1be127f0" id="like-post-wrapper-140353593-1269-5e0dd1be127f0">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-49">
 Capítulo 49 – A Matemática do Backpropagation Through Time (BPTT)
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Estudamos no capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/redes-neurais-recorrentes/" rel="noopener noreferrer" target="_blank">
     anterior
    </a>
   </span>
   as Redes Neurais Recorrentes. Mas para que elas funcionem, o algoritmo de treinamento precisa de um pequeno ajuste, uma vez que esse tipo de rede possui o que podemos chamar de “memória” durante seu treinamento. E o que faz isso acontecer é A Matemática do Backpropagation Through Time (BPTT), assunto deste capítulo do Deep Learning Book.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Um Pouco Mais Sobre as RNNs
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A ideia por trás dos RNNs é fazer uso de informações sequenciais. Em uma rede neural tradicional, assumimos que todas as entradas (e saídas) são independentes umas das outras. Mas para muitas tarefas isso é uma ideia muito ruim. Se você quiser prever a próxima palavra em uma frase, é melhor saber quais palavras vieram antes dela. As RNNs são chamadas de recorrentes porque executam a mesma tarefa para todos os elementos de uma sequência, com a saída sendo dependente dos cálculos anteriores. Outra maneira de pensar sobre RNNs é que elas têm uma “memória” que captura informações sobre o que foi calculado até agora. Em teoria, RNNs podem fazer uso de informações em sequências arbitrariamente longas, mas, na prática, limitam-se a olhar para trás apenas alguns passos (mais sobre isso adiante). Aqui está o que uma RNN típica parece:
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="rnn" class="aligncenter size-full wp-image-1301" data-attachment-id="1301" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rnn" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?fit=795%2C319" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?fit=300%2C120" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?fit=795%2C319" data-orig-size="795,319" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/rnn-3/" data-recalc-dims="1" height="319" sizes="(max-width: 795px) 100vw, 795px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=795%2C319" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?w=795 795w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=300%2C120 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=768%2C308 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=200%2C80 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=690%2C277 690w" width="795"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O diagrama acima mostra uma RNN sendo “desenrolada” ou “desdobrada” (termo unfolded em inglês) em uma rede completa. Ao desenrolar, simplesmente queremos dizer que escrevemos a rede para a sequência completa. Por exemplo, se a sequência que nos interessa é uma sentença de 5 palavras, a rede seria desdobrada em uma rede neural de 5 camadas, uma camada para cada palavra. As fórmulas que governam o cálculo que acontece em uma RNN são as seguintes:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   1. xt é a entrada no passo de tempo t. Por exemplo, x1 poderia ser um vetor one-hot correspondente à segunda palavra de uma sentença.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   2. st é o estado oculto no passo de tempo t. É a “memória” da rede. O termo st é calculado com base no estado oculto anterior e a entrada na etapa atual através da fórmula: st = f(Uxt + Wst-1). A função geralmente é uma não-linearidade, como tanh ou ReLU. Já s -1, que é necessário para calcular o primeiro estado oculto, é tipicamente inicializado com zero.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   3. ot é a saída na etapa t. Por exemplo, se quiséssemos prever a próxima palavra em uma frase, seria um vetor de probabilidades em todo o nosso vocabulário. ot = softmax(Vst).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Há algumas coisas a serem observadas aqui:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você pode pensar no estado oculto st como a memória da rede. st captura informações sobre o que aconteceu em todas as etapas de tempo anteriores. A saída na etapa ot é calculada exclusivamente com base na memória no tempo t. É um pouco mais complicado na prática, porque normalmente não é possível capturar informações de muitas etapas de tempo anteriores.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao contrário de uma rede neural profunda tradicional, que usa parâmetros diferentes em cada camada, uma RNN compartilha os mesmos parâmetros (U, V, W acima) em todas as etapas. Isso reflete o fato de que estamos executando a mesma tarefa em cada etapa, apenas com entradas diferentes. Isso reduz muito o número total de parâmetros que precisamos aprender.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O diagrama acima tem saídas em cada etapa de tempo, mas dependendo da tarefa, isso pode não ser necessário. Por exemplo, ao prever o sentimento de uma frase, podemos nos preocupar apenas com a saída final, não com o sentimento após cada palavra. Da mesma forma, podemos não precisar de entradas em cada etapa de tempo. A principal característica de uma RNN é seu estado oculto, que captura algumas informações sobre uma sequência.
  </span>
 </p>
 <h3>
  <span style="color: #000000;">
   E o Backpropagation?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Lembre-se, o objetivo das redes neurais recorrentes é classificar com precisão uma entrada sequencial (por exemplo, dada uma frase, prever o sentimento ou mesmo a próxima palavra). Contamos com a retropropagação (backpropagation) do erro e o gradiente descendente para fazê-lo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A retropropagação em redes feedforward retrocede do erro final através das saídas, pesos e entradas de cada camada oculta, atribuindo a esses pesos a responsabilidade por uma parte do erro calculando suas derivadas parciais – ∂E / ∂w, ou a relação entre suas taxas de mudança. Essas derivações são então usadas por nossa regra de aprendizado, gradiente descendente, para ajustar os pesos para cima ou para baixo, qualquer que seja a direção que diminua o erro. Já estudamos isso nos capítulos anteriores aqui do Deep Learning Book.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As redes recorrentes dependem de uma extensão da retropropagação, chamada Backpropagation Through Time, ou BPTT. O tempo, neste caso, é simplesmente expresso por uma série ordenada e bem definida de cálculos, ligando um passo de tempo ao seguinte, o que significa que toda a retropropagação precisa funcionar.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Redes neurais, sejam elas recorrentes ou não, são simplesmente funções compostas aninhadas como f(g(h(x))). A adição de um elemento de tempo apenas estende a série de funções para as quais calculamos derivadas com a regra da cadeia (chain rule). Matemática pura!
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   A Matemática do Backpropagation Through Time (BPTT)
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por falar em Matemática, vamos compreender o BPTT através de fórmulas e alguns gráficos. Para as devidas referências, sempre consulte as notas ao final do capítulo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Considere que estamos criando uma rede neural recorrente que seja capaz de prever a próxima palavra em um texto, o que pode ser útil em aplicações de IA para criar petições e assim ajudar advogados a automatizar o trabalho. Algo que já é feito pelo
   <a href="https://rossintelligence.com/" rel="noopener noreferrer" target="_blank">
    <span style="text-decoration: underline;">
     ROSS
    </span>
   </a>
   , o Robô Advogado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos começar com a equação básica de uma Rede Neural Recorrente:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter size-full wp-image-1291" data-attachment-id="1291" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?fit=198%2C79" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?fit=198%2C79" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?fit=198%2C79" data-orig-size="198,79" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form1-6/" data-recalc-dims="1" height="79" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?resize=198%2C79" width="198"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Também definimos nossa perda, ou erro, como a perda de entropia cruzada, dada por:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-1292" data-attachment-id="1292" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?fit=208%2C134" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?fit=208%2C134" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?fit=208%2C134" data-orig-size="208,134" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form2-12/" data-recalc-dims="1" height="134" sizes="(max-width: 208px) 100vw, 208px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?resize=208%2C134" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?w=208 208w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?resize=200%2C129 200w" width="208"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui, yt é a palavra correta no momento do passo t, e y^t é nossa previsão. Normalmente, tratamos a sequência completa (sentença) como um exemplo de treinamento, portanto, o erro total é apenas a soma dos erros em cada etapa de tempo (palavra).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rnn-bptt1" class="aligncenter size-full wp-image-1293" data-attachment-id="1293" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rnn-bptt1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?fit=953%2C550" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?fit=300%2C173" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?fit=953%2C550" data-orig-size="953,550" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/rnn-bptt1/" data-recalc-dims="1" height="550" sizes="(max-width: 953px) 100vw, 953px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=953%2C550" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?w=953 953w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=300%2C173 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=768%2C443 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=200%2C115 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=690%2C398 690w" width="953"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Lembre-se de que nosso objetivo é calcular os gradientes do erro em relação aos nossos parâmetros U, V e W e, em seguida, aprender bons parâmetros usando o Gradiente Descendente Estocástico. Assim como resumimos os erros, também somamos os gradientes em cada etapa de tempo para um exemplo de treinamento:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form3" class="aligncenter size-full wp-image-1294" data-attachment-id="1294" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?fit=101%2C35" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?fit=101%2C35" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?fit=101%2C35" data-orig-size="101,35" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form3-10/" data-recalc-dims="1" height="35" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?resize=101%2C35" width="101"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para calcular esses gradientes, usamos a regra de diferenciação da cadeia. Esse é o algoritmo de retropropagação quando aplicado para trás a partir do erro. Usaremos o E_3 como exemplo, apenas para trabalhar com números concretos.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form4" class="aligncenter size-full wp-image-1295" data-attachment-id="1295" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?fit=148%2C106" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?fit=148%2C106" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?fit=148%2C106" data-orig-size="148,106" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form4-6/" data-recalc-dims="1" height="106" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?resize=148%2C106" width="148"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Acima, z3 = Vs3 e a última linha é o produto externo de dois vetores. Não se preocupe se você não seguir os passos acima, nós pulamos
  </span>
  <span style="color: #000000;">
   vários passos e você pode tentar calcular essas derivadas você mesmo (bom exercício!). O ponto que estou tentando transmitir é que
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="latex" class="aligncenter size-full wp-image-1305" data-attachment-id="1305" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="latex" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?fit=22%2C21" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?fit=22%2C21" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?fit=22%2C21" data-orig-size="22,21" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/latex/" data-recalc-dims="1" height="21" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?resize=22%2C21" width="22"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   depende apenas dos valores no momento atual, yˆ3, y3, s3. Se você tem estes valores, calculando o gradiente para V é uma multiplicação de matriz simples.
  </span>
  <span style="color: #000000;">
   Mas a história é diferente para W (e para U):
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="latex (1)" class="aligncenter size-full wp-image-1306" data-attachment-id="1306" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="latex (1)" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?fit=22%2C21" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?fit=22%2C21" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?fit=22%2C21" data-orig-size="22,21" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/latex-1/" data-recalc-dims="1" height="21" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?resize=22%2C21" width="22"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para entender porque, escrevemos a regra da cadeia, como acima:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form5" class="aligncenter size-full wp-image-1296" data-attachment-id="1296" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?fit=140%2C39" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?fit=140%2C39" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?fit=140%2C39" data-orig-size="140,39" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form5-5/" data-recalc-dims="1" height="39" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?resize=140%2C39" width="140"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora, note que s3 = tanh(Uxt + Ws2) depende de s2, que depende de W e s1, e assim por diante. Então, se pegarmos a derivada em relação a W, não podemos simplesmente tratar s2 como uma constante! Precisamos aplicar a regra da cadeia novamente e o que realmente temos é o seguinte:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form6" class="aligncenter size-full wp-image-1297" data-attachment-id="1297" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form6" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?fit=195%2C49" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?fit=195%2C49" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?fit=195%2C49" data-orig-size="195,49" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form6-4/" data-recalc-dims="1" height="49" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?resize=195%2C49" width="195"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Somamos as contribuições de cada passo de tempo para o gradiente. Em outras palavras, como W é usado em todas as etapas até a saída que nos interessa, precisamos retroceder gradientes na rede de t = 3 até t = 0:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rnn-bptt-with-gradients" class="aligncenter size-full wp-image-1298" data-attachment-id="1298" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rnn-bptt-with-gradients" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?fit=953%2C550" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?fit=300%2C173" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?fit=953%2C550" data-orig-size="953,550" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/rnn-bptt-with-gradients/" data-recalc-dims="1" height="550" sizes="(max-width: 953px) 100vw, 953px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=953%2C550" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?w=953 953w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=300%2C173 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=768%2C443 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=200%2C115 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=690%2C398 690w" width="953"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Observe que isso é exatamente o mesmo que o algoritmo de retropropagação padrão que usamos nas Redes Neurais Profundas da Feedforward. A principal diferença é que resumimos os gradientes para W em cada etapa de tempo. Em um rede neural tradicional, não compartilhamos parâmetros entre camadas, portanto, não precisamos somar nada. Mas, na minha opinião, o BPTT é apenas um nome sofisticado para retropropagação padrão em uma RNN “desenrolada”. Assim como com Backpropagation, você pode definir um vetor delta que você repassa, por exemplo:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form7" class="aligncenter size-full wp-image-1299" data-attachment-id="1299" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form7" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?fit=183%2C30" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?fit=183%2C30" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?fit=183%2C30" data-orig-size="183,30" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form7-3/" data-recalc-dims="1" height="30" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?resize=183%2C30" width="183"/>
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Aqui um exemplo de como seria a implementação do BPTT em Python:
  </span>
 </p>
 <p>
  <a href="https://github.com/dsacademybr" rel="noopener noreferrer" target="_blank">
   <img alt="bptt" class="aligncenter wp-image-1308 size-large" data-attachment-id="1308" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="bptt" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?fit=1024%2C791" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?fit=300%2C232" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?fit=1248%2C964" data-orig-size="1248,964" data-permalink="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/bptt-2/" data-recalc-dims="1" height="791" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=1024%2C791" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=1024%2C791 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=300%2C232 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=768%2C593 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=200%2C154 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=690%2C533 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?w=1248 1248w" width="1024"/>
  </a>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso também deve lhe dar uma ideia do motivo pelo qual as RNNs são difíceis de treinar: as sequências (frases) podem ser bastante longas, talvez 20 palavras ou mais, e, portanto, você precisa retroceder através de várias camadas. Na prática, muitas pessoas truncam a retropropagação em poucos passos. Mas isso é assunto para o próximo capítulo! Até lá.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O BPTT é estudado em detalhes no curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
     Deep Learning II
    </a>
   </span>
   , na Data Science Academy e usado para construir aplicações de tradução de idiomas e geração automática de legendas em vídeos. O conceito também é aplicado no curso de
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz" rel="noopener noreferrer" target="_blank">
     Processamento de Linguagem Natural
    </a>
   </span>
   .
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Cheatsheet
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://skymind.ai/wiki/lstm" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    A Beginner’s Guide to LSTMs and Recurrent Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1289" href="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1289" href="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1289" href="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1289" href="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1289-5e0dd1c52d1d7" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1289&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1289-5e0dd1c52d1d7" id="like-post-wrapper-140353593-1289-5e0dd1c52d1d7">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-50">
 Capítulo 50 – A Matemática da Dissipação do Gradiente e Aplicações das RNNs
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No
   <a href="http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/" rel="noopener noreferrer" target="_blank">
    <span style="text-decoration: underline;">
     Capítulo 34
    </span>
   </a>
   nós discutimos sobre o problema da dissipação do gradiente e a dificuldade em treinar as redes neurais artificiais. Com as RNNs esse problema é ainda mais acentuado e por isso vamos agora estudar A Matemática da Dissipação do Gradiente e Aplicações das RNNs e compreender matematicamente porque o problema acontece.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mencionamos anteriormente que as RNNs têm dificuldades em aprender dependências de longo alcance – interações entre palavras que estão separadas por vários passos, por exemplo. Isso é problemático porque o significado de uma frase em português é geralmente determinado por palavras que não são muito próximas: “O homem que usava uma peruca entrou no bar”. A frase é realmente sobre um homem entrando em um bar, não sobre a peruca. Mas é improvável que uma RNN simples seja capaz de capturar essas informações. Para entender porque, vamos dar uma olhada mais de perto no gradiente que calculamos no
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/" rel="noopener noreferrer" target="_blank">
     capítulo anterior
    </a>
   </span>
   :
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter size-full wp-image-1321" data-attachment-id="1321" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?fit=387%2C96" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?fit=300%2C74" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?fit=387%2C96" data-orig-size="387,96" data-permalink="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form1-7/" data-recalc-dims="1" height="96" sizes="(max-width: 387px) 100vw, 387px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?resize=387%2C96" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?w=387 387w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?resize=300%2C74 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?resize=200%2C50 200w" width="387"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Observe que:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-1322" data-attachment-id="1322" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?fit=43%2C55" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?fit=43%2C55" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?fit=43%2C55" data-orig-size="43,55" data-permalink="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form2-13/" data-recalc-dims="1" height="55" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?resize=43%2C55" width="43"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   é uma regra de cadeia em si! Por exemplo:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form3" class="aligncenter size-full wp-image-1323" data-attachment-id="1323" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?fit=191%2C55" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?fit=191%2C55" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?fit=191%2C55" data-orig-size="191,55" data-permalink="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form3-11/" data-recalc-dims="1" height="55" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?resize=191%2C55" width="191"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Observe também que, como estamos tomando a derivada de uma função vetorial em relação a um vetor, o resultado é uma matriz (chamada de
   <span style="text-decoration: underline;">
    <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant" rel="noopener noreferrer" target="_blank">
     matriz jacobiana
    </a>
   </span>
   ) cujos elementos são todos derivadas
   <em>
    pointwise
   </em>
   . Podemos reescrever o gradiente acima da seguinte forma:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form4" class="aligncenter size-full wp-image-1324" data-attachment-id="1324" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?fit=557%2C102" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?fit=300%2C55" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?fit=557%2C102" data-orig-size="557,102" data-permalink="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form4-7/" data-recalc-dims="1" height="102" sizes="(max-width: 557px) 100vw, 557px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?resize=557%2C102" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?w=557 557w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?resize=300%2C55 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?resize=200%2C37 200w" width="557"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Acontece (esse
   <span style="text-decoration: underline;">
    <a href="http://proceedings.mlr.press/v28/pascanu13.pdf" rel="noopener noreferrer" target="_blank">
     paper
    </a>
   </span>
   explica isso em detalhes) que a norma (na álgebra linear, análise funcional e áreas relacionadas da matemática, uma norma é uma função que atribui um comprimento ou tamanho estritamente positivo a cada vetor em um espaço vetorial – exceto para o vetor zero, ao qual é atribuído um comprimento de zero. Mais detalhes
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener noreferrer" target="_blank">
     aqui
    </a>
   </span>
   .), que você pode pensar como um valor absoluto, da matriz jacobiana acima tem um limite superior de 1. Isso porque a nossa função de ativação tanh (ou sigmóide) mapeia todos os valores em um intervalo entre -1 e 1, e a derivada é limitada por 1 (1/4 no caso de sigmoide) também:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="tanh" class="aligncenter size-full wp-image-1325" data-attachment-id="1325" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="tanh" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?fit=640%2C480" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?fit=300%2C225" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?fit=640%2C480" data-orig-size="640,480" data-permalink="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/tanh/" data-recalc-dims="1" height="480" sizes="(max-width: 640px) 100vw, 640px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?resize=640%2C480" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?w=640 640w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?resize=200%2C150 200w" width="640"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Você pode ver que as funções tanh e sigmoid têm derivadas de 0 em ambas as extremidades, onde se aproximam de uma linha plana. Quando isso acontece, dizemos que os neurônios correspondentes estão saturados. Eles têm um gradiente nulo e conduzem outros gradientes nas camadas anteriores para 0. Assim, com valores pequenos nas multiplicações de matriz e múltiplas matrizes (t-k em particular) os valores de gradiente estão diminuindo exponencialmente rápido, desaparecendo completamente após alguns passos de tempo. Contribuições gradientes de etapas “longínquas” se tornam zero e o estado nessas etapas não contribui para o que a rede está aprendendo: a rede acaba não aprendendo dependências de longo alcance. Dissipações do gradiente não são exclusivos das RNNs e também acontecem em Redes Neurais Profundas Feedforward. Mas as RNNs tendem a ser muito profundas (tão profundas quanto a duração da sentença, em um problema de Processamento de Linguagem Natural por exemplo), o que torna o problema muito mais comum.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   É fácil imaginar que, dependendo de nossas funções de ativação e parâmetros de rede, poderíamos obter explosão em vez de dissipação de gradientes, se os valores da matriz Jacobiana forem grandes. Na verdade, isso é chamado de problema de explosão do gradiente. A razão pela qual as dissipações do gradiente receberam mais atenção do que as explosões é dupla. Por um lado, explosão de gradientes são óbvias. Seus gradientes se tornarão NaN (não um número) e seu programa falhará. Em segundo lugar, recortar os gradientes em um limiar pré-definido é uma solução muito simples e eficaz para evitar a explosão dos gradientes. As dissipações dos gradientes são mais problemáticas porque não são óbvias quando ocorrem ou é mais complicado lidar com elas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Felizmente, existem algumas maneiras de combater o problema da dissipação do gradiente. A inicialização adequada da matriz W pode reduzir o efeito do problema. Ou seja, aplicamos regularização. Uma solução mais interessante é usar as funções de ativação ReLU em vez de tanh ou sigmóide. A derivada ReLU é uma constante de 0 ou 1, por isso não é tão provável que sofra de dissipação do gradiente. Uma solução ainda mais popular é usar as arquiteturas Long Short-Term Memory (LSTM) ou Gated Recurrent Unit (GRU). As LSTMs foram propostas pela primeira vez em 1997 e são os modelos talvez mais amplamente usados em Processamento de Linguagem Natural atualmente. As GRUs, propostas pela primeira vez em 2014, são versões simplificadas das LSTMs. Ambas as arquiteturas RNN foram explicitamente projetadas para lidar com dissipação do gradiente e aprender eficientemente dependências de longo alcance. Vamos cobrir as duas arquiteturas nos próximos capítulos.
  </span>
 </p>
 <p>
 </p>
 <h2 style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Mas o que podemos fazer com as RNNs?
   </span>
  </strong>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As RNNs mostraram grande sucesso em muitas tarefas de Processamento de Linguagem Natural. Neste ponto, devo mencionar que os tipos de RNN mais usados são as LSTMs, que são muito melhores na captura de dependências de longo prazo do que as RNNs em sua arquitetura padrão. Mas não se preocupe, as LSTMs são essencialmente a mesma coisa que as RNNs, mas apenas têm uma maneira diferente de computar o estado oculto. Cobriremos as LSTMs com mais detalhes no próximo capítulo. Aqui estão alguns exemplos de aplicações de RNNs em Processamento de Linguagem Natural (o que não é uma lista definitiva).
  </span>
 </p>
 <h4 style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Modelagem de Linguagem e Geração de Texto
   </span>
  </strong>
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Dada uma sequência de palavras, queremos prever a probabilidade de cada palavra dada às palavras anteriores. Os Modelos de Linguagem nos permitem medir a probabilidade de uma sentença, que é uma entrada importante para a Tradução Automática (já que as sentenças de alta probabilidade estão normalmente corretas). Um efeito colateral de poder prever a próxima palavra é que obtemos um modelo generativo, que nos permite gerar um novo texto por amostragem a partir das probabilidades de saída. E dependendo de quais são nossos dados de treinamento, podemos gerar todos os tipos de coisas. Em Modelagem de Linguagem, nossa entrada é tipicamente uma sequência de palavras (codificadas como vetores únicos), e nossa saída é a sequência de palavras previstas. Ao treinar a rede, definimos ot = x{t + 1}, pois queremos que a saída na etapa t seja a próxima palavra real.
  </span>
 </p>
 <h4 style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Machine Translation
   </span>
  </strong>
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A tradução automática é semelhante à modelagem de linguagem, pois nossa entrada é uma sequência de palavras em nosso idioma de origem (por exemplo português). Queremos produzir uma sequência de palavras em nosso idioma de destino (por exemplo, inglês). A principal diferença é que nossa saída só é iniciada depois de termos visto a entrada completa, porque a primeira palavra de nossas sentenças traduzidas pode exigir informações capturadas da sequência de entrada completa.
  </span>
 </p>
 <h4 style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Reconhecimento de Fala
   </span>
  </strong>
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Dada uma sequência de entrada de sinais acústicos de uma onda sonora, podemos prever uma sequência de segmentos fonéticos juntamente com suas probabilidades.
  </span>
 </p>
 <h4 style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Gerar Descrições de Imagens
   </span>
  </strong>
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Juntamente com as redes neurais convolucionais, as RNNs foram usados como parte de um modelo para gerar descrições de imagens não rotuladas. É incrível como isso parece funcionar. O modelo combinado alinha as palavras geradas com os recursos encontrados nas imagens.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Todos esses exemplos de aplicações das RNNs são mostrados na prática nos cursos
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
     Deep Learning II
    </a>
   </span>
   e
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz" rel="noopener noreferrer" target="_blank">
     Processamento de Linguagem Natural
    </a>
   </span>
   .
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000;" target="_blank">
    <span style="text-decoration: underline;">
     Formação Inteligência Artificial
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.aclweb.org/anthology/P14-1140" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     A Recursive Recurrent Neural Network for Statistical Machine Translation
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Sequence to Sequence Learning with Neural Networks
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent Neural Networks Cheatsheet
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://proceedings.mlr.press/v28/pascanu13.pdf" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     On the difficulty of training recurrent neural networks
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://skymind.ai/wiki/lstm" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     A Beginner’s Guide to LSTMs and Recurrent Neural Networks
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Train longer, generalize better: closing the generalization gap in large batch training of neural networks
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks and Deep Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Recurrent neural network based language model
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled" style="text-align: justify;">
 </div>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1320" href="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1320" href="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1320" href="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1320" href="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1320-5e0dd1c76a067" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1320&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1320-5e0dd1c76a067" id="like-post-wrapper-140353593-1320-5e0dd1c76a067">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-51">
 Capítulo 51 – Arquitetura de Redes Neurais Long Short Term Memory (LSTM)
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Estudamos as redes neurais recorrentes e suas limitações nos capítulos anteriores. Para superar alguns dos problemas das RNNs, podemos usar algumas de suas variações. Uma delas é chamada LSTM ou Long Short Term Memory, um tipo de rede neural recorrente, que é usada em diversos cenários de Processamento de Linguagem Natural. Neste capítulo estudaremos a Arquitetura de Redes Neurais Long Short Term Memory.
  </span>
 </p>
 <h2>
  <span style="color: #000000;">
   Precisamos de Memória
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os humanos não começam a pensar do zero a cada segundo. Ao ler este capítulo, você entende cada palavra com base em sua compreensão das palavras anteriores. Você não joga tudo fora e começa a pensar de novo a cada palavra que você lê. Seus pensamentos têm persistência.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As redes neurais tradicionais não podem fazer isso, o que dificulta sua aplicação para resolver diversos problemas. Por exemplo, imagine que você queira classificar o tipo de evento que está acontecendo em todos os pontos de um filme. Não está claro como uma rede neural tradicional poderia usar o aprendizado sobre eventos anteriores no filme para informar os posteriores.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Redes neurais recorrentes resolvem esse problema. São redes com loops, permitindo que as informações persistam.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esses laços fazem com que as redes neurais recorrentes pareçam misteriosas. No entanto, se você pensar um pouco mais, perceberá que não são tão diferentes de uma rede neural
   <em>
    normal
   </em>
   . Uma rede neural recorrente pode ser imaginada como múltiplas cópias da mesma rede, cada uma passando uma mensagem a um sucessor. Considere o que acontece se desenrolarmos o loop:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="RNN-unrolled" class="aligncenter wp-image-1351 size-large" data-attachment-id="1351" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="RNN-unrolled" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?fit=1024%2C269" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?fit=300%2C79" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?fit=2706%2C711" data-orig-size="2706,711" data-permalink="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/rnn-unrolled/" data-recalc-dims="1" height="269" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=1024%2C269" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=1024%2C269 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=300%2C79 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=768%2C202 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=200%2C53 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=690%2C181 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?w=2340 2340w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa natureza de cadeia revela que redes neurais recorrentes estão intimamente relacionadas a sequências e listas, uma arquitetura natural da rede neural a ser usada para esses dados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nos últimos anos, tem havido um incrível sucesso ao aplicar as RNNs a uma variedade de problemas: reconhecimento de fala, modelagem de idiomas, tradução, legendas de imagens… A lista continua. Deixarei a discussão sobre os incríveis feitos que podemos alcançar com as RNNs com o excelente post de Andrej Karpathy,
   <span style="text-decoration: underline;">
    <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener noreferrer" target="_blank">
     The Unreasonable Effectiveness of Recurrent Neural Networks
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Entretanto, boa parte do sucesso das RNNs se deve a uma de suas variações, as LSTMs, um tipo muito especial de rede neural recorrente que funciona, para muitas tarefas, muito melhor do que a versão padrão. Quase todos os resultados empolgantes baseados em redes neurais recorrentes são alcançados com LSTMs. Vamos então compreender o que torna as LSTMs tão especiais.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Arquitetura da LSTM
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A LSTM é uma arquitetura de rede neural recorrente (RNN) que “
   <em>
    lembra”
   </em>
   valores em intervalos arbitrários. A LSTM é bem adequada para classificar, processar e prever séries temporais com intervalos de tempo de duração desconhecida. A insensibilidade relativa ao comprimento do gap dá uma vantagem à LSTM em relação a RNNs tradicionais (também chamadas “vanilla”), Modelos Ocultos de Markov (MOM) e outros métodos de aprendizado de sequências.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A estrutura de uma RNN é muito semelhante ao Modelo Oculto de Markov. No entanto, a principal diferença é como os parâmetros são calculados e construídos. Uma das vantagens da LSTM é a insensibilidade ao comprimento do gap. RNN e MOM dependem do estado oculto antes da emissão / sequência. Se quisermos prever a sequência após 1.000 intervalos em vez de 10, o modelo esqueceu o ponto de partida até então. Mas um modelo LSTM é capaz de “lembrar” por conta de sua estrutura de células, o diferencial da arquitetura LSTM.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   A LSTM possui uma estrutura em cadeia que contém quatro redes neurais e diferentes blocos de memória chamados células.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="lstmcell" class="aligncenter size-full wp-image-1361" data-attachment-id="1361" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="lstmcell" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?fit=542%2C357" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?fit=300%2C198" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?fit=542%2C357" data-orig-size="542,357" data-permalink="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/lstmcell/" data-recalc-dims="1" height="357" sizes="(max-width: 542px) 100vw, 542px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?resize=542%2C357" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?w=542 542w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?resize=300%2C198 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?resize=200%2C132 200w" width="542"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A informação é retida pelas células e as manipulações de memória são feitas pelos portões (gates). Existem três portões:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Forget Gate
   </strong>
   : As informações que não são mais úteis no estado da célula são removidas com o forget gate. Duas entradas: x_t (entrada no momento específico) e h_t-1 (saída de célula anterior) são alimentadas ao gate e multiplicadas por matrizes de peso, seguidas pela adição do bias. O resultante é passado por uma função de ativação que fornece uma saída binária. Se para um determinado estado de célula a saída for 0, a informação é esquecida e para a saída 1, a informação é retida para uso futuro.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Input Gate
   </strong>
   : A adição de informações úteis ao estado da célula é feita pelo input gate. Primeiro, a informação é regulada usando a função sigmoide que filtra os valores a serem lembrados de forma similar ao forget gate usando as entradas h_t-1 e x_t. Então, um vetor é criado usando a função tanh que dá saída de -1 a +1, que contém todos os valores possíveis de h_t-1 e x_t. Os valores do vetor e os valores regulados são multiplicados para obter as informações úteis
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Output Gate
   </strong>
   : A tarefa de extrair informações úteis do estado da célula atual para ser apresentadas como uma saída é feita pelo output gate. Primeiro, um vetor é gerado aplicando a função tanh na célula. Então, a informação é regulada usando a função sigmóide que filtra os valores a serem lembrados usando as entradas h_t-1 e x_t. Os valores do vetor e os valores regulados são multiplicados para serem enviados como uma saída e entrada para a próxima célula.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A célula RNN recebe duas entradas, a saída do último estado oculto e a observação no tempo = t. Além do estado oculto, não há informações sobre o passado para se lembrar. A memória de longo prazo é geralmente chamada de estado da célula. As setas em loop indicam a natureza recursiva da célula. Isso permite que as informações dos intervalos anteriores sejam armazenadas na célula LSTM. O estado da célula é modificado pelo forget gate colocado abaixo do estado da célula e também ajustado pela porta de modulação de entrada. Da equação, o estado da célula anterior esquece, multiplica-se com a porta do esquecimento e adiciona novas informações através da saída das portas de entrada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Algumas das famosas aplicações das LSTMs incluem:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Modelagem de Linguagem
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Tradução de Idiomas
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Legendas em Imagens
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Geração de Texto
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Chatbots
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Todas as aplicações acima mencionadas são mostrados na prática nos cursos
  </span>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
    Deep Learning II
   </a>
  </span>
  e
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz" rel="noopener noreferrer" target="_blank">
    Processamento de Linguagem Natural
   </a>
  </span>
  .
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Continuamos no próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Formação Inteligência Artificial
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Customizando Redes Neurais com Funções de Ativação Alternativas
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.aclweb.org/anthology/P14-1140" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    A Recursive Recurrent Neural Network for Statistical Machine Translation
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Sequence to Sequence Learning with Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Cheatsheet
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://proceedings.mlr.press/v28/pascanu13.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    On the difficulty of training recurrent neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://skymind.ai/wiki/lstm" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    A Beginner’s Guide to LSTMs and Recurrent Neural Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Long Short-Term Memory (LSTM): Concept
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent neural network based language model
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1350" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1350" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1350" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1350" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1350-5e0dd1ca8d736" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1350&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1350-5e0dd1ca8d736" id="like-post-wrapper-140353593-1350-5e0dd1ca8d736">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-52">
 Capítulo 52 – Arquitetura de Redes Neurais Gated Recurrent Unit (GRU)
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste capítulo estudaremos um tipo realmente fascinante de rede neural. Introduzido por
   <span style="text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1406.1078v3.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Cho, et al.
    </a>
   </span>
   em 2014, a GRU (Gated Recurrent Unit) visa resolver o problema da dissipação do gradiente que é comum em uma rede neural recorrente padrão. A GRU também pode ser considerada uma variação da
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     LSTM
    </a>
   </span>
   porque ambas são projetadas de maneira semelhante e, em alguns casos, produzem resultados igualmente excelentes. Para acompanhar este capítulo você precisa ter concluído os capítulos anteriores. A GRU é estudada na prática
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     aqui
    </a>
   </span>
   .
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   O Problema, Memória de Curto Prazo
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Redes neurais recorrentes sofrem de memória de curto prazo. Se uma sequência for longa o suficiente, elas terão dificuldade em transportar informações das etapas anteriores para as posteriores. Portanto, se você estiver tentando processar um parágrafo de texto para fazer previsões, as RNNs poderão deixar de fora informações importantes desde o início.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Durante a etapa de backpropagation, as redes neurais recorrentes sofrem com o problema da dissipação do gradiente. Gradientes são valores usados para atualizar os pesos das redes neurais. O problema da dissipação do gradiente é quando o gradiente diminui à medida que se propaga novamente ao longo do tempo. Se um valor de gradiente se torna extremamente pequeno, não contribui muito com o aprendizado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim, nas redes neurais recorrentes, as camadas que recebem uma pequena atualização gradiente param de aprender. Portanto, como essas camadas não aprendem, as RNNs podem esquecer o que foi visto em sequências mais longas, tendo assim uma memória de curto prazo.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   LSTM e GRU Como Solução
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   LSTM e GRU foram criadas como a solução para a memória de curto prazo. Elas têm mecanismos internos chamados portões que podem regular o fluxo de informações.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rnns" class="aligncenter wp-image-1386 size-large" data-attachment-id="1386" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rnns" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?fit=1024%2C651" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?fit=300%2C191" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?fit=1516%2C964" data-orig-size="1516,964" data-permalink="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/rnns/" data-recalc-dims="1" height="651" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=1024%2C651" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=1024%2C651 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=300%2C191 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=768%2C488 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=200%2C127 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=690%2C439 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?w=1516 1516w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esses portões podem aprender quais dados em uma sequência são importantes para manter ou jogar fora. Ao fazer isso, eles podem transmitir informações relevantes ao longo de uma longa cadeia de sequências para fazer previsões. Quase todos os resultados de última geração baseados em redes neurais recorrentes são alcançados com essas duas redes. LSTM e GRU podem ser usadas em reconhecimento de voz, síntese de fala e geração de texto. Você pode até usá-las para gerar legendas em vídeos. Essas são aplicações de ponta em
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Inteligência Artificial.
    </a>
   </span>
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Como as GRUs Funcionam?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A GRU é a nova geração de redes neurais recorrentes e é bastante semelhante a uma LSTM. As GRUs se livraram do estado da célula e usaram o estado oculto para transferir informações. Essa arquitetura possui apenas dois portões, um portão de redefinição (reset gate) e um portão de atualização (update date). As GRUs são uma versão melhorada da rede neural recorrente padrão. Mas o que as torna tão especiais e eficazes?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="gru4" class="aligncenter wp-image-1387 size-large" data-attachment-id="1387" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gru4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?fit=1024%2C835" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?fit=300%2C245" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?fit=1042%2C850" data-orig-size="1042,850" data-permalink="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gru4/" data-recalc-dims="1" height="835" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=1024%2C835" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=1024%2C835 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=300%2C245 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=768%2C626 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=200%2C163 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=690%2C563 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?w=1042 1042w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para resolver o problema da dissipação do gradiente de uma RNN padrão, a GRU usa dois portões, reset e update gate. Basicamente, eles são dois vetores que decidem quais informações devem ser passadas para a saída. O que há de especial neles é que eles podem ser treinados para manter informações de muito tempo atrás, sem dissipá-las com o tempo ou remover informações irrelevantes para a previsão.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A estrutura da GRU permite capturar adaptativamente dependências de grandes sequências de dados sem descartar informações de partes anteriores da sequência. Isso é alcançado através de suas unidades de portões, semelhantes às das LSTMs. Esses portões são responsáveis por regular as informações a serem mantidas ou descartadas a cada etapa do tempo.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <p style="text-align: justify;">
   <span style="color: #000000;">
    <img alt="gru5" class="aligncenter size-full wp-image-1393" data-attachment-id="1393" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gru5" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?fit=912%2C477" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?fit=300%2C157" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?fit=912%2C477" data-orig-size="912,477" data-permalink="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gru5/" data-recalc-dims="1" height="477" sizes="(max-width: 912px) 100vw, 912px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=912%2C477" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?w=912 912w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=300%2C157 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=768%2C402 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=200%2C105 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=690%2C361 690w" width="912"/>
   </span>
  </p>
  <p>
  </p>
  <p style="text-align: justify;">
   <span style="color: #000000;">
    A capacidade da GRU de manter dependências ou memória de longo prazo decorre dos cálculos na célula da GRU para produzir o estado oculto. Enquanto as LSTMs têm dois estados diferentes passados entre as células – o estado da célula e o estado oculto, que carregam a memória de longo e curto prazo, respectivamente – as GRUs têm apenas um estado oculto transferido entre as etapas do tempo. Esse estado oculto é capaz de manter as dependências de longo e curto prazo ao mesmo tempo, devido aos mecanismos de restrição e cálculos pelos quais o estado oculto e os dados de entrada passam.
   </span>
  </p>
  <p>
  </p>
  <p style="text-align: justify;">
   <p style="text-align: justify;">
    <span style="color: #000000;">
     <img alt="gruxlstm" class="aligncenter size-full wp-image-1394" data-attachment-id="1394" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gruxlstm" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1024%2C411" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=300%2C120" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1031%2C414" data-orig-size="1031,414" data-permalink="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gruxlstm/" data-recalc-dims="1" height="414" sizes="(max-width: 1031px) 100vw, 1031px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1031%2C414" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?w=1031 1031w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=300%2C120 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=768%2C308 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1024%2C411 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=200%2C80 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=690%2C277 690w" width="1031"/>
    </span>
   </p>
   <p>
   </p>
   <p style="text-align: justify;">
    <span style="color: #000000;">
     Assim como os portões das LSTMs, os portões na GRU são treinados para filtrar seletivamente qualquer informação irrelevante, mantendo o que é útil. Esses portões são essencialmente vetores contendo valores entre 0 e 1 que serão multiplicados com os dados de entrada e / ou estado oculto. Um valor 0 nos vetores indica que os dados correspondentes no estado de entrada ou oculto não são importantes e, portanto, retornarão como zero. Por outro lado, um valor 1 no vetor significa que os dados correspondentes são importantes e serão usados.
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="color: #000000;">
     No próximo capítulo veremos os detalhes matemáticos por trás da GRU, uma das arquiteturas mais interessantes de Deep Learning e que tem obtido resultados formidáveis especialmente em Processamento de Linguagem Natural.
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="color: #000000;">
     Aqui você encontra uma animação que ajuda a compreender o funcionamento das arquiteturas de Deep Learning do tipo recorrente:
     <span style="text-decoration: underline;">
      <a href="https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45" rel="noopener noreferrer" target="_blank">
       Animated RNN, LSTM and GRU.
      </a>
     </span>
    </span>
   </p>
   <p>
    <span style="color: #000000;">
     Até o próximo capítulo!
    </span>
   </p>
   <p>
   </p>
   <p style="text-align: justify;">
    <span style="color: #000000;">
     Referências:
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Formação Inteligência Artificial
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Formação Análise Estatística Para Cientistas de Dados
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Formação Cientista de Dados
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Customizando Redes Neurais com Funções de Ativação Alternativas
      </a>
     </span>
    </span>
   </p>
   <p>
    <span style="text-decoration: underline;">
     <a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be" rel="noopener noreferrer" target="_blank">
      <span style="color: #000000; text-decoration: underline;">
       Understanding GRU Networks
      </span>
     </a>
    </span>
   </p>
   <p>
    <span style="text-decoration: underline;">
     <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noopener noreferrer" target="_blank">
      <span style="color: #000000; text-decoration: underline;">
       Illustrated Guide to LSTM’s and GRU’s: A step by step explanation
      </span>
     </a>
    </span>
   </p>
   <p>
    <span style="text-decoration: underline;">
     <a href="https://blog.floydhub.com/gru-with-pytorch/" rel="noopener noreferrer" target="_blank">
      <span style="color: #000000; text-decoration: underline;">
       Gated Recurrent Unit (GRU)
      </span>
     </a>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://www.aclweb.org/anthology/P14-1140" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       A Recursive Recurrent Neural Network for Statistical Machine Translation
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Sequence to Sequence Learning with Neural Networks
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Recurrent Neural Networks Cheatsheet
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://proceedings.mlr.press/v28/pascanu13.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       On the difficulty of training recurrent neural networks
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://skymind.ai/wiki/lstm" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       A Beginner’s Guide to LSTMs and Recurrent Neural Networks
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Long Short-Term Memory (LSTM): Concept
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Train longer, generalize better: closing the generalization gap in large batch training of neural networks
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Practical Recommendations for Gradient-Based Training of Deep Architectures
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Gradient-Based Learning Applied to Document Recognition
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Neural Networks &amp; The Backpropagation Algorithm, Explained
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Neural Networks and Deep Learning
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Recurrent neural network based language model
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Gradient Descent For Machine Learning
      </a>
     </span>
    </span>
   </p>
   <p style="text-align: justify;">
    <span style="text-decoration: underline;">
     <span style="color: #000000; text-decoration: underline;">
      <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
       Pattern Recognition and Machine Learning
      </a>
     </span>
    </span>
   </p>
   <div class="sharedaddy sd-sharing-enabled">
    <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
     <h3 class="sd-title">
      Compartilhe isso:
     </h3>
     <div class="sd-content">
      <ul>
       <li class="share-twitter">
        <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1381" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
         <span>
         </span>
         <span class="sharing-screen-reader-text">
          Clique para compartilhar no Twitter(abre em nova janela)
         </span>
        </a>
       </li>
       <li class="share-facebook">
        <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1381" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
         <span>
         </span>
         <span class="sharing-screen-reader-text">
          Clique para compartilhar no Facebook(abre em nova janela)
         </span>
        </a>
       </li>
       <li class="share-linkedin">
        <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1381" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
         <span>
         </span>
         <span class="sharing-screen-reader-text">
          Clique para compartilhar no LinkedIn(abre em nova janela)
         </span>
        </a>
       </li>
       <li class="share-pinterest">
        <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1381" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
         <span>
         </span>
         <span class="sharing-screen-reader-text">
          Clique para compartilhar no Pinterest(abre em nova janela)
         </span>
        </a>
       </li>
       <li class="share-tumblr">
        <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
         <span>
         </span>
         <span class="sharing-screen-reader-text">
          Clique para compartilhar no Tumblr(abre em nova janela)
         </span>
        </a>
       </li>
       <li class="share-jetpack-whatsapp">
        <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
         <span>
         </span>
         <span class="sharing-screen-reader-text">
          Clique para compartilhar no WhatsApp(abre em nova janela)
         </span>
        </a>
       </li>
       <li class="share-end">
       </li>
      </ul>
     </div>
    </div>
   </div>
   <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1381-5e0dd1ccafe28" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1381&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1381-5e0dd1ccafe28" id="like-post-wrapper-140353593-1381-5e0dd1ccafe28">
    <h3 class="sd-title">
     Curtir isso:
    </h3>
    <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
     <span class="button">
      <span>
       Curtir
      </span>
     </span>
     <span class="loading">
      Carregando...
     </span>
    </div>
    <span class="sd-text-color">
    </span>
    <a class="sd-link-color">
    </a>
   </div>
   <div class="jp-relatedposts" id="jp-relatedposts">
    <h3 class="jp-relatedposts-headline">
     <em>
      Relacionado
     </em>
    </h3>
   </div>
  </p>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-53">
 Capítulo 53 – Matemática na GRU, Dissipação e Clipping do Gradiente
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A capacidade da rede GRU de manter dependências ou memória de longo prazo decorre dos cálculos na célula na GRU para produzir o estado oculto. As LSTMs têm dois estados diferentes passados entre as células – o estado da célula e o estado oculto, que carregam a memória de longo e curto prazo, respectivamente – as GRUs têm apenas um estado oculto transferido entre as etapas do tempo. Esse estado oculto é capaz de manter as dependências de longo e curto prazo ao mesmo tempo, devido aos mecanismos de restrição (portões) e cálculos pelos quais o estado oculto e os dados de entrada passam.
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="gruxlstm" class="aligncenter size-full wp-image-1394" data-attachment-id="1394" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gruxlstm" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1024%2C411" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=300%2C120" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1031%2C414" data-orig-size="1031,414" data-permalink="http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gruxlstm/" data-recalc-dims="1" height="414" sizes="(max-width: 1031px) 100vw, 1031px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1031%2C414" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?w=1031 1031w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=300%2C120 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=768%2C308 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1024%2C411 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=200%2C80 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=690%2C277 690w" width="1031"/>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A célula GRU contém apenas dois portões: o portão de atualização e o portão de redefinição. Assim como os portões das LSTMs, os portões na GRU são treinados para filtrar seletivamente qualquer informação irrelevante, mantendo o que é útil. Esses portões são essencialmente vetores contendo valores entre 0 e 1 que serão multiplicados com os dados de entrada e/ou estado oculto. Um valor 0 nos vetores indica que os dados correspondentes no estado de entrada ou oculto não são importantes e, portanto, retornarão como zero. Por outro lado, um valor 1 no vetor significa que os dados correspondentes são importantes e serão usados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Usaremos os termos gate e vetor de forma intercambiável para o restante deste capítulo, pois eles se referem à mesma coisa.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A estrutura de uma unidade GRU é mostrada abaixo.
  </span>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="gru1" class="aligncenter size-full wp-image-1408" data-attachment-id="1408" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gru1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?fit=912%2C477" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?fit=300%2C157" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?fit=912%2C477" data-orig-size="912,477" data-permalink="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/gru1/" data-recalc-dims="1" height="477" sizes="(max-width: 912px) 100vw, 912px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=912%2C477" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?w=912 912w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=300%2C157 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=768%2C402 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=200%2C105 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=690%2C361 690w" width="912"/>
  </span>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   E
  </span>
  <span style="color: #000000;">
   mbora a estrutura possa parecer bastante complicada devido ao grande número de conexões, o mecanismo por trás dela pode ser dividido em três etapas principais.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Reset Gate
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na primeira etapa, criamos o portão de redefinição (reset gate). Essa porta é derivada e calculada usando o estado oculto da etapa anterior e os dados de entrada na etapa atual.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Matematicamente, isso é conseguido multiplicando o estado oculto anterior e a entrada atual com seus respectivos pesos e somando-os antes de passar a soma através de uma função sigmóide. A função sigmoide transformará os valores entre 0 e 1, permitindo que o portão filtre entre as informações menos importantes e mais importantes nas etapas subsequentes. A fórmula matemática é representada abaixo:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter size-full wp-image-1409" data-attachment-id="1409" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?fit=978%2C126" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?fit=300%2C39" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?fit=978%2C126" data-orig-size="978,126" data-permalink="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form1-8/" data-recalc-dims="1" height="126" sizes="(max-width: 978px) 100vw, 978px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=978%2C126" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?w=978 978w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=300%2C39 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=768%2C99 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=200%2C26 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=690%2C89 690w" width="978"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Quando toda a rede é treinada através de backpropagation, os pesos na equação serão atualizados de forma que o vetor aprenda a reter apenas os recursos úteis.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O estado oculto anterior será primeiro multiplicado por um peso treinável e passará por uma multiplicação por elementos (produto Hadamard) com o vetor de redefinição. Esta operação decidirá quais informações serão mantidas nas etapas anteriores, juntamente com as novas entradas. Ao mesmo tempo, a entrada atual também será multiplicada por um peso treinável antes de ser somada com o produto do vetor de redefinição e do estado oculto anterior acima. Por fim, uma função tanh de ativação não linear será aplicada ao resultado final para obter r na equação abaixo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-1410" data-attachment-id="1410" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?fit=962%2C146" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?fit=300%2C46" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?fit=962%2C146" data-orig-size="962,146" data-permalink="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form2-14/" data-recalc-dims="1" height="146" sizes="(max-width: 962px) 100vw, 962px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=962%2C146" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?w=962 962w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=300%2C46 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=768%2C117 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=200%2C30 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=690%2C105 690w" width="962"/>
  </span>
 </p>
 <h2>
 </h2>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Update Gate
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em seguida, temos o portão de atualização (update gate). Assim como o reset gate, o update gate é calculado usando o estado oculto anterior e os dados de entrada atuais.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os vetores Update e Reset gate são criados usando a mesma fórmula, mas os pesos multiplicados pela entrada e pelo estado oculto são exclusivos para cada portão, o que significa que os vetores finais para cada portão são diferentes. Isso permite que os portões sirvam a seus propósitos específicos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form3" class="aligncenter size-full wp-image-1411" data-attachment-id="1411" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?fit=1024%2C144" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?fit=300%2C42" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?fit=1052%2C148" data-orig-size="1052,148" data-permalink="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form3-12/" data-recalc-dims="1" height="148" sizes="(max-width: 1052px) 100vw, 1052px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=1052%2C148" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?w=1052 1052w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=300%2C42 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=768%2C108 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=1024%2C144 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=200%2C28 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=690%2C97 690w" width="1052"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O vetor Update será submetido a multiplicação por elementos com o estado oculto anterior para obter u em nossa equação abaixo, que será usada para calcular nossa saída final posteriormente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="" class="aligncenter size-full wp-image-1412" data-attachment-id="1412" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?fit=500%2C166" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?fit=300%2C100" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?fit=500%2C166" data-orig-size="500,166" data-permalink="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form4-8/" data-recalc-dims="1" height="166" sizes="(max-width: 500px) 100vw, 500px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?resize=500%2C166" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?w=500 500w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?resize=300%2C100 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?resize=200%2C66 200w" width="500"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O vetor Update também será usado em outra operação posteriormente ao obter nossa saída final. O objetivo do update gate aqui é ajudar o modelo a determinar quanto das informações passadas armazenadas no estado oculto anterior precisam ser retidas para o futuro.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Combinando as Saídas
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na última etapa, reutilizaremos o portal Update e obteremos o estado oculto atualizado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Desta vez, pegaremos a versão inversa em elementos do mesmo vetor Update (1 – Update gate) e faremos uma multiplicação em elementos com a nossa saída do reset gate, r. O objetivo desta operação é o gate Update determinar qual parte das novas informações deve ser armazenada no estado oculto.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por fim, o resultado das operações acima será resumido com a nossa saída do portão Update na etapa anterior, u. Isso nos dará nosso novo e atualizado estado oculto.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form5" class="aligncenter size-full wp-image-1413" data-attachment-id="1413" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?fit=652%2C114" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?fit=300%2C52" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?fit=652%2C114" data-orig-size="652,114" data-permalink="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form5-6/" data-recalc-dims="1" height="114" sizes="(max-width: 652px) 100vw, 652px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?resize=652%2C114" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?w=652 652w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?resize=300%2C52 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?resize=200%2C35 200w" width="652"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos usar esse novo estado oculto como nossa saída para esse intervalo de tempo, passando-o por uma camada de ativação linear.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Solução do Problema de Dissipação/Explosão do Gradiente
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vimos os portões em ação. Sabemos como eles transformam nossos dados. Agora, vamos revisar seu papel geral no gerenciamento da memória da rede e falar sobre como eles resolvem o problema de dissipação/explosão do gradiente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como vimos nos mecanismos acima, o Reset Gate é responsável por decidir quais partes do estado oculto anterior devem ser combinadas com a entrada atual para propor um novo estado oculto.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   E o Update Gate é responsável por determinar quanto do estado oculto anterior deve ser retido e qual parte do novo estado oculto proposto (derivado do Reset Gate) deve ser adicionado ao estado oculto final. Quando o Update Gate é multiplicado pela primeira vez com o estado oculto anterior, a rede escolhe quais partes do estado oculto anterior ele manterá em sua memória enquanto descarta o restante. Posteriormente, ele corrige as partes ausentes das informações quando usa o inverso do gate Update para filtrar o novo estado oculto proposto a partir do Reset Gate.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso permite que a rede retenha dependências de longo prazo. O Update Gate pode optar por manter a maioria das memórias anteriores no estado oculto se os valores do vetor Update estiverem próximos de 1 sem recalcular ou alterar todo o estado oculto.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O problema de dissipação/explosão do gradiente ocorre durante a propagação de retorno (backpropagation) ao treinar a RNN, especialmente se a RNN estiver processando longas sequências ou tiver várias camadas. O erro do gradiente calculado durante o treinamento é usado para atualizar o peso da rede na direção certa e na magnitude certa. No entanto, esse gradiente é calculado com a regra da cadeia (
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/" rel="noopener noreferrer" target="_blank">
     chain rule
    </a>
   </span>
   ), começando no final da rede. Portanto, durante o backpropagation, os gradientes sofrerão continuamente multiplicações de matrizes e encolherão ou explodirão exponencialmente por sequências longas. Ter um gradiente muito pequeno significa que o modelo não atualiza seus pesos de maneira eficaz, enquanto gradientes extremamente grandes fazem com que o modelo seja instável.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os portões nas LSTM e GRUs ajudam a resolver esse problema devido ao componente aditivo dos portões de atualização. Enquanto as RNNs tradicionais sempre substituem todo o conteúdo do estado oculto a cada etapa, as LSTMs e GRUs mantêm a maior parte do estado oculto existente enquanto adicionam novo conteúdo sobre ele. Isso permite que os erros dos gradientes sejam propagados de volta sem desaparecer ou explodir muito rapidamente devido às operações de adição.
  </span>
 </p>
 <h2>
  <span style="color: #000000;">
   Clipping (Recorte) do Gradiente
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Embora LSTMs e GRUs sejam as correções mais usadas para o problema acima, outra solução para o problema de explosão de gradientes é o clipping do gradiente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O clipping do gradiente é mais comum em redes neurais recorrentes. Quando os gradientes estão sendo propagados no tempo, eles podem desaparecer porque são continuamente multiplicados por números menores que um. Isso é chamado de problema de dissipação do gradiente, podendo ser resolvido por LSTMs e GRUs e, se você estiver usando uma rede profunda feed-forward, isso é resolvido por conexões residuais. Por outro lado, você também pode ter  explosão dos gradientes. É quando eles se tornam exponencialmente grandes por serem multiplicados por números maiores que 1. O clipping do gradiente
   <em>
    cortará
   </em>
   os gradientes entre dois números para impedir que eles fiquem muito grandes.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O clipping define um valor limite definido nos gradientes, o que significa que, mesmo se um gradiente aumentar além do valor predefinido durante o treinamento, seu valor ainda será limitado ao limite definido. Dessa forma, a direção do gradiente permanece inalterada e apenas a magnitude do gradiente é alterada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O clipping do gradiente e os demais temas estudados neste capítulo são abordados em detalhes e na prática no curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
     Deep Learning II
    </a>
   </span>
   e então aplicados em problemas do mundo real no curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz" rel="noopener noreferrer" target="_blank">
     Processamento de Linguagem Natural
    </a>
   </span>
   .
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Até o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000;" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Customizando Redes Neurais com Funções de Ativação Alternativas
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Understanding GRU Networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Illustrated Guide to LSTM’s and GRU’s: A step by step explanation
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://blog.floydhub.com/gru-with-pytorch/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gated Recurrent Unit (GRU)
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.aclweb.org/anthology/P14-1140" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    A Recursive Recurrent Neural Network for Statistical Machine Translation
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Sequence to Sequence Learning with Neural Networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Cheatsheet
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://proceedings.mlr.press/v28/pascanu13.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    On the difficulty of training recurrent neural networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://skymind.ai/wiki/lstm" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    A Beginner’s Guide to LSTMs and Recurrent Neural Networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Long Short-Term Memory (LSTM): Concept
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent neural network based language model
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1407" href="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1407" href="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1407" href="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1407" href="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1407-5e0dd1cfeeef0" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1407&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1407-5e0dd1cfeeef0" id="like-post-wrapper-140353593-1407-5e0dd1cfeeef0">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-54">
 Capítulo 54 – Introdução às Redes Adversárias Generativas (GANs – Generative Adversarial Networks)
</h1>
<div class="entry-content">
 <blockquote>
  <p style="text-align: center;">
   <span style="color: #000000;">
    <strong>
     Você pode não achar que os programadores são artistas, mas a programação é uma profissão extremamente criativa. É criatividade baseada em lógica. – John Romero
    </strong>
   </span>
  </p>
 </blockquote>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Redes Adversárias Generativas (GANs) são arquiteturas de redes neurais profundas compostas por duas redes colocadas uma contra a outra (daí o nome “adversárias”). Esta é uma das arquiteturas mais recentes e mais fascinantes em Deep Learning e que estudaremos a partir de agora!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As GANs foram introduzidos em um
   <span style="text-decoration: underline;">
    <a href="https://arxiv.org/abs/1406.2661" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     artigo
    </a>
   </span>
   de Ian Goodfellow e outros pesquisadores da Universidade de Montreal, incluindo Yoshua Bengio, em 2014. Referindo-se às GANs, o diretor de pesquisa de IA do Facebook, Yann LeCun, chamou o treinamento adversário de “a ideia mais interessante nos últimos 10 anos em Machine Learning”.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O potencial das GANs é enorme porque elas podem aprender a imitar qualquer distribuição de dados. Ou seja, as GANs podem ser ensinadas a criar mundos estranhamente semelhantes aos nossos em qualquer domínio: imagens, música, fala, prosa. Elas são artistas robóticos, em certo sentido, e sua produção é impressionante – até comovente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em uma virada surreal, a
   <span style="text-decoration: underline;">
    <a href="https://twitter.com/ChristiesInc" rel="noopener noreferrer" target="_blank">
     Christie’s
    </a>
   </span>
   (famosa rede inglesa de leilões) vendeu um retrato de US $ 432.000 gerado por uma GAN, com base no código-fonte aberto escrito por Robbie Barrat, de Stanford. Como a maioria dos artistas de verdade, ele não viu nada do dinheiro, que foi para a empresa francesa Obvious.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="GANs" class="aligncenter size-full wp-image-1428" data-attachment-id="1428" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="GANs" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?fit=1024%2C502" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?fit=300%2C147" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?fit=2358%2C1156" data-orig-size="2358,1156" data-permalink="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gans-2/" data-recalc-dims="1" height="574" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=1170%2C574" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?w=2358 2358w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=300%2C147 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=768%2C377 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=1024%2C502 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=200%2C98 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=690%2C338 690w" width="1170"/>
  </span>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Dois dos nus gerados por GANs. Imagens de
   <span style="text-decoration: underline;">
    <a href="https://www.theverge.com/2018/10/23/18013190/ai-art-portrait-auction-christies-belamy-obvious-robbie-barrat-gans" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Robbie Barrat
    </a>
   </span>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em 2019, o DeepMind mostrou que os Autoencoders Variacionais (VAEs) poderiam superar as GANs na geração de faces. Estudaremos os Autoencoders mais a frente aqui no livro. GANs e VAEs são estudadas na prática no curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep Learning II.
    </a>
   </span>
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Algoritmos Generativos vs. Discriminativos
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Antes de entrar nos detalhes, vamos dar uma rápida visão geral do que são feitas as GANs. As Redes Adversárias Generativas pertencem ao conjunto de modelos generativos. Isso significa que eles são capazes de produzir / gerar (veremos como) novo conteúdo. Naturalmente, essa capacidade de gerar novo conteúdo faz com que as GANs pareçam um pouco “mágicas”, pelo menos à primeira vista. Nos capítulos seguintes superaremos a aparente mágica das GANs para mergulhar na matemática e modelagem por trás desses modelos. Não apenas discutiremos as noções fundamentais de que as Redes Adversárias Generativas se baseiam, mas estudaremos os conceitos principais dessa arquitetura passo a passo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para entender as GANs, você deve saber como os algoritmos generativos funcionam e, para isso, contrastá-los com algoritmos discriminativos é instrutivo. Algoritmos discriminativos tentam classificar os dados de entrada; isto é, dados os recursos de uma instância de dados, eles prevêem um rótulo ou categoria à qual esses dados pertencem. Praticamente tudo que estudamos no
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/" rel="noopener noreferrer" target="_blank">
     livro
    </a>
   </span>
   até aqui se refere a algoritmos discriminativos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por exemplo, dadas todas as palavras em um email (a instância de dados), um algoritmo discriminativo poderia prever se a mensagem é spam ou não é spam. O spam é um dos rótulos e o conjunto de palavras coletadas no email são os recursos que constituem os dados de entrada. Quando esse problema é expresso matematicamente, o rótulo é chamado y e os recursos são chamados x. A formulação p (y | x) é usada para significar “a probabilidade de y dado x”, que neste caso seria traduzido para “a probabilidade de um email ser spam, dadas as palavras que ele contém”.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Algoritmos discriminativos mapeiam recursos para rótulos e estão preocupados apenas com essa correlação. Uma maneira de pensar sobre algoritmos generativos é que eles fazem o oposto. Em vez de prever um rótulo com determinados recursos, eles tentam prever os recursos com um determinado rótulo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A pergunta que um algoritmo generativo tenta responder é: Supondo que este email seja spam, qual a probabilidade desses recursos? Enquanto os modelos discriminativos se preocupam com a relação entre y e x, os modelos generativos se preocupam com “como você obtém x”. Eles permitem capturar p (x | y), a probabilidade de x dado y ou a probabilidade de recursos com um rótulo ou categoria. Dito isto, algoritmos generativos também podem ser usados ​​como classificadores. Acontece que eles podem fazer mais do que categorizar dados de entrada. O que mais eles podem fazer? Ainda estamos descobrindo, pois a evolução está acontecendo agora nesse momento com pesquisas em todo mundo e
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
     Cientistas de Dados
    </a>
   </span>
   e
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" target="_blank">
     Engenheiros de IA
    </a>
   </span>
   trabalhando em diferentes aplicações.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Outra maneira de pensar sobre isso é distinguir discriminativo de generativo assim:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Modelos discriminativos aprendem a fronteira entre classes.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Modelos generativos modelam a distribuição de classes individuais.
   </span>
  </li>
 </ul>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Como as GANs Funcionam
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vejamos uma visão geral das GANs e nos próximos capítulos entraremos nos detalhes matemáticos e estatísticos da arquitetura.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma rede neural, chamada de gerador, gera novas instâncias de dados, enquanto a outra, o discriminador, avalia sua autenticidade; ou seja, o discriminador decide se cada instância de dados que ele analisa pertence ou não ao conjunto de dados de treinamento real (a imagem abaixo demonstra isso).
  </span>
 </p>
 <p>
  <img alt="gan_schema" class="aligncenter size-full wp-image-1429" data-attachment-id="1429" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gan_schema" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?fit=1024%2C375" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?fit=300%2C110" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?fit=1406%2C515" data-orig-size="1406,515" data-permalink="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gan_schema/" data-recalc-dims="1" height="429" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=1170%2C429" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?w=1406 1406w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=300%2C110 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=768%2C281 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=1024%2C375 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=200%2C73 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=690%2C253 690w" width="1170"/>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Digamos que estamos tentando fazer algo mais banal do que imitar a Mona Lisa. Geraremos números escritos à mão, como os encontrados no conjunto de dados MNIST, retirado do mundo real. O objetivo do discriminador, quando mostrada uma instância do verdadeiro conjunto de dados MNIST, é reconhecer aqueles que são autênticos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Enquanto isso, o gerador está criando novas imagens sintéticas que são transmitidas ao discriminador. O gerador gera as imagens fake na esperança de que elas também sejam consideradas autênticas, mesmo sendo falsas. O objetivo do gerador é gerar dígitos manuscritos cada vez melhores. O objetivo do discriminador é identificar imagens falsas do gerador. Ou seja, são duas redes adversárias, uma discriminativa (padrão que já estudamos até aqui no livro) e uma generativa que, em termos gerais, faz o oposto das redes discriminativas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui estão as etapas de uma GAN:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    O gerador considera números aleatórios e retorna uma imagem.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Essa imagem gerada é inserida no discriminador ao lado de um fluxo de imagens tiradas do conjunto de dados real e verdadeiro.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    O discriminador obtém imagens reais e falsas e retorna probabilidades, um número entre 0 e 1, com 1 representando uma previsão de imagem autêntica e 0 representando previsão de imagens falsas (geradas pela rede generativa).
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Então você tem um loop de feedback duplo assim:
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="GANs" class="aligncenter size-full wp-image-1430" data-attachment-id="1430" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="GANs" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?fit=1024%2C447" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?fit=300%2C131" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?fit=1213%2C529" data-orig-size="1213,529" data-permalink="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gans-1/" data-recalc-dims="1" height="510" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=1170%2C510" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?w=1213 1213w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=300%2C131 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=768%2C335 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=1024%2C447 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=200%2C87 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=690%2C301 690w" width="1170"/>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para o MNIST, a rede discriminadora é uma rede convolucional padrão que pode categorizar as imagens alimentadas, um classificador binomial que rotula as imagens como reais ou falsas. O gerador é uma rede convolucional inversa, em certo sentido: enquanto um classificador convolucional padrão recebe uma imagem e reduz a amostragem para produzir uma probabilidade, o gerador pega um vetor de ruído aleatório e faz o upsample para uma imagem. O primeiro joga fora os dados por meio de técnicas de downsampling, como o maxpool, e o segundo gera novos dados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ambas as redes estão tentando otimizar uma função objetivo (função de perda) diferente e oposta. À medida que o discriminador muda seu comportamento, o gerador também muda e vice-versa. Suas perdas empurram um contra o outro. Uma ideia simples e genial! Durante o treinamento, a rede generativa vai aprendendo a criar uma imagem fake que fica cada vez mais próxima de uma imagem real. Alguns pesquisadores tem feito o mesmo com a voz, gerando falas que são falsas, mas se parecem muito com falas verdadeiras. Esse é o perigo das Deep Fakes, quando a tecnologia é usada para o mal, infelizmente. Aqui tem um exemplo de Deep Fake gerada com GAN para fala do ex-presidente dos EUA, Barack Obama:
   <span style="text-decoration: underline;">
    <a href="https://www.youtube.com/watch?v=AmUC4m6w1wo" rel="noopener noreferrer" target="_blank">
     Fake Obama created using AI video tool – BBC News
    </a>
   </span>
   .
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="gan" class="aligncenter size-full wp-image-1432" data-attachment-id="1432" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="gan" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?fit=800%2C399" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?fit=300%2C150" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?fit=800%2C399" data-orig-size="800,399" data-permalink="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gan/" data-recalc-dims="1" height="399" sizes="(max-width: 800px) 100vw, 800px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=800%2C399" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?w=800 800w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=300%2C150 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=768%2C383 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=200%2C100 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=690%2C344 690w" width="800"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Quer aprender mais sobre as GANs? Então acompanhe os próximos capítulos!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Customizando Redes Neurais com Funções de Ativação Alternativas
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://skymind.ai/wiki/generative-adversarial-network-gan" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     A Beginner’s Guide to Generative Adversarial Networks (GANs)
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     A Leap into the Future: Generative Adversarial Networks
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Understanding Generative Adversarial Networks (GANs)
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     How A.I. Is Creating Building Blocks to Reshape Music and Art
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Train longer, generalize better: closing the generalization gap in large batch training of neural networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks and Deep Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent neural network based language model
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1426" href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1426" href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1426" href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1426" href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1426-5e0dd1d2016c6" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1426&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1426-5e0dd1d2016c6" id="like-post-wrapper-140353593-1426-5e0dd1d2016c6">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-55">
 Capítulo 55 – Geração de Variáveis Aleatórias – Uma das Bases dos Modelos Generativos em GANs (Generative Adversarial Networks)
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste capítulo discutiremos o processo de geração de variáveis aleatórias a partir de uma determinada distribuição, tema importante para compreender o funcionamento dos modelos generativos das GANs e nos próximos capítulos veremos os detalhes de funcionamento dos modelos generativos e os detalhes matemáticos das GANs. Estamos considerando que você leu o capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     anterior
    </a>
   </span>
   .
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Variáveis ​​aleatórias uniformes podem ser geradas pseudo-aleatoriamente
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos começar discutindo o processo de geração de variáveis ​​aleatórias. Veremos alguns métodos existentes e, mais especificamente, o método de transformação inversa que permite gerar variáveis ​​aleatórias complexas a partir de variáveis ​​aleatórias uniformes simples. Embora tudo isso possa parecer um pouco distante do nosso assunto, GANs, veremos no próximo capítulo o vínculo profundo que existe com os modelos generativos. Se o conceito de Estatística for algo que você esteja vendo pela primeira vez, a DSA oferece cursos completos de Estatística e Matemática na
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
    .
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma variável aleatória é uma variável quantitativa, cujo resultado (valor) depende de fatores aleatórios. Um exemplo de uma variável aleatória é o resultado do lançamento de um dado que pode resultar em qualquer número entre 1 e 6. Embora possamos conhecer os seus possíveis resultados, o resultado em si depende de fatores de sorte (álea). Uma variável aleatória pode ser uma medição de um parâmetro que pode gerar valores diferentes. O conceito de variável aleatória é essencial em Estatística e em outros métodos quantitativos para a representação de fenômenos incertos. Este conceito é estudado em detalhes no curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=analise-estatistica-para-data-science-i" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Análise Estatística Para Data Science I com R e SAS
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   As variáveis aleatórias podem ser classificadas em variáveis aleatórias discretas, contínuas e mistas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os computadores são fundamentalmente determinísticos. Portanto, é teoricamente impossível gerar números realmente aleatórios (a pergunta “o que realmente é aleatoriedade?” é algo difícil de responder). No entanto, é possível definir algoritmos que geram sequências de números cujas propriedades estão muito próximas das propriedades das sequências teóricas de números aleatórios. Em particular, um computador é capaz, usando um gerador de números pseudo-aleatórios, de gerar uma sequência de números que segue aproximadamente uma distribuição aleatória uniforme entre 0 e 1. O caso uniforme é muito simples, no qual variáveis ​​aleatórias mais complexas podem ser construídas.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Variáveis ​​aleatórias expressas como resultado de uma operação ou processo
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Existem diferentes técnicas que visam gerar variáveis ​​aleatórias mais complexas. Entre elas, podemos encontrar, por exemplo, método de transformação inversa, amostragem por rejeição, algoritmo Metropolis-Hastings entre outros. Todos esses métodos se baseiam em diferentes truques matemáticos que consistem principalmente em representar a variável aleatória que queremos gerar como resultado de uma operação (sobre variáveis ​​aleatórias mais simples) ou de um processo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A
   <span style="text-decoration: underline;">
    <a href="https://en.wikipedia.org/wiki/Rejection_sampling" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     amostragem por rejeição
    </a>
   </span>
   expressa a variável aleatória como resultado de um processo que consiste em amostrar não da distribuição complexa, mas de uma distribuição simples bem conhecida e para aceitar ou rejeitar o valor amostrado, dependendo de alguma condição. Repetindo esse processo até que o valor amostrado seja aceito, podemos mostrar que, com a condição correta de aceitação, o valor que será efetivamente amostrado seguirá a distribuição correta.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No algoritmo
   <span style="text-decoration: underline;">
    <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Metropolis-Hastings
    </a>
   </span>
   , a ideia é encontrar uma Cadeia de Markov (MC – Markov Chain) de modo que a distribuição estacionária dessa MC corresponda à distribuição da qual gostaríamos de amostrar nossa variável aleatória. Uma vez encontrada essa MC, podemos simular uma trajetória longa o suficiente sobre ela para considerar que atingimos um estado estacionário e, em seguida, o último valor que obtemos dessa maneira pode ser considerado como extraído da distribuição de interesse.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Não iremos mais adiante nos detalhes da amostragem por rejeição e do Metropolis-Hastings, porque esses métodos não são os que nos levarão à noção por trás das GANs. No entanto, vamos nos concentrar um pouco mais no método de transformação inversa.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   O método de transformação inversa
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A ideia do método de transformação inversa é simplesmente representar nossa “complexa” variável aleatória como resultado de uma função aplicada a um variável aleatória uniforme, que nós sabemos como gerar.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Consideramos abaixo um exemplo unidimensional. Seja X uma variável aleatória complexa da qual queremos amostrar e U seja uma variável aleatória uniforme sobre [0,1] que sabemos como amostrar. Lembramos que uma variável aleatória é totalmente definida por sua Função de Distribuição Cumulativa (CDF). O CDF de uma variável aleatória é uma função do domínio de definição da variável aleatória até o intervalo [0,1] e definido, em uma dimensão, de modo que:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter size-full wp-image-1453" data-attachment-id="1453" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?fit=634%2C42" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?fit=300%2C20" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?fit=634%2C42" data-orig-size="634,42" data-permalink="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form1-9/" data-recalc-dims="1" height="42" sizes="(max-width: 634px) 100vw, 634px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?resize=634%2C42" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?w=634 634w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?resize=300%2C20 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?resize=200%2C13 200w" width="634"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No caso particular de nossa variável aleatória uniforme U, temos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-1454" data-attachment-id="1454" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?fit=746%2C42" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?fit=300%2C17" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?fit=746%2C42" data-orig-size="746,42" data-permalink="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form2-15/" data-recalc-dims="1" height="42" sizes="(max-width: 746px) 100vw, 746px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=746%2C42" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?w=746 746w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=300%2C17 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=200%2C11 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=690%2C39 690w" width="746"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Por uma questão de simplicidade, vamos supor aqui que a função CDF_X é inversível e seu inverso é indicado por:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form3" class="aligncenter size-full wp-image-1456" data-attachment-id="1456" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?fit=134%2C50" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?fit=134%2C50" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?fit=134%2C50" data-orig-size="134,50" data-permalink="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form3-13/" data-recalc-dims="1" height="50" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?resize=134%2C50" width="134"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   (o método pode ser facilmente estendido ao caso não inversível usando o inverso generalizado da função, mas não é realmente o ponto principal em que queremos focar aqui). Então, se definirmos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form4" class="aligncenter size-full wp-image-1457" data-attachment-id="1457" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?fit=282%2C50" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?fit=282%2C50" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?fit=282%2C50" data-orig-size="282,50" data-permalink="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form4-9/" data-recalc-dims="1" height="50" sizes="(max-width: 282px) 100vw, 282px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?resize=282%2C50" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?w=282 282w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?resize=200%2C35 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?resize=280%2C50 280w" width="282"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   temos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form5" class="aligncenter size-full wp-image-1458" data-attachment-id="1458" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?fit=1024%2C38" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?fit=300%2C11" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?fit=1358%2C50" data-orig-size="1358,50" data-permalink="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form5-7/" data-recalc-dims="1" height="43" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=1170%2C43" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?w=1358 1358w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=300%2C11 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=768%2C28 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=1024%2C38 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=200%2C7 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=690%2C25 690w" width="1170"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como podemos ver, Y e X têm o mesmo CDF e depois definem a mesma variável aleatória. Assim, definindo Y como acima (em função de uma variável aleatória uniforme), conseguimos definir uma variável aleatória com a distribuição alvo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para resumir, o método de transformação inversa é uma maneira de gerar uma variável aleatória que segue uma determinada distribuição, fazendo uma variável aleatória uniforme passar por uma “função de transformação” bem projetada (CDF inverso). Essa noção de “método de transformação inversa” pode, de fato, ser estendida à noção de “método de transformação” que consiste, de maneira mais geral, em gerar variáveis ​​aleatórias em função de algumas variáveis ​​aleatórias mais simples (não necessariamente uniformes e, em seguida, a função de transformação é não mais o CDF inverso). Conceitualmente, o objetivo da “função de transformação” é deformar / remodelar a distribuição de probabilidade inicial: a função de transformação começa de onde a distribuição inicial é muito alta em comparação com a distribuição de destino e a coloca onde é muito baixa. Foi exatamente isso que pensou o criador do modelo GAN e muitos consideram o conceito como uma espécie de “hack” na teoria estatística, o que gerou o modelo GAN.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Observe a ilustração do método de transformação inversa abaixo. Em azul: a distribuição uniforme em [0,1]. Em laranja: a distribuição gaussiana (normal) padrão. Em cinza: o mapeamento da distribuição uniforme para a gaussiana (CDF inverso).
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   <img alt="dist" class="aligncenter wp-image-1464 size-large" data-attachment-id="1464" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="dist" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?fit=1024%2C367" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?fit=300%2C108" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?fit=4000%2C1434" data-orig-size="4000,1434" data-permalink="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/dist/" data-recalc-dims="1" height="367" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=1024%2C367" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=1024%2C367 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=300%2C108 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=768%2C275 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=200%2C72 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=690%2C247 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?w=2340 2340w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?w=3510 3510w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Compreendeu o conceito? Isso é o que está por trás dos modelos generativos nas GANs, que veremos no próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Customizando Redes Neurais com Funções de Ativação Alternativas
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://skymind.ai/wiki/generative-adversarial-network-gan" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    A Beginner’s Guide to Generative Adversarial Networks (GANs)
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    A Leap into the Future: Generative Adversarial Networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Understanding Generative Adversarial Networks (GANs)
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    How A.I. Is Creating Building Blocks to Reshape Music and Art
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Train longer, generalize better: closing the generalization gap in large batch training of neural networks
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks and Deep Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent neural network based language model
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1452" href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1452" href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1452" href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1452" href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1452-5e0dd1d401fdf" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1452&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1452-5e0dd1d401fdf" id="like-post-wrapper-140353593-1452-5e0dd1d401fdf">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-56">
 Capítulo 56 – Modelos Generativos – O Diferencial das GANs (Generative Adversarial Networks)
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste capítulo discutiremos o funcionamento dos modelos generativos, o principal diferencial nas GANs (Generative Adversarial Networks). Estamos considerando que você leu o capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/" rel="noopener noreferrer" target="_blank">
     anterior
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Suponha que estamos interessados ​​em gerar imagens quadradas em preto e branco de cães com um tamanho de n por n pixels. Podemos remodelar cada dado como um vetor dimensional N = n x n (empilhando colunas umas sobre as outras), de modo que uma imagem de cachorro possa ser representada por um vetor (quem sabe conseguimos criar um modelo capaz de diferenciar um Muffin de um Chihuahua).
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="dogs" class="aligncenter size-full wp-image-1483" data-attachment-id="1483" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="dogs" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?fit=800%2C350" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?fit=300%2C131" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?fit=800%2C350" data-orig-size="800,350" data-permalink="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/dogs/" data-recalc-dims="1" height="350" sizes="(max-width: 800px) 100vw, 800px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=800%2C350" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=300%2C131 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=768%2C336 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=200%2C88 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=690%2C302 690w" width="800"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No entanto, isso não significa que todos os vetores representem um cão que foi moldado de volta a um quadrado! Portanto, podemos dizer que os vetores dimensionais N que efetivamente geram algo que se parece com um cachorro são distribuídos de acordo com uma distribuição de probabilidade muito específica em todo o espaço vetorial dimensional N (alguns pontos desse espaço provavelmente representam cães, enquanto é improvável para alguns outros). No mesmo espírito, existe, nesse espaço vetorial dimensional N, distribuições de probabilidade para imagens de gatos, pássaros e assim por diante.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Então, o problema de gerar uma nova imagem do cão é equivalente ao problema de gerar um novo vetor após a “distribuição de probabilidade do cão” no espaço vetorial dimensional N. De fato, estamos enfrentando um problema de gerar uma variável aleatória com relação a uma distribuição de probabilidade específica (lembra do que discutimos no capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/" rel="noopener noreferrer" target="_blank">
     anterior
    </a>
   </span>
   ?).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste ponto, podemos mencionar duas coisas importantes. Primeiro, a “distribuição de probabilidade canina” que mencionamos é uma distribuição muito complexa em um espaço muito grande. Segundo, mesmo se pudermos assumir a existência de tal distribuição subjacente (na verdade existem imagens que se parecem com cachorro e outras que não), obviamente não sabemos como expressar explicitamente essa distribuição. Os dois pontos anteriores dificultam bastante o processo de geração de variáveis ​​aleatórias a partir dessa distribuição. Vamos tentar resolver esses dois problemas a seguir.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nosso primeiro problema ao tentar gerar nossa nova imagem de cachorro é que a “distribuição de probabilidade do cachorro” no espaço vetorial dimensional N é muito complexa e não sabemos como gerar diretamente variáveis ​​aleatórias complexas. No entanto, como sabemos muito bem como gerar N variáveis ​​aleatórias uniformes não correlacionadas, poderíamos fazer uso do método de transformação. Para fazer isso, precisamos expressar nossa variável aleatória N dimensional como resultado de uma função muito complexa aplicada a uma variável aleatória N dimensional simples!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui, podemos enfatizar o fato de que encontrar a função de transformação não é tão simples quanto tomar a inversa de forma fechada da Função de Distribuição Cumulativa (que obviamente não sabemos) como fizemos ao descrever o método de transformação inversa no capítulo
   <a href="http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/" rel="noopener noreferrer" target="_blank">
    anterior
   </a>
   . A função de transformação não pode ser expressa explicitamente e, então, precisamos aprender com os dados (essa é uma das razões pelas quais treinamos um modelo de Machine Learning e esse conceito é estudado em detalhes
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-engineer" rel="noopener noreferrer" target="_blank">
     aqui
    </a>
   </span>
   )
  </span>
  <span style="color: #000000;">
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em seguida, a ideia é modelar a função de transformação por uma rede neural que tome como entrada uma variável aleatória uniforme N dimensional simples e que retorne como saída outra variável aleatória N dimensional que deve seguir, após o treinamento, a “distribuição de probabilidade canina” correta . Depois que a arquitetura da rede foi projetada, ainda precisamos treiná-la. Nas próximas duas seções, discutiremos duas maneiras de treinar essas redes generativas, incluindo a ideia de treinamento antagônico por trás das GANs!
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="model1" class="aligncenter wp-image-1487 size-large" data-attachment-id="1487" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="model1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?fit=1024%2C488" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?fit=300%2C143" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?fit=2486%2C1185" data-orig-size="2486,1185" data-permalink="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/model1/" data-recalc-dims="1" height="488" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=1024%2C488" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=1024%2C488 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=300%2C143 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=768%2C366 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=200%2C95 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=690%2C329 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?w=2340 2340w" width="1024"/>
 </p>
 <p>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Redes de Correspondência Generativa (Generative Matching Networks)
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Obs: a denominação de “redes de correspondência generativa” não é padrão. No entanto, podemos encontrar na literatura, por exemplo, “Redes de correspondência de momentos generativos” ou também “Redes de correspondência de recursos generativos”. Só queremos aqui usar uma denominação um pouco mais geral para o que descrevemos abaixo.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Treinando Modelos Generativos
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até agora, mostramos que nosso problema de gerar uma nova imagem de cachorro pode ser reformulado em um problema de gerar um vetor aleatório no espaço vetorial dimensional N que segue a “distribuição de probabilidade canina” e sugerimos o uso de um método de transformação , com uma rede neural para modelar a função de transformação.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora, ainda precisamos treinar (otimizar) a rede para expressar a função de transformação correta. Para isso, podemos sugerir dois métodos diferentes de treinamento: um direto e um indireto. O método de treinamento direto consiste em comparar as distribuições de probabilidade verdadeira e gerada e
   <em>
    retropropagar
   </em>
   a diferença (o erro) através da rede. Essa é a ideia que governa as redes de correspondência generativa (GMNs – Generative Matching Networks)). Para o método de treinamento indireto, não comparamos diretamente as distribuições verdadeiras e geradas. Em vez disso, treinamos a rede generativa, fazendo com que essas duas distribuições passem por um ajuste escolhido de forma que o processo de otimização da rede generativa em relação à tarefa ajustada imponha que a distribuição gerada esteja próxima da verdadeira distribuição. Essa última ideia é a que está por trás das Redes Adversárias Generativas (GANs) que apresentaremos na próxima seção. Mas, por enquanto, vamos começar com o método direto e as GMNs.
  </span>
 </p>
 <p style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Comparando duas distribuições de probabilidade com base em amostras
   </span>
  </strong>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como mencionado, a ideia das GMNs é treinar a rede generativa comparando diretamente a distribuição gerada com a verdadeira. No entanto, não sabemos como expressar explicitamente a verdadeira “distribuição de probabilidade de cães” e também podemos dizer que a distribuição gerada é complexa demais para ser expressa explicitamente. Portanto, comparações baseadas em expressões explícitas não são possíveis. Logo, se tivermos uma maneira de comparar distribuições de probabilidade com base em amostras, podemos usá-la para treinar a rede. De fato, temos uma amostra de dados verdadeiros e podemos, a cada iteração do processo de treinamento, produzir uma amostra de dados gerados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Embora, em teoria, qualquer distância (ou medida de similaridade) capaz de comparar efetivamente duas distribuições baseadas em amostras possa ser usada, podemos mencionar, em particular, a abordagem da Discrepância Média Máxima (MMD). O MMD define uma distância entre duas distribuições de probabilidade que podem ser calculadas (estimadas) com base em amostras dessas distribuições. Embora não esteja totalmente fora do escopo deste capítulo, decidimos não gastar muito mais tempo descrevendo o MMD. No entanto, caso você queira mais detalhes sobre isso, recomendamos esses 3 papers abaixo:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Learning features to compare distributions
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     A Kernel Method for the Two-Sample-Problem
    </span>
   </a>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     A Kernel Two-Sample Test
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    Retropropagação do erro de correspondência de distribuição
   </span>
  </strong>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim, uma vez que definimos uma maneira de comparar duas distribuições com base em amostras, podemos definir o processo de treinamento da rede generativa em GMNs. Dada uma variável aleatória com distribuição de probabilidade uniforme como entrada, queremos que a distribuição de probabilidade da saída gerada seja a “distribuição de probabilidade do cão”. A ideia das GMNs é otimizar a rede repetindo as seguintes etapas:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Gerar algumas entradas uniformes.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Fazer essas entradas passarem pela rede e coletar as saídas geradas.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Comparar a verdadeira “distribuição de probabilidade de cães” e a gerada com base nas amostras disponíveis (por exemplo, calculando a distância MMD entre a amostra de imagens reais de cães e a amostra de imagens geradas).
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Usar retropropagação para fazer uma etapa de descida de gradiente para diminuir a distância (por exemplo, MMD) entre distribuições verdadeiras e geradas.
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Conforme descrito acima, ao seguir estas etapas, aplicamos uma descida do gradiente na rede com uma função de perda que é a distância entre as distribuições verdadeiras e as distribuições geradas na iteração atual.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   O método de treinamento “indireto”
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A abordagem “direta” apresentada acima compara diretamente a distribuição gerada com a verdadeira ao treinar a rede generativa. A brilhante ideia que governa as GANs consiste em substituir essa comparação direta por uma indireta que assume a forma de uma tarefa ajustada sobre essas duas distribuições.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O treinamento da rede generativa é então realizado com relação a essa tarefa, de modo que força a distribuição gerada a se aproximar cada vez mais da distribuição verdadeira.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A tarefa ajustada das GANs é uma tarefa de discriminação entre amostras verdadeiras e geradas. Ou poderíamos dizer uma tarefa de “não discriminação”, pois queremos que a discriminação falhe o máximo possível. Portanto, em uma arquitetura GAN, temos um discriminador, que coleta amostras de dados verdadeiros e gerados e tenta classificá-los da melhor maneira possível, e um gerador treinado para enganar o discriminador o máximo possível. Vamos ver em um exemplo simples porque as abordagens diretas e indiretas que mencionamos devem, em teoria, levar ao mesmo gerador ideal.
  </span>
 </p>
 <p style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    O caso ideal: gerador e discriminador perfeitos
   </span>
  </strong>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para entender melhor por que treinar um gerador para enganar um discriminador levará ao mesmo resultado que treinar diretamente o gerador para corresponder à distribuição de destino, vamos dar um exemplo unidimensional simples. Esquecemos, por enquanto, como o gerador e o discriminador são representados e os consideramos como noções abstratas (que serão especificadas na próxima subseção). Além disso, ambos são supostos “perfeitos” (com capacidades infinitas) no sentido de que não são limitados por nenhum tipo de modelo (parametrizado).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Suponha que tenhamos uma distribuição verdadeira, por exemplo, um gaussiano unidimensional e que desejemos um gerador que faça amostras dessa distribuição de probabilidade. O que chamamos de método de treinamento “direto” consistiria em ajustar iterativamente o gerador (iterações de descida de gradiente) para corrigir a diferença / erro medido entre distribuições verdadeiras e geradas. Por fim, supondo que o processo de otimização seja perfeito, devemos terminar com a distribuição gerada que corresponda exatamente à verdadeira distribuição.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="norm" class="aligncenter wp-image-1480 size-large" data-attachment-id="1480" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="norm" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?fit=1024%2C985" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?fit=300%2C288" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?fit=3684%2C3542" data-orig-size="3684,3542" data-permalink="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/norm/" data-recalc-dims="1" height="985" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=1024%2C985" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=1024%2C985 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=300%2C288 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=768%2C738 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=200%2C192 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=690%2C663 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?w=2340 2340w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?w=3510 3510w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para a abordagem “indireta”, devemos considerar também um discriminador. Presumimos por enquanto que esse discriminador é um tipo de oráculo que sabe exatamente quais são as distribuições verdadeira e gerada e que é capaz, com base nessas informações, de prever uma classe (“verdadeira” ou “gerada”) para qualquer ponto. Se as duas distribuições estiverem distantes, o discriminador será capaz de classificar facilmente e com um alto nível de confiança a maioria dos pontos que apresentamos. Se queremos enganar o discriminador, precisamos aproximar a distribuição gerada da verdadeira. O discriminador terá mais dificuldade em prever a classe quando as duas distribuições serão iguais em todos os pontos: nesse caso, para cada ponto, haverá chances iguais de ser “verdadeiro” ou “gerado” e, em seguida, o discriminador poderá ” fazer melhor do que ser verdadeiro em um caso em dois em média.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="norm2" class="aligncenter wp-image-1481 size-large" data-attachment-id="1481" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="norm2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?fit=1024%2C957" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?fit=300%2C280" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?fit=3791%2C3542" data-orig-size="3791,3542" data-permalink="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/norm2/" data-recalc-dims="1" height="957" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=1024%2C957" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=1024%2C957 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=300%2C280 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=768%2C718 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=200%2C187 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=690%2C645 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?w=2340 2340w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?w=3510 3510w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nesse ponto, parece legítimo se perguntar se esse método indireto é realmente uma boa ideia. De fato, parece ser mais complicado (temos que otimizar o gerador com base em uma tarefa ajustada, em vez de diretamente nas distribuições) e requer um discriminador que consideramos aqui como um determinado oráculo, mas que, na realidade, não é conhecido. Nem perfeito. Para o primeiro ponto, a dificuldade de comparar diretamente duas distribuições de probabilidade com base em amostras contrabalança a aparente maior complexidade do método indireto. Para o segundo ponto, é óbvio que o discriminador não é conhecido. No entanto, pode ser aprendido!
  </span>
 </p>
 <p style="text-align: justify;">
  <strong>
   <span style="color: #000000;">
    A aproximação: redes neurais adversárias
   </span>
  </strong>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos agora descrever a forma específica que assume o gerador e o discriminador na arquitetura das GANs. O gerador é uma rede neural que modela uma função de transformação. Ele assume como entrada uma variável aleatória simples e deve retornar, uma vez treinada, uma variável aleatória que segue a distribuição de destino. Como é muito complicado e desconhecido, decidimos modelar o discriminador com outra rede neural. Essa rede neural modela uma função discriminativa. Ele toma como entrada um ponto (no nosso exemplo de cachorro, um vetor dimensional N) e retorna como saída a probabilidade desse ponto ser “verdadeiro”.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Observe que o fato de impormos agora um modelo parametrizado para expressar tanto o gerador quanto o discriminador (em vez das versões idealizadas na subseção anterior) não tem, na prática, um grande impacto no argumento / intuição teórica acima: apenas trabalhamos em alguns espaços parametrizados em vez de espaços completos ideais e, portanto, os pontos ideais que devemos alcançar no caso ideal podem ser vistos como “arredondados” pela capacidade de precisão dos modelos parametrizados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Uma vez definidas, as duas redes podem ser treinadas em conjunto (ao mesmo tempo) com objetivos opostos:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    O objetivo do gerador é enganar o discriminador; portanto, a rede neural generativa é treinada para maximizar o erro de classificação final (entre dados verdadeiros e gerados).
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    O objetivo do discriminador é detectar dados falsos gerados, para que a rede neural discriminativa seja treinada para minimizar o erro de classificação final.
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Portanto, a cada iteração do processo de treinamento, os pesos da rede generativa são atualizados para aumentar o erro de classificação (subida do gradiente de erro sobre os parâmetros do gerador), enquanto os pesos da rede discriminativa são atualizados para diminuir esse erro (descida do gradiente de erro sobre os parâmetros do discriminador).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Esses objetivos opostos e a noção implícita de treinamento antagônico das duas redes explicam o nome de “redes adversárias”: ambas as redes tentam se derrotar e, ao fazê-lo, estão cada vez melhor. A competição entre elas faz com que essas duas redes “progridam” com relação a seus respectivos objetivos. Do ponto de vista da teoria dos jogos, podemos pensar nessa configuração como um jogo minimax para dois jogadores, em que o estado de equilíbrio corresponde à situação em que o gerador produz dados a partir da distribuição exata e onde o discriminador prediz “verdadeiro” ou “gerado” com probabilidade 1/2 para qualquer ponto que receber.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Os modelos GAN estão entre os mais avançados em Deep Learning e a ideia por trás da sua concepção é simples e brilhante.
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   Agora sim, estamos prontos para compreender a Matemática que faz tudo isso acontecer!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Está gostando do Deep Learning Book? Então ajude a continuarmos esse trabalho e compartilhe o Deep Learning Book entre seus amigos. Quanto mais dividimos o conhecimento, mas ele se multiplica.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até o próximo capítulo!
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Customizando Redes Neurais com Funções de Ativação Alternativas
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://skymind.ai/wiki/generative-adversarial-network-gan" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     A Beginner’s Guide to Generative Adversarial Networks (GANs)
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     A Leap into the Future: Generative Adversarial Networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Understanding Generative Adversarial Networks (GANs)
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     How A.I. Is Creating Building Blocks to Reshape Music and Art
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Train longer, generalize better: closing the generalization gap in large batch training of neural networks
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks and Deep Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent neural network based language model
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1479" href="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1479" href="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1479" href="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1479" href="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1479-5e0dd1d5ea4ca" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1479&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1479-5e0dd1d5ea4ca" id="like-post-wrapper-140353593-1479-5e0dd1d5ea4ca">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-57">
 Capítulo 57 – Os Detalhes Matemáticos das GANs (Generative Adversarial Networks)
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste capítulo vamos concluir nosso estudo das GANs com os detalhes matemáticos, antes de avançar para outra arquitetura de Deep Learning que estudaremos na sequência. Estamos considerando que você leu os capítulos anteriores sobre GANs.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A modelagem de redes neurais requer essencialmente definir duas coisas: uma arquitetura e uma função de perda. Já descrevemos a
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/" rel="noopener noreferrer" target="_blank">
     arquitetura de redes adversárias generativas
    </a>
   </span>
   . Consiste em duas redes:
  </span>
 </p>
 <ol>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Uma rede generativa G que recebe uma entrada aleatória z com densidade p_z e retorna uma saída x_g = G (z) que deve seguir (após o treinamento) a distribuição de probabilidade alvo.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Uma rede discriminativa D que recebe uma entrada x que pode ser uma entrada “verdadeira” (x_t, cuja densidade é denotada p_t) ou uma entrada “gerada” (x_g, cuja densidade p_g é a densidade induzida pela densidade p_z através de G) e que retorna a probabilidade D (x) de x para ser um dado “verdadeiro”.
   </span>
  </li>
 </ol>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos agora examinar mais de perto a função de perda “teórica” das GANs. Se enviarmos ao discriminador dados “verdadeiros” e “gerados” nas mesmas proporções, o erro absoluto esperado do discriminador poderá ser expresso como:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter wp-image-1513 size-full" data-attachment-id="1513" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?fit=854%2C178" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?fit=300%2C63" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?fit=854%2C178" data-orig-size="854,178" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form1-10/" data-recalc-dims="1" height="178" sizes="(max-width: 854px) 100vw, 854px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=854%2C178" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?w=854 854w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=300%2C63 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=768%2C160 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=200%2C42 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=690%2C144 690w" width="854"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O objetivo do gerador é enganar o discriminador cujo objetivo é ser capaz de distinguir entre dados verdadeiros e dados gerados. Portanto, ao treinar o gerador, queremos maximizar esse erro enquanto tentamos minimizá-lo para o discriminador. Isso nos dá a fórmula abaixo:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-1514" data-attachment-id="1514" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?fit=412%2C154" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?fit=300%2C112" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?fit=412%2C154" data-orig-size="412,154" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form2-16/" data-recalc-dims="1" height="154" sizes="(max-width: 412px) 100vw, 412px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?resize=412%2C154" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?w=412 412w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?resize=300%2C112 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?resize=200%2C75 200w" width="412"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para qualquer gerador G (juntamente com a densidade de probabilidade induzida p_g), o melhor discriminador possível é aquele que minimiza a integral abaixo:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form3" class="aligncenter wp-image-1515 size-large" data-attachment-id="1515" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?fit=1024%2C79" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?fit=300%2C23" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?fit=1188%2C92" data-orig-size="1188,92" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form3-14/" data-recalc-dims="1" height="79" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=1024%2C79" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=1024%2C79 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=300%2C23 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=768%2C59 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=200%2C15 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=690%2C53 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?w=1188 1188w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para minimizar (em relação a D) essa integral, podemos minimizar a função dentro da integral para cada valor de x. Em seguida, definimos o melhor discriminador possível para um determinado gerador:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form4" class="aligncenter size-full wp-image-1516" data-attachment-id="1516" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?fit=272%2C108" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?fit=272%2C108" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?fit=272%2C108" data-orig-size="272,108" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form4-10/" data-recalc-dims="1" height="108" sizes="(max-width: 272px) 100vw, 272px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?resize=272%2C108" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?w=272 272w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?resize=200%2C79 200w" width="272"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   (de fato, um dos melhores porque x valores tais que p_t (x) = p_g (x) podem ser manipulados de outra maneira, mas isso não importa para o que segue). Em seguida, pesquisamos G que maximiza:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form5" class="aligncenter wp-image-1517 size-large" data-attachment-id="1517" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?fit=1024%2C87" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?fit=300%2C25" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?fit=1084%2C92" data-orig-size="1084,92" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form5-8/" data-recalc-dims="1" height="87" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=1024%2C87" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=1024%2C87 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=300%2C25 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=768%2C65 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=200%2C17 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=690%2C59 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?w=1084 1084w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Parece complexo? É menos do que parece e explicamos sobre os fundamentos matemáticos por trás dessas fórmulas no curso
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener noreferrer" target="_blank">
     Matemática Para Machine Learning
    </a>
   </span>
   . Novamente, para maximizar (em relação a G) essa integral, podemos maximizar a função dentro da integral para cada valor de x. Como a densidade p_t é independente do gerador G, não podemos fazer melhor do que definir G de modo que:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form7" class="aligncenter size-full wp-image-1518" data-attachment-id="1518" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form7" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?fit=326%2C112" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?fit=300%2C103" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?fit=326%2C112" data-orig-size="326,112" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form7-4/" data-recalc-dims="1" height="112" sizes="(max-width: 326px) 100vw, 326px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?resize=326%2C112" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?w=326 326w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?resize=300%2C103 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?resize=200%2C69 200w" width="326"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Obviamente, como p_g é uma densidade de probabilidade que deve se integrar a 1, necessariamente temos o melhor G:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form8" class="aligncenter size-full wp-image-1519" data-attachment-id="1519" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form8" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?fit=280%2C74" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?fit=280%2C74" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?fit=280%2C74" data-orig-size="280,74" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form8-4/" data-recalc-dims="1" height="74" sizes="(max-width: 280px) 100vw, 280px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?resize=280%2C74" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?w=280 280w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?resize=200%2C53 200w" width="280"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim, mostramos que, em um caso ideal com gerador e discriminador de capacidade ilimitada, o ponto ideal do cenário adversário é tal que o gerador produz a mesma densidade que a densidade real e o discriminador não pode fazer melhor do que ser verdadeiro em um caso a cada dois, exatamente como a intuição nos disse. Por fim, observe também que G maximiza:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form6" class="aligncenter wp-image-1520 size-large" data-attachment-id="1520" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form6" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?fit=1024%2C90" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?fit=300%2C26" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?fit=1112%2C98" data-orig-size="1112,98" data-permalink="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form6-5/" data-recalc-dims="1" height="90" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=1024%2C90" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=1024%2C90 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=300%2C26 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=768%2C68 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=200%2C18 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=690%2C61 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?w=1112 1112w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na fórmula acima vemos que G deseja maximizar a probabilidade esperada de o discriminador estar errado.
  </span>
 </p>
 <p>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Conclusão Sobre as GANs
  </span>
 </h3>
 <p>
  <span style="color: #000000;">
   As GANs são bem recentes e possuem uma ideia inovadora sobre como treinar uma arquitetura de rede neural. Muitos estudos e aplicações vem sendo feitos em todo mundo e já existem até algumas aplicações comerciais usando essa arquitetura de Deep Learning. Caso queira aprender a construir GANs na prática, acesse
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
     aqui
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Pontos mais importantes sobre as GANs:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Computadores podem basicamente gerar variáveis ​​pseudo-aleatórias simples (por exemplo, eles podem gerar variáveis ​​que seguem muito de perto uma distribuição uniforme).
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Existem maneiras diferentes de gerar variáveis ​​aleatórias mais complexas, incluindo a noção de “método de transformação” que consiste em expressar uma variável aleatória em função de algumas variáveis ​​aleatórias mais simples.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    No aprendizado de máquina, os modelos generativos tentam gerar dados de uma determinada distribuição de probabilidade (complexa).
   </span>
   <span style="color: #000000;">
    Modelos generativos de aprendizado profundo são modelados como redes neurais (funções muito complexas) que recebem como entrada uma variável aleatória simples e retornam uma variável aleatória que segue a distribuição direcionada (como “método de transformação”).
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Essas redes generativas podem ser treinadas “diretamente” (comparando a distribuição dos dados gerados com a verdadeira distribuição): esta é a ideia das redes correspondentes generativas
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Essas redes generativas também podem ser treinadas “indiretamente” (tentando enganar outra rede treinada ao mesmo tempo para distinguir dados “gerados” de dados “verdadeiros”): essa é a ideia das redes adversas generativas.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    <span style="color: #000000;">
     Mesmo que o “hype” que rodeia os GANs seja talvez um pouco exagerado, podemos dizer que a ideia de treinamento antagônico sugerida por Ian Goodfellow e seus co-autores é realmente ótima. Essa maneira de distorcer a função de perda, passando de uma comparação direta para uma indireta, é realmente algo que pode ser muito inspirador para futuros trabalhos na área de aprendizado profundo.
    </span>
   </span>
   <p style="text-align: justify;">
   </p>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Customizando Redes Neurais com Funções de Ativação Alternativas
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://skymind.ai/wiki/generative-adversarial-network-gan" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     A Beginner’s Guide to Generative Adversarial Networks (GANs)
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     A Leap into the Future: Generative Adversarial Networks
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Understanding Generative Adversarial Networks (GANs)
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     How A.I. Is Creating Building Blocks to Reshape Music and Art
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1705.08741.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Train longer, generalize better: closing the generalization gap in large batch training of neural networks
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks and Deep Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent neural network based language model
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1512" href="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1512" href="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1512" href="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1512" href="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1512-5e0dd1d80a886" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1512&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1512-5e0dd1d80a886" id="like-post-wrapper-140353593-1512-5e0dd1d80a886">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-58">
 Capítulo 58 – Introdução aos Autoencoders
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até aqui neste livro descrevemos arquiteturas de redes neurais profundas (Deep Learning) usadas em aprendizagem supervisionada, na qual rotulamos exemplos de treinamento, fornecendo ao algoritmo dados de entrada (X) e de saída (Y). Agora, suponha que tenhamos apenas um conjunto de exemplos de treinamento não rotulados {x1, x2, x3,…}, onde x (i) ∈ℜn. Uma rede neural Autoencoder é um algoritmo de aprendizado não supervisionado que aplica backpropagation, definindo os valores de destino como iguais às entradas. Ou seja, usa y (i) = x (i). Estudaremos essa arquitetura de Deep Learning a partir de agora.
  </span>
 </p>
 <h3>
  <span style="color: #000000;">
   O que são Autoencoders?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os Autoencoders são uma técnica de aprendizado não supervisionado, na qual usamos as redes neurais para a tarefa de aprendizado de representação. Especificamente, projetaremos uma arquitetura de rede neural de modo a impor um
   <em>
    gargalo
   </em>
   na rede que força uma representação de conhecimento compactada da entrada original. Se os recursos de entrada fossem independentes um do outro, essa compressão e reconstrução subsequente seriam uma tarefa muito difícil. No entanto, se houver algum tipo de estrutura nos dados (ou seja, correlações entre os recursos de entrada), essa estrutura poderá ser aprendida e consequentemente aproveitada ao forçar a entrada através do
   <em>
    gargalo
   </em>
   da rede.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Autoencoders (AE) são redes neurais que visam copiar suas entradas para suas saídas. Eles trabalham compactando a entrada em uma representação de espaço latente e, em seguida, reconstruindo a saída dessa representação. Esse tipo de rede é composto de duas partes:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Codificador (Encoder)
   </strong>
   : é a parte da rede que compacta a entrada em uma representação de espaço latente (codificando a entrada). Pode ser representado por uma função de codificação h = f (x).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Decodificador (Decoder)
   </strong>
   : Esta parte tem como objetivo reconstruir a entrada da representação do espaço latente. Pode ser representado por uma função de decodificação r = g (h).
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="mushroom_encoder" class="aligncenter wp-image-1545 size-full" data-attachment-id="1545" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="mushroom_encoder" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?fit=878%2C341" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?fit=300%2C117" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?fit=878%2C341" data-orig-size="878,341" data-permalink="http://deeplearningbook.com.br/introducao-aos-autoencoders/mushroom_encoder/" data-recalc-dims="1" height="341" sizes="(max-width: 878px) 100vw, 878px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=878%2C341" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?w=878 878w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=300%2C117 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=768%2C298 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=200%2C78 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=690%2C268 690w" width="878"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O Autoencoder tenta aprender uma função h W, b (x) ≈ x. Em outras palavras, ele está tentando aprender uma aproximação com a função de identidade, de modo a gerar x’ semelhante a x. A função de identidade parece uma função particularmente trivial para tentar aprender; mas colocando restrições na rede, como limitando o número de unidades ocultas, podemos descobrir uma estrutura interessante sobre os dados. Como um exemplo concreto, suponha que as entradas x sejam os valores de intensidade de pixel de uma imagem 10 × 10 (100 pixels), portanto n = 100 e haja s = 50 unidades ocultas na camada L2. Observe que também temos y∈ℜ100. Como existem apenas 50 unidades ocultas, a rede é forçada a aprender uma representação “compactada” da entrada. Ou seja, dado apenas o vetor de ativações de unidades ocultas, ele deve tentar ”reconstruir” a entrada de 100 pixels x. Se a entrada fosse completamente aleatória – digamos, cada xi provém de um ID Gaussiano independente dos outros recursos, essa tarefa de compactação seria muito difícil. Mas se houver estrutura nos dados, por exemplo, se alguns dos recursos de entrada estiverem correlacionados, esse algoritmo poderá descobrir algumas dessas correlações. De fato, esse Autoencoder simples geralmente acaba aprendendo uma representação de baixa dimensão muito semelhante aos PCAs (
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener noreferrer" target="_blank">
     Principal Component Analysis
    </a>
   </span>
   ).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nosso argumento acima se baseava no número de unidades ocultas s sendo pequenas. Mas mesmo quando o número de unidades ocultas é grande (talvez até maior que o número de pixels de entrada), ainda podemos descobrir uma estrutura interessante, impondo outras restrições à rede. Em particular, se impusermos uma restrição de “esparsidade” nas unidades ocultas, o Autoencoder ainda descobrirá uma estrutura interessante nos dados, mesmo que o número de unidades ocultas seja grande.
  </span>
 </p>
 <p>
 </p>
 <p>
  <img alt="autoencoders" class="aligncenter wp-image-1537 size-large" data-attachment-id="1537" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="autoencoders" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?fit=1024%2C252" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?fit=300%2C74" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?fit=1248%2C307" data-orig-size="1248,307" data-permalink="http://deeplearningbook.com.br/introducao-aos-autoencoders/autoencoders/" data-recalc-dims="1" height="252" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=1024%2C252" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=1024%2C252 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=300%2C74 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=768%2C189 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=200%2C49 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=690%2C170 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?w=1248 1248w" width="1024"/>
 </p>
 <p>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Por que copiar a entrada para a saída?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Se o único objetivo dos Autoencoders fosse copiar a entrada para a saída, eles seriam inúteis. De fato, esperamos que, treinando o Autoencoder para copiar a entrada para a saída, a representação latente h tenha propriedades úteis.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Isso pode ser conseguido criando restrições na tarefa de cópia. Uma maneira de obter recursos úteis do Autoencoder é restringir h a ter dimensões menores que x; nesse caso, o Autoencoder é chamado de incompleto. Ao treinar uma representação incompleta, forçamos o Autoencoder a aprender os recursos mais importantes dos dados de treinamento. Se for dada muita capacidade ao Autoencoder, ele poderá aprender a executar a tarefa de cópia sem extrair nenhuma informação útil sobre a distribuição dos dados. Isso também pode ocorrer se a dimensão da representação latente for a mesma que a entrada e, no caso de excesso de conclusão, em que a dimensão da representação latente for maior que a entrada. Nesses casos, mesmo um codificador linear e um decodificador linear podem aprender a copiar a entrada na saída sem aprender nada útil sobre a distribuição de dados. Idealmente, alguém poderia treinar qualquer arquitetura de Autoencoder com sucesso, escolhendo a dimensão do código e a capacidade do codificador e decodificador com base na complexidade da distribuição a ser modelada.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Para que são usados ​​os Autoencoders?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Atualmente, o denoising de dados (remoção de ruídos) e a redução de dimensionalidade para visualização de dados são considerados duas principais aplicações práticas interessantes de Autoencoders. Com restrições de dimensionalidade e esparsidade apropriadas, os Autoencoders podem aprender projeções de dados mais interessantes que o PCA ou outras técnicas básicas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os Autoencoders aprendem automaticamente a partir de exemplos de dados. Isso significa que é fácil treinar instâncias especializadas do algoritmo que terão bom desempenho em um tipo específico de entrada e que não requer nenhuma nova engenharia de recursos, apenas os dados de treinamento apropriados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Outra aplicação dos Autoencoders é como tarefa preliminar ao reconhecimento de imagens com CNNs (Redes Neurais Convolucionais). Observando a imagem acima você percebe que a saída do Autoencoder é a imagem do número 4 muito mais suave. Ou seja, aplicamos os Autoencoders para remover ruído dos dados (denoising) e depois usamos a saída dos Autoencoders para treinar um modelo CNN.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os Autoencoders são treinados para preservar o máximo de informações possível quando uma entrada é passada pelo codificador e depois pelo decodificador, mas também são treinados para fazer com que a nova representação tenha várias propriedades agradáveis. Diferentes tipos de Autoencoders visam atingir diferentes tipos de propriedades.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Autoencoders são estudados na prática em
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
     Deep Learning II
    </a>
    .
   </span>
   Nos próximos capítulos estudaremos os detalhes matemáticos e estatísticos dessa fascinante arquitetura de Deep Learning.
  </span>
 </p>
 <p>
 </p>
 <p>
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Customizando Redes Neurais com Funções de Ativação Alternativas
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Autoencoders – Unsupervised Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep inside: Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.jeremyjordan.me/autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Introduction to Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent neural network based language model
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1536" href="http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1536" href="http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1536" href="http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1536" href="http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1536-5e0dd1da07767" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1536&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1536-5e0dd1da07767" id="like-post-wrapper-140353593-1536-5e0dd1da07767">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-59">
 Capítulo 59 – Principais Tipos de Redes Neurais Artificiais Autoencoders
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste capítulo vamos estudar os tipos principais de Autoencoders (estamos considerando que você leu o capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/introducao-aos-autoencoders/" rel="noopener noreferrer" target="_blank">
     anterior
    </a>
   </span>
   ):
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os Autoencoders codificam os valores de entrada x usando uma função f. Em seguida, decodificam os valores codificados f (x) usando uma função g para criar valores de saída idênticos aos valores de entrada. O objetivo do Autoencoder é minimizar o erro de reconstrução entre a entrada e a saída. Isso ajuda os Autoencoders a aprender os recursos importantes presentes nos dados. Quando uma representação permite uma boa reconstrução de sua entrada, ela retém grande parte das informações presentes na entrada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   E existem diferentes tipos de Autoencoders. Confira:
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   1. Autoencoder Padrão
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na sua forma mais simples, o Autoencoder é uma rede neural artificial de três camadas, isto é, uma rede neural com uma camada de entrada, uma oculta e uma camada de saída. A entrada e a saída são as mesmas e aprendemos a reconstruir a entrada, por exemplo, usando o otimizador adam e a função de perda de erro quadrático médio.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   2. Autoencoder Multicamada
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Se uma camada oculta não for suficiente, obviamente podemos estender o Autoencoder para mais camadas ocultas. Nossa implementação poderia usar 3 camadas ocultas em vez de apenas uma. Qualquer uma das camadas ocultas pode ser escolhida como representação de recurso, mas o ideal é tornar a rede simétrica e usar a camada mais intermediária.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   3. Autoencoder Convolucional
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Também podemos nos perguntar: os Autoencoders podem ser usados com convoluções em vez de camadas totalmente conectadas?
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A resposta é sim e o princípio é o mesmo, mas usando imagens (vetores 3D) em vez de vetores 1D
   <em>
    achatados (flattened)
   </em>
   . A imagem de entrada é reduzida para fornecer uma representação latente de dimensões menores e forçar o Autoencoder a aprender uma versão compactada das imagens.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   4. Autoencoder Regularizado
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Existem outras maneiras pelas quais podemos restringir a reconstrução de um Autoencoder, além de impor uma camada oculta de menor dimensão que a entrada. Em vez de limitar a capacidade do modelo mantendo o codificador e o decodificador rasos e o tamanho do código pequeno, os Autoencoders regularizados usam uma função de perda que incentiva o modelo a ter outras propriedades além da capacidade de copiar sua entrada para sua saída. Na prática, geralmente encontramos dois tipos de Autoencoder Regularizado: o Autoencoder Esparso e o Autoencoder Denoising.
  </span>
 </p>
 <h4 style="text-align: justify;">
  <span style="color: #000000;">
   4.1. Autoencoder Esparso
  </span>
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os Autoencoders Esparsos geralmente são usados ​​para aprender recursos para outra tarefa, como classificação. Um Autoencoder que foi regularizado para ser esparso deve responder a recursos estatísticos exclusivos do conjunto de dados em que foi treinado, em vez de simplesmente atuar como uma função de identidade. Dessa forma, o treinamento para executar a tarefa de cópia com uma penalidade de escassez pode produzir um modelo que aprendeu recursos úteis como subproduto.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Outra maneira de restringir a reconstrução do Autoencoder é impor uma restrição à sua perda. Poderíamos, por exemplo, adicionar um termo de reguralização na função de perda. Isso fará com que nosso Autoencoder aprenda representação esparsa de dados.
  </span>
  <span style="color: #000000;">
   Em nossa camada oculta, podemos adicionar um regularizador de atividades L1, que aplicará uma penalidade na função de perda durante a fase de otimização. Como resultado, a representação será mais esparsa em comparação com o Autoencoder Padrão. Abaixo uma representação do Autoencoder Esparso:
  </span>
 </p>
 <h4 style="text-align: justify;">
 </h4>
 <h4 style="text-align: justify;">
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="esparso" class="aligncenter size-full wp-image-1567" data-attachment-id="1567" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="esparso" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?fit=495%2C693" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?fit=214%2C300" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?fit=495%2C693" data-orig-size="495,693" data-permalink="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/esparso/" data-recalc-dims="1" height="693" sizes="(max-width: 495px) 100vw, 495px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?resize=495%2C693" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?w=495 495w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?resize=214%2C300 214w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?resize=200%2C280 200w" width="495"/>
  </span>
 </p>
 <h4 style="text-align: justify;">
 </h4>
 <h4 style="text-align: justify;">
  <span style="color: #000000;">
   4.2. Autoencoder Denoising
  </span>
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em vez de adicionar uma penalidade à função de perda, podemos obter um Autoencoder que aprende algo útil alterando o termo do erro de reconstrução da função de perda. Isso pode ser feito adicionando algum ruído à imagem de entrada e fazendo o Autoencoder aprender a removê-la. Dessa maneira, o Autoencoder extrairá os recursos mais importantes e aprenderá uma representação robusta dos dados.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Denoising refere-se à adição intencional de ruído à entrada bruta antes de fornecê-la à rede. Pode-se obter denoising usando o mapeamento estocástico. Abaixo uma representação do Autoencoder Denoising:
  </span>
 </p>
 <h4 style="text-align: justify;">
 </h4>
 <h4 style="text-align: justify;">
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="denoising" class="aligncenter size-full wp-image-1568" data-attachment-id="1568" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="denoising" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?fit=414%2C516" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?fit=241%2C300" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?fit=414%2C516" data-orig-size="414,516" data-permalink="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/denoising/" data-recalc-dims="1" height="516" sizes="(max-width: 414px) 100vw, 414px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?resize=414%2C516" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?w=414 414w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?resize=241%2C300 241w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?resize=200%2C249 200w" width="414"/>
  </span>
 </p>
 <h3 style="text-align: justify;">
 </h3>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   5. Contractive Autoencoders(CAE)
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O objetivo do Autoencoder Contrativo (CAE) é ter uma representação aprendida robusta, menos sensível a pequenas variações nos dados. A robustez da representação para os dados é feita aplicando um termo de penalidade à função de perda. O termo da penalidade é a norma Frobenius da matriz jacobiana. A norma de Frobenius da matriz jacobiana para a camada oculta é calculada em relação à entrada. A norma de Frobenius da matriz jacobiana é a soma do quadrado de todos os elementos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O Autoencoder Contrativo é outra técnica de regularização, como os Autoencoders Esparsos e os Autoencoders Denoising.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O CAE supera os resultados obtidos pela regularização do Autoencoder usando decaimento de peso ou denoising. O CAE é uma escolha melhor do que o Autoencoder Denoising para aprender a extração de recursos úteis. O termo de penalidade gera mapeamento que contrai fortemente os dados e, portanto, o nome Autoencoder Contrativo.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   6. Deep Autoencoders
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Deep Autoencoders consistem em duas redes de crenças profundas idênticas (Deep Belief Networks). Uma rede para codificação e outra para decodificação. Os Autoencoders tipicamente profundos têm de 4 a 5 camadas para codificação e as próximas 4 a 5 camadas para decodificação. Usamos camada não supervisionada por camada, pré-treinamento.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A Máquina Boltzmann Restrita (RBM) é o alicerce básico das Deep Belief Networks. Na figura abaixo, tiramos uma imagem com 784 pixels. Treinamos usando uma pilha de 4 RBMs, desenrolamos e ajustamos com Backpropagation. A camada de codificação final é compacta e rápida!
  </span>
 </p>
 <h4 style="text-align: justify;">
 </h4>
 <h4 style="text-align: justify;">
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="deep" class="aligncenter size-full wp-image-1571" data-attachment-id="1571" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="deep" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?fit=711%2C367" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?fit=300%2C155" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?fit=711%2C367" data-orig-size="711,367" data-permalink="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/deep/" data-recalc-dims="1" height="367" sizes="(max-width: 711px) 100vw, 711px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=711%2C367" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?w=711 711w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=300%2C155 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=200%2C103 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=690%2C356 690w" width="711"/>
  </span>
 </p>
 <h4 style="text-align: justify;">
 </h4>
 <h4 style="text-align: justify;">
 </h4>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   7. Variational Autoencoders (VAEs)
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nos últimos anos, modelos generativos baseados em aprendizado profundo ganharam cada vez mais interesse devido a (e implicando) algumas melhorias surpreendentes no campo. Contando com uma enorme quantidade de dados, arquiteturas de rede bem projetadas e técnicas de treinamento inteligentes, os modelos geradores profundos demonstraram uma capacidade incrível de produzir peças de conteúdo altamente realistas de vários tipos, como imagens, textos e sons. Entre esses modelos geradores profundos, duas famílias principais se destacam e merecem uma atenção especial: Redes Adversárias Generativas (GANs) e Autoencoders Variacionais (VAEs). As GANs já estudamos nos capítulos anteriores.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   E a VAE é tão especial que merece um capítulo inteiro. Não perca o próximo capítulo.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000;" target="_blank">
    <span style="text-decoration: underline;">
     Formação Inteligência Artificial
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Customizando Redes Neurais com Funções de Ativação Alternativas
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Autoencoders – Unsupervised Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep inside: Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://medium.com/datadriveninvestor/deep-learning-different-types-of-autoencoders-41d4fa5f7570" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep Learning — Different Types of Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Contractive Auto-Encoders – Explicit Invariance During Feature Extraction
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.jeremyjordan.me/autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Introduction to Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent neural network based language model
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1565" href="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1565" href="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1565" href="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1565" href="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1565-5e0dd224c0f8b" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1565&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1565-5e0dd224c0f8b" id="like-post-wrapper-140353593-1565-5e0dd224c0f8b">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-60">
 Capítulo 60 – Variational Autoencoders (VAEs) – Definição, Redução de Dimensionalidade, Espaço Latente e Regularização
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nos últimos anos, modelos generativos baseados em aprendizado profundo ganharam cada vez mais interesse devido a (e implicando) algumas melhorias surpreendentes em Inteligência Artificial. Contando com uma enorme quantidade de dados, arquiteturas de rede bem projetadas e técnicas de treinamento inteligentes, os modelos generativos profundos demonstraram uma capacidade incrível de produzir peças de conteúdo altamente realistas de vários tipos, como imagens, textos e sons. Entre esses modelos, duas famílias principais se destacam e merecem uma atenção especial: Redes Adversárias Generativas (GANs) e Autoencoders Variacionais (VAEs). O primeiro já estudamos e agora estudaremos o segundo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em um
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     capítulo anterior
    </a>
   </span>
   , discutimos em profundidade as Redes Adversárias Generativas (GANs) e mostramos, em particular, como o treinamento antagônico pode opor duas redes, um gerador e um discriminador, para pressionar ambas a melhorar iteração após iteração. Apresentamos agora, neste capítulo, outro tipo de modelo generativo profundo: Autoencoders Variacionais (VAEs – Variational Autoencoders).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em resumo, um VAE é um Autoencoder cuja distribuição de codificações é regularizada durante o treinamento, a fim de garantir que seu espaço latente tenha boas propriedades, o que nos permite gerar novos dados. Além disso, o termo “variacional” vem da estreita relação que existe entre a regularização e o método de inferência variacional em Estatística. Essa arquitetura de Deep Learning é estudada na prática em
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep Learning II
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas o conceito por trás dos VAEs também pode levantar muitas questões. Qual é o espaço latente e por que regularizá-lo? Como gerar novos dados a partir dos VAEs? Qual é a ligação entre VAEs e inferência variacional? Para descrever os VAEs da melhor maneira possível, tentaremos responder a todas essas perguntas (e muitas outras!) e fornecer a você o máximo de informação e conhecimento possível (variando de intuições básicas a detalhes matemáticos mais avançados). Assim, o objetivo deste capítulo não é apenas discutir as noções fundamentais dos Variational Autoencoders (VAEs), mas também construir passo a passo e começar desde o início o raciocínio que leva a essas noções (como sempre fazemos em nossos cursos na
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/todos-os-cursos-dsa" rel="noopener noreferrer" target="_blank">
     Data Science Academy
    </a>
   </span>
   ). E vamos começar com o conceito de redução de dimensionalidade.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   O Que é Redução de Dimensionalidade?
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No aprendizado de máquina, a redução de dimensionalidade é o processo de redução do número de recursos (atributos) que descrevem alguns dados. Essa redução é feita por seleção (apenas alguns recursos existentes são conservados) ou por extração (um número reduzido de novos recursos é criado com base nos recursos antigos) e pode ser útil em muitas situações que exigem dados de baixa dimensionalidade (visualização de dados, armazenamento, computação pesada, etc…). Embora existam muitos métodos diferentes de redução de dimensionalidade, podemos definir uma estrutura global que seja compatível com a maioria desses métodos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Primeiro, vamos codificar o processo que produz a representação de “novos recursos” a partir da representação de “recursos antigos” (por seleção ou por extração) e decodificar o processo inverso. A redução de dimensionalidade pode então ser interpretada como compactação de dados, onde o codificador compacta os dados (do espaço inicial para o espaço codificado, também chamado de espaço latente) enquanto o decodificador os descompacta. Obviamente, dependendo da distribuição inicial dos dados, da dimensão do espaço latente e da definição do codificador, essa compactação pode ser perdida, o que significa que uma parte da informação é perdida durante o processo de codificação e não pode ser recuperada durante a decodificação.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="reduc" class="aligncenter size-large wp-image-1606" data-attachment-id="1606" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="reduc" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?fit=1024%2C560" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?fit=300%2C164" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?fit=1586%2C868" data-orig-size="1586,868" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/reduc/" data-recalc-dims="1" height="560" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=1024%2C560" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=1024%2C560 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=300%2C164 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=768%2C420 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=1536%2C841 1536w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=200%2C109 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=690%2C378 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?w=1586 1586w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O principal objetivo de um método de redução de dimensionalidade é encontrar o melhor par codificador / decodificador entre uma determinada família. Em outras palavras, para um determinado conjunto de codificadores e decodificadores possíveis, estamos procurando o par que mantém o máximo de informações ao codificar e, portanto, tem o mínimo de erro de reconstrução ao decodificar. Se denotarmos respectivamente E e D as famílias de codificadores e decodificadores que estamos considerando, o problema da redução de dimensionalidade pode ser escrito:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter wp-image-1608 size-medium" data-attachment-id="1608" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?fit=592%2C158" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?fit=300%2C80" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?fit=592%2C158" data-orig-size="592,158" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/form1-11/" data-recalc-dims="1" height="80" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?resize=300%2C80" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?resize=300%2C80 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?resize=200%2C53 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?w=592 592w" width="300"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Onde:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter wp-image-1609 size-full" data-attachment-id="1609" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?fit=270%2C108" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?fit=270%2C108" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?fit=270%2C108" data-orig-size="270,108" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/form2-17/" data-recalc-dims="1" height="108" sizes="(max-width: 270px) 100vw, 270px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?resize=270%2C108" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?w=270 270w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?resize=200%2C80 200w" width="270"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   define a medida do erro de reconstrução entre os dados de entrada x e os dados codificados e decodificados d (e (x)).
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Análise de Componentes Principais (PCA) e Autoencoders
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Um dos primeiros métodos que vêm à mente quando se fala em redução de dimensionalidade é a análise de componentes principais (PCA). Para mostrar como ele se encaixa na estrutura que acabamos de descrever e criar o link para os Autoencoders, vamos dar uma visão geral de como o PCA funciona, deixando a maioria dos detalhes de lado (caso queira estudar o PCA em detalhes e na prática, há um capítulo inteiro dedicado a esta técnica em
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Matemática Para Machine Learning
    </a>
   </span>
   ) .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A ideia do PCA é construir novos recursos independentes, que são combinações lineares dos novos recursos antigos e, de modo que as projeções dos dados no subespaço definido por esses novos recursos sejam o mais próximo possível dos dados iniciais (em termos de distância euclidiana). Em outras palavras, o PCA está procurando o melhor subespaço linear do espaço inicial (descrito por uma base ortogonal de novos recursos), de modo que o erro de aproximar os dados por suas projeções nesse subespaço seja o menor possível.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Traduzido em nossa estrutura global, procuramos um codificador na família E das matrizes n_e por n_d (transformação linear) cujas linhas são ortonormais (independência de recursos) e pelo decodificador associado entre a família D de matrizes n_d por n_e. Pode-se mostrar que os autovetores unitários correspondentes aos n_e maiores autovalores (em norma) da matriz de características de covariância são ortogonais (ou podem ser escolhidos assim) e definem o melhor subespaço da dimensão n_e para projetar dados com erro mínimo de aproximação. Assim, esses n_e autovetores podem ser escolhidos como nossos novos recursos e, portanto, o problema de redução de dimensão pode ser expresso como um problema de autovalor / autovetor. Além disso, também pode ser mostrado que, nesse caso, a matriz decodificadora é a transposta da matriz codificadora (se esses conceitos de Matemática parecem estranhos a você recomendamos
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Matemática Para Machine Learning
    </a>
    )
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Os Autoencoders são em essência, redes neurais para redução de dimensionalidade. A ideia geral dos Autoencoders é bastante simples e consiste em definir um codificador e um decodificador como redes neurais e aprender o melhor esquema de codificação-decodificação usando um processo de otimização iterativo. Assim, a cada iteração, alimentamos a arquitetura do autoencoder (o codificador seguido pelo decodificador) com alguns dados, comparamos a saída decodificada com os dados iniciais e retropropagamos o erro na arquitetura para atualizar os pesos das redes.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim, intuitivamente, a arquitetura geral do autoencoder (codificador + decodificador) cria um gargalo de dados que garante que apenas a parte estruturada principal da informação possa passar e ser reconstruída. Observando nossa estrutura geral, a família E dos codificadores considerados é definida pela arquitetura da rede do codificador, a família D dos decodificadores considerados é definida pela arquitetura da rede do decodificador e a busca do codificador e decodificador que minimiza o erro de reconstrução é feita por descida do gradiente sobre os parâmetros dessas redes.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="autoencoder" class="aligncenter size-large wp-image-1611" data-attachment-id="1611" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="autoencoder" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?fit=1024%2C560" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?fit=300%2C164" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?fit=1262%2C690" data-orig-size="1262,690" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/autoencoder/" data-recalc-dims="1" height="560" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=1024%2C560" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=1024%2C560 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=300%2C164 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=768%2C420 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=200%2C109 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=690%2C377 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?w=1262 1262w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos primeiro supor que nossas arquiteturas de codificador e decodificador tenham apenas uma camada sem não linearidade (autoencoder linear). Esse codificador e decodificador são transformações lineares simples que podem ser expressas como matrizes. Em tal situação, podemos ver um vínculo claro com o PCA no sentido de que, assim como o PCA, estamos procurando o melhor subespaço linear para projetar dados com o mínimo de perda de informações possível ao fazê-lo. As matrizes de codificação e decodificação obtidas com o PCA definem naturalmente uma das soluções que gostaríamos de alcançar por descida do gradiente, mas devemos destacar que essa não é a única. De fato, várias bases podem ser escolhidas para descrever o mesmo subespaço ideal e, portanto, vários pares de codificador / decodificador podem fornecer o erro de reconstrução ideal. Além disso, para autoencoders lineares e, ao contrário do PCA, os novos recursos não precisam ser independentes (sem restrições de ortogonalidade nas redes neurais).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Agora, vamos supor que o codificador e o decodificador sejam profundos e não lineares. Nesse caso, quanto mais complexa a arquitetura, mais o autoencoder pode prosseguir para uma alta redução de dimensionalidade, mantendo baixa a perda de reconstrução. Intuitivamente, se nosso codificador e nosso decodificador tiver graus de liberdade suficientes, podemos reduzir qualquer dimensionalidade inicial para 1. De fato, um codificador com “poder infinito” poderia teoricamente pegar nossos N pontos de dados iniciais e codificá-los como 1, 2, 3, … até N (ou mais geralmente, como N inteiro no eixo real) e o decodificador associado pode fazer a transformação reversa, sem perda durante o processo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui, porém, devemos ter duas coisas em mente. Primeiro, uma importante redução de dimensionalidade sem perda de reconstrução costuma ter um preço: a falta de estruturas interpretáveis ​​e exploráveis ​​no espaço latente (falta de regularidade). Segundo, na maioria das vezes, o objetivo final da redução da dimensionalidade não é apenas reduzir o número de dimensões dos dados, mas reduzir esse número de dimensões, mantendo a maior parte das informações da estrutura de dados nas representações reduzidas. Por essas duas razões, a dimensão do espaço latente e a “profundidade” dos autoencoders (que definem o grau e a qualidade da compressão) devem ser cuidadosamente controladas e ajustadas, dependendo do objetivo final da redução da dimensionalidade.
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Variational Autoencoders (VAEs)
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/introducao-aos-autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     capítulo 58
    </a>
   </span>
   introduzimos o conceito de Autoencoders, que são arquiteturas de codificador-decodificador que podem ser treinadas por descida do gradiente. Vamos agora fazer o link com o problema de geração de conteúdo, ver as limitações dos Autoencoders em sua forma atual para esse problema e introduzir os Autoencoders Variacionais.
  </span>
 </p>
 <h4 style="text-align: justify;">
  <span style="color: #000000;">
   Limitações de Autoencoders para Geração de Conteúdo
  </span>
 </h4>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nesse ponto, uma pergunta natural que vem à mente é “qual é o link entre os Autoencoders e a geração de conteúdo?”. De fato, uma vez que o Autoencoder foi treinado, temos um codificador e um decodificador, mas ainda não há uma maneira real de produzir qualquer novo conteúdo. À primeira vista, poderíamos ficar tentados a pensar que, se o espaço latente for regular o suficiente (bem “organizado” pelo codificador durante o processo de treinamento), poderíamos pegar um ponto aleatoriamente nesse espaço latente e decodificá-lo para obter um novo conteúdo. O decodificador agiria mais ou menos como o gerador de uma Rede Adversária Generativa .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="vae" class="aligncenter wp-image-1597 size-large" data-attachment-id="1597" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="vae" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?fit=1024%2C533" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?fit=300%2C156" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?fit=1389%2C723" data-orig-size="1389,723" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae/" data-recalc-dims="1" height="533" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=1024%2C533" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=1024%2C533 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=300%2C156 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=768%2C400 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=200%2C104 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=690%2C359 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?w=1389 1389w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No entanto, a regularidade do espaço latente para Autoencoders é um ponto difícil que depende da distribuição dos dados no espaço inicial, da dimensão do espaço latente e da arquitetura do codificador. Portanto, é bastante difícil (se não impossível) garantir, a priori, que o codificador organize o espaço latente de maneira inteligente, compatível com o processo generativo que acabamos de descrever.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para ilustrar esse ponto, vamos considerar o exemplo no qual descrevemos um codificador e um decodificador suficientemente poderosos para colocar N dados de treinamento inicial no eixo real (cada ponto de dados sendo codificado como um valor real) e decodificá-los sem nenhum perda de reconstrução. Nesse caso, o alto grau de liberdade do Autoencoder que possibilita a codificação e decodificação sem perda de informações (apesar da baixa dimensionalidade do espaço latente) leva a uma super adaptação severa, o que implica que alguns pontos do espaço latente fornecerão conteúdo sem sentido uma vez decodificado. Se esse exemplo unidimensional tiver sido voluntariamente escolhido para ser extremo, podemos notar que o problema da regularidade espacial latente dos Autoencoders é muito mais geral do que isso e merece uma atenção especial.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="vae2" class="aligncenter wp-image-1598 size-large" data-attachment-id="1598" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="vae2" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?fit=1024%2C341" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?fit=300%2C100" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?fit=1804%2C600" data-orig-size="1804,600" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae2/" data-recalc-dims="1" height="341" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=1024%2C341" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=1024%2C341 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=300%2C100 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=768%2C255 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=1536%2C511 1536w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=200%2C67 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=690%2C229 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?w=1804 1804w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao pensar nisso por um minuto, essa falta de estrutura entre os dados codificados no espaço latente é bastante normal. De fato, o Autoencoder é treinado apenas para codificar e decodificar com o mínimo de perdas possível, independentemente da organização do espaço latente. Portanto, se não tomarmos cuidado com a definição da arquitetura, é natural que, durante o treinamento, a rede aproveite todas as possibilidades de sobreajuste para realizar sua tarefa da melhor forma possível … a menos que a regularizemos explicitamente!
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Definição de Autoencoders Variacionais
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Portanto, para poder usar o decodificador de nosso autoencoder para fins generativos, precisamos ter certeza de que o espaço latente é regular o suficiente. Uma solução possível para obter essa regularidade é introduzir regularização explícita durante o processo de treinamento. Assim, como mencionamos brevemente na introdução deste capítulo, um autoencoder variacional pode ser definido como um autoencoder cujo treinamento é regularizado para evitar sobreajuste e garantir que o espaço latente tenha boas propriedades que possibilitem processos generativos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim como um autoencoder padrão, um autoencoder variacional é uma arquitetura composta por um codificador e um decodificador, treinada para minimizar o erro de reconstrução entre os dados decodificados e os dados iniciais. No entanto, para introduzir alguma regularização do espaço latente, procedemos a uma ligeira modificação do processo de codificação / decodificação: em vez de codificar uma entrada como um único ponto, a codificamos como uma distribuição no espaço latente. O modelo é treinado da seguinte maneira:
  </span>
 </p>
 <ul style="text-align: justify;">
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Primeiro, a entrada é codificada como distribuição no espaço latente.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Segundo, um ponto do espaço latente é amostrado a partir dessa distribuição.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Terceiro, o ponto amostrado é decodificado e o erro de reconstrução pode ser calculado.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Finalmente, o erro de reconstrução é retropropagado pela rede.
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="vae3" class="aligncenter size-full wp-image-1599" data-attachment-id="1599" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="vae3" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?fit=1024%2C338" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?fit=300%2C99" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?fit=1804%2C596" data-orig-size="1804,596" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae3/" data-recalc-dims="1" height="387" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=1170%2C387" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?w=1804 1804w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=300%2C99 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=1024%2C338 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=768%2C254 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=1536%2C507 1536w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=200%2C66 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=690%2C228 690w" width="1170"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na prática, as distribuições codificadas são escolhidas para serem normais, de modo que o codificador possa ser treinado para retornar a média e a matriz de covariância que descrevem esses gaussianos. A razão pela qual uma entrada é codificada como uma distribuição com alguma variância em vez de um único ponto é que torna possível expressar muito naturalmente a regularização do espaço latente: as distribuições retornadas pelo codificador são impostas para estarem próximas a uma distribuição normal padrão. Veremos na próxima subseção que garantimos dessa maneira uma regularização local e global do espaço latente (local por causa do controle de variância e global por causa do controle médio).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Assim, a função de perda que é minimizada ao treinar um VAE é composta de um “termo de reconstrução” (na camada final), que tende a tornar o esquema de codificação-decodificação o mais eficiente possível e um “termo de regularização” (no camada latente), que tende a regularizar a organização do espaço latente, tornando as distribuições retornadas pelo codificador próximas a uma distribuição normal padrão. Esse termo de regularização é expresso como a divergência de Kulback-Leibler entre a distribuição retornada e uma gaussiana padrão e será mais justificado na próxima seção. Podemos notar que a divergência de Kullback-Leibler entre duas distribuições gaussianas tem uma forma fechada que pode ser expressa diretamente em termos das médias e das matrizes de covariância das duas distribuições.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="vae4" class="aligncenter size-full wp-image-1600" data-attachment-id="1600" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="vae4" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?fit=1024%2C497" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?fit=300%2C146" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?fit=1548%2C752" data-orig-size="1548,752" data-permalink="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae4/" data-recalc-dims="1" height="568" sizes="(max-width: 1170px) 100vw, 1170px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=1170%2C568" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?w=1548 1548w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=300%2C146 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=1024%2C497 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=768%2C373 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=1536%2C746 1536w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=200%2C97 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=690%2C335 690w" width="1170"/>
  </span>
 </p>
 <h2 style="text-align: justify;">
  <span style="color: #000000;">
   Intuições Sobre a Regularização
  </span>
 </h2>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A regularidade que se espera do espaço latente para possibilitar o processo generativo pode ser expressa por meio de duas propriedades principais: continuidade (dois pontos de fechamento no espaço latente não devem fornecer dois conteúdos completamente diferentes uma vez decodificados) e integridade (para uma distribuição escolhida , um ponto amostrado no espaço latente deve fornecer conteúdo “significativo” depois de decodificado).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O único fato de que os VAEs codificam entradas como distribuições, em vez de pontos simples, não é suficiente para garantir continuidade e integridade. Sem um termo de regularização bem definido, o modelo pode aprender, a fim de minimizar seu erro de reconstrução, “ignorar” o fato de que as distribuições são retornadas e se comportam quase como os autoencoders clássicos (levando ao super ajuste). Para fazer isso, o codificador pode retornar distribuições com pequenas variações (que tendem a ser distribuições pontuais) ou retornar distribuições com meios muito diferentes (que ficariam muito distantes um do outro no espaço latente). Nos dois casos, as distribuições são usadas da maneira errada (cancelando o benefício esperado) e a continuidade e / ou a integridade não são satisfeitas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Portanto, para evitar esses efeitos, precisamos regularizar a matriz de covariância e a média das distribuições retornadas pelo codificador. Na prática, essa regularização é feita impondo distribuições para estar perto de uma distribuição normal padrão (centralizada e reduzida). Dessa forma, exigimos que as matrizes de covariância estejam próximas da identidade, impedindo distribuições pontuais e com a média próxima de 0, impedindo que as distribuições codificadas estejam muito distantes umas das outras.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com esse termo de regularização, impedimos que o modelo codifique dados distantes no espaço latente e incentivamos o máximo possível as distribuições retornadas a “se sobrepor”, satisfazendo dessa maneira as condições de continuidade e integridade esperadas. Naturalmente, como em qualquer termo de regularização, isso tem o preço de um erro de reconstrução mais alto nos dados de treinamento. A troca entre o erro de reconstrução e a divergência de KL pode, no entanto, ser ajustada e veremos no próximo capítulo como a expressão emerge naturalmente de nossa derivação formal.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para concluir, podemos observar que a continuidade e a integridade obtidas com a regularização tendem a criar um “gradiente” sobre as informações codificadas no espaço latente. Por exemplo, um ponto do espaço latente que estaria a meio caminho entre as médias de duas distribuições codificadas provenientes de diferentes dados de treinamento deve ser decodificado em algo que esteja em algum lugar entre os dados que deram a primeira distribuição e os dados que deram a segunda distribuição como pode ser amostrado pelo autoencoder em ambos os casos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Essa é uma das arquiteturas mais avançadas de Deep Learning e para uma melhor compreensão, precisamos da nossa amiga, a Matemática. No próximo capítulo traremos um pouco da Matemática por trás dos VAEs. Até lá.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Customizando Redes Neurais com Funções de Ativação Alternativas
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Autoencoders – Unsupervised Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Understanding Variational Autoencoders (VAEs)
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Deep inside: Autoencoders
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://medium.com/datadriveninvestor/deep-learning-different-types-of-autoencoders-41d4fa5f7570" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Deep Learning — Different Types of Autoencoders
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Contractive Auto-Encoders – Explicit Invariance During Feature Extraction
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.jeremyjordan.me/autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Introduction to Autoencoders
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent neural network based language model
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1579" href="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1579" href="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1579" href="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1579" href="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1579-5e0dd226c3523" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1579&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1579-5e0dd226c3523" id="like-post-wrapper-140353593-1579-5e0dd226c3523">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-61">
 Capítulo 61 – A Matemática dos Variational Autoencoders (VAEs)
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   No capítulo
   <span style="text-decoration: underline;">
    <a href="http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/" rel="noopener noreferrer" target="_blank">
     anterior
    </a>
   </span>
   , fornecemos a seguinte visão geral intuitiva: Os VAEs são Autoencoders que codificam entradas como distribuições em vez de pontos e cuja “organização” do espaço latente é regularizada restringindo as distribuições retornadas pelo codificador a estarem próximas de um gaussiano padrão. Neste capítulo, forneceremos uma visão matemática dos VAEs que nos permitirá justificar o termo de regularização com mais rigor. Para isso, definiremos uma estrutura probabilística clara e usaremos, em particular, a técnica de inferência variacional.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A base matemática e estatística por trás dos conceitos deste capítulo pode ser obtida na
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" target="_blank">
    <span style="text-decoration: underline;">
     Formação Análise Estatística Para Cientistas de Dados
    </span>
   </a>
   . Os conceitos aqui abordados também são estudados
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=machine-learning-engineer" rel="noopener noreferrer" target="_blank">
     Machine Learning
    </a>
   </span>
   e
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" target="_blank">
     Deep Learning II
    </a>
   </span>
   .
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Estrutura Probabilística e Premissas
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos começar definindo um modelo gráfico probabilístico para descrever nossos dados. Denotamos por x a variável que representa nossos dados e assumimos que x é gerado a partir de uma variável latente z (a representação codificada) que não é diretamente observada. Assim, para cada ponto de dados, é assumido o seguinte processo generativo de duas etapas:
  </span>
 </p>
 <ul>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Primeiro, uma representação latente z é amostrada da distribuição anterior p(z).
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Segundo, os dados x são amostrados da distribuição de probabilidade condicional definido por p(x | z).
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Com esse modelo probabilístico em mente, podemos redefinir nossas noções de codificador e decodificador. De fato, ao contrário de um Autoencoder simples que considera codificador e decodificador determinístico, consideraremos agora versões probabilísticas desses dois objetos. O “decodificador probabilístico” é definido naturalmente por p(x | z), que descreve a distribuição da variável decodificada dada a codificada, enquanto o “codificador probabilístico” é definido por p(z | x), que descreve a distribuição de a variável codificada, dada a decodificada.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Neste ponto, já podemos notar que a regularização do espaço latente que nos faltava em Autoencoders simples aparece naturalmente aqui na definição do processo de geração de dados: presume-se que representações codificadas z no espaço latente sigam a distribuição anterior p(z). Caso contrário, também podemos lembrar o conhecido teorema de Bayes, que faz a ligação entre o anterior p(z), a probabilidade p(x | z) e o posterior p(z | x):
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form1" class="aligncenter size-full wp-image-1637" data-attachment-id="1637" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?fit=732%2C176" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?fit=300%2C72" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?fit=732%2C176" data-orig-size="732,176" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form1-12/" data-recalc-dims="1" height="176" sizes="(max-width: 732px) 100vw, 732px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=732%2C176" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?w=732 732w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=300%2C72 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=200%2C48 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=690%2C166 690w" width="732"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos agora assumir que p(z) é uma distribuição gaussiana padrão e que p(x | z) é uma distribuição gaussiana cuja média é definida por uma função determinística f da variável de z e cuja matriz de covariância tem a forma de uma constante positiva c que multiplica a matriz de identidade I. Supõe-se que a função f pertence a uma família de funções denotadas F que é deixada não especificada no momento e que será escolhida posteriormente. Assim, temos:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form2" class="aligncenter size-full wp-image-1638" data-attachment-id="1638" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?fit=894%2C156" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?fit=300%2C52" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?fit=894%2C156" data-orig-size="894,156" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form2-18/" data-recalc-dims="1" height="156" sizes="(max-width: 894px) 100vw, 894px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=894%2C156" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?w=894 894w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=300%2C52 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=768%2C134 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=200%2C35 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=690%2C120 690w" width="894"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Vamos considerar, por enquanto, que f está bem definido e fixo. Em teoria, como conhecemos p(z) e p(x | z), podemos usar o Teorema de Bayes para calcular p(z | x): este é um problema clássico de inferência bayesiana. No entanto, esse tipo de computação geralmente é intratável (por causa da integral no denominador) e requer o uso de técnicas de aproximação, como inferência variacional.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Nota: Aqui podemos mencionar que p(z) e p(x | z) são ambas distribuições gaussianas, implicando que p(z | x) também deve seguir uma distribuição gaussiana. Em teoria, poderíamos “apenas” tentar expressar a média e a matriz de covariância de p(z | x) com relação às médias e às matrizes de covariância de p(z) e p(x | z). No entanto, na prática, esses valores dependem da função f que pode ser complexa e que não está definida por enquanto (mesmo que tenhamos assumido o contrário). Além disso, o uso de uma técnica de aproximação como inferência variacional torna a abordagem bastante geral e mais robusta a algumas mudanças na hipótese do modelo.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Formulação de Inferência Variacional
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em Estatística, a inferência variacional é uma técnica para aproximar distribuições complexas. A ideia é definir uma família de distribuição parametrizada (por exemplo, a família de Gaussianos, cujos parâmetros são a média e a covariância) e procurar a melhor aproximação de nossa distribuição de destino entre essa família. O melhor elemento da família é aquele que minimiza uma determinada medição de erro de aproximação (na maioria das vezes a divergência de Kullback-Leibler entre aproximação e alvo) e é encontrada por descida do gradiente sobre os parâmetros que descrevem a família. Para mais detalhes, você encontra um artigo sobe isso na seção de referências ao final do capítulo.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Aqui vamos aproximar p(z | x) por uma distribuição gaussiana q_x(z) cuja média e covariância são definidas por duas funções, g e h, do parâmetro x. Essas duas funções devem pertencer, respectivamente, às famílias de funções G e H que serão especificadas mais tarde, mas que devem ser parametrizadas. Assim, podemos denotar:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form3" class="aligncenter size-full wp-image-1639" data-attachment-id="1639" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form3" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?fit=868%2C100" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?fit=300%2C35" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?fit=868%2C100" data-orig-size="868,100" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form3-15/" data-recalc-dims="1" height="100" sizes="(max-width: 868px) 100vw, 868px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=868%2C100" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?w=868 868w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=300%2C35 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=768%2C88 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=200%2C23 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=690%2C79 690w" width="868"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Portanto, definimos dessa maneira uma família de candidatos à inferência variacional e precisamos agora encontrar a melhor aproximação entre essa família otimizando as funções g e h (de fato, seus parâmetros) para minimizar a divergência de Kullback-Leibler entre a aproximação e o alvo p(z | x). Em outras palavras, estamos procurando a aproximação ideal g* e h* de modo que:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form4" class="aligncenter size-large wp-image-1640" data-attachment-id="1640" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form4" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?fit=1024%2C319" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?fit=300%2C93" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?fit=1626%2C506" data-orig-size="1626,506" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form4-11/" data-recalc-dims="1" height="319" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=1024%2C319" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=1024%2C319 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=300%2C93 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=768%2C239 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=1536%2C478 1536w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=200%2C62 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=690%2C215 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?w=1626 1626w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na penúltima equação, podemos observar a troca existente – ao aproximar-se do p(z | x) posterior – entre maximizar a probabilidade das “observações” (maximização da probabilidade logarítmica esperada para o primeiro termo) e permanecer próximo à distribuição anterior (minimização da divergência de KL entre q_x(z) e p(z), para o segundo termo). Essa troca é natural para o problema de inferência bayesiana e expressa o equilíbrio que precisa ser encontrado entre a confiança que temos nos dados e a confiança que temos no passado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até agora, assumimos a função f conhecida e fixa e mostramos que, com tais premissas, podemos aproximar o p(z | x) posterior usando a técnica de inferência variacional. No entanto, na prática, essa função f, que define o decodificador, não é conhecida e também precisa ser escolhida. Para fazer isso, lembre-se de que nosso objetivo inicial é encontrar um esquema de codificação e decodificação com desempenho, cujo espaço latente seja regular o suficiente para ser usado para fins generativos. Se a regularidade é regida principalmente pela distribuição anterior assumida no espaço latente, o desempenho do esquema geral de codificação / decodificação depende muito da escolha da função f. De fato, como p(z | x) pode ser aproximado (por inferência variacional) de p(z) e p(x | z) e como p(z) é um gaussiano padrão simples, as duas únicas alavancas que temos à nossa disposição em nosso modelo para fazer otimizações são o parâmetro c (que define a variância da probabilidade) e a função f (que define a média da probabilidade).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Portanto, vamos considerar que, como discutimos anteriormente, podemos obter para qualquer função f em F (cada uma definindo um decodificador probabilístico diferente p(x | z)) a melhor aproximação de p(z | x), denotada q*_x(z) Apesar de sua natureza probabilística, estamos procurando um esquema de codificação-decodificação o mais eficiente possível e, em seguida, queremos escolher a função f que maximize a probabilidade logarítmica esperada de x dado z quando z é amostrado de q*_x(z) Em outras palavras, para uma dada entrada x, queremos maximizar a probabilidade de ter x̂ = x quando amostramos z da distribuição q*_x(z) e depois amostramos x̂ da distribuição p(x | z). Assim, procuramos o f* ideal para que:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form5" class="aligncenter size-full wp-image-1641" data-attachment-id="1641" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form5" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?fit=748%2C282" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?fit=300%2C113" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?fit=748%2C282" data-orig-size="748,282" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form5-9/" data-recalc-dims="1" height="282" sizes="(max-width: 748px) 100vw, 748px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=748%2C282" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?w=748 748w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=300%2C113 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=200%2C75 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=690%2C260 690w" width="748"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   onde q*_x (z) depende da função f e é obtido como descrito anteriormente. Reunindo todas as peças, estamos procurando as aproximações
  </span>
  <span style="color: #000000;">
   f*,  g* e h* ideais para que:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form6" class="aligncenter size-large wp-image-1642" data-attachment-id="1642" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form6" data-large-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?fit=1024%2C83" data-medium-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?fit=300%2C24" data-orig-file="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?fit=1280%2C104" data-orig-size="1280,104" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form6-6/" data-recalc-dims="1" height="83" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=1024%2C83" srcset="https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=1024%2C83 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=300%2C24 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=768%2C62 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=200%2C16 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=690%2C56 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?w=1280 1280w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Podemos identificar nesta função objetivo os elementos introduzidos na descrição intuitiva dos VAEs dados no capítulo anterior: o erro de reconstrução entre x e f(z) e o termo de regularização dado pela divergência KL entre q_x(z) e p(z) ) (que é um gaussiano padrão). Também podemos notar a constante c que regula o equilíbrio entre os dois termos anteriores. Quanto maior c for, mais assumimos uma alta variação em torno de f(z) para o decodificador probabilístico em nosso modelo e, portanto, mais favorecemos o termo de regularização sobre o termo de reconstrução (e o oposto se c for baixo).
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Trazendo Redes Neurais Para o Modelo
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Até o momento, definimos um modelo probabilístico que depende de três funções, f, g e h, e expressamos, usando inferência variacional, o problema de otimização a ser resolvido para obter f*, g* e h* que ofereçam o melhor esquema de codificação-decodificação com este modelo. Como não podemos otimizar facilmente todo o espaço de funções, restringimos o domínio de otimização e decidimos expressar f, g e h como redes neurais. Assim, F, G e H correspondem, respectivamente, às famílias de funções definidas pelas arquiteturas de rede e a otimização é feita sobre os parâmetros dessas redes.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Na prática, g e h não são definidos por duas redes completamente independentes, mas compartilham uma parte de sua arquitetura e seus pesos, de modo que temos:
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form7" class="aligncenter size-large wp-image-1643" data-attachment-id="1643" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form7" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?fit=1024%2C41" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?fit=300%2C12" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?fit=1046%2C42" data-orig-size="1046,42" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form7-5/" data-recalc-dims="1" height="41" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=1024%2C41" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=1024%2C41 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=300%2C12 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=768%2C31 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=200%2C8 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=690%2C28 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?w=1046 1046w" width="1024"/>
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Como isso define a matriz de covariância de q_x(z), h(x) deve ser uma matriz quadrada. Entretanto, para simplificar o cálculo e reduzir o número de parâmetros, assumimos que nossa aproximação de p(z | x), q_x(z), é uma distribuição gaussiana multidimensional com matriz de covariância diagonal (assunção de independência de variáveis). Com essa suposição, h(x) é simplesmente o vetor dos elementos diagonais da matriz de covariância e, então, tem o mesmo tamanho de g(x). No entanto, reduzimos dessa maneira a família de distribuições que consideramos para inferência variacional e, portanto, a aproximação de p(z | x) obtida pode ser menos precisa.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form8" class="aligncenter size-large wp-image-1644" data-attachment-id="1644" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form8" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?fit=1024%2C474" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?fit=300%2C139" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?fit=1286%2C595" data-orig-size="1286,595" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form8-5/" data-recalc-dims="1" height="474" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=1024%2C474" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=1024%2C474 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=300%2C139 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=768%2C355 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=200%2C93 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=690%2C319 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?w=1286 1286w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Ao contrário da parte do codificador que modela p(z | x) e para a qual consideramos um gaussiano com média e covariância que são funções de x(g e h), nosso modelo assume para p(x | z) um gaussiano com covariância. A função f da variável z que define a média desse gaussiano é modelada por uma rede neural e pode ser representada da seguinte forma:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form9" class="aligncenter size-large wp-image-1645" data-attachment-id="1645" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form9" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?fit=1024%2C415" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?fit=300%2C122" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?fit=1286%2C521" data-orig-size="1286,521" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form9-2/" data-recalc-dims="1" height="415" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=1024%2C415" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=1024%2C415 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=300%2C122 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=768%2C311 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=200%2C81 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=690%2C280 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?w=1286 1286w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A arquitetura geral é então obtida concatenando o codificador e as partes do decodificador. No entanto, ainda precisamos ter muito cuidado com a maneira como coletamos amostras da distribuição retornada pelo codificador durante o treinamento. O processo de amostragem deve ser expresso de forma a permitir que o erro seja retropropagado pela rede. Um truque simples, chamado truque de reparametrização, é usado para tornar possível a descida do gradiente, apesar da amostragem aleatória que ocorre na metade da arquitetura e consiste em usar o fato de que se z é uma variável aleatória após uma distribuição gaussiana com média g(x) e com covariância h(x), pode ser expresso como:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form10" class="aligncenter size-full wp-image-1646" data-attachment-id="1646" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form10" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?fit=680%2C118" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?fit=300%2C52" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?fit=680%2C118" data-orig-size="680,118" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form10-2/" data-recalc-dims="1" height="118" sizes="(max-width: 680px) 100vw, 680px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?resize=680%2C118" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?w=680 680w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?resize=300%2C52 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?resize=200%2C35 200w" width="680"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form11" class="aligncenter size-large wp-image-1648" data-attachment-id="1648" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form11" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?fit=1024%2C388" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?fit=300%2C114" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?fit=1852%2C701" data-orig-size="1852,701" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form11/" data-recalc-dims="1" height="388" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=1024%2C388" srcset="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=1024%2C388 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=300%2C114 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=768%2C291 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=1536%2C581 1536w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=200%2C76 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=690%2C261 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?w=1852 1852w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Finalmente, a função objetivo da arquitetura de Autoencoder Variacional obtida dessa maneira é dada pela última equação da subseção anterior, na qual a expectativa teórica é substituída por uma aproximação de Monte-Carlo mais ou menos precisa que consiste, na maioria das vezes, em um sorteio único. Assim, considerando essa aproximação e denotando C = 1 / (2c), recuperamos a função de perda derivada intuitivamente na seção anterior, composta por um termo de reconstrução, um termo de regularização e uma constante para definir os pesos relativos desses dois termos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="form12" class="aligncenter size-large wp-image-1649" data-attachment-id="1649" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="form12" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?fit=1024%2C617" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?fit=300%2C181" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?fit=1664%2C1002" data-orig-size="1664,1002" data-permalink="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form12/" data-recalc-dims="1" height="617" sizes="(max-width: 1024px) 100vw, 1024px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=1024%2C617" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=1024%2C617 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=300%2C181 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=768%2C462 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=1536%2C925 1536w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=200%2C120 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=690%2C415 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?w=1664 1664w" width="1024"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Autoencoders Variacionais (VAEs) são Autoencoders que resolvem o problema da irregularidade do espaço latente, fazendo com que o codificador retorne uma distribuição sobre o espaço latente em vez de um único ponto e adicionando à função de perda um termo de regularização sobre a distribuição retornada para garantir uma melhor organização do espaço latente
  </span>
  <span style="color: #000000;">
   assumindo um modelo probabilístico simples para descrever nossos dados, a função de perda bastante intuitiva dos VAEs, composta por um termo de reconstrução e um termo de regularização, pode ser cuidadosamente derivada, usando em particular a técnica estatística de inferência variacional (daí o nome Autoencoder “Variacional”).
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Para concluir, podemos destacar que, durante os últimos anos, as GANs se beneficiaram de muito mais contribuições científicas do que os VAEs. Entre outras razões, o maior interesse demonstrado pela comunidade por GANs pode ser parcialmente explicado pelo maior grau de complexidade na base teórica dos VAEs (modelo probabilístico e inferência variacional) em comparação à simplicidade do conceito de treinamento adversário que rege os GANs. Com este capítulo, esperamos que tenhamos compartilhado intuições valiosas e fortes fundamentos teóricos para tornar os VAEs mais acessíveis aos recém-chegados. No entanto, agora que discutimos em profundidade os dois, resta uma pergunta … qual arquitetura você achou mais interessante, GANs ou VAEs?
  </span>
 </p>
 <p>
  <span style="color: #000000;">
   No próximo capítulo começamos a estudar a Aprendizagem Por Reforço! Até lá.
  </span>
 </p>
 <p>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Inteligência Artificial
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Análise Estatística Para Cientistas de Dados
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Formação Cientista de Dados
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Customizando Redes Neurais com Funções de Ativação Alternativas
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Autoencoders – Unsupervised Learning
    </a>
   </span>
  </span>
 </p>
 <p>
  <span style="text-decoration: underline;">
   <a href="https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29" rel="noopener noreferrer" target="_blank">
    <span style="color: #000000; text-decoration: underline;">
     Bayesian inference problem, MCMC and variational inference
    </span>
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Understanding Variational Autoencoders (VAEs)
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep inside: Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://medium.com/datadriveninvestor/deep-learning-different-types-of-autoencoders-41d4fa5f7570" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep Learning — Different Types of Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Contractive Auto-Encoders – Explicit Invariance During Feature Extraction
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.jeremyjordan.me/autoencoders/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Introduction to Autoencoders
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Practical Recommendations for Gradient-Based Training of Deep Architectures
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient-Based Learning Applied to Document Recognition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Neural Networks &amp; The Backpropagation Algorithm, Explained
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Recurrent neural network based language model
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Gradient Descent For Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline;">
   <span style="color: #000000; text-decoration: underline;">
    <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Pattern Recognition and Machine Learning
    </a>
   </span>
  </span>
 </p>
 <p style="text-align: justify;">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Compartilhe isso:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1636" href="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Twitter(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1636" href="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Facebook(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-linkedin">
       <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1636" href="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no LinkedIn(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-pinterest">
       <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1636" href="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Pinterest(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no Tumblr(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-jetpack-whatsapp">
       <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Clique para compartilhar no WhatsApp(abre em nova janela)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1636-5e0dd2292a2bf" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1636&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1636-5e0dd2292a2bf" id="like-post-wrapper-140353593-1636-5e0dd2292a2bf">
   <h3 class="sd-title">
    Curtir isso:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Curtir
     </span>
    </span>
    <span class="loading">
     Carregando...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Relacionado
    </em>
   </h3>
  </div>
 </p>
</div>
<br/><hr><br/><h1 class="entry-title" id="capitulo-62">
 Capítulo 62 – O Que é Aprendizagem Por Reforço?
</h1>
<div class="entry-content">
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Todas as arquiteturas de Deep Learning que estudamos até aqui neste livro podem ser classificadas em duas categorias de aprendizagem de máquina (você já sabe que Deep Learning é sub-categoria de Machine Learning, que por sua vez é uma sub-categoria de
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Inteligência Artificial
    </a>
   </span>
   ):
  </span>
 </p>
 <ul style="text-align: justify;">
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Aprendizagem Supervisionada – quando apresentamos ao algoritmo dados de entrada e as respectivas saídas.
   </span>
  </li>
  <li style="text-align: justify;">
   <span style="color: #000000;">
    Aprendizagem Não Supervisionada – quando apresentamos somente os dados de entrada e o algoritmo descobre as saídas.
   </span>
  </li>
 </ul>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Mas existe uma terceira categoria de aprendizagem, chamada de Aprendizagem Por Reforço (ou Reinforcement Learning), muito usada em Games e Robótica e que vem obtendo resultados cada vez melhores. A Aprendizagem Por Reforço é a principal técnica por trás do AlphaGo e está muito bem retratada no documentário do mesmo nome:
   <span style="text-decoration: underline;">
    <a href="https://www.imdb.com/title/tt6700846/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     AlphaGo
    </a>
   </span>
   .
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Serão diversos capítulos dedicados a esta técnica e à sua extensão, o Deep Reinforcement Learning, que são estudados na prática em
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Deep Learning II
    </a>
   </span>
   . Você também encontra um exemplo completo no curso gratuito
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/cursos-gratuitos-1" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Python Fundamentos Para Análise de Dados
    </a>
   </span>
   . Vamos começar definindo o que é Aprendizagem Por Reforço.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   O Que é Aprendizagem Por Reforço?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   A Aprendizagem Por Reforço é o treinamento de modelos de aprendizado de máquina para tomar uma sequência de decisões. O agente aprende a atingir uma meta em um ambiente incerto e potencialmente complexo. No aprendizado por reforço, o sistema de inteligência artificial enfrenta uma situação. O computador utiliza tentativa e erro para encontrar uma solução para o problema. Para que a máquina faça o que o programador deseja, a inteligência artificial recebe recompensas ou penalidades pelas ações que executa. Seu objetivo é maximizar a recompensa total.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Embora o
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
     Cientista de Dados
    </a>
   </span>
   defina a política de recompensa – isto é, as regras do jogo – ele não dá ao modelo nenhuma dica ou sugestão de como resolver o jogo. Cabe ao modelo descobrir como executar a tarefa para maximizar a recompensa, começando com testes totalmente aleatórios e terminando com táticas sofisticadas. Ao alavancar o poder da pesquisa e de muitas tentativas, o aprendizado por reforço é atualmente a maneira mais eficaz de sugerir a criatividade da máquina. Ao contrário dos seres humanos, a inteligência artificial pode reunir experiência de milhares de jogos paralelos se um algoritmo de aprendizado por reforço for executado em uma infraestrutura de computador suficientemente poderosa.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Exemplo:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O problema é o seguinte: Temos um agente e uma recompensa, com muitos obstáculos no meio, como nesta imagem abaixo. O agente deve encontrar o melhor caminho possível para alcançar a recompensa e quando encontrar um obstáculo, deve ser penalizado (pois ele deve escolher o caminho sem obstáculos). Com a Aprendizagem Por Reforço, podemos treinar o agente para encontrar o melhor caminho.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="rl" class="aligncenter wp-image-1670 size-medium" data-attachment-id="1670" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="rl" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?fit=996%2C689" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?fit=300%2C208" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?fit=996%2C689" data-orig-size="996,689" data-permalink="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/rl/" data-recalc-dims="1" height="208" sizes="(max-width: 300px) 100vw, 300px" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=300%2C208" srcset="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=300%2C208 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=768%2C531 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=200%2C138 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=690%2C477 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?w=996 996w" width="300"/>
  </span>
 </p>
 <h3 style="text-align: justify;">
 </h3>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   Desafios do Aprendizado Por Reforço
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O principal desafio do aprendizado por reforço está na preparação do ambiente de simulação, que depende muito da tarefa a ser executada. Quando o modelo é treinado em jogos de Xadrez, Go ou Atari, a preparação do ambiente de simulação é relativamente simples. Quando se trata de construir um modelo capaz de dirigir um carro autônomo, a construção de um simulador realista é crucial antes de deixar o carro andar na rua. O modelo precisa descobrir como frear ou evitar uma colisão em um ambiente seguro. Transferir o modelo do ambiente de treinamento para o mundo real é onde as coisas ficam complicadas.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Escalar e ajustar a rede neural que controla o agente é outro desafio. Não há como se comunicar com a rede a não ser através do sistema de recompensas e penalidades. Isso pode levar a um esquecimento catastrófico, em que a aquisição de novos conhecimentos faz com que alguns dos antigos sejam apagados da rede. Ou seja, precisamos guardar o aprendizado na “memória” do agente.
   <br/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Outro desafio é alcançar um ótimo local – ou seja, o agente executa a tarefa como está, mas não da maneira ideal ou necessária. Um “saltador” pulando como um canguru em vez de fazer o que se esperava dele com pequenos saltos é um ótimo exemplo. Por fim, existem agentes que otimizarão o prêmio sem executar a tarefa para a qual foram projetados.
  </span>
 </p>
 <h3 style="text-align: justify;">
  <span style="color: #000000;">
   O Que Distingue o Aprendizado Por Reforço do Aprendizado Profundo e do Aprendizado de Máquina?
  </span>
 </h3>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   De fato, não há uma divisão clara entre aprendizado de máquina, aprendizado profundo e aprendizado por reforço. É como uma relação paralelogramo – retângulo – quadrado, em que o aprendizado de máquina é a categoria mais ampla e o aprendizado por reforço é o mais estreito.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Da mesma forma, o aprendizado por reforço é uma aplicação especializada de técnicas de Deep Learning e Machine Learning, projetada para resolver problemas de uma maneira específica.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Embora as ideias pareçam divergir, não há uma divisão acentuada entre esses subtipos. Além disso, eles se mesclam nos projetos, pois os modelos são projetados para não se ater ao “tipo puro”, mas para executar a tarefa da maneira mais eficaz possível. Portanto, “o que distingue precisamente o aprendizado de máquina, o aprendizado profundo e o aprendizado por reforço” é, na verdade, uma pergunta difícil de responder. Mas vamos definir cada um deles!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <strong>
    Aprendizado de Máquina
   </strong>
   é uma forma de IA na qual os computadores têm a capacidade de melhorar progressivamente o desempenho de uma tarefa específica com dados, sem serem diretamente programados. Essa é a definição de
   <span style="text-decoration: underline;">
    <a href="https://en.wikipedia.org/wiki/Arthur_Samuel" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
     Arthur Lee Samuel
    </a>
    .
   </span>
   Ele cunhou o termo “aprendizado de máquina”, do qual existem dois tipos, aprendizado de máquina supervisionado e não supervisionado.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O aprendizado de máquina supervisionado acontece quando um programador pode fornecer um rótulo para cada entrada de treinamento no sistema de aprendizado de máquina.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O aprendizado não supervisionado ocorre quando o modelo é fornecido apenas com os dados de entrada, mas sem rótulos explícitos. Ele precisa pesquisar os dados e encontrar a estrutura ou os relacionamentos ocultos. O
   <span style="text-decoration: underline;">
    <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" target="_blank">
     Cientista de Dados
    </a>
   </span>
   pode não saber qual é a estrutura ou o que o modelo de aprendizado de máquina irá encontrar.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O
   <strong>
    Aprendizado Profundo
   </strong>
   consiste em várias camadas de redes neurais, projetadas para executar tarefas mais sofisticadas. A construção de modelos de aprendizado profundo foi inspirada no design do cérebro humano, mas simplificada. Os modelos de aprendizado profundo consistem em algumas camadas de rede neural que são, em princípio, responsáveis por aprender gradualmente recursos mais abstratos sobre dados específicos.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Embora as soluções de aprendizado profundo sejam capazes de fornecer resultados maravilhosos, em termos de escala, elas não são páreo para o cérebro humano. Cada camada usa o resultado de uma anterior como entrada e toda a rede é treinada como um todo. O conceito central de criar uma rede neural artificial não é novo, mas apenas recentemente o hardware moderno forneceu energia computacional suficiente para treinar efetivamente essas redes, expondo um número suficiente de exemplos. A adoção estendida trouxe estruturas como TensorFlow, Keras e PyTorch, as quais tornaram a construção de modelos de aprendizado de máquina muito mais conveniente.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   O
   <strong>
    Aprendizado Por Reforço
   </strong>
   , como declarado acima, emprega um sistema de recompensas e penalidades para obrigar o computador a resolver um problema sozinho. O envolvimento humano é limitado à mudança do ambiente e ao ajuste do sistema de recompensas e penalidades. Como o computador maximiza a recompensa, ele está propenso a procurar maneiras inesperadas de fazê-lo. O envolvimento humano é focado em impedir que ele explore o sistema e motive a máquina a executar a tarefa da maneira esperada. O aprendizado por reforço é útil quando não existe uma “maneira adequada” de executar uma tarefa, mas existem regras que o modelo deve seguir para desempenhar corretamente suas tarefas. Abaixo a performance de um agente sendo treinado em um jogo clássico do Atari.
  </span>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Performance inicial do agente:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="breakout_0" class="aligncenter size-full wp-image-1674" data-attachment-id="1674" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="breakout_0" data-large-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?fit=160%2C210" data-medium-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?fit=160%2C210" data-orig-file="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?fit=160%2C210" data-orig-size="160,210" data-permalink="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/breakout_0/" data-recalc-dims="1" height="210" src="https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?resize=160%2C210" width="160"/>
  </span>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Após 15 minutos de treinamento:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="breakout_1" class="aligncenter size-full wp-image-1673" data-attachment-id="1673" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="breakout_1" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?fit=160%2C210" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?fit=160%2C210" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?fit=160%2C210" data-orig-size="160,210" data-permalink="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/breakout_1/" data-recalc-dims="1" height="210" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?resize=160%2C210" width="160"/>
  </span>
 </p>
 <p style="text-align: center;">
  <span style="color: #000000;">
   Após 30 minutos de treinamento:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   <img alt="breakout_2" class="aligncenter size-full wp-image-1675" data-attachment-id="1675" data-comments-opened="0" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="breakout_2" data-large-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?fit=160%2C210" data-medium-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?fit=160%2C210" data-orig-file="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?fit=160%2C210" data-orig-size="160,210" data-permalink="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/breakout_2/" data-recalc-dims="1" height="210" src="https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?resize=160%2C210" width="160"/>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Em particular, se a inteligência artificial vai dirigir um carro ou aprender a jogar alguns clássicos do Atari, pode ser considerado um marco intermediário significativo. Uma aplicação potencial do aprendizado por reforço em veículos autônomos é uma das aplicações mais trabalhadas nos dias de hoje em todo mundo. Um desenvolvedor é incapaz de prever todas as situações futuras da estrada, portanto, deixar o modelo treinar-se com um sistema de penalidades e recompensas em um ambiente variado é possivelmente a maneira mais eficaz da IA ampliar a experiência que possui e coleta e assim aprender a conduzir um veículo autônomo sem que seja explicitamente programada para isso.
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Continuaremos no próximo capítulo!
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="color: #000000;">
   Referências:
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Inteligência Artificial
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Análise Estatística Para Cientistas de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Formação Cientista de Dados
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Customizando Redes Neurais com Funções de Ativação Alternativas
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    What is reinforcement learning? The complete guide
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.geeksforgeeks.org/what-is-reinforcement-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Reinforcement learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Practical Recommendations for Gradient-Based Training of Deep Architectures
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient-Based Learning Applied to Document Recognition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Neural Networks &amp; The Backpropagation Algorithm, Explained
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Recurrent neural network based language model
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Gradient Descent For Machine Learning
   </a>
  </span>
 </p>
 <p style="text-align: justify;">
  <span style="text-decoration: underline; color: #000000;">
   <a href="https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning" rel="noopener noreferrer" style="color: #000000; text-decoration: underline;" target="_blank">
    Pattern Recognition and Machine Learning
   </a>
  </span>
 </p>
 <div class="sharedaddy sd-sharing-enabled">
  <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
   <h3 class="sd-title">
    Compartilhe isso:
   </h3>
   <div class="sd-content">
    <ul>
     <li class="share-twitter">
      <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-1666" href="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Twitter">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Twitter(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-facebook">
      <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-1666" href="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Facebook">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Facebook(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-linkedin">
      <a class="share-linkedin sd-button share-icon no-text" data-shared="sharing-linkedin-1666" href="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no LinkedIn">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no LinkedIn(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-pinterest">
      <a class="share-pinterest sd-button share-icon no-text" data-shared="sharing-pinterest-1666" href="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=pinterest" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Pinterest">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Pinterest(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-tumblr">
      <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no Tumblr">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no Tumblr(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-jetpack-whatsapp">
      <a class="share-jetpack-whatsapp sd-button share-icon no-text" data-shared="" href="http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=jetpack-whatsapp" rel="nofollow noopener noreferrer" target="_blank" title="Clique para compartilhar no WhatsApp">
       <span>
       </span>
       <span class="sharing-screen-reader-text">
        Clique para compartilhar no WhatsApp(abre em nova janela)
       </span>
      </a>
     </li>
     <li class="share-end">
     </li>
    </ul>
   </div>
  </div>
 </div>
 <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-140353593-1666-5e0dd22bb6e3e" data-src="https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1666&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1666-5e0dd22bb6e3e" id="like-post-wrapper-140353593-1666-5e0dd22bb6e3e">
  <h3 class="sd-title">
   Curtir isso:
  </h3>
  <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
   <span class="button">
    <span>
     Curtir
    </span>
   </span>
   <span class="loading">
    Carregando...
   </span>
  </div>
  <span class="sd-text-color">
  </span>
  <a class="sd-link-color">
  </a>
 </div>
 <div class="jp-relatedposts" id="jp-relatedposts">
  <h3 class="jp-relatedposts-headline">
   <em>
    Relacionado
   </em>
  </h3>
 </div>
</div>
<br/><hr><br/></body></html>