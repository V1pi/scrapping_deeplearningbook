{"1": "<h1 class=\"entry-title\" id=\"capitulo-1\">\n Cap\u00c3\u00adtulo 1 \u2013 Deep Learning e a Tempestade Perfeita\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  O interesse pela Aprendizagem de M\u00c3\u00a1quina (Machine Learning) explodiu na \u00c3\u00baltima d\u00c3\u00a9cada. O mundo a nossa volta est\u00c3\u00a1 passando por uma transforma\u00c3\u00a7\u00c3\u00a3o e vemos uma intera\u00c3\u00a7\u00c3\u00a3o cada vez maior das aplica\u00c3\u00a7\u00c3\u00b5es de computador com os seres humanos. Softwares de detec\u00c3\u00a7\u00c3\u00a3o de spam, sistemas de recomenda\u00c3\u00a7\u00c3\u00a3o, marca\u00c3\u00a7\u00c3\u00a3o em fotos de redes sociais, assistentes pessoais ativados por voz, carros aut\u00c3\u00b4nomos, smartphones com reconhecimento facial e muito mais.\n </p>\n <p style=\"text-align: justify;\">\n  E o interesse por Machine Learning se mostra ainda mais evidente pelo n\u00c3\u00bamero cada vez maior de confer\u00c3\u00aancias, meetups, artigos, livros, cursos, buscas no Google e profissionais e empresas procurando compreender o que \u00c3\u00a9 e como usar aprendizagem de m\u00c3\u00a1quina, embora muitos ainda confundem o que podem fazer com o que desejam fazer. N\u00c3\u00a3o h\u00c3\u00a1 como ficar indiferente a esta revolu\u00c3\u00a7\u00c3\u00a3o trazida pela aprendizagem de m\u00c3\u00a1quina e, segundo o Gartner, at\u00c3\u00a9 2020 todos os softwares corporativos ter\u00c3\u00a3o alguma funcionalidade ligada a Machine Learning.\n </p>\n <p style=\"text-align: justify;\">\n  Fundamentalmente, Machine Learning \u00c3\u00a9 a utiliza\u00c3\u00a7\u00c3\u00a3o de algoritmos para extrair informa\u00c3\u00a7\u00c3\u00b5es de dados brutos e represent\u00c3\u00a1-los atrav\u00c3\u00a9s de algum tipo de modelo matem\u00c3\u00a1tico. Usamos ent\u00c3\u00a3o este modelo para fazer infer\u00c3\u00aancias a partir de outros conjuntos de dados. Existem muitos algoritmos que permitem fazer isso, mas um tipo em especial vem se destacando, as redes neurais artificiais.\n </p>\n <p style=\"text-align: justify;\">\n  As redes neurais artificiais n\u00c3\u00a3o s\u00c3\u00a3o necessariamente novas, existem pelo menos desde a d\u00c3\u00a9cada de 1950. Mas durante v\u00c3\u00a1rias d\u00c3\u00a9cadas, embora a arquitetura desses modelos tivesse evolu\u00c3\u00addo, ainda faltavam ingredientes que fizessem os modelos realmente funcionar. E esses ingredientes surgiram quase ao mesmo tempo. Um deles voc\u00c3\u00aa j\u00c3\u00a1 deve ter ouvido: Big Data. O volume de dados, gerado em variedade e velocidade cada vez maiores, permite criar modelos e atingir altos n\u00c3\u00adveis de precis\u00c3\u00a3o. Mas ainda falta um ingrediente. Faltava! Como processar grandes modelos de Machine Learning com grandes quantidades de dados? As CPUs n\u00c3\u00a3o conseguiam dar conta do recado.\n </p>\n <p style=\"text-align: justify;\">\n  Foi quando os gamers e sua avidez por poder computacional e gr\u00c3\u00a1ficos perfeitos, nos ajudaram a encontrar o segundo ingrediente: Programa\u00c3\u00a7\u00c3\u00a3o Paralela em GPUs. As unidades de processamento gr\u00c3\u00a1fico, que permitem realizar opera\u00c3\u00a7\u00c3\u00b5es matem\u00c3\u00a1ticas de forma paralela, principalmente opera\u00c3\u00a7\u00c3\u00b5es com matrizes e vetores, elementos presentes em modelos de redes neurais artificias, formaram a tempestade perfeita, que permitiu a evolu\u00c3\u00a7\u00c3\u00a3o na qual nos encontramos hoje: Big Data + Processamento Paralelo + Modelos de Aprendizagem de M\u00c3\u00a1quina = Intelig\u00c3\u00aancia Artificial.\n </p>\n <p style=\"text-align: justify;\">\n  A unidade fundamental de uma rede neural artificial \u00c3\u00a9 um n\u00c3\u00b3 (ou neur\u00c3\u00b4nio matem\u00c3\u00a1tico), que por sua vez \u00c3\u00a9 baseado no neur\u00c3\u00b4nio biol\u00c3\u00b3gico. As conex\u00c3\u00b5es entre esses neur\u00c3\u00b4nios matem\u00c3\u00a1ticos tamb\u00c3\u00a9m foram inspiradas em c\u00c3\u00a9rebros biol\u00c3\u00b3gicos, especialmente na forma como essas conex\u00c3\u00b5es se desenvolvem ao longo do tempo com \u201ctreinamento\u201d. Em meados da d\u00c3\u00a9cada de 1980 e in\u00c3\u00adcio da d\u00c3\u00a9cada de 1990, muitos avan\u00c3\u00a7os importantes na arquitetura das redes neurais artificias ocorreram. No entanto, a quantidade de tempo e dados necess\u00c3\u00a1rios para obter bons resultados retardou a ado\u00c3\u00a7\u00c3\u00a3o e, portanto, o interesse foi arrefecido, com o que ficou conhecimento como AI Winter (Inverno da IA).\n </p>\n <p style=\"text-align: justify;\">\n  No in\u00c3\u00adcio dos anos 2000, o poder computacional expandiu exponencialmente e o mercado viu uma \u201cexplos\u00c3\u00a3o\u201d de t\u00c3\u00a9cnicas computacionais que n\u00c3\u00a3o eram poss\u00c3\u00adveis antes disso. Foi quando o aprendizado profundo (Deep Learning) emergiu do crescimento computacional explosivo dessa d\u00c3\u00a9cada como o principal mecanismo de constru\u00c3\u00a7\u00c3\u00a3o de sistemas de Intelig\u00c3\u00aancia Artificial, ganhando muitas competi\u00c3\u00a7\u00c3\u00b5es importantes de aprendizagem de m\u00c3\u00a1quina. O interesse por Deep Learning n\u00c3\u00a3o para de crescer e hoje vemos o termo aprendizado profundo sendo mencionado com frequ\u00c3\u00aancia cada vez maior e solu\u00c3\u00a7\u00c3\u00b5es comerciais surgindo a todo momento.\n </p>\n <p style=\"text-align: justify;\">\n  Este livro online, gratuito e em portugu\u00c3\u00aas, \u00c3\u00a9 uma iniciativa da\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br\" rel=\"noopener\" target=\"_blank\">\n    Data Science Academy\n   </a>\n  </span>\n  para ajudar aqueles que buscam conhecimento avan\u00c3\u00a7ado e de qualidade em nosso idioma. Ser\u00c3\u00a3o mais de 50 cap\u00c3\u00adtulos, publicados no formato de posts e lan\u00c3\u00a7ados semanalmente. Desta forma, esperamos contribuir para o crescimento do Deep Learning e Intelig\u00c3\u00aancia Artificial no Brasil.\n </p>\n <p>\n  Nos acompanhe nesta incr\u00c3\u00advel jornada!\n </p>\n <p>\n  Equipe DSA\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br\" rel=\"noopener\" target=\"_blank\">\n    www.datascienceacademy.com.br\n   </a>\n  </span>\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-25\" href=\"http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-25\" href=\"http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-25\" href=\"http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-25\" href=\"http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-25-5e0b6a00f2562\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=25&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-25-5e0b6a00f2562\" id=\"like-post-wrapper-140353593-25-5e0b6a00f2562\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "2": "<h1 class=\"entry-title\" id=\"capitulo-2\">\n Cap\u00edtulo 2 \u2013 Uma Breve Hist\u00f3ria das Redes Neurais Artificiais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Para compreender onde estamos hoje, precisamos olhar para o passado e analisar como chegamos at\u00e9 aqui. Vejamos ent\u00e3o Uma Breve Hist\u00f3ria das Redes Neurais Artificiais.\n </p>\n <p style=\"text-align: justify;\">\n  O c\u00e9rebro humano \u00e9 uma m\u00e1quina altamente poderosa e complexa capaz de processar uma grande quantidade de informa\u00e7\u00f5es em tempo m\u00ednimo. As unidades principais do c\u00e9rebro s\u00e3o os neur\u00f4nios e \u00e9 por meio deles que as informa\u00e7\u00f5es s\u00e3o transmitidas e processadas. As tarefas realizadas pelo c\u00e9rebro intrigam os pesquisadores, como por exemplo, a capacidade do c\u00e9rebro de reconhecer um rosto familiar dentre uma multid\u00e3o em apenas mil\u00e9simos de segundo. As respostas sobre alguns enigmas do funcionamento do c\u00e9rebro ainda n\u00e3o foram respondidas e se perpetuam ate os dias de hoje. O que \u00e9 conhecido sobre o funcionamento do c\u00e9rebro \u00e9 que o mesmo desenvolve suas regras atrav\u00e9s da experi\u00eancia adquirida em situa\u00e7\u00f5es vividas anteriormente.\n </p>\n <p>\n  <img alt=\"C\u00e9rebro Humano\" class=\"aligncenter size-medium wp-image-74\" data-attachment-id=\"74\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"1\"}' data-image-title=\"C\u00e9rebro Humano\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/ce\u0301rebro-humano.jpg?fit=660%2C528\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/ce\u0301rebro-humano.jpg?fit=300%2C240\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/ce\u0301rebro-humano.jpg?fit=660%2C528\" data-orig-size=\"660,528\" data-permalink=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/cerebro-humano/\" data-recalc-dims=\"1\" height=\"240\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/ce\u0301rebro-humano.jpg?resize=300%2C240\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/ce\u0301rebro-humano.jpg?resize=300%2C240 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/ce\u0301rebro-humano.jpg?resize=200%2C160 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/ce\u0301rebro-humano.jpg?w=660 660w\" width=\"300\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig1 \u2013 C\u00e9rebro humano, a m\u00e1quina mais fant\u00e1stica que existe no Planeta Terra.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O desenvolvimento do c\u00e9rebro humano ocorre principalmente nos dois primeiros anos de vida, mas se arrasta por toda a vida. Inspirando-se neste modelo, diversos pesquisadores tentaram simular o funcionamento do c\u00e9rebro, principalmente o processo de aprendizagem por experi\u00eancia, a fim de criar sistemas inteligentes capazes de realizar tarefas como classifica\u00e7\u00e3o, reconhecimento de padr\u00f5es, processamento de imagens, entre outras atividades. Como resultado destas pesquisas surgiu o modelo do neur\u00f4nio artificial e posteriormente um sistema com v\u00e1rios neur\u00f4nios interconectados, a chamada Rede Neural.\n </p>\n <p style=\"text-align: justify;\">\n  Em 1943, o neurofisiologista Warren McCulloch e o matem\u00e1tico Walter Pitts escreveram um artigo sobre como os neur\u00f4nios poderiam funcionar e para isso, eles modelaram uma rede neural simples usando circuitos el\u00e9tricos.\n </p>\n <p style=\"text-align: justify;\">\n  Warren McCulloch e Walter Pitts criaram um modelo computacional para redes neurais baseadas em matem\u00e1tica e algoritmos denominados l\u00f3gica de limiar (threshold logic). Este modelo abriu o caminho para a pesquisa da rede neural dividida em duas abordagens: uma abordagem focada em processos biol\u00f3gicos no c\u00e9rebro, enquanto a outra focada na aplica\u00e7\u00e3o de redes neurais \u00e0 intelig\u00eancia artificial.\n </p>\n <p style=\"text-align: justify;\">\n  Em 1949, Donald Hebb escreveu\n  <em>\n   The Organization of Behavior\n  </em>\n  , uma obra que apontou o fato de que os caminhos neurais s\u00e3o fortalecidos cada vez que s\u00e3o usados, um conceito fundamentalmente essencial para a maneira como os humanos aprendem. Se dois nervos dispararem ao mesmo tempo, argumentou, a conex\u00e3o entre eles \u00e9 melhorada.\n </p>\n <p style=\"text-align: justify;\">\n  \u00c0 medida que os computadores se tornaram mais avan\u00e7ados na d\u00e9cada de 1950, finalmente foi poss\u00edvel simular uma hipot\u00e9tica rede neural. O primeiro passo para isso foi feito por Nathanial Rochester dos laborat\u00f3rios de pesquisa da IBM. Infelizmente para ele, a primeira tentativa de faz\u00ea-lo falhou.\n </p>\n <p style=\"text-align: justify;\">\n  No entanto, ao longo deste tempo, os defensores das \u201cm\u00e1quinas pensantes\u201d continuaram a argumentar suas pesquisas. Em 1956, o Projeto de Pesquisa de Ver\u00e3o de Dartmouth sobre Intelig\u00eancia Artificial proporcionou um impulso tanto \u00e0 Intelig\u00eancia Artificial como \u00e0s Redes Neurais. Um dos resultados deste processo foi estimular a pesquisa em IA na parte de processamento neural.\n </p>\n <p style=\"text-align: justify;\">\n  Nos anos seguintes ao Projeto Dartmouth, John von Neumann sugeriu imitar fun\u00e7\u00f5es simples de neur\u00f4nios usando rel\u00e9s telegr\u00e1ficos ou tubos de v\u00e1cuo. Al\u00e9m disso, Frank Rosenblatt, um neurobiologista, come\u00e7ou a trabalhar no Perceptron. Ele estava intrigado com o funcionamento do olho de uma mosca. Grande parte do processamento feito por uma mosca ao decidir fugir, \u00e9 feito em seus olhos. O Perceptron, que resultou dessa pesquisa, foi constru\u00eddo em hardware e \u00e9 a mais antiga rede neural ainda em uso hoje. Um Percetron de camada \u00fanica foi \u00fatil para classificar um conjunto de entradas de valor cont\u00ednuo em uma de duas classes. O Perceptron calcula uma soma ponderada das entradas, subtrai um limite e passa um dos dois valores poss\u00edveis como resultado. Infelizmente, o Perceptron \u00e9 limitado e foi comprovado como tal durante os \u201canos desiludidos\u201d por Marvin Minsky e o livro de Seymour Papert de 1969, Perceptrons.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Redes Neurais\" class=\"aligncenter size-full wp-image-78\" data-attachment-id=\"78\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Redes Neurais\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?fit=967%2C617\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?fit=300%2C191\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?fit=967%2C617\" data-orig-size=\"967,617\" data-permalink=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/neuralnetworks/\" data-recalc-dims=\"1\" height=\"617\" sizes=\"(max-width: 967px) 100vw, 967px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=967%2C617\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?w=967 967w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=300%2C191 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=768%2C490 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=200%2C128 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks.png?resize=690%2C440 690w\" width=\"967\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig2 \u2013 Algumas Arquiteturas de Redes Neurais\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Em 1959, Bernard Widrow e Marcian Hoff, de Stanford, desenvolveram modelos denominados \u201cADALINE\u201d e \u201cMADALINE\u201d. Em uma exibi\u00e7\u00e3o t\u00edpica do amor de Stanford por siglas, os nomes prov\u00eam do uso de m\u00faltiplos elementos ADAptive LINear. ADALINE foi desenvolvido para reconhecer padr\u00f5es bin\u00e1rios de modo que, se ele estivesse lendo bits de transmiss\u00e3o de uma linha telef\u00f4nica, poderia prever o pr\u00f3ximo bit. MADALINE foi a primeira rede neural aplicada a um problema do mundo real, usando um filtro adaptativo que elimina ecos nas linhas telef\u00f4nicas. Embora o sistema seja t\u00e3o antigo como os sistemas de controle de tr\u00e1fego a\u00e9reo, ele ainda est\u00e1 em uso comercial.\n </p>\n <p style=\"text-align: justify;\">\n  Infelizmente, esses sucessos anteriores levaram as pessoas a exagerar o potencial das redes neurais, particularmente \u00e0 luz da limita\u00e7\u00e3o na eletr\u00f4nica, ent\u00e3o dispon\u00edvel na \u00e9poca. Este exagero excessivo, que decorreu do mundo acad\u00eamico e t\u00e9cnico, infectou a literatura geral da \u00e9poca. Muitas promessas foram feitas, mas o resultado foi o desapontamento. Al\u00e9m disso, muitos escritores come\u00e7aram a refletir sobre o efeito que teria \u201cm\u00e1quinas pensantes\u201d no homem. A s\u00e9rie de Asimov em rob\u00f4s revelou os efeitos sobre a moral e os valores do homem quando m\u00e1quinas fossem capazes de fazer todo o trabalho da humanidade. Outros escritores criaram computadores mais sinistros, como HAL do filme 2001.\n </p>\n <p style=\"text-align: justify;\">\n  Toda essa discuss\u00e3o sobre o efeito da Intelig\u00eancia Artificial sobre a vida humana, aliada aos poucos progressos, fizeram vozes respeitadas criticar a pesquisa em redes neurais. O resultado foi a redu\u00e7\u00e3o dr\u00e1stica de grande parte do financiamento em pesquisas. Esse per\u00edodo de crescimento atrofiado durou at\u00e9 1981, sendo conhecido como o Inverno da IA (AI Winter).\n </p>\n <p style=\"text-align: justify;\">\n  Em 1982, v\u00e1rios eventos provocaram um renovado interesse. John Hopfield da Caltech apresentou um documento \u00e0 Academia Nacional de Ci\u00eancias. A abordagem de Hopfield n\u00e3o era simplesmente modelar c\u00e9rebros, mas criar dispositivos \u00fateis. Com clareza e an\u00e1lise matem\u00e1tica, ele mostrou como essas redes poderiam funcionar e o que poderiam fazer. No entanto, o maior recurso de Hopfield foi seu carisma. Ele era articulado e simp\u00e1tico e isso colaborou bastante para que ele fosse ouvido.\n </p>\n <p style=\"text-align: justify;\">\n  Em 1985, o Instituto Americano de F\u00edsica come\u00e7ou o que se tornou uma reuni\u00e3o anual \u2013 Redes Neurais para Computa\u00e7\u00e3o. Em 1987, a primeira Confer\u00eancia Internacional sobre Redes Neurais do Institute of Electrical and Electronic Engineer\u2019s (IEEE) atraiu mais de 1.800 participantes.\n </p>\n <p style=\"text-align: justify;\">\n  Em 1986, com redes neurais de v\u00e1rias camadas nas not\u00edcias, o problema era como estender a regra Widrow-Hoff para v\u00e1rias camadas. Tr\u00eas grupos independentes de pesquisadores, dentre os quais David Rumelhart, ex-membro do departamento de psicologia de Stanford, apresentaram ideias semelhantes que agora s\u00e3o chamadas de redes Backpropagation porque distribuem erros de reconhecimento de padr\u00f5es em toda a rede. As redes h\u00edbridas utilizavam apenas duas camadas, essas redes de Backpropagation utilizam muitas. O resultado \u00e9 que as redes de Backpropagation \u201caprendem\u201d de forma mais lenta, pois necessitam, possivelmente, de milhares de itera\u00e7\u00f5es para aprender, mas geram um resultado muito preciso.\n </p>\n <p style=\"text-align: justify;\">\n  Agora, as redes neurais s\u00e3o usadas em v\u00e1rias aplica\u00e7\u00f5es. A ideia fundamental por tr\u00e1s da natureza das redes neurais \u00e9 que, se ela funcionar na natureza, deve ser capaz de funcionar em computadores. O futuro das redes neurais, no entanto, reside no desenvolvimento de hardware. As redes neurais r\u00e1pidas e eficientes dependem do hardware especificado para seu eventual uso.\n </p>\n <p style=\"text-align: justify;\">\n  O diagrama abaixo mostra alguns marcos importantes na evolu\u00e7\u00e3o e pesquisa das redes neurais artificiais. O fato, \u00e9 que ainda estamos escrevendo esta hist\u00f3ria e muita evolu\u00e7\u00e3o est\u00e1 ocorrendo neste momento, atrav\u00e9s do trabalho de milhares de pesquisadores e profissionais de Intelig\u00eancia Artificial em todo mundo. E voc\u00ea, n\u00e3o quer ajudar a escrever esta hist\u00f3ria?\n </p>\n <p>\n  <img alt=\"Timeline das Redes Neurais\" class=\"aligncenter wp-image-76 size-full\" data-attachment-id=\"76\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Timeline das Redes Neurais\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?fit=1024%2C481\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?fit=300%2C141\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?fit=2133%2C1002\" data-orig-size=\"2133,1002\" data-permalink=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/nn_timeline/\" data-recalc-dims=\"1\" height=\"550\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=1170%2C550\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?w=2133 2133w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=300%2C141 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=768%2C361 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=1024%2C481 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=200%2C94 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/nn_timeline.jpg?resize=690%2C324 690w\" width=\"1170\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig3 \u2013 Marcos no desenvolvimento das redes neurais.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Podemos resumir assim os principais marcos na pesquisa e evolu\u00e7\u00e3o das redes neurais artificiais at\u00e9 chegarmos ao Deep Learning:\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   1943\n  </strong>\n  : Warren McCulloch e Walter Pitts criam um modelo computacional para redes neurais baseadas em matem\u00e1tica e algoritmos denominados l\u00f3gica de limiar.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   1958\n  </strong>\n  : Frank Rosenblatt cria o Perceptron, um algoritmo para o reconhecimento de padr\u00f5es baseado em uma rede neural computacional de duas camadas usando simples adi\u00e7\u00e3o e subtra\u00e7\u00e3o. Ele tamb\u00e9m prop\u00f4s camadas adicionais com nota\u00e7\u00f5es matem\u00e1ticas, mas isso n\u00e3o seria realizado at\u00e9 1975.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   1980\n  </strong>\n  : Kunihiko Fukushima prop\u00f5e a Neoconitron, uma rede neural de hierarquia, multicamada, que foi utilizada para o reconhecimento de caligrafia e outros problemas de reconhecimento de padr\u00f5es.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   1989\n  </strong>\n  : os cientistas conseguiram criar algoritmos que usavam redes neurais profundas, mas os tempos de treinamento para os sistemas foram medidos em dias, tornando-os impratic\u00e1veis \u200b\u200bpara o uso no mundo real.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   1992\n  </strong>\n  : Juyang Weng publica o Cresceptron, um m\u00e9todo para realizar o reconhecimento de objetos 3-D automaticamente a partir de cenas desordenadas.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   Meados dos anos 2000\n  </strong>\n  : o termo \u201caprendizagem profunda\u201d come\u00e7a a ganhar popularidade ap\u00f3s um artigo de Geoffrey Hinton e Ruslan Salakhutdinov mostrar como uma rede neural de v\u00e1rias camadas poderia ser pr\u00e9-treinada uma camada por vez.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   2009\n  </strong>\n  : acontece o NIPS Workshop sobre Aprendizagem Profunda para Reconhecimento de Voz e descobre-se que com um conjunto de dados suficientemente grande, as redes neurais n\u00e3o precisam de pr\u00e9-treinamento e as taxas de erro caem significativamente.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   2012\n  </strong>\n  : algoritmos de reconhecimento de padr\u00f5es artificiais alcan\u00e7am desempenho em n\u00edvel humano em determinadas tarefas. E o algoritmo de aprendizagem profunda do Google \u00e9 capaz de identificar gatos.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   2014\n  </strong>\n  : o Google compra a Startup de Intelig\u00eancia Artificial chamada DeepMind, do Reino Unido, por \u00a3 400m\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   2015\n  </strong>\n  : Facebook coloca a tecnologia de aprendizado profundo \u2013 chamada DeepFace \u2013 em opera\u00e7\u00e3o para marcar e identificar automaticamente usu\u00e1rios do Facebook em fotografias. Algoritmos executam tarefas superiores de reconhecimento facial usando redes profundas que levam em conta 120 milh\u00f5es de par\u00e2metros.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   2016\n  </strong>\n  : o algoritmo do Google DeepMind, AlphaGo, mapeia a arte do complexo jogo de tabuleiro Go e vence o campe\u00e3o mundial de Go, Lee Sedol, em um torneio altamente divulgado em Seul.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   2017\n  </strong>\n  : ado\u00e7\u00e3o em massa do Deep Learning em diversas aplica\u00e7\u00f5es corporativas e mobile, al\u00e9m do avan\u00e7o em pesquisas. Todos os eventos de tecnologia ligados a Data Science, IA e Big Data, apontam Deep Learning como a principal tecnologia para cria\u00e7\u00e3o de sistemas inteligentes.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A promessa do aprendizado profundo n\u00e3o \u00e9 que os computadores comecem a pensar como seres humanos. Isso \u00e9 como pedir uma ma\u00e7\u00e3 para se tornar uma laranja. Em vez disso, demonstra que, dado um conjunto de dados suficientemente grande, processadores r\u00e1pidos e um algoritmo suficientemente sofisticado, os computadores podem come\u00e7ar a realizar tarefas que at\u00e9 ent\u00e3o s\u00f3 podiam ser realizadas apenas por seres humanos, como reconhecer imagens e voz, criar obras de arte ou tomar decis\u00f5es por si mesmo.\n </p>\n <p style=\"text-align: justify;\">\n  Os estudos sobre as redes neurais sofreram uma grande revolu\u00e7\u00e3o a partir dos anos 80 e esta \u00e1rea de estudos tem se destacado, seja pelas promissoras caracter\u00edsticas apresentadas pelos modelos de redes neurais propostos, seja pelas condi\u00e7\u00f5es tecnol\u00f3gicas atuais de implementa\u00e7\u00e3o que permitem desenvolver arrojadas implementa\u00e7\u00f5es de arquiteturas neurais paralelas em hardwares dedicado, obtendo assim \u00f3timas performances destes sistemas (bastante superiores aos sistemas convencionais). A evolu\u00e7\u00e3o natural das redes neurais, s\u00e3o as redes neurais profundas (ou Deep Learning). Mas isso \u00e9 o que vamos discutir no pr\u00f3ximo cap\u00edtulo! At\u00e9 l\u00e1.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  Christopher D. Manning. (2015). Computational Linguistics and Deep Learning Computational Linguistics, 41(4), 701\u2013707.\n </p>\n <p>\n  F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.\n </p>\n <p>\n  W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115\u2013133, 1943.\n </p>\n <p>\n  The organization of behavior: A neuropsychological theory. D. O. Hebb. John Wiley And Sons, Inc., New York, 1949\n </p>\n <p>\n  B. Widrow et al. Adaptive \u201dAdaline\u201d neuron using chemical \u201dmemistors\u201d. Number Technical Report 1553-2. Stanford Electron. Labs., Stanford, CA, October 1960.\n </p>\n <p>\n  \u201cNew Navy Device Learns By Doing\u201d, New York Times, July 8, 1958.\n </p>\n <p>\n  Perceptrons. An Introduction to Computational Geometry. MARVIN MINSKY and SEYMOUR PAPERT. M.I.T. Press, Cambridge, Mass., 1969.\n </p>\n <p>\n  Minsky, M. (1952). A neural-analogue calculator based upon a probability model of reinforcement. Harvard University Pychological Laboratories internal report.\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-58\" href=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-58\" href=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-58\" href=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-58\" href=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-58-5e0dd03f69491\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=58&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-58-5e0dd03f69491\" id=\"like-post-wrapper-140353593-58-5e0dd03f69491\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "3": "<h1 class=\"entry-title\" id=\"capitulo-3\">\n Cap\u00edtulo 3 \u2013 O Que S\u00e3o Redes Neurais Artificiais Profundas ou Deep Learning?\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Aprendizagem Profunda ou Deep Learning, \u00e9 uma sub-\u00e1rea da Aprendizagem de M\u00e1quina, que emprega algoritmos para processar dados e imitar o processamento feito pelo c\u00e9rebro humano. Mas O Que S\u00e3o Redes Neurais Artificiais Profundas ou Deep Learning? \u00c9 o que veremos neste cap\u00edtulo. N\u00e3o se preocupe se alguns termos mais t\u00e9cnicos n\u00e3o fizerem sentido agora. Todos eles ser\u00e3o estudados ao longo deste livro online.\n </p>\n <p style=\"text-align: justify;\">\n  Deep Learning\u00a0 usa camadas de neur\u00f4nios matem\u00e1ticos para processar dados, compreender a fala humana e reconhecer objetos visualmente. A informa\u00e7\u00e3o \u00e9 passada atrav\u00e9s de cada camada, com a sa\u00edda da camada anterior fornecendo entrada para a pr\u00f3xima camada. A primeira camada em uma rede \u00e9 chamada de camada de entrada, enquanto a \u00faltima \u00e9 chamada de camada de sa\u00edda. Todas as camadas entre as duas s\u00e3o referidas como camadas ocultas. Cada camada \u00e9 tipicamente um algoritmo simples e uniforme contendo um tipo de fun\u00e7\u00e3o de ativa\u00e7\u00e3o.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Neural Network\" class=\"aligncenter size-full wp-image-87\" data-attachment-id=\"87\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Neural Network\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?fit=690%2C259\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?fit=300%2C113\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?fit=690%2C259\" data-orig-size=\"690,259\" data-permalink=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/neural/\" data-recalc-dims=\"1\" height=\"259\" sizes=\"(max-width: 690px) 100vw, 690px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?resize=690%2C259\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?w=690 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?resize=300%2C113 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neural.png?resize=200%2C75 200w\" width=\"690\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig4 \u2013 Rede Neural Simples e Rede Neural Profunda (Deep Learning)\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A aprendizagem profunda \u00e9 respons\u00e1vel por avan\u00e7os recentes em vis\u00e3o computacional, reconhecimento de fala, processamento de linguagem natural e reconhecimento de \u00e1udio. O aprendizado profundo \u00e9 baseado no conceito de redes neurais artificiais, ou sistemas computacionais que imitam a maneira como o c\u00e9rebro humano funciona.\n </p>\n <p style=\"text-align: justify;\">\n  A extra\u00e7\u00e3o de recursos \u00e9 outro aspecto da Aprendizagem Profunda. A extra\u00e7\u00e3o de recursos usa um algoritmo para construir automaticamente \u201crecursos\u201d significativos dos dados para fins de treinamento, aprendizado e compreens\u00e3o. Normalmente, o Cientista de Dados, ou Engenheiro de IA, \u00e9 respons\u00e1vel pela extra\u00e7\u00e3o de recursos.\n </p>\n <p style=\"text-align: justify;\">\n  O aumento r\u00e1pido e o aparente dom\u00ednio do aprendizado profundo sobre os m\u00e9todos tradicionais de aprendizagem de m\u00e1quina em uma variedade de tarefas tem sido surpreendente de testemunhar e, \u00e0s vezes, dif\u00edcil de explicar. Deep Learning \u00e9 uma evolu\u00e7\u00e3o das Redes Neurais, que por sua vez possuem uma hist\u00f3ria fascinante que remonta \u00e0 d\u00e9cada de 1940, cheia de altos e baixos, voltas e reviravoltas, amigos e rivais, sucessos e fracassos. Em uma hist\u00f3ria digna de um filme dos anos 90, uma ideia que j\u00e1 foi uma esp\u00e9cie de patinho feio floresceu para se tornar a bola da vez.\n </p>\n <p style=\"text-align: justify;\">\n  Consequentemente, o interesse em aprendizagem profunda tem disparado, com cobertura constante na m\u00eddia popular. A pesquisa de aprendizagem profunda agora aparece rotineiramente em revistas como Science, Nature, Nature Methods e Forbes apenas para citar alguns. O aprendizado profundo conquistou Go, aprendeu a dirigir um carro, diagnosticou c\u00e2ncer de pele e autismo, tornou-se um falsificador de arte mestre e pode at\u00e9 alucinar imagens fotorrealistas.\n </p>\n <p style=\"text-align: justify;\">\n  Os primeiros algoritmos de aprendizagem profunda que possu\u00edam m\u00faltiplas camadas de caracter\u00edsticas n\u00e3o-lineares podem ser rastreados at\u00e9 Alexey Grigoryevich Ivakhnenko (desenvolveu o M\u00e9todo do Grupo de Manipula\u00e7\u00e3o de Dados) e Valentin Grigor\u2019evich Lapa (autor de Cybernetics and Forecasting Techniques) em 1965 (Figura 5), que usaram modelos finos mas profundos com fun\u00e7\u00f5es de ativa\u00e7\u00e3o polinomial os quais eles analisaram com m\u00e9todos estat\u00edsticos. Em cada camada, eles selecionavam os melhores recursos atrav\u00e9s de m\u00e9todos estat\u00edsticos e encaminhavam para a pr\u00f3xima camada. Eles n\u00e3o usaram Backpropagation para treinar a rede de ponta a ponta, mas utilizaram m\u00ednimos quadrados camada-por-camada, onde as camadas anteriores foram independentemente instaladas em camadas posteriores (um processo lento e manual).\n </p>\n <p>\n  <img alt=\"GMDH-network\" class=\"aligncenter size-full wp-image-86\" data-attachment-id=\"86\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"GMDH-network\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?fit=662%2C501\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?fit=300%2C227\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?fit=662%2C501\" data-orig-size=\"662,501\" data-permalink=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/gmdh-network/\" data-recalc-dims=\"1\" height=\"501\" sizes=\"(max-width: 662px) 100vw, 662px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?resize=662%2C501\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?w=662 662w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?resize=300%2C227 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/GMDH-network.png?resize=200%2C151 200w\" width=\"662\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig5 \u2013 Arquitetura da primeira rede profunda conhecida treinada por Alexey Grigorevich\u00a0Ivakhnenko em 1965.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  No final da d\u00e9cada de 1970, o primeiro inverno de AI come\u00e7ou, resultado de promessas que n\u00e3o poderiam ser mantidas. O impacto desta falta de financiamento limitou a pesquisa em Redes Neurais Profundas e Intelig\u00eancia Artificial. Felizmente, houve indiv\u00edduos que realizaram a pesquisa sem financiamento.\n </p>\n <p style=\"text-align: justify;\">\n  As primeiras \u201credes neurais convolutivas\u201d foram usadas por Kunihiko Fukushima. Fukushima concebeu redes neurais com m\u00faltiplas camadas de agrupamento e convolu\u00e7\u00f5es. Em 1979, ele desenvolveu uma rede neural artificial, chamada Neocognitron, que usava um design hier\u00e1rquico e multicamadas. Este design permitiu ao computador \u201caprender\u201d a reconhecer padr\u00f5es visuais. As redes se assemelhavam a vers\u00f5es modernas, mas foram treinadas com uma estrat\u00e9gia de refor\u00e7o de ativa\u00e7\u00e3o recorrente em m\u00faltiplas camadas, que ganhou for\u00e7a ao longo do tempo. Al\u00e9m disso, o design de Fukushima permitiu que os recursos importantes fossem ajustados manualmente aumentando o \u201cpeso\u201d de certas conex\u00f5es.\n </p>\n <p style=\"text-align: justify;\">\n  Muitos dos conceitos de Neocognitron continuam a ser utilizados. O uso de conex\u00f5es de cima para baixo e novos m\u00e9todos de aprendizagem permitiram a realiza\u00e7\u00e3o de uma variedade de redes neurais. Quando mais de um padr\u00e3o \u00e9 apresentado ao mesmo tempo, o Modelo de Aten\u00e7\u00e3o Seletiva pode separar e reconhecer padr\u00f5es individuais deslocando sua aten\u00e7\u00e3o de um para o outro (o mesmo processo que usamos em multitarefa). Um Neocognitron moderno n\u00e3o s\u00f3 pode identificar padr\u00f5es com informa\u00e7\u00f5es faltantes (por exemplo, um n\u00famero 5 desenhado de maneira incompleta), mas tamb\u00e9m pode completar a imagem adicionando as informa\u00e7\u00f5es que faltam. Isso pode ser descrito como \u201cinfer\u00eancia\u201d.\n </p>\n <p style=\"text-align: justify;\">\n  O Backpropagation, o uso de erros no treinamento de modelos de Deep Learning, evoluiu significativamente em 1970. Foi quando Seppo Linnainmaa escreveu sua tese de mestrado, incluindo um c\u00f3digo FORTRAN para Backpropagation. Infelizmente, o conceito n\u00e3o foi aplicado \u00e0s redes neurais at\u00e9 1985. Foi quando Rumelhart, Williams e Hinton demonstraram o Backpropagation em uma rede neural que poderia fornecer representa\u00e7\u00f5es de distribui\u00e7\u00e3o \u201cinteressantes\u201d. Filosoficamente, essa descoberta trouxe \u00e0 luz a quest\u00e3o dentro da psicologia cognitiva de saber se a compreens\u00e3o humana depende da l\u00f3gica simb\u00f3lica (computacionalismo) ou de representa\u00e7\u00f5es distribu\u00eddas (conex\u00e3o). Em 1989, Yann LeCun forneceu a primeira demonstra\u00e7\u00e3o pr\u00e1tica de Backpropagation no Bell Labs. Ele combinou redes neurais convolutivas com Backpropagation para ler os d\u00edgitos \u201cmanuscritos\u201d (assunto do pr\u00f3ximo cap\u00edtulo). Este sistema foi usado para ler o n\u00famero de cheques manuscritos.\n </p>\n <p>\n  <img alt=\"Deep Learning\" class=\"aligncenter size-full wp-image-111\" data-attachment-id=\"111\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Deep Learning\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?fit=1024%2C768\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?fit=300%2C225\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?fit=2048%2C1536\" data-orig-size=\"2048,1536\" data-permalink=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/deeplearningpioneersatnipsconference2014inmontreal/\" data-recalc-dims=\"1\" height=\"878\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=1170%2C878\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?w=2048 2048w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=768%2C576 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=1024%2C768 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=200%2C150 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/deeplearningpioneersatnipsconference2014inmontreal.jpg?resize=690%2C518 690w\" width=\"1170\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig6 \u2013 Os pioneiros da Intelig\u00eancia Artificial. Da esquerda para a direita: Yann LeCun, Geoffrey Hinton,\u00a0Yoshua Bengio e Andrew Ng\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Por\u00e9m, tivemos neste per\u00edodo o que ficou conhecido como segundo Inverno da IA, que ocorreu entre 1985-1990, que tamb\u00e9m afetou pesquisas em Redes Neurais e Aprendizagem Profunda. V\u00e1rios indiv\u00edduos excessivamente otimistas haviam exagerado o potencial \u201cimediato\u201d da Intelig\u00eancia Artificial, quebrando as expectativas e irritando os investidores. A raiva era t\u00e3o intensa, que a frase Intelig\u00eancia Artificial atingiu o status de pseudoci\u00eancia. Felizmente, algumas pessoas continuaram trabalhando em IA e Deep Learning, e alguns avan\u00e7os significativos foram feitos. Em 1995, Dana Cortes e Vladimir Vapnik desenvolveram a m\u00e1quina de vetor de suporte ou Support Vector Machine (um sistema para mapear e reconhecer dados semelhantes). O LSTM (Long-Short Term Memory) para redes neurais recorrentes foi desenvolvido em 1997, por Sepp Hochreiter e Juergen Schmidhuber.\n </p>\n <p style=\"text-align: justify;\">\n  O pr\u00f3ximo passo evolutivo significativo para Deep Learning ocorreu em 1999, quando os computadores come\u00e7aram a se tornar mais r\u00e1pidos no processamento de dados e GPUs (unidades de processamento de gr\u00e1fico) foram desenvolvidas. O uso de GPUs significou um salto no tempo de processamento, resultando em um aumento das velocidades computacionais em 1000 vezes ao longo de um per\u00edodo de 10 anos. Durante esse per\u00edodo, as redes neurais come\u00e7aram a competir com m\u00e1quinas de vetor de suporte. Enquanto uma rede neural poderia ser lenta em compara\u00e7\u00e3o com uma m\u00e1quina de vetor de suporte, as redes neurais ofereciam melhores resultados usando os mesmos dados. As redes neurais tamb\u00e9m t\u00eam a vantagem de continuar a melhorar \u00e0 medida que mais dados de treinamento s\u00e3o adicionados.\n </p>\n <p style=\"text-align: justify;\">\n  Em torno do ano 2000, apareceu o problema conhecido como Vanishing Gradient. Foi descoberto que as \u201ccaracter\u00edsticas\u201d aprendidas em camadas mais baixas n\u00e3o eram aprendidas pelas camadas superiores, pois nenhum sinal de aprendizado alcan\u00e7ou essas camadas. Este n\u00e3o era um problema fundamental para todas as redes neurais, apenas aquelas com m\u00e9todos de aprendizagem baseados em gradientes. A origem do problema acabou por ser certas fun\u00e7\u00f5es de ativa\u00e7\u00e3o. Uma s\u00e9rie de fun\u00e7\u00f5es de ativa\u00e7\u00e3o condensavam sua entrada, reduzindo, por sua vez, a faixa de sa\u00edda de forma um tanto ca\u00f3tica. Isso produziu grandes \u00e1reas de entrada mapeadas em uma faixa extremamente pequena. Nessas \u00e1reas de entrada, uma grande mudan\u00e7a ser\u00e1 reduzida a uma pequena mudan\u00e7a na sa\u00edda, resultando em um gradiente em queda. Duas solu\u00e7\u00f5es utilizadas para resolver este problema foram o pr\u00e9-treino camada-a-camada e o desenvolvimento de uma mem\u00f3ria longa e de curto prazo.\n </p>\n <p style=\"text-align: justify;\">\n  Em 2001, um relat\u00f3rio de pesquisa do Grupo META (agora chamado Gartner) descreveu os desafios e oportunidades no crescimento do volume de dados. O relat\u00f3rio descreveu o aumento do volume de dados e a crescente velocidade de dados como o aumento da gama de fontes e tipos de dados. Este foi um apelo para se preparar para a investida do Big Data, que estava apenas come\u00e7ando.\n </p>\n <p style=\"text-align: justify;\">\n  Em 2009, Fei-Fei Li, professora de IA em Stanford na Calif\u00f3rnia, lan\u00e7ou o ImageNet e montou uma base de dados gratuita de mais de 14 milh\u00f5es de imagens etiquetadas. Eram necess\u00e1rias imagens marcadas para \u201ctreinar\u201d as redes neurais. A professora Li disse: \u201cNossa vis\u00e3o \u00e9 que o Big Data mudar\u00e1 a maneira como a aprendizagem de m\u00e1quina funciona. Data drives learning.\u201d. Ela acertou em cheio!\n </p>\n <p style=\"text-align: justify;\">\n  At\u00e9 2011, a velocidade das GPUs aumentou significativamente, possibilitando a forma\u00e7\u00e3o de redes neurais convolutivas \u201csem\u201d o pr\u00e9-treino camada por camada. Com o aumento da velocidade de computa\u00e7\u00e3o, tornou-se \u00f3bvio que Deep Learning tinha vantagens significativas em termos de efici\u00eancia e velocidade. Um exemplo \u00e9 a AlexNet, uma rede neural convolutiva, cuja arquitetura ganhou v\u00e1rias competi\u00e7\u00f5es internacionais durante 2011 e 2012. As unidades lineares retificadas foram usadas para melhorar a velocidade.\n </p>\n <p style=\"text-align: justify;\">\n  Tamb\u00e9m em 2012, o Google Brain lan\u00e7ou os resultados de um projeto incomum conhecido como The Cat Experiment. O projeto de esp\u00edrito livre explorou as dificuldades de \u201caprendizagem sem supervis\u00e3o\u201d. A Aprendizagem profunda usa \u201caprendizagem supervisionada\u201d, o que significa que a rede neural convolutiva \u00e9 treinada usando dados rotulados. Usando a aprendizagem sem supervis\u00e3o, uma rede neural convolucional \u00e9 alimentada com dados n\u00e3o marcados, e \u00e9 ent\u00e3o solicitada a busca de padr\u00f5es recorrentes.\n </p>\n <p style=\"text-align: justify;\">\n  O Cat Experiment usou uma rede neural distribu\u00edda por mais de 1.000 computadores. Dez milh\u00f5es de imagens \u201csem etiqueta\u201d foram tiradas aleatoriamente do YouTube, mostradas ao sistema e, em seguida, o software de treinamento foi autorizado a ser executado. No final do treinamento, um neur\u00f4nio na camada mais alta foi encontrado para responder fortemente \u00e0s imagens de gatos. Andrew Ng, o fundador do projeto, disse: \u201cN\u00f3s tamb\u00e9m encontramos um neur\u00f4nio que respondeu fortemente aos rostos humanos\u201d. A aprendizagem n\u00e3o supervisionada continua a ser um um campo ativo de pesquisa em Aprendizagem Profunda.\n </p>\n <p style=\"text-align: justify;\">\n  Atualmente, o processamento de Big Data e a evolu\u00e7\u00e3o da Intelig\u00eancia Artificial s\u00e3o ambos dependentes da Aprendizagem Profunda. Com Deep Learning podemos construir sistemas inteligentes e estamos nos aproximando da cria\u00e7\u00e3o de uma IA totalmente aut\u00f4noma. Isso vai gerar impacto em todas os segmentos da sociedade e aqueles que souberem trabalhar com a tecnologia, ser\u00e3o os l\u00edderes desse novo mundo que se apresenta diante de n\u00f3s.\n </p>\n <p style=\"text-align: justify;\">\n  No pr\u00f3ximo cap\u00edtulo voc\u00ea vai come\u00e7ar a compreender tecnicamente como funciona a Aprendizagem Profunda. At\u00e9 o cap\u00edtulo 4.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  Deep Learning in a Nutshell: History and Training from NVIDIA\n </p>\n <p>\n  Linnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master\u2019s thesis, Univ. Helsinki.\n </p>\n <p>\n  P. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University, Cambridge, MA, 1974.\n </p>\n <p>\n  Werbos, P.J. (2006). Backwards differentiation in AD and neural nets: Past links and new opportunities. In Automatic Differentiation: Applications, Theory, and Implementations, pages 15-34. Springer.\n </p>\n <p>\n  Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533\u2013536.\n </p>\n <p>\n  Widrow, B., &amp; Lehr, M. (1990). 30 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proceedings of the IEEE, 78(9), 1415-1442.\n </p>\n <p>\n  D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1, David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group (Eds.). MIT Press, Cambridge, MA, USA 318-362\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-60\" href=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-60\" href=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-60\" href=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-60\" href=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-60-5e0dd041939bb\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=60&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-60-5e0dd041939bb\" id=\"like-post-wrapper-140353593-60-5e0dd041939bb\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "4": "<h1 class=\"entry-title\" id=\"capitulo-4\">\n Cap\u00edtulo 4 \u2013 O Neur\u00f4nio, Biol\u00f3gico e Matem\u00e1tico\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Para compreender a l\u00f3gica de funcionamento das redes neurais, alguns conceitos b\u00e1sicos referentes ao funcionamento do c\u00e9rebro humano e seus componentes, os neur\u00f4nios, s\u00e3o de fundamental import\u00e2ncia. A forma\u00e7\u00e3o das conex\u00f5es entre as c\u00e9lulas e algumas considera\u00e7\u00f5es sobre como se concebe teoricamente o funcionamento matem\u00e1tico, ajudam a entender as bases da aprendizagem de m\u00e1quina e das redes neurais. Vejamos como funciona o neur\u00f4nio biol\u00f3gico deixando Machine Learning de lado por um instante!\n </p>\n <p>\n </p>\n <h2>\n  O Neur\u00f4nio Biol\u00f3gico\n </h2>\n <p style=\"text-align: justify;\">\n  O neur\u00f4nio \u00e9 a unidade b\u00e1sica do c\u00e9rebro humano, sendo uma c\u00e9lula especializada na transmiss\u00e3o de informa\u00e7\u00f5es, pois nelas est\u00e3o introduzidas propriedades de excitabilidade e condu\u00e7\u00e3o de mensagens nervosas. O neur\u00f4nio \u00e9 constitu\u00eddo por 3 partes principais: a soma ou corpo celular, do qual emanam algumas ramifica\u00e7\u00f5es denominadas de dendritos, e por uma outra ramifica\u00e7\u00e3o descendente da soma, por\u00e9m mais extensa, chamada de ax\u00f4nio. Nas extremidades dos ax\u00f4nios est\u00e3o os nervos terminais, pelos quais \u00e9 realizada a transmiss\u00e3o das informa\u00e7\u00f5es para outros neur\u00f4nios. Esta transmiss\u00e3o \u00e9 conhecida como sinapse.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Neur\u00f4nio Biol\u00f3gico\" class=\"aligncenter size-full wp-image-126\" data-attachment-id=\"126\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Neur\u00f4nio Biol\u00f3gico\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?fit=800%2C357\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?fit=300%2C134\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?fit=800%2C357\" data-orig-size=\"800,357\" data-permalink=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/neuronio/\" data-recalc-dims=\"1\" height=\"357\" sizes=\"(max-width: 800px) 100vw, 800px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=800%2C357\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=300%2C134 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=768%2C343 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=200%2C89 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpg?resize=690%2C308 690w\" width=\"800\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig7 \u2013 Representa\u00e7\u00e3o Simplificada do Neur\u00f4nio Biol\u00f3gico\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Nosso c\u00e9rebro \u00e9 formado por bilh\u00f5es de neur\u00f4nios. Mas eles n\u00e3o est\u00e3o isolados. Pelo contr\u00e1rio, existem centenas de bilh\u00f5es de conex\u00f5es entre eles, formando uma enorme rede de comunica\u00e7\u00e3o, a rede neural. Cada neur\u00f4nio possui um corpo central, diversos dendritos e um ax\u00f4nio. Os dendritos recebem sinais el\u00e9tricos de outros neur\u00f4nios atrav\u00e9s das sinapses, que constitui o processo de comunica\u00e7\u00e3o entre neur\u00f4nios. O corpo celular processa a informa\u00e7\u00e3o e envia para outro neur\u00f4nio.\n </p>\n <p style=\"text-align: justify;\">\n  Observe que a soma e os dendritos formam a superf\u00edcie de entrada do neur\u00f4nio e o ax\u00f4nio a superf\u00edcie de sa\u00edda do fluxo de informa\u00e7\u00e3o (esse fluxo de informa\u00e7\u00e3o \u00e9 importante para compreender o neur\u00f4nio matem\u00e1tico daqui a pouco). A informa\u00e7\u00e3o transmitida pelos neur\u00f4nios na realidade s\u00e3o impulsos el\u00e9tricos. O impulso el\u00e9trico \u00e9 a mensagem que os neur\u00f4nios transmitem uns aos outros, ou seja, \u00e9 a propaga\u00e7\u00e3o de um est\u00edmulo ao longo dos neur\u00f4nios que pode ser qualquer sinal captado pelos receptores nervosos.\n </p>\n <p style=\"text-align: justify;\">\n  Os dendritos t\u00eam como fun\u00e7\u00e3o, receber informa\u00e7\u00f5es, ou impulsos nervosos, oriundos de outros neur\u00f4nios e conduzi-los at\u00e9 o corpo celular. Ali, a informa\u00e7\u00e3o \u00e9 processada e novos impulsos s\u00e3o gerados. Estes impulsos s\u00e3o transmitidos a outros neur\u00f4nios, passando pelo ax\u00f4nio e atingindo os dendritos dos neur\u00f4nios seguintes.\u00a0O corpo do neur\u00f4nio \u00e9 respons\u00e1vel por coletar e combinar informa\u00e7\u00f5es vindas de outros neur\u00f4nios.\n </p>\n <p style=\"text-align: justify;\">\n  O ponto de contato entre a termina\u00e7\u00e3o ax\u00f4nica de um neur\u00f4nio e o dendrito de outro \u00e9 chamado sinapse. \u00c9 pelas sinapses que os neur\u00f4nios se unem funcionalmente, formando as redes neurais. As sinapses funcionam como v\u00e1lvulas, sendo capazes de controlar a transmiss\u00e3o de impulsos, isto \u00e9, o fluxo da informa\u00e7\u00e3o entre os neur\u00f4nios na rede neural. O efeito das sinapses \u00e9 vari\u00e1vel e \u00e9 esta varia\u00e7\u00e3o que d\u00e1 ao neur\u00f4nio capacidade de adapta\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Sinais el\u00e9tricos gerados nos sensores (retina ocular, papilas gustativas, etc\u2026) caminham pelos ax\u00f4nios. Se esses sinais forem superiores a um limiar de disparo (threshold), seguem pelo ax\u00f4nio. Caso contr\u00e1rio, s\u00e3o bloqueados e n\u00e3o prosseguem (s\u00e3o considerados irrelevantes).\u00a0A passagem desses sinais n\u00e3o \u00e9 el\u00e9trica, mas qu\u00edmica (atrav\u00e9s da subst\u00e2ncia serotonina). Se o sinal for superior a certo limite (threshold), vai em frente; caso contr\u00e1rio \u00e9 bloqueado e n\u00e3o segue. Estamos falando aqui do neur\u00f4nio biol\u00f3gico e preste bastante aten\u00e7\u00e3o a palavra threshold, pois ela \u00e9 a ess\u00eancia do neur\u00f4nio matem\u00e1tico.\n </p>\n <p style=\"text-align: justify;\">\n  Um neur\u00f4nio recebe sinais atrav\u00e9s de in\u00fameros dendritos, os quais s\u00e3o ponderados e enviados para o ax\u00f4nio, podendo ou n\u00e3o seguir adiante (threshold). Na passagem por um neur\u00f4nio, um sinal pode ser amplificado ou atenuado, dependendo do dendrito de origem, pois a cada condutor, est\u00e1 associado um peso pelo qual o sinal \u00e9 multiplicado. Os pesos s\u00e3o o que chamamos de mem\u00f3ria.\n </p>\n <p style=\"text-align: justify;\">\n  Cada regi\u00e3o do c\u00e9rebro \u00e9 especializada em uma dada fun\u00e7\u00e3o, como processamento de sinais auditivos, sonoros, elabora\u00e7\u00e3o de pensamentos, desejos, etc\u2026 Esse processamento se d\u00e1 atrav\u00e9s de redes particulares interligadas entre si, realizando processamento paralelo. Cada regi\u00e3o do c\u00e9rebro possui uma arquitetura de rede diferente: varia o n\u00famero de neur\u00f4nios, de sinapses por neur\u00f4nio, valor dos thresholds e dos pesos, etc\u2026Os valores dos pesos s\u00e3o estabelecidos por meio de treinamento recebido pelo c\u00e9rebro durante a vida \u00fatil. \u00c9 a memoriza\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Inspirados no neur\u00f4nio biol\u00f3gico, os pesquisadores desenvolveram um modelo de neur\u00f4nio matem\u00e1tico que se tornou a base da Intelig\u00eancia Artificial. A ideia era simples: \u201cSe redes neurais formam a intelig\u00eancia humana, vamos reproduzir isso e criar Intelig\u00eancia Artificial\u201d. E assim nasceu o neur\u00f4nio matem\u00e1tico, o qual descrevemos abaixo.\n </p>\n <p>\n </p>\n <h2>\n  O Neur\u00f4nio Matem\u00e1tico\n </h2>\n <p style=\"text-align: justify;\">\n  A partir da estrutura e funcionamento do neur\u00f4nio biol\u00f3gico, pesquisadores tentaram simular este sistema em computador. O modelo mais bem aceito foi proposto por Warren McCulloch e Walter Pitts em 1943, o qual implementa de maneira simplificada os componentes e o funcionamento de um neur\u00f4nio biol\u00f3gico. Em termos simples, um neur\u00f4nio matem\u00e1tico de uma rede neural artificial \u00e9 um componente que calcula a soma ponderada de v\u00e1rios inputs, aplica uma fun\u00e7\u00e3o e passa o resultado adiante.\n </p>\n <p style=\"text-align: justify;\">\n  Neste modelo de neur\u00f4nio matem\u00e1tico, os impulsos el\u00e9tricos provenientes de outros neur\u00f4nios s\u00e3o representados pelos chamados sinais de entrada (a letra x nesse diagrama abaixo, que nada mais s\u00e3o do que os dados que alimentam seu modelo de rede neural artificial). Dentre os v\u00e1rios est\u00edmulos recebidos, alguns excitar\u00e3o mais e outros menos o neur\u00f4nio receptor e essa medida de qu\u00e3o excitat\u00f3rio \u00e9 o est\u00edmulo \u00e9 representada no modelo de Warren McCulloch e Walter Pitts atrav\u00e9s dos pesos sin\u00e1pticos. Quanto maior o valor do peso, mais excitat\u00f3rio \u00e9 o est\u00edmulo. Os pesos sin\u00e1pticos s\u00e3o representados por wkn neste diagrama abaixo, onde k representa o \u00edndice do neur\u00f4nio em quest\u00e3o e n se refere ao terminal de entrada da sinapse a qual o peso sin\u00e1ptico se refere.\n </p>\n <p style=\"text-align: justify;\">\n  A soma ou corpo da c\u00e9lula \u00e9 representada por uma composi\u00e7\u00e3o de dois m\u00f3dulos, o primeiro \u00e9 uma jun\u00e7\u00e3o aditiva, somat\u00f3rio dos est\u00edmulos (sinais de entrada) multiplicado pelo seu fator excitat\u00f3rio (pesos sin\u00e1pticos), e posteriormente uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o, que definir\u00e1 com base nas entradas e pesos sin\u00e1pticos, qual ser\u00e1 a sa\u00edda do neur\u00f4nio. O ax\u00f4nio \u00e9 aqui representado pela sa\u00edda (yk) obtida pela aplica\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o. Assim como no modelo biol\u00f3gico, o est\u00edmulo pode ser excitat\u00f3rio ou inibit\u00f3rio, representado pelo peso sin\u00e1ptico positivo ou negativo respectivamente.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Neur\u00f4nio Matem\u00e1tico\" class=\"aligncenter size-full wp-image-128\" data-attachment-id=\"128\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Neur\u00f4nio Matem\u00e1tico\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?fit=543%2C300\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?fit=300%2C166\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?fit=543%2C300\" data-orig-size=\"543,300\" data-permalink=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/neuronio-matematico/\" data-recalc-dims=\"1\" height=\"300\" sizes=\"(max-width: 543px) 100vw, 543px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?resize=543%2C300\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?w=543 543w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?resize=300%2C166 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio-matematico.png?resize=200%2C110 200w\" width=\"543\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig8 \u2013 Representa\u00e7\u00e3o Simplificada do Neur\u00f4nio Matem\u00e1tico\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O modelo proposto possui uma natureza bin\u00e1ria. Tanto os sinais de entrada quanto a sa\u00edda, s\u00e3o valores bin\u00e1rios. McCulloch acreditava que o funcionamento do sistema nervoso central possu\u00eda um carater bin\u00e1rio, ou seja, um neur\u00f4nio infuencia ou n\u00e3o outro neur\u00f4nio, mas posteriormente mostrou-se que n\u00e3o era dessa forma.\n </p>\n <p style=\"text-align: justify;\">\n  O neur\u00f4nio matem\u00e1tico \u00e9 um modelo simplificado do neur\u00f4nio biol\u00f3gico. Tais modelos inspirados a partir da an\u00e1lise da gera\u00e7\u00e3o e propaga\u00e7\u00e3o de impulsos el\u00e9tricos pela membrana celular dos neur\u00f4nios. O neur\u00f4nio matem\u00e1tico recebe um ou mais sinais de entrada e devolve um \u00fanico sinal de sa\u00edda, que pode ser distribu\u00eddo como sinal de sa\u00edda da rede, ou como sinal de entrada para um ou v\u00e1rios outros neur\u00f4nios da camada posterior (que formam a rede neural artificial).\u00a0Os dendritos e ax\u00f4nios s\u00e3o representados matematicamente apenas pelas sinapses, e a intensidade da liga\u00e7\u00e3o \u00e9 representada por uma grandeza denominada peso sin\u00e1ptico, simbolizada pela letra w. Quando as entradas, x s\u00e3o apresentadas ao neur\u00f4nio, elas s\u00e3o multiplicadas pelos pesos sin\u00e1pticos correspondentes, gerando as entradas ponderadas, ou seja, x1 que multiplica w1, etc\u2026 Isso descreve uma das bases matem\u00e1ticas do funcionamento de uma rede neural artificial, a multiplica\u00e7\u00e3o de matrizes:\n </p>\n <p>\n  <img alt=\"Matriz\" class=\"aligncenter wp-image-130 size-medium\" data-attachment-id=\"130\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Matriz\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?fit=668%2C328\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?fit=300%2C147\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?fit=668%2C328\" data-orig-size=\"668,328\" data-permalink=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/matriz/\" data-recalc-dims=\"1\" height=\"147\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?resize=300%2C147\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?resize=300%2C147 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?resize=200%2C98 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/matriz.png?w=668 668w\" width=\"300\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig9 \u2013 Multiplica\u00e7\u00e3o de Matrizes Entre Sinais de Entrada x e Pesos Sin\u00e1pticos w (vers\u00e3o simplificada)\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O neur\u00f4nio ent\u00e3o totaliza todos os produtos gerando um \u00fanico resultado. A esta fun\u00e7\u00e3o se denomina fun\u00e7\u00e3o de combina\u00e7\u00e3o. Este valor \u00e9 ent\u00e3o apresentado a uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o ou fun\u00e7\u00e3o de transfer\u00eancia, que tem, dentre outras, a finalidade de evitar o acr\u00e9scimo progressivo dos valores de sa\u00edda ao longo das camadas da rede, visto que tais fun\u00e7\u00f5es possuem valores m\u00e1ximos e m\u00ednimos contidos em intervalos determinados. O uso de fun\u00e7\u00f5es de transfer\u00eancia n\u00e3o-lineares torna a rede neural uma ferramenta poderosa. Sabe-se que uma rede perceptron de duas camadas com fun\u00e7\u00e3o de transfer\u00eancia n\u00e3o-linear como a fun\u00e7\u00e3o sigm\u00f3ide (que veremos mais adiante), \u00e9 denominada de aproximador universal.\n </p>\n <p style=\"text-align: justify;\">\n  Um neur\u00f4nio dispara quando a soma dos impulsos que ele recebe ultrapassa o seu limiar de excita\u00e7\u00e3o chamado de threshold. O corpo do neur\u00f4nio, por sua vez, \u00e9 emulado por um mecanismo simples que faz a soma dos valores xi e wi recebidos pelo neur\u00f4nio (soma ponderada) e decide se o neur\u00f4nio deve ou n\u00e3o disparar (sa\u00edda igual a 1 ou a 0) comparando a soma obtida ao limiar ou threshold do neur\u00f4nio. A ativa\u00e7\u00e3o do neur\u00f4nio \u00e9 obtida atrav\u00e9s da aplica\u00e7\u00e3o de uma \u201cfun\u00e7\u00e3o de ativa\u00e7\u00e3o\u201d, que ativa a sa\u00edda ou n\u00e3o, dependendo do valor da soma ponderada das suas entradas.\n </p>\n <p style=\"text-align: justify;\">\n  Note que este modelo matem\u00e1tico simplificado de um neur\u00f4nio \u00e9 est\u00e1tico, ou seja, n\u00e3o considera a din\u00e2mica do neur\u00f4nio natural. No neur\u00f4nio biol\u00f3gico, os sinais s\u00e3o enviados em pulsos e alguns componentes dos neur\u00f4nios biol\u00f3gicos, a exemplo do ax\u00f4nio, funcionam como filtros de frequ\u00eancia.\n </p>\n <p style=\"text-align: justify;\">\n  O modelo do neur\u00f4nio matem\u00e1tico tamb\u00e9m pode incluir uma polariza\u00e7\u00e3o ou bias de entrada. Esta vari\u00e1vel \u00e9 inclu\u00edda ao somat\u00f3rio da fun\u00e7\u00e3o de ativa\u00e7\u00e3o, com o intuito de aumentar o grau de liberdade desta fun\u00e7\u00e3o e, consequentemente, a capacidade de aproxima\u00e7\u00e3o da rede. O valor do bias \u00e9 ajustado da mesma forma que os pesos sin\u00e1pticos. O bias possibilita que um neur\u00f4nio apresente sa\u00edda n\u00e3o nula ainda que todas as suas entradas sejam nulas. Por exemplo, caso n\u00e3o houvesse o bias e todas as entradas de um neur\u00f4nio fossem nulas, ent\u00e3o o valor da fun\u00e7\u00e3o de ativa\u00e7\u00e3o seria nulo. Desta forma n\u00e3o poder\u00edamos, por exemplo, fazer com o que o neur\u00f4nio aprendesse a rela\u00e7\u00e3o pertinente ao \u201dou exclusivo\u201d da l\u00f3gica. Em resumo, temos esses componentes em um neur\u00f4nio matem\u00e1tico:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Resumo do Neur\u00f4nio\" class=\"aligncenter size-medium wp-image-135\" data-attachment-id=\"135\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Resumo do Neur\u00f4nio\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?fit=800%2C365\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?fit=300%2C137\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?fit=800%2C365\" data-orig-size=\"800,365\" data-permalink=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/neuronio-2/\" data-recalc-dims=\"1\" height=\"137\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=300%2C137\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=300%2C137 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=768%2C350 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=200%2C91 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=690%2C315 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?w=800 800w\" width=\"300\"/>\n </p>\n <p style=\"text-align: center;\">\n  Fig10 \u2013 Representa\u00e7\u00e3o do Neur\u00f4nio Matem\u00e1tico\n </p>\n <p>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   Sinais de entrada { X1, X2, \u2026, Xn }: S\u00e3o os sinais externos normalmente normalizados para incrementar a efici\u00eancia computacional dos algoritmos de aprendizagem. S\u00e3o os dados que alimentam seu modelo preditivo.\n  </li>\n  <li style=\"text-align: justify;\">\n   Pesos sin\u00e1pticos { W1, W2, \u2026, Wn }: S\u00e3o valores para ponderar os sinais de cada entrada da rede. Esses valores s\u00e3o aprendidos durante o treinamento.\n  </li>\n  <li style=\"text-align: justify;\">\n   Combinador linear { \u03a3 }: Agregar todos sinais de entrada que foram ponderados pelos respectivos pesos sin\u00e1pticos a fim de produzir um potencial de ativa\u00e7\u00e3o.\n  </li>\n  <li style=\"text-align: justify;\">\n   Limiar de ativa\u00e7\u00e3o { \u0398 }: Especifica qual ser\u00e1 o patamar apropriado para que o resultado produzido pelo combinador linear possa gerar um valor de disparo de ativa\u00e7\u00e3o.\n  </li>\n  <li style=\"text-align: justify;\">\n   Potencial de ativa\u00e7\u00e3o { u }: \u00c9 o resultado obtido pela diferen\u00e7a do valor produzido entre o combinador linear e o limiar de ativa\u00e7\u00e3o. Se o valor for positivo, ou seja, se u \u2265 0 ent\u00e3o o neur\u00f4nio produz um potencial excitat\u00f3rio; caso contr\u00e1rio, o potencial ser\u00e1 inibit\u00f3rio.\n  </li>\n  <li style=\"text-align: justify;\">\n   Fun\u00e7\u00e3o de ativa\u00e7\u00e3o { g }: Seu objetivo \u00e9 limitar a sa\u00edda de um neur\u00f4nio em um intervalo valores.\n  </li>\n  <li style=\"text-align: justify;\">\n   Sinal de sa\u00edda { y}: \u00c9 o valor final de sa\u00edda podendo ser usado como entrada de outros neur\u00f4nios que est\u00e3o sequencialmente interligados.\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  Os modelos baseados em redes neurais artificiais s\u00e3o os que mais ganharam aten\u00e7\u00e3o nos \u00faltimos anos por conseguirem resolver problemas de IA nos quais se conseguia pouco avan\u00e7o com outras t\u00e9cnicas. A partir da concep\u00e7\u00e3o do neur\u00f4nio matem\u00e1tico, v\u00e1rias arquiteturas e modelos com diferentes combina\u00e7\u00f5es entre esses neur\u00f4nios, e aplicando diferentes t\u00e9cnicas matem\u00e1ticas e estat\u00edsticas, surgiram e propiciaram a cria\u00e7\u00e3o de arquiteturas avan\u00e7adas de Deep Learning como Redes Neurais Convolucionais, Redes Neurais Recorrentes, Auto Encoders, Generative Adversarial Networks, Memory Networks, entre outras, que estudaremos ao longo deste livro online.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.khanacademy.org/science/biology/human-biology/neuron-nervous-system/v/anatomy-of-a-neuron\" rel=\"noopener\" target=\"_blank\">\n    Anatomia de um Neur\u00f4nio\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://datascienceacademy.com.br/blog/bibliografia-machine-learning-e-inteligencia-artificial/\" rel=\"noopener\" target=\"_blank\">\n    Bibliografia Machine Learning e IA\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/abs/1404.7828\" rel=\"noopener\" target=\"_blank\">\n    Deep Learning in Neural Networks: An Overview\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.manning.com/books/grokking-deep-learning\" rel=\"noopener\" target=\"_blank\">\n    Grokking Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  HAYKIN, S. Redes Neurais, princ\u00edpios e pr\u00e1ticas. Porto Alegre: Bookman, 2001.\n </p>\n <p>\n  JAIN, A. K, MAO, J., MOHIUDDIN, K.M. Artificial neural networks: a tutorial. IEEE Computer, v. 29, n. 3, p. 56-63, 1996.\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-124\" href=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-124\" href=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-124\" href=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-124\" href=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-124-5e0dd044ad0f9\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=124&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-124-5e0dd044ad0f9\" id=\"like-post-wrapper-140353593-124-5e0dd044ad0f9\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "5": "<h1 class=\"entry-title\" id=\"capitulo-5\">\n Cap\u00edtulo 5 \u2013 Usando Redes Neurais Para Reconhecer D\u00edgitos Manuscritos\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  O sistema visual humano \u00e9 uma das maravilhas do mundo. Considere a seguinte sequ\u00eancia de d\u00edgitos manuscritos:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"D\u00edgitos\" class=\"aligncenter wp-image-91 size-medium\" data-attachment-id=\"91\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"D\u00edgitos\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?fit=623%2C128\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?fit=300%2C62\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?fit=623%2C128\" data-orig-size=\"623,128\" data-permalink=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/digits/\" data-recalc-dims=\"1\" height=\"62\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?resize=300%2C62\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?resize=300%2C62 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?resize=200%2C41 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/digits.png?w=623 623w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A maioria das pessoas reconhece sem esfor\u00e7o esses d\u00edgitos como 504192. Essa facilidade \u00e9 enganosa. Em cada hemisf\u00e9rio do nosso c\u00e9rebro, os seres humanos t\u00eam um c\u00f3rtex visual prim\u00e1rio, tamb\u00e9m conhecido como V1, contendo 140 milh\u00f5es de neur\u00f4nios, com dezenas de bilh\u00f5es de conex\u00f5es entre eles. E, no entanto, a vis\u00e3o humana envolve n\u00e3o apenas V1, mas uma s\u00e9rie inteira de cortices visuais \u2013 V2, V3, V4 e V5 \u2013 fazendo processamento de imagem progressivamente mais complexo. N\u00f3s carregamos em nossas cabe\u00e7as um supercomputador, sintonizado pela evolu\u00e7\u00e3o ao longo de centenas de milh\u00f5es de anos, e soberbamente adaptado para entender o mundo visual. Reconhecer os d\u00edgitos manuscritos n\u00e3o \u00e9 f\u00e1cil. Em vez disso, n\u00f3s humanos somos estupendos, surpreendentemente bons, em entender o que nossos olhos nos mostram. Mas quase todo esse trabalho \u00e9 feito inconscientemente. E, portanto, geralmente n\u00e3o apreciamos o qu\u00e3o dif\u00edcil \u00e9 o problema dos nossos sistemas visuais.\n </p>\n <p style=\"text-align: justify;\">\n  A dificuldade de reconhecimento do padr\u00e3o visual torna-se evidente se voc\u00ea tentar escrever um programa de computador para reconhecer d\u00edgitos como os acima. O que parece f\u00e1cil quando n\u00f3s seres humanos fazemos, de repente, se torna extremamente dif\u00edcil. Intui\u00e7\u00f5es simples sobre como reconhecemos formas \u2013 \u201cum 9 tem um loop no topo e um curso vertical no canto inferior direito\u201d \u2013 n\u00e3o \u00e9 t\u00e3o simples de se expressar algoritmicamente. Quando voc\u00ea tenta construir essas regras de forma precisa, voc\u00ea se perde rapidamente em diversas exce\u00e7\u00f5es, ressalvas e casos especiais. \u00c9 meio desesperador.\n </p>\n <p style=\"text-align: justify;\">\n  As redes neurais abordam o problema de uma maneira diferente. A ideia \u00e9 tomar uma grande quantidade de d\u00edgitos manuscritos, conhecidos como exemplos de treinamento, e em seguida, desenvolver um sistema que possa aprender com esses exemplos de treinamento. Em outras palavras, a rede neural usa os exemplos para inferir automaticamente regras para o reconhecimento de d\u00edgitos manuscritos. Al\u00e9m disso, ao aumentar o n\u00famero de exemplos de treinamento, a rede pode aprender mais sobre a caligrafia, e assim melhorar sua precis\u00e3o. Podemos construir um reconhecedor de d\u00edgitos manuscritos melhor usando milhares, milh\u00f5es ou bilh\u00f5es de exemplos de treinamento.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"MNIST\" class=\"aligncenter size-full wp-image-93\" data-attachment-id=\"93\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"MNIST\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?fit=255%2C204\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?fit=255%2C204\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?fit=255%2C204\" data-orig-size=\"255,204\" data-permalink=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/mnist_100_digits/\" data-recalc-dims=\"1\" height=\"204\" sizes=\"(max-width: 255px) 100vw, 255px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?resize=255%2C204\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?w=255 255w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/mnist_100_digits.png?resize=200%2C160 200w\" width=\"255\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Ao longo dos pr\u00f3ximos cap\u00edtulos come\u00e7aremos nossa jornada rumo \u00e0s arquiteturas mais avan\u00e7adas de Deep Learning, desenvolvendo um programa de computador implementando uma rede neural\u00a0que aprende a reconhecer os d\u00edgitos manuscritos. O programa n\u00e3o usar\u00e1 bibliotecas de redes neurais especiais (usaremos apenas linguagem Python). Mas este programa pode reconhecer d\u00edgitos com uma precis\u00e3o de mais de 96%, sem interven\u00e7\u00e3o humana. Al\u00e9m disso, em cap\u00edtulos posteriores, desenvolveremos ideias que podem melhorar a precis\u00e3o para mais de 99%. Na verdade, as melhores redes neurais comerciais s\u00e3o agora t\u00e3o boas que s\u00e3o usadas pelos bancos para processar cheques e por ag\u00eancias de correio para reconhecer endere\u00e7os.\n </p>\n <p style=\"text-align: justify;\">\n  Estamos nos concentrando no reconhecimento de manuscrito porque \u00e9 um excelente problema prot\u00f3tipo para aprender sobre redes neurais em geral. Como um prot\u00f3tipo, ele atinge um ponto interessante: \u00e9 desafiador \u2013 n\u00e3o \u00e9 t\u00e3o simples reconhecer os d\u00edgitos manuscritos \u2013 mas tamb\u00e9m n\u00e3o \u00e9 t\u00e3o dif\u00edcil e nem requer uma solu\u00e7\u00e3o extremamente complicada, ou um tremendo poder computacional. Al\u00e9m disso, \u00e9 uma \u00f3tima maneira de desenvolver t\u00e9cnicas mais avan\u00e7adas, como a aprendizagem profunda. E assim, ao longo do livro, retornaremos repetidamente ao problema do reconhecimento de d\u00edgitos manuscritos. Mais tarde, no livro, vamos discutir como essas ideias podem ser aplicadas a outros problemas em vis\u00e3o computacional, e tamb\u00e9m em reconhecimento da fala, processamento de linguagem natural e outras \u00e1reas.\n </p>\n <p style=\"text-align: justify;\">\n  Ao longo do caminho, desenvolveremos muitas ideias-chave sobre as redes neurais, incluindo dois tipos importantes de neur\u00f4nios artificiais (o perceptron e o neur\u00f4nio sigm\u00f3ide) e o algoritmo de aprendizagem padr\u00e3o para redes neurais, conhecido como descida estoc\u00e1stica do gradiente. Explicaremos porque as coisas s\u00e3o feitas da maneira que elas s\u00e3o e na constru\u00e7\u00e3o de sua intui\u00e7\u00e3o de redes neurais. Isso requer uma discuss\u00e3o mais longa do que apenas apresentar a mec\u00e2nica b\u00e1sica do que est\u00e1 acontecendo, mas vale a pena para o entendimento mais profundo que voc\u00ea alcan\u00e7ar\u00e1. E ao final deste livro, voc\u00ea ter\u00e1 uma boa compreens\u00e3o do que \u00e9 aprendizado profundo e como isso est\u00e1 transformando o mundo!\n </p>\n <p>\n  Caso voc\u00ea n\u00e3o tenha conhecimento em linguagem Python, recomendamos o curso gratuito\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener\" target=\"_blank\">\n    Python Fundamentos Para An\u00e1lise de Dados\n   </a>\n  </span>\n  . Ele vai fornecer uma \u00f3tima base de tudo que voc\u00ea precisa para come\u00e7ar a desenvolver suas redes neurais.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n  (alguns trechos extra\u00eddos e traduzidos com autoriza\u00e7\u00e3o do autor\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://michaelnielsen.org/\" rel=\"noopener\" target=\"_blank\">\n    Michael Nielsen\n   </a>\n  </span>\n  )\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-62\" href=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-62\" href=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-62\" href=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-62\" href=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/usando-redes-neurais-para-reconhecer-digitos-manuscritos/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-62-5e0dd04676444\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=62&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-62-5e0dd04676444\" id=\"like-post-wrapper-140353593-62-5e0dd04676444\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "6": "<h1 class=\"entry-title\" id=\"capitulo-6\">\n Cap\u00edtulo 6 \u2013 O Perceptron \u2013 Parte 1\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;\">\n   Voc\u00ea sabe quais s\u00e3o as principais arquiteturas de redes neurais artificias? N\u00e3o. Ent\u00e3o analise cuidadosamente a imagem abaixo (excelente trabalho criado pela equipe do Asimov Institute, cujo link voc\u00ea encontra na se\u00e7\u00e3o de refer\u00eancias ao final deste cap\u00edtulo):\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Neural Network Zoo\" class=\"aligncenter size-large wp-image-154\" data-attachment-id=\"154\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Neural Network Zoo\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?fit=683%2C1024\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?fit=200%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?fit=2000%2C3000\" data-orig-size=\"2000,3000\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-1/neuralnetworks-2/\" data-recalc-dims=\"1\" height=\"1024\" sizes=\"(max-width: 683px) 100vw, 683px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=683%2C1024\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=683%2C1024 683w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=200%2C300 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=768%2C1152 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?resize=690%2C1035 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuralnetworks-1.png?w=2000 2000w\" width=\"683\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Incr\u00edvel, n\u00e3o? S\u00e3o diversas arquiteturas, usadas para resolver diferentes tipos de problemas, como por exemplo as arquiteturas de redes neurais convolucionais usadas em problemas de Vis\u00e3o Computacional e as redes neurais recorrentes usadas em problemas de Processamento de Linguagem Natural. Estudaremos quase todas essas arquiteturas aqui neste livro. Sim, isso mesmo que voc\u00ea leu. Estamos apenas come\u00e7ando!! Caso queira aprender a construir modelos e projetos usando essas arquiteturas e trabalhando com linguagem Python e Google TensorFlow, clique\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/cursos-online/\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Embora todas essas arquiteturas sejam de redes neurais artificias, nem todas s\u00e3o de Deep Learning. O que caracteriza modelos de aprendizagem profunda, como o nome sugere, s\u00e3o redes neurais artificias com muitas camadas ocultas (ou intermedi\u00e1rias). Mas antes de chegarmos l\u00e1, precisamos passar pela arquitetura mais simples de uma rede neural artificial, o Perceptron. Como diz o ditado: \u201cToda grande caminhada come\u00e7a pelo primeiro passo\u201d.\n </p>\n <p style=\"text-align: justify;\">\n  O Modelo Perceptron foi desenvolvido nas d\u00e9cadas de 1950 e 1960 pelo cientista Frank Rosenblatt, inspirado em trabalhos anteriores de Warren McCulloch e Walter Pitts. Hoje, \u00e9 mais comum usar outros modelos de neur\u00f4nios artificiais, mas o Perceptron permite uma compreens\u00e3o clara de como funciona uma rede neural em termos matem\u00e1ticos, sendo uma excelente introdu\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Ent\u00e3o, como funcionam os Perceptrons? Um Perceptron \u00e9 um modelo matem\u00e1tico que recebe v\u00e1rias entradas, x1, x2, \u2026 e produz uma \u00fanica sa\u00edda bin\u00e1ria:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Perceptron\" class=\"aligncenter size-full wp-image-97\" data-attachment-id=\"97\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Perceptron\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?fit=280%2C138\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?fit=280%2C138\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?fit=280%2C138\" data-orig-size=\"280,138\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-1/perceptron/\" data-recalc-dims=\"1\" height=\"138\" sizes=\"(max-width: 280px) 100vw, 280px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?resize=280%2C138\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?w=280 280w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?resize=200%2C99 200w\" width=\"280\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  No exemplo mostrado, o Perceptron possui tr\u00eas entradas: x1, x2, x3. Rosenblatt prop\u00f4s uma regra simples para calcular a sa\u00edda. Ele introduziu pesos, w1, w2, \u2026, n\u00fameros reais expressando a import\u00e2ncia das respectivas entradas para a sa\u00edda. A sa\u00edda do neur\u00f4nio, 0 ou 1, \u00e9 determinada pela soma ponderada,\n  <strong>\n   \u03a3jwjxj\n  </strong>\n  , menor ou maior do que algum valor limiar (threshold). Assim como os pesos, o threshold \u00e9 um n\u00famero real que \u00e9 um par\u00e2metro do neur\u00f4nio. Para coloc\u00e1-lo em termos alg\u00e9bricos mais precisos:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Output\" class=\"aligncenter size-full wp-image-99\" data-attachment-id=\"99\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Output\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?fit=762%2C186\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?fit=300%2C73\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?fit=762%2C186\" data-orig-size=\"762,186\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-1/output/\" data-recalc-dims=\"1\" height=\"186\" sizes=\"(max-width: 762px) 100vw, 762px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=762%2C186\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?w=762 762w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=300%2C73 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=200%2C49 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?resize=690%2C168 690w\" width=\"762\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Esse \u00e9 o modelo matem\u00e1tico b\u00e1sico. Uma maneira de pensar sobre o Perceptron \u00e9 que \u00e9 um dispositivo que toma decis\u00f5es ao comprovar evid\u00eancias. Deixe-me dar um exemplo. N\u00e3o \u00e9 um exemplo muito realista, mas \u00e9 f\u00e1cil de entender, e logo chegaremos a exemplos mais realistas. Suponha que o fim de semana esteja chegando e voc\u00ea ouviu falar que haver\u00e1 um festival de queijo em sua cidade. Voc\u00ea gosta de queijo e est\u00e1 tentando decidir se deve ou n\u00e3o ir ao festival. Voc\u00ea pode tomar sua decis\u00e3o pesando tr\u00eas fatores:\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   O tempo est\u00e1 bom?\n  </li>\n  <li style=\"text-align: justify;\">\n   Seu namorado ou namorada quer acompanh\u00e1-lo(a)?\n  </li>\n  <li style=\"text-align: justify;\">\n   O festival est\u00e1 perto de transporte p\u00fablico? (Voc\u00ea n\u00e3o possui um carro)\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  Podemos representar estes tr\u00eas fatores pelas vari\u00e1veis bin\u00e1rias correspondentes x1, x2 e x3. Por exemplo, ter\u00edamos x1 = 1 se o tempo estiver bom e x1 = 0 se o tempo estiver ruim. Da mesma forma, x2 = 1 se seu namorado ou namorada quiser ir ao festival com voc\u00ea, e x2 = 0, se n\u00e3o. E similarmente para x3 e transporte p\u00fablico.\n </p>\n <p style=\"text-align: justify;\">\n  Agora, suponha que voc\u00ea adore queijo e est\u00e1 disposto a ir ao festival, mesmo que seu namorado ou namorada n\u00e3o esteja interessado e o festival fica em um lugar de dif\u00edcil acesso e sem transporte p\u00fablico amplamente dispon\u00edvel. Al\u00e9m disso, voc\u00ea realmente detesta mau tempo, e n\u00e3o h\u00e1 como ir ao festival se o tempo estiver ruim. Voc\u00ea pode usar Perceptrons para modelar esse tipo de tomada de decis\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Uma maneira de fazer isso \u00e9 escolher um peso w1 = 6 para o tempo e w2 = 2 e w3 = 2 para as outras condi\u00e7\u00f5es. O valor maior de w1 indica que o tempo \u00e9 muito importante para voc\u00ea, muito mais do que se seu namorado ou namorada vai acompanh\u00e1-lo(a) ou se o festival \u00e9 pr\u00f3ximo do transporte p\u00fablico. Finalmente, suponha que voc\u00ea escolha um threshold de 5 para o Perceptron. Com essas escolhas, o Perceptron implementa o modelo de tomada de decis\u00e3o desejado, produzindo 1 sempre que o tempo estiver bom e 0 sempre que o tempo estiver ruim. N\u00e3o faz diferen\u00e7a para o resultado se seu namorado ou namorada quer ir, ou se o transporte p\u00fablico est\u00e1 acess\u00edvel.\n </p>\n <p style=\"text-align: justify;\">\n  Variando os pesos e o limiar, podemos obter diferentes modelos de tomada de decis\u00e3o. Por exemplo, suponha que escolhemos um threshold de 3. Ent\u00e3o, o Perceptron decidir\u00e1 que voc\u00ea deveria ir ao festival sempre que o tempo estiver bom ou quando o festival estiver perto do transporte p\u00fablico e seu namorado ou namorada estiver disposto a se juntar a voc\u00ea. Em outras palavras, seria um modelo diferente de tomada de decis\u00e3o. Reduzir o threshold significa que voc\u00ea est\u00e1 mais propenso a ir ao festival.\n </p>\n <p style=\"text-align: justify;\">\n  Obviamente, o Perceptron n\u00e3o \u00e9 um modelo completo de tomada de decis\u00e3o humana! Mas o que o exemplo ilustra \u00e9 como um Perceptron pode pesar diferentes tipos de evid\u00eancias para tomar decis\u00f5es. E deve parecer plaus\u00edvel que uma rede complexa de Perceptrons possa tomar decis\u00f5es bastante sutis.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"Rede\" class=\"aligncenter size-full wp-image-100\" data-attachment-id=\"100\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Rede\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?fit=540%2C211\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?fit=300%2C117\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?fit=540%2C211\" data-orig-size=\"540,211\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-1/rede/\" data-recalc-dims=\"1\" height=\"211\" sizes=\"(max-width: 540px) 100vw, 540px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?resize=540%2C211\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?w=540 540w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?resize=300%2C117 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/rede.png?resize=200%2C78 200w\" width=\"540\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Nesta rede, a primeira coluna de Perceptrons \u2013 o que chamaremos de primeira camada de Perceptrons \u2013 est\u00e1 tomando tr\u00eas decis\u00f5es muito simples, pesando a evid\u00eancia de entrada. E quanto aos Perceptrons na segunda camada? Cada um desses Perceptrons est\u00e1 tomando uma decis\u00e3o ponderando os resultados da primeira camada de tomada de decis\u00e3o. Desta forma, um Perceptron na segunda camada pode tomar uma decis\u00e3o em um n\u00edvel mais complexo e mais abstrato do que os Perceptrons na primeira camada. E as decis\u00f5es ainda mais complexas podem ser feitas pelos Perceptrons na terceira camada. Desta forma, uma rede de Perceptrons de v\u00e1rias camadas pode envolver-se em uma tomada de decis\u00e3o sofisticada.\n </p>\n <p style=\"text-align: justify;\">\n  Ali\u00e1s, quando definimos os Perceptrons, dissemos que um Perceptron possui apenas uma sa\u00edda. Na rede acima, os Perceptrons parecem ter m\u00faltiplos resultados. Na verdade, eles ainda s\u00e3o de sa\u00edda \u00fanica. As setas de sa\u00edda m\u00faltiplas s\u00e3o meramente uma maneira \u00fatil de indicar que a sa\u00edda de um Perceptron est\u00e1 sendo usada como entrada para v\u00e1rios outros Perceptrons.\n </p>\n <p style=\"text-align: justify;\">\n  Vamos simplificar a maneira como descrevemos os Perceptrons. No limite de condi\u00e7\u00e3o\n  <strong>\n   \u03a3jwjxj &gt; threshold\n  </strong>\n  podemos fazer duas mudan\u00e7as de nota\u00e7\u00e3o para simplific\u00e1-lo. A primeira mudan\u00e7a \u00e9 escrever \u03a3jwjxj como um produto (dot product), w\u22c5x\u2261\u03a3jwjxj, onde w e x s\u00e3o vetores cujos componentes s\u00e3o os pesos e entradas, respectivamente. A segunda mudan\u00e7a \u00e9 mover o\u00a0threshold para o outro lado da equa\u00e7\u00e3o e substitu\u00ed-lo pelo que \u00e9 conhecido como o vi\u00e9s (bias) do Perceptron, ou b \u2261 -threshold. Usando o vi\u00e9s em vez do threshold, a regra Perceptron pode ser reescrita:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"F\u00f3rmula Perceptron\" class=\"aligncenter size-full wp-image-157\" data-attachment-id=\"157\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"F\u00f3rmula Perceptron\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?fit=295%2C81\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?fit=295%2C81\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?fit=295%2C81\" data-orig-size=\"295,81\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-1/formula/\" data-recalc-dims=\"1\" height=\"81\" sizes=\"(max-width: 295px) 100vw, 295px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?resize=295%2C81\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?w=295 295w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?resize=200%2C55 200w\" width=\"295\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Voc\u00ea pode pensar no vi\u00e9s como uma medida de qu\u00e3o f\u00e1cil \u00e9 obter o Perceptron para produzir um 1. Ou para coloc\u00e1-lo em termos mais biol\u00f3gicos, o vi\u00e9s \u00e9 uma medida de qu\u00e3o f\u00e1cil \u00e9 fazer com que o Perceptron dispare. Para um Perceptron com um vi\u00e9s realmente grande, \u00e9 extremamente f\u00e1cil para o Perceptron emitir um 1. Mas se o vi\u00e9s \u00e9 muito negativo, ent\u00e3o \u00e9 dif\u00edcil para o Perceptron emitir um 1. Obviamente, a introdu\u00e7\u00e3o do vi\u00e9s \u00e9 apenas uma pequena mudan\u00e7a em como descrevemos Perceptrons, mas veremos mais adiante que isso leva a outras simplifica\u00e7\u00f5es de nota\u00e7\u00e3o. Por isso, no restante do livro, n\u00e3o usaremos o threshold, usaremos sempre o vi\u00e9s.\n </p>\n <p style=\"text-align: justify;\">\n  Agora come\u00e7a a ficar mais f\u00e1cil compreender o conceito por tr\u00e1s das redes neurais artificiais e isso ser\u00e1 muito \u00fatil quando estudarmos arquiteturas mais avan\u00e7adas!\u00a0Um Perceptron segue o modelo \u201cfeed-forward\u201d, o que significa que as entradas s\u00e3o enviadas para o neur\u00f4nio, processadas e resultam em uma sa\u00edda. No diagrama abaixo, isso significa que a rede (um neur\u00f4nio) l\u00ea da esquerda para a direita.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Neur\u00f4nio\" class=\"aligncenter wp-image-162 size-full\" data-attachment-id=\"162\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Neur\u00f4nio\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?fit=945%2C417\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?fit=300%2C132\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?fit=945%2C417\" data-orig-size=\"945,417\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-1/neuronio-4/\" data-recalc-dims=\"1\" height=\"417\" sizes=\"(max-width: 945px) 100vw, 945px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=945%2C417\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?w=945 945w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=300%2C132 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=768%2C339 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=200%2C88 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/neuronio-1.png?resize=690%2C304 690w\" width=\"945\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O processo de treinamento de um modelo Perceptron consiste em fazer com que o modelo aprenda os valores ideais de pesos e bias. Apresentamos ao modelo os dados de entrada e as poss\u00edveis sa\u00eddas, treinamos o modelo e pesos e bias s\u00e3o aprendidos. Com o modelo treinado, podemos apresentar novos dados de entrada e o modelo ser\u00e1 capaz de prever a sa\u00edda. Veremos isso em breve quando criarmos nosso primeiro modelo usando linguagem Python.\n </p>\n <p style=\"text-align: justify;\">\n  Perceptron \u00e9 uma rede neural de camada \u00fanica e um Perceptron de v\u00e1rias camadas \u00e9 chamado de Rede Neural Artificial. O Perceptron \u00e9 um classificador linear (bin\u00e1rio). Al\u00e9m disso, \u00e9 usado na aprendizagem supervisionada e pode ser usado para classificar os dados de entrada fornecidos.\n </p>\n <p>\n  Mas o Perceptron tem ainda outras caracter\u00edsticas importantes, como a representa\u00e7\u00e3o de condicionais l\u00f3gicos (and, or, xor), problemas com dados n\u00e3o linearmente separ\u00e1veis e as fun\u00e7\u00f5es de ativa\u00e7\u00e3o. Mas esses s\u00e3o temas para o pr\u00f3ximo cap\u00edtulo. At\u00e9 l\u00e1!\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.asimovinstitute.org/neural-network-zoo/\" rel=\"noopener\" target=\"_blank\">\n    The Neural Network Zoo\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n  (alguns trechos extra\u00eddos e traduzidos com autoriza\u00e7\u00e3o do autor\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://michaelnielsen.org/\" rel=\"noopener\" target=\"_blank\">\n    Michael Nielsen\n   </a>\n  </span>\n  )\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-95\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-1/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-95\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-1/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-95\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-1/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-95\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-1/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-1/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-1/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-95-5e0dd04872547\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=95&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-95-5e0dd04872547\" id=\"like-post-wrapper-140353593-95-5e0dd04872547\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "7": "<h1 class=\"entry-title\" id=\"capitulo-7\">\n Cap\u00edtulo 7 \u2013 O Perceptron \u2013 Parte 2\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  O Perceptron \u00e9 um modelo matem\u00e1tico de um neur\u00f4nio biol\u00f3gico. Enquanto nos neur\u00f4nios reais o dendrito recebe sinais el\u00e9tricos dos ax\u00f4nios de outros neur\u00f4nios, no Perceptron estes sinais el\u00e9tricos s\u00e3o representados como valores num\u00e9ricos. Nas sinapses entre dendritos e ax\u00f4nio, os sinais el\u00e9tricos s\u00e3o modulados em v\u00e1rias quantidades. Isso tamb\u00e9m \u00e9 modelado no Perceptron multiplicando cada valor de entrada por um valor chamado peso. Um neur\u00f4nio real dispara um sinal de sa\u00edda somente quando a for\u00e7a total dos sinais de entrada excede um certo limiar. N\u00f3s modelamos esse fen\u00f4meno em um Perceptron calculando a soma ponderada das entradas para representar a for\u00e7a total dos sinais de entrada e aplicando uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o na soma para determinar sua sa\u00edda. Tal como nas redes neurais biol\u00f3gicas, esta sa\u00edda \u00e9 alimentada em outros Perceptrons. Estudamos tudo isso no cap\u00edtulo anterior. Agora vamos continuar nossa discuss\u00e3o sobre o Perceptron compreendendo mais alguns conceitos, que ser\u00e3o fundamentais mais a frente quando estudarmos as arquiteturas avan\u00e7adas de Deep Learning.\n </p>\n <p style=\"text-align: justify;\">\n  Antes de iniciar, vamos definir dois conceitos que voc\u00ea vai ver com frequ\u00eancia daqui em diante, vetor de entrada e vetor de pesos:\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   Vetor de entrada\n  </strong>\n  \u2013\u00a0 todos os valores de entrada de cada Perceptron s\u00e3o coletivamente chamados de vetor de entrada desse Perceptron. Esses s\u00e3o seus dados de entrada.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   Vetor de pesos\n  </strong>\n  \u2013 de forma semelhante, todos os valores de peso de cada Perceptron s\u00e3o coletivamente chamados de vetor de peso desse Perceptron. Iniciamos nossa rede neural artificial com valores aleat\u00f3rios de pesos e durante o treinamento a rede neural aprende os valores de peso ideais. Como veremos, existem muitas formas de realizar esse processo.\n </p>\n <p style=\"text-align: justify;\">\n  Boa parte do trabalho de uma rede neural vai girar em torno das opera\u00e7\u00f5es alg\u00e9bricas entre o vetor de entrada e o vetor de pesos. Em seguida, vamos adicionando outras camadas matem\u00e1ticas ou estat\u00edsticas para realizar diferentes opera\u00e7\u00f5es, de acordo com o problema que estamos tentando resolver com o modelo de rede neural. Voc\u00ea vai perceber que tudo n\u00e3o passa de Matem\u00e1tica, que pode ser implementada com linguagens de programa\u00e7\u00e3o, grandes conjuntos de dados e processamento paralelo, para formar sistemas de Intelig\u00eancia Artificial.\n </p>\n <p>\n </p>\n <h2 style=\"text-align: justify;\">\n  Mas o que um Perceptron pode fazer afinal?\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/capitulo-6-o-perceptron-parte-1/\" rel=\"noopener\" target=\"_blank\">\n    No cap\u00edtulo anterior\n   </a>\n  </span>\n  descrevemos os Perceptrons como um m\u00e9todo para pesar evid\u00eancias a fim de tomar decis\u00f5es. Outra forma em que os Perceptrons podem ser usados \u00e9 para calcular as fun\u00e7\u00f5es l\u00f3gicas elementares tais como AND, OR e NAND (caso tenha d\u00favidas sobre as opera\u00e7\u00f5es l\u00f3gicas, consulte as refer\u00eancias ao final deste cap\u00edtulo). Por exemplo, suponha que tenhamos um Perceptron com duas entradas, cada uma com peso -2 e um vi\u00e9s de 3. Aqui est\u00e1 o nosso Perceptron:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Perceptron\" class=\"aligncenter size-full wp-image-177\" data-attachment-id=\"177\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Perceptron\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?fit=250%2C104\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?fit=250%2C104\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?fit=250%2C104\" data-orig-size=\"250,104\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-2/perceptron-2/\" data-recalc-dims=\"1\" height=\"104\" sizes=\"(max-width: 250px) 100vw, 250px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?resize=250%2C104\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?w=250 250w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/perceptron.png?resize=200%2C83 200w\" width=\"250\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Ent\u00e3o vemos que a entrada 00 produziria a sa\u00edda 1, uma vez que (-2) * 0 + (- 2) * 0 + 3 = 3, \u00e9 positivo (resultado positivo, gera sa\u00edda 1 do Perceptron, lembra do cap\u00edtulo anterior?). Aqui, inclu\u00edmos o s\u00edmbolo * para tornar as multiplica\u00e7\u00f5es expl\u00edcitas. C\u00e1lculos similares mostram que as entradas 01 e 10 produzem a sa\u00edda 1. Mas a entrada 11 produz a sa\u00edda 0, uma vez que (-2) * 1 + (- 2) * 1 + 3 = -1, \u00e9 negativo. E assim nosso Perceptron implementa um \u201cport\u00e3o\u201d NAND, ou uma opera\u00e7\u00e3o l\u00f3gica bin\u00e1ria NAND.\n </p>\n <p style=\"text-align: justify;\">\n  O exemplo NAND mostra que podemos usar Perceptrons para calcular fun\u00e7\u00f5es l\u00f3gicas simples. Na verdade, podemos usar redes de Perceptrons para calcular qualquer fun\u00e7\u00e3o l\u00f3gica. A raz\u00e3o \u00e9 que o port\u00e3o NAND \u00e9 universal para computa\u00e7\u00e3o, ou seja, podemos construir qualquer computa\u00e7\u00e3o com port\u00f5es NAND.\n </p>\n <p style=\"text-align: justify;\">\n  Uma rede de Perceptrons pode ser usada para simular um circuito contendo muitos port\u00f5es NAND. E como os port\u00f5es NAND s\u00e3o universais para a computa\u00e7\u00e3o, segue-se que os Perceptrons tamb\u00e9m s\u00e3o universais para a computa\u00e7\u00e3o. Considerando que o Perceptron \u00e9 o modelo mais simples de rede neural, imagine o que pode ser feito com modelos bem mais avan\u00e7ados! Acertou se voc\u00ea pensou em Intelig\u00eancia Artificial.\n </p>\n <p style=\"text-align: justify;\">\n  A universalidade computacional dos Perceptrons \u00e9 simultaneamente reconfortante e decepcionante. \u00c9 reconfortante porque nos diz que redes de Perceptrons podem ser t\u00e3o poderosas como qualquer outro dispositivo de computa\u00e7\u00e3o. Mas tamb\u00e9m \u00e9 decepcionante, porque parece que os Perceptrons s\u00e3o meramente um novo tipo de port\u00e3o NAND. Isso n\u00e3o \u00e9 uma grande noticia!\n </p>\n <p style=\"text-align: justify;\">\n  No entanto, a situa\u00e7\u00e3o \u00e9 melhor do que esta vis\u00e3o sugere. Acontece que podemos conceber algoritmos de aprendizado que podem ajustar automaticamente os pesos e os vieses de uma rede de neur\u00f4nios artificiais. Este ajuste ocorre em resposta a est\u00edmulos externos, sem interven\u00e7\u00e3o direta de um programador. Esses algoritmos de aprendizagem nos permitem usar neur\u00f4nios artificiais de uma maneira que \u00e9 radicalmente diferente dos port\u00f5es l\u00f3gicos convencionais. Em vez de colocar explicitamente um circuito de NAND e outros port\u00f5es, nossas redes neurais podem simplesmente aprender a resolver problemas, \u00e0s vezes problemas em que seriam extremamente dif\u00edceis de projetar diretamente usando um circuito convencional de l\u00f3gica.\n </p>\n <p>\n </p>\n <h2>\n  Opera\u00e7\u00f5es L\u00f3gicas e Regi\u00f5es Linearmente Separ\u00e1veis\n </h2>\n <p style=\"text-align: justify;\">\n  Conforme mencionado acima, um Perceptron calcula a soma ponderada dos valores de entrada. Por simplicidade, suponhamos que existem dois valores de entrada, x e y para um certo Perceptron P. Vamos definir os pesos de x e y, como sendo A e B, respectivamente. A soma ponderada pode ser representada como: A x + B y.\n </p>\n <p style=\"text-align: justify;\">\n  Uma vez que o Perceptron produz um valor n\u00e3o-zero somente quando a soma ponderada excede um certo limite C, pode-se escrever a sa\u00edda deste Perceptron da seguinte maneira:\n </p>\n <p>\n  <img alt=\"Regra Perceptron\" class=\"aligncenter wp-image-192 size-full\" data-attachment-id=\"192\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Regra Perceptron\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?fit=882%2C256\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?fit=300%2C87\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?fit=882%2C256\" data-orig-size=\"882,256\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-2/regra/\" data-recalc-dims=\"1\" height=\"256\" sizes=\"(max-width: 882px) 100vw, 882px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=882%2C256\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?w=882 882w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=300%2C87 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=768%2C223 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=200%2C58 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/regra.png?resize=690%2C200 690w\" width=\"882\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Considerando que A x + B y &gt; C e A x + B y &lt; C s\u00e3o as duas regi\u00f5es no plano xy separadas pela linha A x + B y + C = 0, e se considerarmos ainda a entrada (x, y) como um ponto em um plano, ent\u00e3o o Perceptron realmente nos diz qual regi\u00e3o no plano a que esse ponto pertence. Tais regi\u00f5es, uma vez que s\u00e3o separadas por uma \u00fanica linha, s\u00e3o chamadas de regi\u00f5es linearmente separ\u00e1veis.\n </p>\n <p style=\"text-align: justify;\">\n  Um \u00fanico Perceptron consegue resolver somente fun\u00e7\u00f5es linearmente separ\u00e1veis. Em fun\u00e7\u00f5es n\u00e3o linearmente separ\u00e1veis, o Perceptron n\u00e3o consegue gerar um hiperplano, esta linha nos gr\u00e1ficos abaixo, para separar os dados. A quest\u00e3o \u00e9 que no mundo real raramente os dados s\u00e3o linearmente separ\u00e1veis, fazendo com o que o Perceptron n\u00e3o seja muito \u00fatil para atividades pr\u00e1ticas (mas sendo ideal para iniciar o estudo em redes neurais artificiais). E como separamos os dados n\u00e3o linearmente separ\u00e1veis? Continue acompanhando este livro e voc\u00ea ir\u00e1 descobrir.\n </p>\n <p>\n  <img alt=\"Linear e N\u00e3o-Linear\" class=\"aligncenter wp-image-194 size-full\" data-attachment-id=\"194\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Linear e N\u00e3o-Linear\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?fit=771%2C369\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?fit=300%2C144\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?fit=771%2C369\" data-orig-size=\"771,369\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-2/linsep_new/\" data-recalc-dims=\"1\" height=\"369\" sizes=\"(max-width: 771px) 100vw, 771px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=771%2C369\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?w=771 771w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=300%2C144 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=768%2C368 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=200%2C96 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/linsep_new.png?resize=690%2C330 690w\" width=\"771\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Mas ainda assim o Perceptron tem sua utilidade, porque resulta em algumas fun\u00e7\u00f5es l\u00f3gicas, como os operadores booleanos AND, OR e NOT, que s\u00e3o linearmente separ\u00e1veis, isto \u00e9, eles podem ser realizadas usando um \u00fanico Perceptron. Podemos ilustrar porque eles s\u00e3o linearmente separ\u00e1veis ao tra\u00e7ar cada um deles em um gr\u00e1fico:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Fun\u00e7\u00f5es L\u00f3gicas\" class=\"aligncenter wp-image-183 size-medium\" data-attachment-id=\"183\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Fun\u00e7\u00f5es L\u00f3gicas\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?fit=816%2C172\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?fit=300%2C63\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?fit=816%2C172\" data-orig-size=\"816,172\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-2/functions/\" data-recalc-dims=\"1\" height=\"63\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=300%2C63\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=300%2C63 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=768%2C162 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=200%2C42 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?resize=690%2C145 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/functions.png?w=816 816w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Nos gr\u00e1ficos acima, os dois eixos s\u00e3o as entradas que podem levar o valor de 0 ou 1 e os n\u00fameros no gr\u00e1fico s\u00e3o a sa\u00edda esperada para uma entrada espec\u00edfica. Usando um vetor de peso apropriado para cada caso, um \u00fanico Perceptron pode executar todas essas fun\u00e7\u00f5es.\n </p>\n <p style=\"text-align: justify;\">\n  No entanto, nem todos os operadores de l\u00f3gica s\u00e3o linearmente separ\u00e1veis. Por exemplo, o operador XOR n\u00e3o \u00e9 linearmente separ\u00e1vel e n\u00e3o pode ser alcan\u00e7ado por um \u00fanico Perceptron. No entanto, esse problema poderia ser superado usando mais de um Perceptron organizado em redes neurais feed-forward, que veremos mais a frente nos pr\u00f3ximos cap\u00edtulos.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"xor\" class=\"aligncenter size-full wp-image-184\" data-attachment-id=\"184\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"xor\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?fit=108%2C71\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?fit=108%2C71\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?fit=108%2C71\" data-orig-size=\"108,71\" data-permalink=\"http://deeplearningbook.com.br/o-perceptron-parte-2/xor/\" data-recalc-dims=\"1\" height=\"71\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/xor.jpg?resize=108%2C71\" width=\"108\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Uma vez que \u00e9 imposs\u00edvel desenhar uma linha para dividir as regi\u00f5es contendo 1 ou 0, a fun\u00e7\u00e3o XOR n\u00e3o \u00e9 linearmente separ\u00e1vel, conforme pode ser visto no gr\u00e1fico acima.\n </p>\n <p style=\"text-align: justify;\">\n  Agora fica mais f\u00e1cil compreender porque precisamos de arquiteturas mais avan\u00e7adas de redes neurais artificiais, uma vez que temos problemas complexos no mundo real, como Vis\u00e3o Computacional, Processamento de Linguagem Natural, Tradu\u00e7\u00e3o, Detec\u00e7\u00e3o de Fraudes, Classifica\u00e7\u00e3o e muitos outros. E veremos essas arquiteturas em detalhes. Mas antes, precisamos falar sobre um componente fundamental das redes neurais, a Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o. N\u00e3o perca o pr\u00f3ximo cap\u00edtulo. At\u00e9 l\u00e1.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Porta_AND\" rel=\"noopener\" target=\"_blank\">\n    Opera\u00e7\u00e3o L\u00f3gica AND\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Porta_OR\" rel=\"noopener\" target=\"_blank\">\n    Opera\u00e7\u00e3o L\u00f3gica OR\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Porta_NAND\" rel=\"noopener\" target=\"_blank\">\n    Opera\u00e7\u00e3o L\u00f3gica NAND\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Porta_XOR\" rel=\"noopener\" target=\"_blank\">\n    Opera\u00e7\u00e3o L\u00f3gica XOR\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n  (alguns trechos extra\u00eddos e traduzidos com autoriza\u00e7\u00e3o do autor\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://michaelnielsen.org/\" rel=\"noopener\" target=\"_blank\">\n    Michael Nielsen\n   </a>\n  </span>\n  )\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-175\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-2/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-175\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-2/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-175\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-2/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-175\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-2/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-2/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-perceptron-parte-2/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-175-5e0dd04a5505a\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=175&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-175-5e0dd04a5505a\" id=\"like-post-wrapper-140353593-175-5e0dd04a5505a\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "8": "<h1 class=\"entry-title\" id=\"capitulo-8\">\n Cap\u00edtulo 8 \u2013 Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Neste cap\u00edtulo estudaremos um importante componente de uma rede neural artificial, a Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o. Este cap\u00edtulo \u00e9 uma introdu\u00e7\u00e3o ao tema e voltaremos a ele mais adiante quando estudarmos as arquiteturas avan\u00e7adas de\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/capitulo-1-deep-learning-a-tempestade-perfeita/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Deep Learning\n   </a>\n  </span>\n  . Este cap\u00edtulo pode ser um pouco desafiador, pois come\u00e7aremos a introduzir conceitos mais avan\u00e7ados, que ser\u00e3o muito \u00fateis na sequ\u00eancia dos cap\u00edtulos. Relaxe, fa\u00e7a a leitura e aprenda um pouco mais sobre redes neurais artificiais.\n </p>\n <p style=\"text-align: justify;\">\n  Antes de mergulhar nos detalhes das fun\u00e7\u00f5es de ativa\u00e7\u00e3o, vamos fazer uma pequena revis\u00e3o do que s\u00e3o\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/capitulo-3-o-que-sao-redes-neurais-artificiais-profundas/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    redes neurais artificiais\n   </a>\n  </span>\n  e como funcionam. Uma rede neural \u00e9 um mecanismo de aprendizado de m\u00e1quina (Machine Learning) muito poderoso que imita basicamente como um c\u00e9rebro humano aprende. O c\u00e9rebro recebe o est\u00edmulo do mundo exterior, faz o processamento e gera o resultado.\u00a0\u00c0 medida que a tarefa se torna complicada, v\u00e1rios neur\u00f4nios formam uma rede complexa, transmitindo informa\u00e7\u00f5es entre si. Usando uma rede neural artificial, tentamos imitar um comportamento semelhante. A rede que voc\u00ea v\u00ea abaixo \u00e9 uma rede neural artificial composta de neur\u00f4nios interligados.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Neural Network\" class=\"aligncenter wp-image-215 size-full\" data-attachment-id=\"215\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Neural Network\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?fit=572%2C312\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?fit=300%2C164\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?fit=572%2C312\" data-orig-size=\"572,312\" data-permalink=\"http://deeplearningbook.com.br/funcao-de-ativacao/nn/\" data-recalc-dims=\"1\" height=\"312\" sizes=\"(max-width: 572px) 100vw, 572px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?resize=572%2C312\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?w=572 572w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?resize=300%2C164 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/nn.png?resize=200%2C109 200w\" width=\"572\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Os c\u00edrculos negros na imagem acima s\u00e3o neur\u00f4nios. Cada neur\u00f4nio \u00e9 caracterizado pelo peso, bias e a fun\u00e7\u00e3o de ativa\u00e7\u00e3o. Os dados de entrada s\u00e3o alimentados na camada de entrada. Os neur\u00f4nios fazem uma transforma\u00e7\u00e3o linear na entrada pelos pesos e bias. A transforma\u00e7\u00e3o n\u00e3o linear \u00e9 feita pela fun\u00e7\u00e3o de ativa\u00e7\u00e3o. A informa\u00e7\u00e3o se move da camada de entrada para as camadas ocultas. As camadas ocultas fazem o processamento e enviam a sa\u00edda final para a camada de sa\u00edda. Este \u00e9 o movimento direto da informa\u00e7\u00e3o conhecido como propaga\u00e7\u00e3o direta. Mas e se o resultado gerado estiver longe do valor esperado? Em uma rede neural, atualizar\u00edamos os pesos e bias dos neur\u00f4nios com base no erro. Este processo \u00e9 conhecido como backpropagation. Uma vez que todos os dados passaram por este processo, os pesos e bias finais s\u00e3o usados para previs\u00f5es.\n </p>\n <p style=\"text-align: justify;\">\n  Calma, calma, calma. Muita informa\u00e7\u00e3o em um \u00fanico par\u00e1grafo, eu sei! Vamos por partes. As entradas, os pesos e bias n\u00f3s j\u00e1 discutimos nos cap\u00edtulos anteriores. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o vamos discutir agora e a propaga\u00e7\u00e3o direta e o backpropagation discutimos nos pr\u00f3ximos cap\u00edtulos!\n </p>\n <p>\n </p>\n <h2>\n  Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o\n </h2>\n <p style=\"text-align: justify;\">\n  Os algoritmos de aprendizagem s\u00e3o fant\u00e1sticos. Mas como podemos elaborar esses algoritmos para uma rede neural artificial? Suponhamos que tenhamos uma rede de Perceptrons que gostar\u00edamos de usar para aprender a resolver algum problema. Por exemplo, as entradas para a rede poderiam ser os dados de pixel de uma imagem digitalizada, escrita \u00e0 m\u00e3o, de um d\u00edgito. Gostar\u00edamos que a rede aprendesse pesos e bias para que a sa\u00edda da rede classifique corretamente o d\u00edgito. Para ver como a aprendizagem pode funcionar, suponha que fa\u00e7amos uma pequena altera\u00e7\u00e3o em algum peso (ou bias) na rede. O que queremos \u00e9 que esta pequena mudan\u00e7a de peso cause apenas uma pequena altera\u00e7\u00e3o correspondente na sa\u00edda da rede. Como veremos em um momento, esta propriedade tornar\u00e1 poss\u00edvel a aprendizagem. Esquematicamente, aqui est\u00e1 o que queremos (obviamente, esta rede \u00e9 muito simples para fazer reconhecimento de escrita, mas fique tranquilo que veremos redes bem mais complexas).\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Esquema\" class=\"aligncenter size-full wp-image-207\" data-attachment-id=\"207\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Esquema\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?fit=487%2C270\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?fit=300%2C166\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?fit=487%2C270\" data-orig-size=\"487,270\" data-permalink=\"http://deeplearningbook.com.br/funcao-de-ativacao/tikz8/\" data-recalc-dims=\"1\" height=\"270\" sizes=\"(max-width: 487px) 100vw, 487px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?resize=487%2C270\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?w=487 487w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?resize=300%2C166 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/tikz8.png?resize=200%2C111 200w\" width=\"487\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Se fosse verdade que uma pequena altera\u00e7\u00e3o em um peso (ou bias) fizesse com que tiv\u00e9ssemos apenas uma pequena altera\u00e7\u00e3o no resultado, ent\u00e3o poder\u00edamos usar esse fato para modificar os pesos e os valores de bias para que a nossa rede se comporte mais da maneira que queremos. Por exemplo, suponha que a rede classificasse equivocadamente uma imagem como \u201c8\u201d quando deveria ser um \u201c9\u201d. Podemos descobrir como fazer uma pequena mudan\u00e7a nos pesos e bias para que a rede fique um pouco mais pr\u00f3xima da classifica\u00e7\u00e3o da imagem como \u201c9\u201d. E ent\u00e3o, repetir\u00edamos isso, mudando os pesos e os valores de bias repetidamente para produzir melhor e melhor resultado. A rede estaria aprendendo.\n </p>\n <p style=\"text-align: justify;\">\n  O problema \u00e9 que isso n\u00e3o \u00e9 o que acontece quando nossa rede cont\u00e9m apenas Perceptrons, conforme estudamos nos cap\u00edtulos anteriores. De fato, uma pequena altera\u00e7\u00e3o nos pesos de um \u00fanico Perceptron na rede pode, por vezes, fazer com que a sa\u00edda desse Perceptron mude completamente, digamos de 0 a 1. Essa mudan\u00e7a pode ent\u00e3o causar o comportamento do resto da rede mudar completamente de uma maneira muito complicada. Ent\u00e3o, enquanto o seu \u201c9\u201d pode agora ser classificado corretamente, o comportamento da rede em todas as outras imagens provavelmente mudar\u00e1 completamente de maneira dif\u00edcil de controlar. Talvez haja uma maneira inteligente de resolver esse problema. Sim, h\u00e1. E \u00e9 conhecida como fun\u00e7\u00e3o de ativa\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Podemos superar esse problema atrav\u00e9s da introdu\u00e7\u00e3o de um componente matem\u00e1tico em nosso neur\u00f4nio artificial, chamado fun\u00e7\u00e3o de ativa\u00e7\u00e3o. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o permitem que pequenas mudan\u00e7as nos pesos e bias causem apenas uma pequena altera\u00e7\u00e3o no output. Esse \u00e9 o fato crucial que permitir\u00e1 que uma rede de neur\u00f4nios artificiais aprenda.\n </p>\n <p style=\"text-align: justify;\">\n  Vejamos como isso funciona:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o\" class=\"aligncenter wp-image-211 size-large\" data-attachment-id=\"211\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?fit=1024%2C426\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?fit=300%2C125\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?fit=1100%2C458\" data-orig-size=\"1100,458\" data-permalink=\"http://deeplearningbook.com.br/funcao-de-ativacao/activation-function-2/\" data-recalc-dims=\"1\" height=\"426\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=1024%2C426\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=1024%2C426 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=300%2C125 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=768%2C320 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=200%2C83 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?resize=690%2C287 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/activation-function-1.png?w=1100 1100w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o um elemento extremamente importante das redes neurais artificiais. Elas basicamente decidem se um neur\u00f4nio deve ser ativado ou n\u00e3o. Ou seja, se a informa\u00e7\u00e3o que o neur\u00f4nio est\u00e1 recebendo \u00e9 relevante para a informa\u00e7\u00e3o fornecida ou deve ser ignorada. Veja na f\u00f3rmula abaixo como a fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 mais uma camada matem\u00e1tica no processamento.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o\" class=\"aligncenter wp-image-217 size-full\" data-attachment-id=\"217\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?fit=406%2C49\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?fit=300%2C36\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?fit=406%2C49\" data-orig-size=\"406,49\" data-permalink=\"http://deeplearningbook.com.br/funcao-de-ativacao/act/\" data-recalc-dims=\"1\" height=\"49\" sizes=\"(max-width: 406px) 100vw, 406px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?resize=406%2C49\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?w=406 406w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?resize=300%2C36 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/act.png?resize=200%2C24 200w\" width=\"406\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 a transforma\u00e7\u00e3o n\u00e3o linear que fazemos ao longo do sinal de entrada. Esta sa\u00edda transformada \u00e9 ent\u00e3o enviada para a pr\u00f3xima camada de neur\u00f4nios como entrada.\u00a0Quando n\u00e3o temos a fun\u00e7\u00e3o de ativa\u00e7\u00e3o, os pesos e bias simplesmente fazem uma transforma\u00e7\u00e3o linear. Uma equa\u00e7\u00e3o linear \u00e9 simples de resolver, mas \u00e9 limitada na sua capacidade de resolver problemas complexos. Uma rede neural sem fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 essencialmente apenas um modelo de regress\u00e3o linear. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o faz a transforma\u00e7\u00e3o n\u00e3o-linear nos dados de entrada, tornando-o capaz de aprender e executar tarefas mais complexas. Queremos que nossas redes neurais funcionem em tarefas complicadas, como tradu\u00e7\u00f5es de idiomas (Processamento de Linguagem Natural) e classifica\u00e7\u00f5es de imagens (Vis\u00e3o Computacional). As transforma\u00e7\u00f5es lineares nunca seriam capazes de executar tais tarefas.\n </p>\n <p style=\"text-align: justify;\">\n  As fun\u00e7\u00f5es de ativa\u00e7\u00e3o tornam poss\u00edvel a propaga\u00e7\u00e3o posterior desde que os gradientes sejam fornecidos juntamente com o erro para atualizar os pesos e bias. Sem a fun\u00e7\u00e3o n\u00e3o linear diferenci\u00e1vel, isso n\u00e3o seria poss\u00edvel. Caso o termo gradiente n\u00e3o seja familiar, aguarde os pr\u00f3ximos cap\u00edtulos, quando vamos explicar este conceito em detalhes, visto que ele \u00e9 a ess\u00eancia do processo de aprendizagem em redes neurais artificiais.\n </p>\n <p style=\"text-align: justify;\">\n  Mas n\u00e3o existe apenas um tipo de fun\u00e7\u00e3o de ativa\u00e7\u00e3o. Na verdade existem v\u00e1rios, cada qual a ser usado em diferentes situa\u00e7\u00f5es. Vamos a uma breve descri\u00e7\u00e3o dos tipos mais populares.\n </p>\n <p>\n </p>\n <h2>\n  Tipos Populares de Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o\n </h2>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 um componente matem\u00e1tico inclu\u00eddo na estrutura de redes neurais artificiais a fim de permitir a solu\u00e7\u00e3o de problemas complexos. Existem diversos tipos de fun\u00e7\u00f5es de ativa\u00e7\u00e3o e esta \u00e9 uma \u00e1rea de pesquisa ativa, \u00e0 medida que a Intelig\u00eancia Artificial evolui (n\u00e3o \u00e9 maravilhoso estar participando desta evolu\u00e7\u00e3o, que vai transformar completamente o mundo?). Vejamos quais s\u00e3o os tipos mais populares.\n </p>\n <p>\n </p>\n <h3>\n  Fun\u00e7\u00e3o de Etapa Bin\u00e1ria (Binary Step Function)\n </h3>\n <p style=\"text-align: justify;\">\n  A primeira coisa que vem \u00e0 nossa mente quando temos uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o seria um classificador baseado em limiar (threshold), ou seja, se o neur\u00f4nio deve ou n\u00e3o ser ativado. Se o valor Y estiver acima de um valor de limite determinado, ative o neur\u00f4nio sen\u00e3o deixa desativado. Simples! Essa seria a regra:\n </p>\n <p>\n  <strong>\n   f(x) = 1, x&gt;=0\n  </strong>\n </p>\n <p>\n  <strong>\n   f(x) = 0, x&lt;0\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o de etapa bin\u00e1ria \u00e9 isso mesmo, extremamente simples. Ela pode ser usada ao criar um classificador bin\u00e1rio. Quando simplesmente precisamos dizer sim ou n\u00e3o para uma \u00fanica classe, a fun\u00e7\u00e3o de etapa seria a melhor escolha, pois ativaria o neur\u00f4nio ou deixaria zero.\n </p>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o \u00e9 mais te\u00f3rica do que pr\u00e1tica, pois, na maioria dos casos, classificamos os dados em v\u00e1rias classes do que apenas uma \u00fanica classe. A fun\u00e7\u00e3o de etapa n\u00e3o seria capaz de fazer isso.\n </p>\n <p style=\"text-align: justify;\">\n  Al\u00e9m disso, o gradiente da fun\u00e7\u00e3o de etapa \u00e9 zero. Isso faz com que a fun\u00e7\u00e3o de etapa n\u00e3o seja t\u00e3o \u00fatil durante o backpropagation quando os gradientes das fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o enviados para c\u00e1lculos de erro para melhorar e otimizar os resultados. O gradiente da fun\u00e7\u00e3o de etapa reduz tudo para zero e a melhoria dos modelos realmente n\u00e3o acontece. Lembrando, mais uma vez, que veremos em detalhes os conceitos de gradiente e backpropagation mais adiante, nos pr\u00f3ximos cap\u00edtulos!\n </p>\n <p>\n </p>\n <h3>\n  Fun\u00e7\u00e3o Linear\n </h3>\n <p style=\"text-align: justify;\">\n  N\u00f3s vimos o problema com a fun\u00e7\u00e3o step, o gradiente sendo zero, \u00e9 imposs\u00edvel atualizar o gradiente durante a backpropagation. Em vez de uma fun\u00e7\u00e3o de passo simples, podemos tentar usar uma fun\u00e7\u00e3o linear. Podemos definir a fun\u00e7\u00e3o como:\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   f(x) = ax\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  A derivada de uma fun\u00e7\u00e3o linear \u00e9 constante, isto \u00e9, n\u00e3o depende do valor de entrada x. Isso significa que toda vez que fazemos backpropagation, o gradiente seria o mesmo. E este \u00e9 um grande problema, n\u00e3o estamos realmente melhorando o erro, j\u00e1 que o gradiente \u00e9 praticamente o mesmo. E n\u00e3o apenas suponha que estamos tentando realizar uma tarefa complicada para a qual precisamos de m\u00faltiplas camadas em nossa rede. Agora, se cada camada tiver uma transforma\u00e7\u00e3o linear, n\u00e3o importa quantas camadas n\u00f3s tenhamos, a sa\u00edda final n\u00e3o \u00e9 sen\u00e3o uma transforma\u00e7\u00e3o linear da entrada. Portanto, a fun\u00e7\u00e3o linear pode ser ideal para tarefas simples, onde a interpretabilidade \u00e9 altamente desejada.\n </p>\n <p>\n </p>\n <h3>\n  Sigm\u00f3ide\n </h3>\n <p>\n  Sigm\u00f3ide \u00e9 uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o amplamente utilizada. \u00c9 da forma:\n </p>\n <p>\n  <strong>\n   f (x) = 1 / (1 + e ^ -x)\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  Esta \u00e9 uma fun\u00e7\u00e3o suave e \u00e9 continuamente diferenci\u00e1vel. A maior vantagem sobre a fun\u00e7\u00e3o de etapa e a fun\u00e7\u00e3o linear \u00e9 que n\u00e3o \u00e9 linear. Esta \u00e9 uma caracter\u00edstica incrivelmente interessante da fun\u00e7\u00e3o sigm\u00f3ide. Isto significa essencialmente que quando eu tenho v\u00e1rios neur\u00f4nios com fun\u00e7\u00e3o sigm\u00f3ide como fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u2013 a sa\u00edda tamb\u00e9m n\u00e3o \u00e9 linear. A fun\u00e7\u00e3o varia de 0 a 1 tendo um formato S.\n </p>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o essencialmente tenta empurrar os valores de Y para os extremos. Esta \u00e9 uma qualidade muito desej\u00e1vel quando tentamos classificar os valores para uma classe espec\u00edfica.\n </p>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o sigm\u00f3ide ainda \u00e9 amplamente utilizada at\u00e9 hoje, mas ainda temos problemas que precisamos abordar. Com a sigm\u00f3ide temos problemas quando os gradientes se tornam muito pequenos. Isso significa que o gradiente est\u00e1 se aproximando de zero e a rede n\u00e3o est\u00e1 realmente aprendendo.\n </p>\n <p style=\"text-align: justify;\">\n  Outro problema que a fun\u00e7\u00e3o sigm\u00f3ide sofre \u00e9 que os valores variam apenas de 0 a 1. Esta medida que a fun\u00e7\u00e3o sigm\u00f3ide n\u00e3o \u00e9 sim\u00e9trica em torno da origem e os valores recebidos s\u00e3o todos positivos. Nem sempre desejamos que os valores enviados ao pr\u00f3ximo neur\u00f4nio sejam todos do mesmo sinal. Isso pode ser abordado pela amplia\u00e7\u00e3o da fun\u00e7\u00e3o sigm\u00f3ide. Isso \u00e9 exatamente o que acontece na fun\u00e7\u00e3o tanh.\n </p>\n <p>\n </p>\n <h3>\n  Tanh\n </h3>\n <p>\n  A fun\u00e7\u00e3o tanh \u00e9 muito semelhante \u00e0 fun\u00e7\u00e3o sigm\u00f3ide. Na verdade, \u00e9 apenas uma vers\u00e3o escalonada da fun\u00e7\u00e3o sigm\u00f3ide.\n </p>\n <p>\n  <strong>\n   Tanh (x) = 2sigmoides (2x) -1\n  </strong>\n </p>\n <p>\n  Pode ser escrito diretamente como:\n </p>\n <p>\n  <strong>\n   tanh (x) = 2 / (1 + e ^ (- 2x)) -1\n  </strong>\n </p>\n <p>\n  Tanh funciona de forma semelhante \u00e0 fun\u00e7\u00e3o sigm\u00f3ide, mas sim sim\u00e9trico em rela\u00e7\u00e3o \u00e0 origem. varia de -1 a 1.\n </p>\n <p style=\"text-align: justify;\">\n  Basicamente, soluciona o nosso problema dos valores, sendo todos do mesmo sinal. Todas as outras propriedades s\u00e3o as mesmas da fun\u00e7\u00e3o sigmoide. \u00c9 cont\u00ednuo e diferenci\u00e1vel em todos os pontos. A fun\u00e7\u00e3o n\u00e3o \u00e9 linear, ent\u00e3o podemos fazer o backpropagation facilmente nos erros.\n </p>\n <h3>\n </h3>\n <h3>\n  ReLU\n </h3>\n <p>\n  A fun\u00e7\u00e3o ReLU \u00e9 a unidade linear rectificada. \u00c9 definida como:\n </p>\n <p>\n  <strong>\n   f(x) = max (0, x)\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  ReLU \u00e9 a fun\u00e7\u00e3o de ativa\u00e7\u00e3o mais amplamente utilizada ao projetar redes neurais atualmente. Primeiramente, a fun\u00e7\u00e3o ReLU \u00e9 n\u00e3o linear, o que significa que podemos facilmente copiar os erros para tr\u00e1s e ter v\u00e1rias camadas de neur\u00f4nios ativados pela fun\u00e7\u00e3o ReLU.\n </p>\n <p style=\"text-align: justify;\">\n  A principal vantagem de usar a fun\u00e7\u00e3o ReLU sobre outras fun\u00e7\u00f5es de ativa\u00e7\u00e3o \u00e9 que ela n\u00e3o ativa todos os neur\u00f4nios ao mesmo tempo. O que isto significa ? Se voc\u00ea olhar para a fun\u00e7\u00e3o ReLU e a entrada for negativa, ela ser\u00e1 convertida em zero e o neur\u00f4nio n\u00e3o ser\u00e1 ativado. Isso significa que, ao mesmo tempo, apenas alguns neur\u00f4nios s\u00e3o ativados, tornando a rede esparsa e eficiente e f\u00e1cil para a computa\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Mas ReLU tamb\u00e9m pode ter problemas com os gradientes que se deslocam em dire\u00e7\u00e3o a zero. Mas quando temos um problema, sempre podemos pensar em uma solu\u00e7\u00e3o. Ali\u00e1s, isso \u00e9 o que as empresas mais procuram nos dias de hoje: \u201cresolvedores de problemas\u201d. Seja um e sua empregabilidade estar\u00e1 garantida!\n </p>\n <h3>\n </h3>\n <h3>\n  Leaky ReLU\n </h3>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o Leaky ReLU n\u00e3o passa de uma vers\u00e3o melhorada da fun\u00e7\u00e3o ReLU. Na fun\u00e7\u00e3o ReLU, o gradiente \u00e9 0 para x &lt; 0, o que fez os neur\u00f4nios morrerem por ativa\u00e7\u00f5es nessa regi\u00e3o. Leaky ReLU ajuda a resolver este problema. Em vez de definir a fun\u00e7\u00e3o Relu como 0 para x inferior a 0, definimos como um pequeno componente linear de x. Pode ser definido como:\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   f(x) = ax, x &lt; 0\n  </strong>\n  <br/>\n  <strong>\n   f(x) = x, x &gt; = 0\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  O que fizemos aqui \u00e9 que simplesmente substitu\u00edmos a linha horizontal por uma linha n\u00e3o-zero, n\u00e3o horizontal. Aqui um \u00e9 um valor pequeno como 0,01 ou algo parecido. A principal vantagem de substituir a linha horizontal \u00e9 remover o gradiente zero.\n </p>\n <h3>\n </h3>\n <h3>\n  Softmax\n </h3>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o softmax tamb\u00e9m \u00e9 um tipo de fun\u00e7\u00e3o sigm\u00f3ide, mas \u00e9 \u00fatil quando tentamos lidar com problemas de classifica\u00e7\u00e3o. A fun\u00e7\u00e3o sigm\u00f3ide como vimos anteriormente \u00e9 capaz de lidar com apenas duas classes. O que devemos fazer quando estamos tentando lidar com v\u00e1rias classes? Apenas classificar sim ou n\u00e3o para uma \u00fanica classe n\u00e3o ajudaria. A fun\u00e7\u00e3o softmax transforma as sa\u00eddas para cada classe para valores entre 0 e 1 e tamb\u00e9m divide pela soma das sa\u00eddas. Isso essencialmente d\u00e1 a probabilidade de a entrada estar em uma determinada classe. Pode ser definido como:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"Softmax\" class=\"aligncenter size-full wp-image-219\" data-attachment-id=\"219\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Softmax\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?fit=274%2C56\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?fit=274%2C56\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?fit=274%2C56\" data-orig-size=\"274,56\" data-permalink=\"http://deeplearningbook.com.br/funcao-de-ativacao/softmax/\" data-recalc-dims=\"1\" height=\"56\" sizes=\"(max-width: 274px) 100vw, 274px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?resize=274%2C56\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?w=274 274w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/softmax.png?resize=200%2C41 200w\" width=\"274\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Digamos, por exemplo, que temos as sa\u00eddas como [1.2, 0.9, 0.75], quando aplicamos a fun\u00e7\u00e3o softmax, obter\u00edamos [0.42, 0.31, 0.27]. Ent\u00e3o, agora podemos us\u00e1-los como probabilidades de que o valor seja de cada classe.\n </p>\n <p style=\"text-align: justify;\">\n  A fun\u00e7\u00e3o softmax \u00e9 idealmente usada na camada de sa\u00edda do classificador, onde realmente estamos tentando gerar as probabilidades para definir a classe de cada entrada.\n </p>\n <p>\n </p>\n <h3>\n  Escolhendo a Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o Correta\n </h3>\n <p style=\"text-align: justify;\">\n  Ufa! Muita coisa, n\u00e3o? E ainda n\u00e3o vimos as quest\u00f5es matem\u00e1ticas envolvidas nessas fun\u00e7\u00f5es. Mas n\u00e3o tenhamos pressa, n\u00e3o existe atalho para o aprendizado e estudaremos tudo passo a passo, item a item, no padr\u00e3o dos cursos na\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Data Science Academy\n   </a>\n  </span>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Agora que j\u00e1 vimos tantas fun\u00e7\u00f5es de ativa\u00e7\u00e3o, precisamos de alguma l\u00f3gica/heur\u00edstica para saber qual fun\u00e7\u00e3o de ativa\u00e7\u00e3o deve ser usada em qual situa\u00e7\u00e3o. N\u00e3o h\u00e1 uma regra de ouro e a escolha depende do problema no qual voc\u00ea estiver trabalhando.\n </p>\n <p style=\"text-align: justify;\">\n  No entanto, dependendo das propriedades do problema, poderemos fazer uma melhor escolha para uma converg\u00eancia f\u00e1cil e r\u00e1pida da rede neural.\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   Fun\u00e7\u00f5es Sigm\u00f3ide e suas combina\u00e7\u00f5es geralmente funcionam melhor no caso de classificadores.\n  </li>\n  <li style=\"text-align: justify;\">\n   Fun\u00e7\u00f5es Sigm\u00f3ide e Tanh \u00e0s vezes s\u00e3o evitadas devido ao problema de Vanishing Gradient (que estudaremos no cap\u00edtulo sobre redes neurais recorrentes).\n  </li>\n  <li style=\"text-align: justify;\">\n   A fun\u00e7\u00e3o ReLU \u00e9 uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o geral e \u00e9 usada na maioria dos casos atualmente.\n  </li>\n  <li style=\"text-align: justify;\">\n   Se encontrarmos um caso de neur\u00f4nios deficientes em nossas redes, a fun\u00e7\u00e3o Leaky ReLU \u00e9 a melhor escolha.\n  </li>\n  <li style=\"text-align: justify;\">\n   Tenha sempre em mente que a fun\u00e7\u00e3o ReLU deve ser usada apenas nas camadas ocultas.\n  </li>\n  <li style=\"text-align: justify;\">\n   Como regra geral, voc\u00ea pode come\u00e7ar usando a fun\u00e7\u00e3o ReLU e depois passar para outras fun\u00e7\u00f5es de ativa\u00e7\u00e3o no caso da ReLU n\u00e3o fornecer resultados \u00f3timos.\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  Est\u00e1 come\u00e7ando a sentir a vibra\u00e7\u00e3o em trabalhar com Intelig\u00eancia Artificial? Ent\u00e3o continue acompanhando, pois estamos apenas no come\u00e7o! At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Fun\u00e7\u00e3o Sigm\u00f3ide\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Vanishing Gradient Problem\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n  (alguns trechos extra\u00eddos e traduzidos com autoriza\u00e7\u00e3o do autor\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://michaelnielsen.org/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Michael Nielsen\n   </a>\n  </span>\n  )\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-206\" href=\"http://deeplearningbook.com.br/funcao-de-ativacao/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-206\" href=\"http://deeplearningbook.com.br/funcao-de-ativacao/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-206\" href=\"http://deeplearningbook.com.br/funcao-de-ativacao/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-206\" href=\"http://deeplearningbook.com.br/funcao-de-ativacao/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/funcao-de-ativacao/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/funcao-de-ativacao/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-206-5e0dd04c1c665\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=206&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-206-5e0dd04c1c665\" id=\"like-post-wrapper-140353593-206-5e0dd04c1c665\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "9": "<h1 class=\"entry-title\" id=\"capitulo-9\">\n Cap\u00edtulo 9 \u2013 A Arquitetura das Redes Neurais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  No cap\u00edtulo 11 vamos desenvolver uma rede neural para classifica\u00e7\u00e3o de d\u00edgitos manuscritos, usando linguagem Python (caso ainda n\u00e3o saiba trabalhar com a linguagem, comece agora mesmo com nosso curso online totalmente gratuito\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener\" target=\"_blank\">\n    Python Fundamentos Para An\u00e1lise de Dados\n   </a>\n  </span>\n  ). Mas antes, vamos compreender a terminologia que ser\u00e1 muito \u00fatil quando estivermos desenvolvendo nosso modelo, estudando a\u00a0Arquitetura das Redes Neurais. Suponha que tenhamos a rede abaixo:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Rede\" class=\"aligncenter wp-image-236 size-full\" data-attachment-id=\"236\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Rede\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?fit=396%2C211\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?fit=300%2C160\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?fit=396%2C211\" data-orig-size=\"396,211\" data-permalink=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/rede-2/\" data-recalc-dims=\"1\" height=\"211\" sizes=\"(max-width: 396px) 100vw, 396px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?resize=396%2C211\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?w=396 396w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?resize=300%2C160 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede.png?resize=200%2C107 200w\" width=\"396\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A camada mais \u00e0 esquerda nesta rede \u00e9 chamada de camada de entrada e os neur\u00f4nios dentro da camada s\u00e3o chamados de neur\u00f4nios de entrada. A camada mais \u00e0 direita ou a sa\u00edda cont\u00e9m os neur\u00f4nios de sa\u00edda ou, como neste caso, um \u00fanico neur\u00f4nio de sa\u00edda. A camada do meio \u00e9 chamada de camada oculta, j\u00e1 que os neur\u00f4nios nessa camada n\u00e3o s\u00e3o entradas ou sa\u00eddas. O termo \u201coculto\u201d talvez soe um pouco misterioso \u2013 a primeira vez que ouvi o termo, pensei que devesse ter algum significado filos\u00f3fico ou matem\u00e1tico profundo \u2013 mas isso realmente n\u00e3o significa nada mais do que \u201cuma camada que n\u00e3o \u00e9 entrada ou sa\u00edda\u201d. A rede acima tem apenas uma \u00fanica camada oculta, mas algumas redes possuem m\u00faltiplas camadas ocultas. Por exemplo, a seguinte rede de quatro camadas tem duas camadas ocultas:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Rede\" class=\"aligncenter wp-image-237 size-full\" data-attachment-id=\"237\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Rede\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?fit=597%2C324\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?fit=300%2C163\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?fit=597%2C324\" data-orig-size=\"597,324\" data-permalink=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/rede2/\" data-recalc-dims=\"1\" height=\"324\" sizes=\"(max-width: 597px) 100vw, 597px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?resize=597%2C324\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?w=597 597w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?resize=300%2C163 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rede2.png?resize=200%2C109 200w\" width=\"597\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Tais redes de camadas m\u00faltiplas s\u00e3o chamados de Perceptrons Multicamadas ou MLPs (Multilayer Perceptrons), ou seja, uma rede neural formada por Perceptrons (embora na verdade seja uma rede de neur\u00f4nios sigm\u00f3ides, como veremos mais adiante).\n </p>\n <p style=\"text-align: justify;\">\n  O design das camadas de entrada e sa\u00edda em uma rede geralmente \u00e9 direto. Por exemplo, suponha que estamos tentando determinar se uma imagem manuscrita representa um \u201c9\u201d ou n\u00e3o. Uma maneira natural de projetar a rede \u00e9 codificar as intensidades dos pixels da imagem nos neur\u00f4nios de entrada. Se a imagem for uma imagem em escala de cinza 64 x 64, ter\u00edamos 64 \u00d7 64 =\u00a04.096 \u00a0neur\u00f4nios de entrada, com as intensidades dimensionadas adequadamente entre 0 e 1. A camada de sa\u00edda conter\u00e1 apenas um \u00fanico neur\u00f4nio com valores inferiores a 0,5 indicando que \u201ca imagem de entrada n\u00e3o \u00e9 um 9\u201d e valores maiores que 0,5 indicando que \u201ca imagem de entrada \u00e9 um 9\u201d.\n </p>\n <p style=\"text-align: justify;\">\n  Embora o design das camadas de entrada e sa\u00edda de uma rede neural seja frequentemente direto, pode haver bastante varia\u00e7\u00e3o para o design das camadas ocultas. Em particular, n\u00e3o \u00e9 poss\u00edvel resumir o processo de design das camadas ocultas com poucas regras simples. Em vez disso, pesquisadores de redes neurais desenvolveram muitas heur\u00edsticas de design para as camadas ocultas, que ajudam as pessoas a obter o comportamento que querem de suas redes. Conheceremos v\u00e1rias heur\u00edsticas de design desse tipo mais adiante ao longo dos pr\u00f3ximos cap\u00edtulos. O design das camadas ocultas \u00e9 um dos pontos cruciais em modelos de Deep Learning.\n </p>\n <p style=\"text-align: justify;\">\n  At\u00e9 agora, estamos discutindo redes neurais onde a sa\u00edda de uma camada \u00e9 usada como entrada para a pr\u00f3xima camada. Essas redes s\u00e3o chamadas de redes neurais feedforward. Isso significa que n\u00e3o h\u00e1 loops na rede \u2013 as informa\u00e7\u00f5es sempre s\u00e3o alimentadas para a frente, nunca s\u00e3o enviadas de volta. Se tiv\u00e9ssemos loops, acabar\u00edamos com situa\u00e7\u00f5es em que a entrada para a fun\u00e7\u00e3o \u03c3 dependeria da sa\u00edda. Isso seria dif\u00edcil de entender e, portanto, n\u00e3o permitimos tais loops.\n </p>\n <p style=\"text-align: justify;\">\n  No entanto, existem outros modelos de redes neurais artificiais em que os circuitos de feedback s\u00e3o poss\u00edveis. Esses modelos s\u00e3o chamados de redes neurais recorrentes. A ideia nestes modelos \u00e9 ter neur\u00f4nios que disparem por algum per\u00edodo de tempo limitado. Disparar pode estimular outros neur\u00f4nios, que podem disparar um pouco mais tarde, tamb\u00e9m por uma dura\u00e7\u00e3o limitada. Isso faz com que ainda mais neur\u00f4nios disparem e, ao longo do tempo, conseguimos uma cascata de disparos de neur\u00f4nios. Loops n\u00e3o causam problemas em tal modelo, uma vez que a sa\u00edda de um neur\u00f4nio afeta apenas sua entrada em algum momento posterior, n\u00e3o instantaneamente.\n </p>\n <p style=\"text-align: justify;\">\n  Geralmente, as arquiteturas de redes neurais podem ser colocadas em 3 categorias espec\u00edficas:\n </p>\n <h3 style=\"text-align: justify;\">\n  1 \u2013 Redes Neurais Feed-Forward\n </h3>\n <p style=\"text-align: justify;\">\n  Estes s\u00e3o o tipo mais comum de rede neural em aplica\u00e7\u00f5es pr\u00e1ticas. A primeira camada \u00e9 a entrada e a \u00faltima camada \u00e9 a sa\u00edda. Se houver mais de uma camada oculta, n\u00f3s as chamamos de redes neurais \u201cprofundas\u201d (ou Deep Learning). Esses tipos de redes neurais calculam uma s\u00e9rie de transforma\u00e7\u00f5es que alteram as semelhan\u00e7as entre os casos. As atividades dos neur\u00f4nios em cada camada s\u00e3o uma fun\u00e7\u00e3o n\u00e3o-linear das atividades na camada anterior.\n </p>\n <h3 style=\"text-align: justify;\">\n  2 \u2013 Redes Recorrentes\n </h3>\n <p style=\"text-align: justify;\">\n  Estes tipos de redes neurais t\u00eam ciclos direcionados em seu grafo de conex\u00e3o. Isso significa que \u00e0s vezes voc\u00ea pode voltar para onde voc\u00ea come\u00e7ou seguindo as setas. Eles podem ter uma din\u00e2mica complicada e isso pode torn\u00e1-los muito dif\u00edceis de treinar. Entretanto, estes tipos s\u00e3o mais biologicamente realistas.\n </p>\n <p style=\"text-align: justify;\">\n  Atualmente, h\u00e1 muito interesse em encontrar formas eficientes de treinamento de redes recorrentes. As redes neurais recorrentes s\u00e3o uma maneira muito natural de modelar dados sequenciais. Eles s\u00e3o equivalentes a redes muito profundas com uma camada oculta por fatia de tempo; exceto que eles usam os mesmos pesos em cada fatia de tempo e recebem entrada em cada fatia. Eles t\u00eam a capacidade de lembrar informa\u00e7\u00f5es em seu estado oculto por um longo per\u00edodo de tempo, mas \u00e9 muito dif\u00edcil trein\u00e1-las para usar esse potencial.\n </p>\n <h3 style=\"text-align: justify;\">\n  3 \u2013 Redes Conectadas Simetricamente\n </h3>\n <p style=\"text-align: justify;\">\n  Estas s\u00e3o como redes recorrentes, mas as conex\u00f5es entre as unidades s\u00e3o sim\u00e9tricas (elas t\u00eam o mesmo peso em ambas as dire\u00e7\u00f5es). As redes sim\u00e9tricas s\u00e3o muito mais f\u00e1ceis de analisar do que as redes recorrentes. Elas tamb\u00e9m s\u00e3o mais restritas no que podem fazer porque obedecem a uma fun\u00e7\u00e3o de energia. As redes conectadas simetricamente sem unidades ocultas s\u00e3o chamadas de \u201cRedes Hopfield\u201d. As redes conectadas simetricamente com unidades ocultas s\u00e3o chamadas de \u201cM\u00e1quinas de Boltzmann\u201d.\n </p>\n <hr/>\n <p>\n  Dentre estas 3 categorias, podemos listar 10 arquiteturas principais de redes neurais:\n </p>\n <ul>\n  <li>\n   Redes Multilayer Perceptron\n  </li>\n  <li>\n   Redes Neurais Convolucionais\n  </li>\n  <li>\n   Redes Neurais Recorrentes\n  </li>\n  <li>\n   Long Short-Term Memory (LSTM)\n  </li>\n  <li>\n   Redes de Hopfield\n  </li>\n  <li>\n   M\u00e1quinas de Boltzmann\n  </li>\n  <li>\n   Deep Belief Network\n  </li>\n  <li>\n   Deep Auto-Encoders\n  </li>\n  <li>\n   Generative Adversarial Network\n  </li>\n  <li>\n   Deep Neural Network Capsules (este \u00e9 um tipo completamente novo de rede neural, lan\u00e7ado no final de 2017)\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  Quer aprender a construir essas arquiteturas de redes neurais de forma eficiente, profissional e totalmente pr\u00e1tica, com mini-projetos para solu\u00e7\u00e3o de problemas do mundo real, em vis\u00e3o computacional, processamento de linguagem natural, detec\u00e7\u00e3o de fraudes, previs\u00e3o de s\u00e9ries temporais e muito mais? Ent\u00e3o confira os \u00fanicos cursos online do Brasil, 100% em portugu\u00eas, onde voc\u00ea aprende tudo sobre essas arquiteturas. Clique nos links abaixo para acessar os programas completos:\n </p>\n <p>\n </p>\n <h1 style=\"text-align: center;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-i\" rel=\"noopener\" target=\"_blank\">\n    Deep Learning I\n   </a>\n  </span>\n </h1>\n <p>\n </p>\n <h1 style=\"text-align: center;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n    Deep Learning II\n   </a>\n  </span>\n </h1>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  No pr\u00f3ximo cap\u00edtulo, daremos a voc\u00ea uma vis\u00e3o geral sobre cada uma dessas 10 arquiteturas e ao longo dos cap\u00edtulos seguintes, estudaremos todas elas. Cada umas dessas arquiteturas tem sido usada para resolver diferentes problemas e criar sistemas de Intelig\u00eancia Artificial. Saber trabalhar com IA de forma eficiente, ser\u00e1 determinante para seu futuro profissional.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide\" rel=\"noopener\" target=\"_blank\">\n    Fun\u00e7\u00e3o Sigm\u00f3ide\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\" rel=\"noopener\" target=\"_blank\">\n    Vanishing Gradient Problem\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n  (alguns trechos extra\u00eddos e traduzidos com autoriza\u00e7\u00e3o do autor\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://michaelnielsen.org/\" rel=\"noopener\" target=\"_blank\">\n    Michael Nielsen\n   </a>\n  </span>\n  )\n </p>\n <p>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-235\" href=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-235\" href=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-235\" href=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-235\" href=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-235-5e0dd04e044c7\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=235&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-235-5e0dd04e044c7\" id=\"like-post-wrapper-140353593-235-5e0dd04e044c7\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "10": "<h1 class=\"entry-title\" id=\"capitulo-10\">\n Cap\u00edtulo 10 \u2013 As 10 Principais Arquiteturas de Redes Neurais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  O Aprendizado de M\u00e1quina (\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n  ) \u00e9 necess\u00e1rio para resolver tarefas que s\u00e3o muito complexas para os humanos. Algumas tarefas s\u00e3o t\u00e3o complexas que \u00e9 impratic\u00e1vel, sen\u00e3o imposs\u00edvel, que os seres humanos consigam explicar todas as nuances envolvidas. Ent\u00e3o, em vez disso, fornecemos uma grande quantidade de dados para um algoritmo de aprendizado de m\u00e1quina e deixamos que o algoritmo funcione, explorando esses dados e buscando um modelo que alcance o que os\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Cientistas de Dados\n   </a>\n  </span>\n  e\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Engenheiros de IA\n   </a>\n  </span>\n  determinaram como objetivo.\u00a0Vejamos estes dois exemplos:\n </p>\n <p>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   \u00c9 muito dif\u00edcil escrever programas que solucionem problemas como reconhecer um objeto tridimensional a partir de um novo ponto de vista em novas condi\u00e7\u00f5es de ilumina\u00e7\u00e3o em uma cena desordenada. N\u00f3s n\u00e3o sabemos qual programa de computador escrever porque n\u00e3o sabemos como ocorre o processo em nosso c\u00e9rebro. Mesmo se tiv\u00e9ssemos uma boa ideia sobre como faz\u00ea-lo, o programa poderia ser incrivelmente complicado.\n  </li>\n </ul>\n <p>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   \u00c9 dif\u00edcil escrever um programa para calcular a probabilidade de uma transa\u00e7\u00e3o de cart\u00e3o de cr\u00e9dito ser fraudulenta. Pode n\u00e3o haver regras que sejam simples e confi\u00e1veis. Precisamos combinar um n\u00famero muito grande de regras fracas. A fraude \u00e9 um alvo em movimento, mas o programa precisa continuar mudando.\n  </li>\n </ul>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  \u00c9 onde Machine Learning pode ser aplicado com sucesso. Em vez de escrever um programa \u00e0 m\u00e3o para cada tarefa espec\u00edfica, n\u00f3s coletamos muitos exemplos que especificam a sa\u00edda correta para uma determinada entrada. Um algoritmo de aprendizagem de m\u00e1quina recebe esses exemplos e produz um programa que faz o trabalho. O programa produzido pelo algoritmo de aprendizagem pode parecer muito diferente de um programa t\u00edpico escrito \u00e0 m\u00e3o. Pode conter milh\u00f5es de n\u00fameros. Se o fizermos corretamente, o programa funciona para novos casos (novos dados). Se os dados mudarem, o programa tamb\u00e9m pode mudar ao treinar em novos dados. E com a redu\u00e7\u00e3o de custos de computa\u00e7\u00e3o (principalmente usando processamento em nuvem), grande quantidade de dados (Big Data) e processamento paralelo em GPU, temos as condi\u00e7\u00f5es perfeitas para a evolu\u00e7\u00e3o de Machine Learning. O maior problema, por incr\u00edvel que pare\u00e7a, ser\u00e1 a falta de profissionais qualificados em n\u00famero suficiente para atender as demandas do mercado.\n </p>\n <p>\n  Alguns exemplos de tarefas melhor resolvidas pela aprendizagem de m\u00e1quina incluem:\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   Reconhecimento de padr\u00f5es: objetos em cenas reais, identidades faciais ou express\u00f5es faciais, palavras escritas ou faladas.\n  </li>\n  <li style=\"text-align: justify;\">\n   Detec\u00e7\u00e3o de anomalias: sequ\u00eancias incomuns de transa\u00e7\u00f5es de cart\u00e3o de cr\u00e9dito, padr\u00f5es incomuns de leituras de sensores em m\u00e1quinas de uma ind\u00fastria t\u00eaxtil.\n  </li>\n  <li style=\"text-align: justify;\">\n   Previs\u00e3o: pre\u00e7os de a\u00e7\u00f5es futuros ou taxas de c\u00e2mbio, quais filmes uma pessoa gostaria de assistir, previs\u00e3o de vendas.\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  Machine Learning \u00e9 um campo abrangente dentro da Intelig\u00eancia Artificial. Mas uma sub-\u00e1rea de Machine Learning, o\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/capitulo-3-o-que-sao-redes-neurais-artificiais-profundas/\" rel=\"noopener\" target=\"_blank\">\n    Deep Learning\n   </a>\n  </span>\n  (ou Redes Neurais Profundas), vem conseguindo resultados no estado da arte para as tarefas acima mencionadas. Neste cap\u00edtulo voc\u00ea encontra\u00a0As 10 Principais Arquiteturas de Redes Neurais, dentre elas as principais arquiteturas de Deep Learning.\n </p>\n <p>\n </p>\n <h2>\n  1-\u00a0Redes Multilayer Perceptrons\n </h2>\n <p style=\"text-align: justify;\">\n  O Perceptron, conforme estudamos nos cap\u00edtulos anteriores, \u00e9 um algoritmo simples destinado a realizar a classifica\u00e7\u00e3o bin\u00e1ria; isto \u00e9, prev\u00ea se a entrada pertence a uma determinada categoria de interesse ou n\u00e3o: fraude ou n\u00e3o_fraude, gato ou n\u00e3o_gato.\n </p>\n <p>\n  <img alt=\"Multilayer Perceptrons (MLP)\" class=\"aligncenter wp-image-258 size-large\" data-attachment-id=\"258\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Multilayer Perceptrons (MLP)\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?fit=1024%2C583\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?fit=300%2C171\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?fit=1024%2C583\" data-orig-size=\"1024,583\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/mlp/\" data-recalc-dims=\"1\" height=\"583\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=1024%2C583\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?w=1024 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=300%2C171 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=768%2C437 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=200%2C114 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/mlp.jpg?resize=690%2C393 690w\" width=\"1024\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Um Perceptron \u00e9 um classificador linear; ou seja, \u00e9 um algoritmo que classifica a entrada separando duas categorias com uma linha reta. A entrada geralmente \u00e9 um vetor de recursos\n  <strong>\n   x\n  </strong>\n  multiplicado por pesos\n  <strong>\n   w\n  </strong>\n  e adicionado a um vi\u00e9s (ou bias)\n  <strong>\n   b\n  </strong>\n  . Aqui um exemplo do Perceptron: y = w * x + b. Um Perceptron produz uma \u00fanica sa\u00edda com base em v\u00e1rias entradas de valor real, formando uma combina\u00e7\u00e3o linear usando os pesos (e \u00e0s vezes passando a sa\u00edda atrav\u00e9s de uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o n\u00e3o linear).\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Frank_Rosenblatt\" rel=\"noopener\" target=\"_blank\">\n    Rosenblatt\n   </a>\n  </span>\n  construiu um Perceptron de uma camada. Ou seja, seu algoritmo n\u00e3o inclui m\u00faltiplas camadas, o que permite que as redes neurais modelem uma hierarquia de recursos. Isso impede que o Perceptron consiga realizar classifica\u00e7\u00e3o n\u00e3o linear, como a fun\u00e7\u00e3o XOR (um disparador do operador XOR quando a entrada exibe uma caracter\u00edstica ou outra, mas n\u00e3o ambas, significa \u201cOR exclusivo\u201d \u201c), como\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://mitpress.mit.edu/books/perceptrons\" rel=\"noopener\" target=\"_blank\">\n    Minsky e Papert\n   </a>\n  </span>\n  mostraram em seu livro.\n </p>\n <p style=\"text-align: justify;\">\n  Um Multilayer Perceptron (MLP) \u00e9 uma rede neural artificial composta por mais de um Perceptron. Eles s\u00e3o compostos por uma camada de entrada para receber o sinal, uma camada de sa\u00edda que toma uma decis\u00e3o ou previs\u00e3o sobre a entrada, e entre esses dois, um n\u00famero arbitr\u00e1rio de camadas ocultas que s\u00e3o o verdadeiro mecanismo computacional do MLP. MLPs com uma camada oculta s\u00e3o capazes de aproximar qualquer fun\u00e7\u00e3o cont\u00ednua.\n </p>\n <p style=\"text-align: justify;\">\n  O Multilayer Perceptron \u00e9 uma esp\u00e9cie de \u201cHello World\u201d da aprendizagem profunda: uma boa forma de come\u00e7ar quando voc\u00ea est\u00e1 aprendendo sobre Deep Learning.\n </p>\n <p style=\"text-align: justify;\">\n  Os MLPs s\u00e3o frequentemente aplicados a problemas de aprendizagem supervisionados: treinam em um conjunto de pares entrada-sa\u00edda e aprendem a modelar a correla\u00e7\u00e3o (ou depend\u00eancias) entre essas entradas e sa\u00eddas. O treinamento envolve o ajuste dos par\u00e2metros, ou os pesos e bias, do modelo para minimizar o erro. O backpropagation \u00e9 usado para fazer os ajustes dos pesos e de bias em rela\u00e7\u00e3o ao erro, e o pr\u00f3prio erro pode ser medido de v\u00e1rias maneiras, inclusive pelo erro quadr\u00e1tico m\u00e9dio (MSE \u2013 Mean Squared Error).\n </p>\n <p style=\"text-align: justify;\">\n  As redes feed forward, como MLPs, s\u00e3o como ping-pong. Elas s\u00e3o principalmente envolvidas em dois movimentos, uma constante de ida e volta.\u00a0 Na passagem para a frente, o fluxo de sinal se move da camada de entrada atrav\u00e9s das camadas ocultas para a camada de sa\u00edda e a decis\u00e3o da camada de sa\u00edda \u00e9 medida em rela\u00e7\u00e3o \u00e0s sa\u00eddas esperadas.\n </p>\n <p style=\"text-align: justify;\">\n  Na passagem para tr\u00e1s, usando o backpropagation e a regra da cadeia (Chain Rule), derivadas parciais da fun\u00e7\u00e3o de erro dos v\u00e1rios pesos e bias s\u00e3o reproduzidos atrav\u00e9s do MLP. Esse ato de diferencia\u00e7\u00e3o nos d\u00e1 um gradiente, ao longo do qual os par\u00e2metros podem ser ajustados \u00e0 medida que movem o MLP um passo mais perto do erro m\u00ednimo. Isso pode ser feito com qualquer algoritmo de otimiza\u00e7\u00e3o baseado em gradiente, como descida estoc\u00e1stica do gradiente. A rede continua jogando aquele jogo de ping-pong at\u00e9 que o erro n\u00e3o possa mais ser reduzido (chegou ao m\u00ednimo poss\u00edvel). Este estado \u00e9 conhecido como converg\u00eancia.\n </p>\n <p style=\"text-align: justify;\">\n  Parece muita coisa? Sim, \u00e9. Veremos esse processo em mais detalhes aqui mesmo neste livro e caso queira aprender a construir modelos MLP para aplica\u00e7\u00f5es pr\u00e1ticas, atrav\u00e9s de v\u00eddeos em portugu\u00eas, clique\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-i\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  .\n </p>\n <p>\n </p>\n <h2>\n  2- Redes Neurais Convolucionais\n </h2>\n <p style=\"text-align: justify;\">\n  Em 1998,\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/\" rel=\"noopener\" target=\"_blank\">\n    Yann LeCun\n   </a>\n  </span>\n  e seus colaboradores desenvolveram um reconhecedor, realmente bom, para d\u00edgitos manuscritos chamado\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/lenet/\" rel=\"noopener\" target=\"_blank\">\n    LeNet\n   </a>\n  </span>\n  . Ele usou o backpropagation em uma rede feed forward com muitas camadas ocultas, muitos mapas de unidades replicadas em cada camada, agrupando as sa\u00eddas de unidades pr\u00f3ximas, formando uma rede ampla que pode lidar com v\u00e1rios caracteres ao mesmo tempo, mesmo se eles se sobrep\u00f5em e uma inteligente maneira de treinar um sistema completo, n\u00e3o apenas um reconhecedor. Mais tarde, esta arquitetura foi formalizada sob o nome de redes neurais convolucionais.\n </p>\n <p style=\"text-align: justify;\">\n  As Redes Neurais Convolucionais\u00a0(ConvNets ou CNNs) s\u00e3o redes neurais artificiais profundas que podem ser usadas para classificar imagens, agrup\u00e1-las por similaridade (busca de fotos) e realizar reconhecimento de objetos dentro de cenas. S\u00e3o algoritmos que podem identificar rostos, indiv\u00edduos, sinais de rua, cenouras, ornitorrincos e muitos outros aspectos dos dados visuais.\n </p>\n <p style=\"text-align: justify;\">\n  As redes convolucionais realizam o reconhecimento \u00f3ptico de caracteres (OCR) para digitalizar texto e tornar poss\u00edvel o processamento de linguagem natural em documentos anal\u00f3gicos e manuscritos, onde as imagens s\u00e3o s\u00edmbolos a serem transcritos. CNNs tamb\u00e9m podem ser aplicadas a arquivos de \u00e1udio quando estes s\u00e3o representados visualmente como um espectrograma. Mais recentemente, as redes convolucionais foram aplicadas diretamente \u00e0 an\u00e1lise de texto, bem como dados gr\u00e1ficos.\n </p>\n <p style=\"text-align: justify;\">\n  A efic\u00e1cia das redes convolucionais no reconhecimento de imagem \u00e9 uma das principais raz\u00f5es pelas quais o mundo testemunhou a efic\u00e1cia do aprendizado profundo. Este tipo de rede est\u00e1 impulsionando grandes avan\u00e7os em\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-visao-computacional-e-reconhecimento-de-imagens\" rel=\"noopener\" target=\"_blank\">\n    Vis\u00e3o Computacional\n   </a>\n  </span>\n  , que tem aplica\u00e7\u00f5es \u00f3bvias em carros aut\u00f4nomos, rob\u00f3tica, drones, seguran\u00e7a, diagn\u00f3sticos m\u00e9dicos e tratamentos para deficientes visuais.\n </p>\n <p style=\"text-align: justify;\">\n  As redes convolucionais ingerem e processam imagens como tensores e tensores s\u00e3o matrizes de n\u00fameros com v\u00e1rias dimens\u00f5es. Eles podem ser dif\u00edceis de visualizar, ent\u00e3o vamos abord\u00e1-los por analogia. Um escalar \u00e9 apenas um n\u00famero, como 7; um vetor \u00e9 uma lista de n\u00fameros (por exemplo, [7,8,9]); e uma matriz \u00e9 uma grade retangular de n\u00fameros que ocupam v\u00e1rias linhas e colunas como uma planilha. Geometricamente, se um escalar \u00e9 um ponto de dimens\u00e3o zero, ent\u00e3o um vetor \u00e9 uma linha unidimensional, uma matriz \u00e9 um plano bidimensional, uma pilha de matrizes \u00e9 um cubo tridimensional e quando cada elemento dessas matrizes tem uma pilha de mapas de recursos ligados a ele, voc\u00ea entra na quarta dimens\u00e3o. Calma, n\u00e3o se desespere (ainda). Veremos isso mais a frente com calma, quando estudarmos exclusivamente esta arquitetura. Em nossos cursos na Data Science Academy inclu\u00edmos aulas completas sobre \u00c1lgebra Linear, onde escalares, vetores, matrizes e tensores s\u00e3o estudados na teoria e pr\u00e1tica, pois este conhecimento \u00e9 fundamental na constru\u00e7\u00e3o de redes neurais profundas.\n </p>\n <p>\n  A primeira coisa a saber sobre redes convolucionais \u00e9 que elas n\u00e3o percebem imagens como os humanos. Portanto, voc\u00ea ter\u00e1 que pensar de uma maneira diferente sobre o que uma imagem significa quando \u00e9 alimentada e processada por uma rede convolucional.\n </p>\n <p>\n  <img alt=\"CNN\" class=\"aligncenter size-full wp-image-260\" data-attachment-id=\"260\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"CNN\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?fit=1000%2C341\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?fit=300%2C102\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?fit=1000%2C341\" data-orig-size=\"1000,341\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/cnn/\" data-recalc-dims=\"1\" height=\"341\" sizes=\"(max-width: 1000px) 100vw, 1000px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=1000%2C341\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?w=1000 1000w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=300%2C102 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=768%2C262 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=200%2C68 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/cnn.png?resize=690%2C235 690w\" width=\"1000\"/>\n </p>\n <p style=\"text-align: justify;\">\n  As redes convolucionais percebem imagens como volumes; isto \u00e9, objetos tridimensionais, em vez de estruturas planas a serem medidas apenas por largura e altura. Isso porque as imagens de cores digitais t\u00eam uma codifica\u00e7\u00e3o vermelho-verde-azul (RGB \u2013 Red-Green-Blue), misturando essas tr\u00eas cores para produzir o espectro de cores que os seres humanos percebem. Uma rede\u00a0convolucional recebe imagens como tr\u00eas estratos separados de cores empilhados um em cima do outro.\n </p>\n <p style=\"text-align: justify;\">\n  Assim, uma rede convolucional recebe uma imagem como uma caixa retangular cuja largura e altura s\u00e3o medidas pelo n\u00famero de pixels ao longo dessas dimens\u00f5es e cuja profundidade \u00e9 de tr\u00eas camadas profundas, uma para cada letra em RGB. Essas camadas de profundidade s\u00e3o referidas como canais.\n </p>\n <p style=\"text-align: justify;\">\n  \u00c0 medida que as imagens se movem atrav\u00e9s de uma rede convolucional, descrevemos em termos de volumes de entrada e sa\u00edda, expressando-as matematicamente como matrizes de m\u00faltiplas dimens\u00f5es dessa forma: 30x30x3. De camada em camada, suas dimens\u00f5es mudam \u00e0 medida que atravessam a rede neural convolucional at\u00e9 gerar uma s\u00e9rie de probabilidades na camada de sa\u00edda, sendo uma probabilidade para cada poss\u00edvel classe de sa\u00edda. Aquela com maior probabilidade, ser\u00e1 a classe definida para a imagem de entrada, um p\u00e1ssaro por exemplo.\n </p>\n <p style=\"text-align: justify;\">\n  Voc\u00ea precisar\u00e1 prestar muita aten\u00e7\u00e3o \u00e0s medidas de cada dimens\u00e3o do volume da imagem, porque elas s\u00e3o a base das opera\u00e7\u00f5es de \u00e1lgebra linear usadas para processar imagens. Poder\u00edamos dedicar dois cap\u00edtulos inteiros somente a esta arquitetura. Ali\u00e1s, \u00e9 o que faremos mais \u00e0 frente aqui no livro e o que j\u00e1 fazemos na pr\u00e1tica\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-i\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  .\n </p>\n <p>\n </p>\n <h2>\n  3- Redes Neurais Recorrentes\n </h2>\n <p style=\"text-align: justify;\">\n  As redes recorrentes s\u00e3o um poderoso conjunto de algoritmos de redes neurais artificiais especialmente \u00fateis para o processamento de dados sequenciais, como som, dados de s\u00e9ries temporais ou linguagem natural. Uma vers\u00e3o de redes recorrentes foi usada pelo\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://deepmind.com/\" rel=\"noopener\" target=\"_blank\">\n    DeepMind\n   </a>\n  </span>\n  no projeto de videogames com agentes aut\u00f4nomos.\n </p>\n <p style=\"text-align: justify;\">\n  As redes recorrentes diferem das redes feed forward porque incluem um loop de feedback, pelo qual a sa\u00edda do passo n-1 \u00e9 alimentada de volta \u00e0 rede para afetar o resultado do passo n, e assim por diante para cada etapa subsequente. Por exemplo, se uma rede \u00e9 exposta a uma palavra letra por letra, e \u00e9 solicitado a adivinhar cada letra a seguir, a primeira letra de uma palavra ajudar\u00e1 a determinar o que uma rede recorrente pensa que a segunda letra pode ser.\n </p>\n <p style=\"text-align: justify;\">\n  Isso difere de uma rede feed forward, que aprende a classificar cada n\u00famero manuscrito por exemplo, independentemente, e de acordo com os pixels de que \u00e9 exposto a partir de um \u00fanico exemplo, sem se referir ao exemplo anterior para ajustar suas previs\u00f5es. As redes feed forward aceitam uma entrada por vez e produzem uma sa\u00edda. As redes recorrentes n\u00e3o enfrentam a mesma restri\u00e7\u00e3o um-para-um.\n </p>\n <p style=\"text-align: justify;\">\n  Embora algumas formas de dados, como imagens, n\u00e3o pare\u00e7am ser sequenciais, elas podem ser entendidas como sequ\u00eancias quando alimentadas em uma rede recorrente. Considere uma imagem de uma palavra manuscrita. Assim como as redes recorrentes processam a escrita manual, convertendo cada imagem em uma letra e usando o in\u00edcio de uma palavra para adivinhar como essa palavra terminar\u00e1, ent\u00e3o as redes podem tratar parte de qualquer imagem como letras em uma sequ\u00eancia. Uma rede neural que percorre uma imagem grande pode aprender a partir de cada regi\u00e3o, o que as regi\u00f5es vizinhas, s\u00e3o mais prov\u00e1veis \u200b\u200bde ser.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"Redes Neurais Recorrentes\" class=\"aligncenter size-full wp-image-262\" data-attachment-id=\"262\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Redes Neurais Recorrentes\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?fit=400%2C318\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?fit=300%2C239\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?fit=400%2C318\" data-orig-size=\"400,318\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/rnn/\" data-recalc-dims=\"1\" height=\"318\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/rnn.gif?resize=400%2C318\" width=\"400\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  As redes recorrentes e as redes feed forward \u201clembram\u201d algo sobre o mundo, modelando os dados que est\u00e3o expostos. Mas elas se lembram de maneiras muito diferentes. Ap\u00f3s o treinamento, a rede feed forward produz um modelo est\u00e1tico dos dados e esse modelo pode ent\u00e3o aceitar novos exemplos e classific\u00e1-los ou agrup\u00e1-los com precis\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Em contraste, as redes recorrentes produzem modelos din\u00e2micos \u2013 ou seja, modelos que mudam ao longo do tempo \u2013 de formas que produzem classifica\u00e7\u00f5es precisas dependentes do contexto dos exemplos que est\u00e3o expostos.\n </p>\n <p style=\"text-align: justify;\">\n  Para ser preciso, um modelo recorrente inclui o estado oculto que determinou a classifica\u00e7\u00e3o anterior em uma s\u00e9rie. Em cada etapa subsequente, esse estado oculto \u00e9 combinado com os dados de entrada do novo passo para produzir a) um novo estado oculto e, em seguida, b) uma nova classifica\u00e7\u00e3o. Cada estado oculto \u00e9 reciclado para produzir seu sucessor modificado.\n </p>\n <p style=\"text-align: justify;\">\n  As mem\u00f3rias humanas tamb\u00e9m s\u00e3o conscientes do contexto, reciclando a consci\u00eancia de estados anteriores para interpretar corretamente novos dados. Por exemplo, vamos considerar dois indiv\u00edduos. Um est\u00e1 ciente de que ele est\u00e1 perto da casa de Bob. O outro est\u00e1 ciente de que entrou em um avi\u00e3o. Eles interpretar\u00e3o os sons \u201cOi Bob!\u201d de duas formas muito diferentes, precisamente porque ret\u00e9m um estado oculto afetado por suas mem\u00f3rias de curto prazo e sensa\u00e7\u00f5es precedentes.\n </p>\n <p style=\"text-align: justify;\">\n  Diferentes lembran\u00e7as de curto prazo devem ser recontadas em momentos diferentes, a fim de atribuir o significado certo \u00e0 entrada atual. Algumas dessas mem\u00f3rias ter\u00e3o sido forjadas recentemente e outras mem\u00f3rias ter\u00e3o forjado muitos passos antes de serem necess\u00e1rios. A rede recorrente que efetivamente associa mem\u00f3rias e entrada remota no tempo \u00e9 chamada de Mem\u00f3ria de Longo Prazo (LSTM), a qual veremos em seguida.\n </p>\n <p>\n </p>\n <h2>\n  4- Long Short-Term Memory (LSTM)\n </h2>\n <p style=\"text-align: justify;\">\n  Em meados dos anos 90, a proposta dos pesquisadores alem\u00e3es\n  <a href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\" rel=\"noopener\" target=\"_blank\">\n   Sepp Hochreiter e Juergen Schmidhuber\n  </a>\n  apresentou uma varia\u00e7\u00e3o da rede recorrente com as chamadas unidades de Long Short-Term Memory, como uma solu\u00e7\u00e3o para o problema do vanishing gradient, problema comum em redes neurais recorrentes.\n </p>\n <p style=\"text-align: justify;\">\n  Os LSTMs ajudam a preservar o erro que pode ser copiado por tempo e camadas. Ao manter um erro mais constante, eles permitem que as redes recorrentes continuem aprendendo durante v\u00e1rios passos de tempo (mais de 1000), abrindo assim um canal para vincular causas e efeitos remotamente. Este \u00e9 um dos desafios centrais para a aprendizagem de m\u00e1quina e a IA, uma vez que os algoritmos s\u00e3o frequentemente confrontados por ambientes onde os sinais de recompensa s\u00e3o escassos e atrasados, como a pr\u00f3pria vida. (Os pensadores religiosos abordaram este mesmo problema com ideias de karma ou recompensas divinas, teorizando consequ\u00eancias invis\u00edveis e distantes para nossas a\u00e7\u00f5es).\n </p>\n <p style=\"text-align: justify;\">\n  Os LSTMs cont\u00eam informa\u00e7\u00f5es fora do fluxo normal da rede recorrente em uma c\u00e9lula fechada. As informa\u00e7\u00f5es podem ser armazenadas, escritas ou lidas a partir de uma c\u00e9lula, como dados na mem\u00f3ria de um computador. A c\u00e9lula toma decis\u00f5es sobre o que armazenar, e quando permitir leituras, grava\u00e7\u00f5es e exclus\u00f5es, atrav\u00e9s de port\u00f5es abertos e fechados. Ao contr\u00e1rio do armazenamento digital em computadores, no entanto, esses port\u00f5es s\u00e3o anal\u00f3gicos, implementados com a multiplica\u00e7\u00e3o de elementos por sigm\u00f3ides, que est\u00e3o todos na faixa de 0-1. Anal\u00f3gico tem a vantagem sobre o digital de ser diferenci\u00e1vel e, portanto, adequado para backpropagation.\n </p>\n <p style=\"text-align: justify;\">\n  Esses port\u00f5es atuam sobre os sinais que recebem e, de forma semelhante aos n\u00f3s da rede neural, eles bloqueiam ou transmitem informa\u00e7\u00f5es com base em sua for\u00e7a e importa\u00e7\u00e3o, que eles filtram com seus pr\u00f3prios conjuntos de pesos. Esses pesos, como os pesos que modulam a entrada e estados ocultos, s\u00e3o ajustados atrav\u00e9s do processo de aprendizagem das redes recorrentes. Ou seja, as c\u00e9lulas aprendem quando permitir que os dados entrem, saiam ou sejam exclu\u00eddos atrav\u00e9s do processo iterativo de fazer suposi\u00e7\u00f5es, calculando o erro durante o backpropagation e ajustando pesos atrav\u00e9s da descida do gradiente.\n </p>\n <p style=\"text-align: justify;\">\n  O diagrama abaixo ilustra como os dados fluem atrav\u00e9s de uma c\u00e9lula de mem\u00f3ria e s\u00e3o controlados por seus port\u00f5es.\n </p>\n <p>\n  <img alt=\"Long Short-Term Memory\" class=\"aligncenter wp-image-269 size-full\" data-attachment-id=\"269\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Long Short-Term Memory\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?fit=793%2C453\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?fit=300%2C171\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?fit=793%2C453\" data-orig-size=\"793,453\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/lstm/\" data-recalc-dims=\"1\" height=\"453\" sizes=\"(max-width: 793px) 100vw, 793px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=793%2C453\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?w=793 793w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=300%2C171 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=768%2C439 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=200%2C114 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/lstm.png?resize=690%2C394 690w\" width=\"793\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Os LSTM\u2019s possuem muitas aplica\u00e7\u00f5es pr\u00e1ticas, incluindo processamento de linguagem natural, gera\u00e7\u00e3o autom\u00e1tica de texto e an\u00e1lise de s\u00e9ries temporais. Caso queira ver esses exemplos na pr\u00e1tica, clique\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  . Teremos um cap\u00edtulo inteiro dedicado aos LSTM\u2019s aqui no livro.\n </p>\n <p>\n </p>\n <h2>\n  5- Redes de Hopfield\n </h2>\n <p style=\"text-align: justify;\">\n  Redes recorrentes de unidades n\u00e3o lineares geralmente s\u00e3o muito dif\u00edceis de analisar. Elas podem se comportar de muitas maneiras diferentes: se estabelecer em um estado est\u00e1vel, oscilar ou seguir trajet\u00f3rias ca\u00f3ticas que n\u00e3o podem ser preditas no futuro. Uma Rede Hopfield \u00e9 composta por unidades de limite bin\u00e1rio com conex\u00f5es recorrentes entre elas. Em 1982, John Hopfield percebeu que, se as conex\u00f5es s\u00e3o sim\u00e9tricas, existe uma fun\u00e7\u00e3o de energia global. Cada \u201cconfigura\u00e7\u00e3o\u201d bin\u00e1ria de toda a rede possui energia, enquanto a regra de decis\u00e3o do limite bin\u00e1rio faz com que a rede se conforme com um m\u00ednimo desta fun\u00e7\u00e3o de energia. Uma excelente maneira de usar esse tipo de computa\u00e7\u00e3o \u00e9 usar mem\u00f3rias como energia m\u00ednima para a rede neural. Usar m\u00ednimos de energia para representar mem\u00f3rias resulta em uma mem\u00f3ria endere\u00e7\u00e1vel ao conte\u00fado. Um item pode ser acessado por apenas conhecer parte do seu conte\u00fado. \u00c9 robusto contra danos no hardware.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Hopfield\" class=\"aligncenter size-full wp-image-279\" data-attachment-id=\"279\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Hopfield\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?fit=520%2C366\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?fit=300%2C211\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?fit=520%2C366\" data-orig-size=\"520,366\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/hopfield/\" data-recalc-dims=\"1\" height=\"366\" sizes=\"(max-width: 520px) 100vw, 520px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?resize=520%2C366\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?w=520 520w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?resize=300%2C211 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/hopfield.png?resize=200%2C141 200w\" width=\"520\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Cada vez que memorizamos uma configura\u00e7\u00e3o, esperamos criar um novo m\u00ednimo de energia. Mas e se dois m\u00ednimos pr\u00f3ximos est\u00e3o em um local intermedi\u00e1rio? Isso limita a capacidade de uma Rede Hopfield. Ent\u00e3o, como aumentamos a capacidade de uma Rede Hopfield? Os f\u00edsicos adoram a ideia de que a matem\u00e1tica que eles j\u00e1 conhecem pode explicar como o c\u00e9rebro funciona. Muitos artigos foram publicados em revistas de f\u00edsica sobre Redes Hopfield e sua capacidade de armazenamento. Eventualmente,\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.baginsky.de/eli/eg_portr.html\" rel=\"noopener\" target=\"_blank\">\n    Elizabeth Gardner\n   </a>\n  </span>\n  descobriu que havia uma regra de armazenamento muito melhor que usa a capacidade total dos pesos. Em vez de tentar armazenar vetores de uma s\u00f3 vez, ela percorreu o conjunto de treinamento muitas vezes e usou o procedimento de converg\u00eancia Perceptron para treinar cada unidade para ter o estado correto, dado os estados de todas as outras unidades nesse vetor.\n  <strong>\n   Os estat\u00edsticos chamam essa t\u00e9cnica de \u201cpseudo-probabilidade\u201d\n  </strong>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Existe outro papel computacional para as Redes Hopfield. Em vez de usar a rede para armazenar mem\u00f3rias, usamos para construir interpreta\u00e7\u00f5es de entrada sensorial. A entrada \u00e9 representada pelas unidades vis\u00edveis, a interpreta\u00e7\u00e3o \u00e9 representada pelos estados das unidades ocultas e o erro da interpreta\u00e7\u00e3o \u00e9 representado pela energia.\n </p>\n <p>\n </p>\n <h2>\n  6-\u00a0M\u00e1quinas de Boltzmann\n </h2>\n <p style=\"text-align: justify;\">\n  Uma M\u00e1quina de Boltzmann \u00e9 um tipo de rede neural recorrente estoc\u00e1stica. Pode ser visto como a contrapartida estoc\u00e1stica e generativa das Redes Hopfield. Foi uma das primeiras redes neurais capazes de aprender representa\u00e7\u00f5es internas e \u00e9 capaz de representar e resolver problemas combinat\u00f3rios dif\u00edceis.\n </p>\n <p style=\"text-align: justify;\">\n  O objetivo do aprendizado do algoritmo da M\u00e1quina de Boltzmann \u00e9 maximizar o produto das probabilidades que a M\u00e1quina de Boltzmann atribui aos vetores bin\u00e1rios no conjunto de treinamento. Isso equivale a maximizar a soma das probabilidades de log que a M\u00e1quina de Boltzmann atribui aos vetores de treinamento. Tamb\u00e9m \u00e9 equivalente a maximizar a probabilidade de obtermos exatamente os N casos de treinamento se fiz\u00e9ssemos o seguinte: 1) Deixar a rede se estabelecer em sua distribui\u00e7\u00e3o estacion\u00e1ria no tempo N diferente, sem entrada externa e 2) Mudar o vetor vis\u00edvel uma vez em cada passada.\n </p>\n <p style=\"text-align: justify;\">\n  Um procedimento eficiente de aprendizado de mini-lote foi proposto para as\u00a0M\u00e1quinas de Boltzmann por\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Salakhutdinov e Hinton em 2012\n   </a>\n  </span>\n  .\n </p>\n <p>\n  <img alt=\"Boltzmann Machine Network\" class=\"aligncenter wp-image-281 size-large\" data-attachment-id=\"281\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Boltzmann Machine Network\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?fit=1024%2C576\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?fit=300%2C169\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?fit=1280%2C720\" data-orig-size=\"1280,720\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/maquinas/\" data-recalc-dims=\"1\" height=\"576\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=1024%2C576\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=1024%2C576 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=300%2C169 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=768%2C432 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=200%2C113 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?resize=690%2C388 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/maquinas.jpeg?w=1280 1280w\" width=\"1024\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Em uma M\u00e1quina de Boltzmann geral, as atualiza\u00e7\u00f5es estoc\u00e1sticas de unidades precisam ser sequenciais. Existe uma arquitetura especial que permite alternar atualiza\u00e7\u00f5es paralelas que s\u00e3o muito mais eficientes (sem conex\u00f5es dentro de uma camada, sem conex\u00f5es de camada ignorada). Este procedimento de mini-lote torna as atualiza\u00e7\u00f5es da M\u00e1quina de Boltzmann mais paralelas. Isso \u00e9 chamado de Deep Boltzmann Machines (DBM), uma M\u00e1quina de Boltzmann geral, mas com muitas conex\u00f5es ausentes.\n </p>\n <p style=\"text-align: justify;\">\n  Em 2014, Salakhutdinov e Hinton apresentaram outra atualiza\u00e7\u00e3o para seu modelo, chamando-o de M\u00e1quinas Boltzmann Restritas. Elas restringem a conectividade para facilitar a infer\u00eancia e a aprendizagem (apenas uma camada de unidades escondidas e sem conex\u00f5es entre unidades ocultas). Em um RBM, \u00e9 preciso apenas um passo para alcan\u00e7ar o equil\u00edbrio.\n </p>\n <p>\n </p>\n <h2>\n  7- Deep Belief Network\n </h2>\n <p class=\"graf graf--p graf-after--figure\" id=\"1989\" style=\"text-align: justify;\">\n  O backpropagation \u00e9 considerado o m\u00e9todo padr\u00e3o em redes neurais artificiais para calcular a contribui\u00e7\u00e3o de erro de cada neur\u00f4nio ap\u00f3s processar um lote de dados (teremos um cap\u00edtulo inteiro sobre isso). No entanto, existem alguns problemas importantes no backpropagation. Em primeiro lugar, requer dados de treinamento rotulados; enquanto quase todos os dados est\u00e3o sem r\u00f3tulos. Em segundo lugar, o tempo de aprendizagem n\u00e3o escala bem, o que significa que \u00e9 muito lento em redes com m\u00faltiplas camadas ocultas. Em terceiro lugar, pode ficar preso em um \u201clocal optima\u201d. Portanto, para redes profundas, o backpropagation est\u00e1 longe de ser \u00f3timo.\n </p>\n <p class=\"graf graf--p graf-after--p\" id=\"a0aa\" style=\"text-align: justify;\">\n  Para superar as limita\u00e7\u00f5es do backpropagation, os pesquisadores consideraram o uso de abordagens de aprendizado sem supervis\u00e3o. Isso ajuda a manter a efici\u00eancia e a simplicidade de usar um m\u00e9todo de gradiente para ajustar os pesos, mas tamb\u00e9m us\u00e1-lo para modelar a estrutura da entrada sensorial. Em particular, eles ajustam os pesos para maximizar a probabilidade de um modelo gerador ter gerado a entrada sensorial. A quest\u00e3o \u00e9 que tipo de modelo generativo devemos aprender? Pode ser um modelo baseado em energia como uma M\u00e1quina de Boltzmann? Ou um modelo causal feito de neur\u00f4nios? Ou um h\u00edbrido dos dois?\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Deep Belief Network\" class=\"aligncenter size-large wp-image-283\" data-attachment-id=\"283\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Deep Belief Network\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?fit=1024%2C528\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?fit=300%2C155\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?fit=1600%2C825\" data-orig-size=\"1600,825\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/dbn/\" data-recalc-dims=\"1\" height=\"528\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=1024%2C528\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=1024%2C528 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=300%2C155 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=768%2C396 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=200%2C103 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?resize=690%2C356 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/dbn.png?w=1600 1600w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Uma Deep Belief Network pode ser definida como uma pilha de M\u00e1quinas de Boltzmann Restritas (RBM \u2013 Restricted Boltzmann Machines), em que cada camada RBM se comunica com as camadas anterior e posterior. Os n\u00f3s de qualquer camada \u00fanica n\u00e3o se comunicam lateralmente.\n </p>\n <p style=\"text-align: justify;\">\n  Esta pilha de RBMs pode terminar com uma camada Softmax para criar um classificador, ou simplesmente pode ajudar a agrupar dados n\u00e3o gravados em um cen\u00e1rio de aprendizado sem supervis\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Com a exce\u00e7\u00e3o das camadas inicial e final, cada camada em uma Deep Belief Network tem uma fun\u00e7\u00e3o dupla: ela serve como a camada oculta para os n\u00f3s que vem antes, e como a camada de entrada (ou \u201cvis\u00edvel\u201d) para a n\u00f3s que vem depois. \u00c9 uma rede constru\u00edda de redes de camada \u00fanica.\n </p>\n <p style=\"text-align: justify;\">\n  As Deep Belief Networks s\u00e3o usadas para reconhecer, agrupar e gerar imagens, sequ\u00eancias de v\u00eddeos e dados de captura de movimento.\n  <span style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;\">\n   Outra aplica\u00e7\u00e3o das Deep Belief Networks \u00e9 no\n  </span>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-processamento-de-linguagem-natural-e-reconhecimento-de-voz\" rel=\"noopener\" target=\"_blank\">\n    Processamento de Linguagem Natural\n   </a>\n  </span>\n  <span style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;\">\n   .\u00a0Esse tipo de rede foi apresentado por Geoff Hinton e seus alunos em 2006.\n  </span>\n </p>\n <p>\n </p>\n <h2>\n  8- Deep Auto-Encoders\n </h2>\n <p style=\"text-align: justify;\">\n  Um Deep Auto-Encoder \u00e9 composto por duas redes sim\u00e9tricas Deep Belief que tipicamente t\u00eam quatro ou cinco camadas rasas que representam a metade da codifica\u00e7\u00e3o (encoder) da rede e o segundo conjunto de quatro ou cinco camadas que comp\u00f5em a metade da decodifica\u00e7\u00e3o (decoder).\n </p>\n <p style=\"text-align: justify;\">\n  As camadas s\u00e3o M\u00e1quinas de Boltzmann Restritas, os blocos de constru\u00e7\u00e3o das Deep Belief Networks, com v\u00e1rias peculiaridades que discutiremos abaixo. Aqui est\u00e1 um esquema simplificado da estrutura de um Deep Auto-Encoder:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Deep Auto-Encoder\" class=\"aligncenter size-full wp-image-285\" data-attachment-id=\"285\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Deep Auto-Encoder\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?fit=540%2C365\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?fit=300%2C203\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?fit=540%2C365\" data-orig-size=\"540,365\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/deep_autoencoder/\" data-recalc-dims=\"1\" height=\"365\" sizes=\"(max-width: 540px) 100vw, 540px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?resize=540%2C365\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?w=540 540w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?resize=300%2C203 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/deep_autoencoder.png?resize=200%2C135 200w\" width=\"540\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Os Deep Auto-Encoders s\u00e3o uma maneira muito agrad\u00e1vel de reduzir a dimensionalidade n\u00e3o linear devido a alguns motivos: eles fornecem mapeamentos flex\u00edveis em ambos os sentidos. O tempo de aprendizagem \u00e9 linear (ou melhor) no n\u00famero de casos de treinamento. E o modelo de codifica\u00e7\u00e3o final \u00e9 bastante compacto e r\u00e1pido. No entanto, pode ser muito dif\u00edcil otimizar Deep Auto-Encoders usando backpropagation. Com pequenos pesos iniciais, o gradiente do backpropagation morre. Mas temos maneiras de otimiz\u00e1-las, usando o pr\u00e9-treinamento camada-por-camada sem supervis\u00e3o ou apenas inicializando os pesos com cuidado.\n </p>\n <p style=\"text-align: justify;\">\n  Os Deep Auto-Encoders s\u00e3o \u00fateis na modelagem de t\u00f3picos ou modelagem estat\u00edstica de t\u00f3picos abstratos que s\u00e3o distribu\u00eddos em uma cole\u00e7\u00e3o de documentos. Isso, por sua vez, \u00e9 um passo importante em sistemas de perguntas e respostas como o IBM Watson.\n </p>\n <p style=\"text-align: justify;\">\n  Em resumo, cada documento em uma cole\u00e7\u00e3o \u00e9 convertido em um Bag-of-Words (ou seja, um conjunto de contagens de palavras) e essas contagens de palavras s\u00e3o dimensionadas para decimais entre 0 e 1, o que pode ser pensado como a probabilidade de uma palavra ocorrer no documento.\n </p>\n <p style=\"text-align: justify;\">\n  As contagens de palavras em escala s\u00e3o ent\u00e3o alimentadas em uma Deep Belief Network, uma pilha de M\u00e1quinas de Boltzmann Restritas, que elas mesmas s\u00e3o apenas um subconjunto de Autoencoders. Essas Deep Belief Networks, ou DBNs, comprimem cada documento para um conjunto de 10 n\u00fameros atrav\u00e9s de uma s\u00e9rie de transforma\u00e7\u00f5es sigm\u00f3ides que o mapeiam no espa\u00e7o de recursos.\n </p>\n <p style=\"text-align: justify;\">\n  O conjunto de n\u00fameros de cada documento, ou vetor, \u00e9 ent\u00e3o introduzido no mesmo espa\u00e7o vetorial, e sua dist\u00e2ncia de qualquer outro vetor de documento medido. Em termos aproximados, os vetores de documentos pr\u00f3ximos se enquadram no mesmo t\u00f3pico. Por exemplo, um documento poderia ser a \u201cpergunta\u201d e outros poderiam ser as \u201crespostas\u201d, uma combina\u00e7\u00e3o que o software faria usando medidas de espa\u00e7o vetorial.\n </p>\n <p style=\"text-align: justify;\">\n  Em resumo, existem agora muitas maneiras diferentes de fazer pr\u00e9-treinamento camada-por-camada de recursos. Para conjuntos de dados que n\u00e3o possuem um grande n\u00famero de casos rotulados, o pr\u00e9-treinamento ajuda a aprendizagem discriminativa subsequente. Para conjuntos de dados muito grandes e rotulados, n\u00e3o \u00e9 necess\u00e1rio inicializar os pesos utilizados na aprendizagem supervisionada usando pr\u00e9-treinamento n\u00e3o supervisionado, mesmo para redes profundas. O pr\u00e9-treinamento foi o primeiro bom caminho para inicializar os pesos para redes profundas, mas agora existem outras formas. Mas se constru\u00edmos redes muito maiores, precisaremos de pr\u00e9-treinamento novamente! Se quiser aprender a construir Deep Auto-Encoders em Python, clique\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  .\n </p>\n <p>\n </p>\n <h2>\n  9- Generative Adversarial Network\n </h2>\n <p style=\"text-align: justify;\">\n  As Generative Adversarial Networks (GANs) s\u00e3o arquiteturas de redes neurais profundas compostas por duas redes, colocando uma contra a outra (da\u00ed o nome, \u201cadvers\u00e1ria\u201d).\n </p>\n <p style=\"text-align: justify;\">\n  Os GANs foram introduzidos em um artigo de Ian Goodfellow e outros pesquisadores da Universidade de Montreal no Canad\u00e1, incluindo Yoshua Bengio, em 2014. Referindo-se aos GANs, o diretor de pesquisa de IA do Facebook, Yann LeCun, chamou de treinamento advers\u00e1rio \u201ca ideia mais interessante nos \u00faltimos 10 anos em Machine Learning\u201d.\n </p>\n <p style=\"text-align: justify;\">\n  O potencial de GANs \u00e9 enorme, porque eles podem aprender a imitar qualquer distribui\u00e7\u00e3o de dados. Ou seja, os GANs podem ser ensinados a criar mundos estranhamente semelhantes aos nossos em qualquer dom\u00ednio: imagens, m\u00fasica, fala, prosa. Eles s\u00e3o artistas rob\u00f4s em um sentido, e sua produ\u00e7\u00e3o \u00e9 impressionante \u2013 at\u00e9 mesmo pungente.\n </p>\n <p style=\"text-align: justify;\">\n  Para entender os GANs, voc\u00ea deve saber como os algoritmos geradores funcionam, e para isso, contrast\u00e1-los com algoritmos discriminat\u00f3rios \u00e9 \u00fatil. Os algoritmos discriminat\u00f3rios tentam classificar dados de entrada; isto \u00e9, dados os recursos de uma inst\u00e2ncia de dados, eles predizem um r\u00f3tulo ou categoria a que esses dados pertencem.\n </p>\n <p style=\"text-align: justify;\">\n  Por exemplo, tendo em conta todas as palavras em um e-mail, um algoritmo discriminat\u00f3rio pode prever se a mensagem \u00e9 spam ou not_spam. O spam \u00e9 um dos r\u00f3tulos, e o saco de palavras (Bag of Words) coletadas do e-mail s\u00e3o os recursos que constituem os dados de entrada. Quando este problema \u00e9 expresso matematicamente, o r\u00f3tulo \u00e9 chamado y e os recursos s\u00e3o chamados de x. A formula\u00e7\u00e3o p (y | x) \u00e9 usada para significar \u201ca probabilidade de y dado x\u201d, que neste caso seria traduzido para \u201ca probabilidade de um email ser spam com as palavras que cont\u00e9m\u201d.\n </p>\n <p style=\"text-align: justify;\">\n  Portanto, algoritmos discriminat\u00f3rios mapeiam recursos para r\u00f3tulos. Eles est\u00e3o preocupados apenas com essa correla\u00e7\u00e3o. Uma maneira de pensar sobre algoritmos generativos \u00e9 que eles fazem o contr\u00e1rio. Em vez de prever um r\u00f3tulo com determinados recursos, eles tentam prever os recursos com um determinado r\u00f3tulo.\n </p>\n <p style=\"text-align: justify;\">\n  A quest\u00e3o que um algoritmo gerador tenta responder \u00e9: assumir que este e-mail \u00e9 spam, qual a probabilidade dos recursos? Enquanto os modelos discriminativos se preocupam com a rela\u00e7\u00e3o entre y e x, os modelos generativos se preocupam com \u201ccomo voc\u00ea obt\u00e9m x\u201d. Eles permitem que voc\u00ea capture p (x | y), a probabilidade de x dado y, ou a probabilidade de caracter\u00edsticas oferecidas em uma classe . (Dito isto, os algoritmos geradores tamb\u00e9m podem ser usados \u200b\u200bcomo classificadores, embora eles podem fazer mais do que categorizar dados de entrada.)\n </p>\n <p style=\"text-align: justify;\">\n  Outra maneira de pensar sobre isso \u00e9 distinguir discriminativo de gerador assim:\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   Modelos discriminativos aprendem o limite entre as classes\n  </li>\n  <li style=\"text-align: justify;\">\n   Modelos generativos modelam a distribui\u00e7\u00e3o de classes individuais\n  </li>\n </ul>\n <p>\n </p>\n <p>\n  <img alt=\"Generative Adversarial Network\" class=\"aligncenter size-large wp-image-270\" data-attachment-id=\"270\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Generative Adversarial Network\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?fit=1024%2C447\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?fit=300%2C131\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?fit=1213%2C529\" data-orig-size=\"1213,529\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/gans/\" data-recalc-dims=\"1\" height=\"447\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=1024%2C447\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=1024%2C447 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=300%2C131 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=768%2C335 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=200%2C87 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?resize=690%2C301 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/GANs.png?w=1213 1213w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Uma rede neural, chamada de gerador, gera novas inst\u00e2ncias de dados, enquanto a outra, o discriminador, as avalia por autenticidade; ou seja, o discriminador decide se cada inst\u00e2ncia de dados que revisa pertence ao conjunto de dados de treinamento real ou n\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Digamos que estamos tentando fazer algo mais banal do que imitar a Mona Lisa. Vamos gerar n\u00fameros escritos \u00e0 m\u00e3o como os encontrados no conjunto de dados\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"noopener\" target=\"_blank\">\n    MNIST\n   </a>\n  </span>\n  , que \u00e9 retirado do mundo real. O objetivo do discriminador, quando mostrado uma inst\u00e2ncia do verdadeiro conjunto de dados MNIST, \u00e9 reconhec\u00ea-los como aut\u00eanticos.\n </p>\n <p style=\"text-align: justify;\">\n  Enquanto isso, o gerador est\u00e1 criando novas imagens que passa para o discriminador. Isso acontece com a esperan\u00e7a de que eles, tamb\u00e9m, sejam considerados aut\u00eanticos, embora sejam falsos. O objetivo do gerador \u00e9 gerar d\u00edgitos \u200b\u200bescritos \u00e0 m\u00e3o por si mesmo. O objetivo do discriminador \u00e9 identificar as imagens provenientes do gerador como falsas.\n </p>\n <p style=\"text-align: justify;\">\n  Aqui est\u00e3o os passos que um GAN realiza:\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   O gerador recebe n\u00fameros aleat\u00f3rios e retorna uma imagem.\n  </li>\n  <li style=\"text-align: justify;\">\n   Essa imagem gerada \u00e9 alimentada no discriminador ao lado de um fluxo de imagens tiradas do conjunto de dados real.\n  </li>\n  <li style=\"text-align: justify;\">\n   O discriminador assume imagens reais e falsas e retorna probabilidades, um n\u00famero entre 0 e 1, com 1 representando uma previs\u00e3o de autenticidade e 0 representando falsas.\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  Ent\u00e3o voc\u00ea tem um loop de feedback duplo:\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   O discriminador est\u00e1 em um loop de feedback com as imagens verdadeiras, que conhecemos.\n  </li>\n  <li style=\"text-align: justify;\">\n   O gerador est\u00e1 em um loop de feedback com o discriminador.\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  Quer aprender como construir GANs, uma das arquiteturas mais incr\u00edveis de Deep Learning, 100% em portugu\u00eas e 100% online, para gerar imagens de forma autom\u00e1tica? Clique\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  .\n </p>\n <p>\n </p>\n <h2>\n  10- Deep Neural Network Capsules\n </h2>\n <p style=\"text-align: justify;\">\n  No final de 2017, Geoffrey Hinton e sua equipe publicaram dois artigos que introduziram um novo tipo de rede neural chamada\n  <em>\n   <strong>\n    Capsules\n   </strong>\n  </em>\n  . Al\u00e9m disso, a equipe publicou um algoritmo, denominado roteamento din\u00e2mico entre c\u00e1psulas, que permite treinar essa rede.\n </p>\n <p style=\"text-align: justify;\">\n  Para todos na comunidade de Deep Learning, esta \u00e9 uma grande not\u00edcia, e por v\u00e1rias raz\u00f5es. Em primeiro lugar, Hinton \u00e9 um dos fundadores do Deep Learning e um inventor de in\u00fameros modelos e algoritmos que hoje s\u00e3o amplamente utilizados. Em segundo lugar, esses artigos apresentam algo completamente novo, e isso \u00e9 muito emocionante porque provavelmente estimular\u00e1 a onda adicional de pesquisas e aplicativos muito inovadores.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Capsule\" class=\"aligncenter size-full wp-image-271\" data-attachment-id=\"271\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Capsule\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?fit=1000%2C303\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?fit=300%2C91\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?fit=1000%2C303\" data-orig-size=\"1000,303\" data-permalink=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/capsule/\" data-recalc-dims=\"1\" height=\"303\" sizes=\"(max-width: 1000px) 100vw, 1000px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=1000%2C303\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?w=1000 1000w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=300%2C91 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=768%2C233 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=200%2C61 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/02/capsule.png?resize=690%2C209 690w\" width=\"1000\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  As\n  <em>\n   <strong>\n    Capsules\n   </strong>\n  </em>\n  introduzem um novo bloco de constru\u00e7\u00e3o que pode ser usado na aprendizagem profunda para modelar melhor as rela\u00e7\u00f5es hier\u00e1rquicas dentro da representa\u00e7\u00e3o do conhecimento interno de uma rede neural. A intui\u00e7\u00e3o por tr\u00e1s deles \u00e9 muito simples e elegante.\n </p>\n <p style=\"text-align: justify;\">\n  Hinton e sua equipe propuseram uma maneira de treinar essa rede composta de c\u00e1psulas e treinou-a com \u00eaxito em um conjunto de dados simples, alcan\u00e7ando desempenho de ponta. Isso \u00e9 muito encorajador. No entanto, h\u00e1 desafios. As implementa\u00e7\u00f5es atuais s\u00e3o muito mais lentas do que outros modelos modernos de aprendizado profundo. O tempo mostrar\u00e1 se as redes\n  <em>\n   <strong>\n    Capsules\n   </strong>\n  </em>\n  podem ser treinadas de forma r\u00e1pida e eficiente. Al\u00e9m disso, precisamos ver se elas funcionam bem em conjuntos de dados mais dif\u00edceis e em diferentes dom\u00ednios.\n </p>\n <p style=\"text-align: justify;\">\n  Em qualquer caso, a rede\n  <em>\n   <strong>\n    Capsule\n   </strong>\n  </em>\n  \u00e9 um modelo muito interessante e j\u00e1 funcionando, que definitivamente se desenvolver\u00e1 ao longo do tempo e contribuir\u00e1 para uma maior expans\u00e3o de aplica\u00e7\u00f5es de aprendizagem profunda.\n </p>\n <p style=\"text-align: justify;\">\n  Inclu\u00edmos as Capsules entre as 10 principais arquiteturas de redes neurais, pois elas representam a inova\u00e7\u00e3o e o avan\u00e7o na incr\u00edvel e vibrante \u00e1rea de Deep Learning e sistemas de Intelig\u00eancia Artificial. Profissionais que realmente desejem abra\u00e7ar IA como carreira, devem estar atentos aos movimentos e inova\u00e7\u00f5es na \u00e1rea.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Esta n\u00e3o \u00e9 uma lista definitiva de arquiteturas e existem outras, tais como\u00a0Word2Vec, Doc2vec, Neural Embeddings e varia\u00e7\u00f5es das arquiteturas aqui apresentadas, como\u00a0Denoising Autoencoders, Variational Autoencoders, al\u00e9m de outras categorias como Deep Reinforcement Learning. Exatamente para auxiliar aqueles que buscam conhecimento de ponta 100% em portugu\u00eas e 100% online, que n\u00f3s criamos a\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n  , o \u00fanico programa do Brasil completo, com todas as ferramentas que o aluno precisa para aprender a trabalhar com IA de forma eficiente. O aluno aprende programa\u00e7\u00e3o paralela em GPU, Deep Learning e seus frameworks, estuda as principais arquiteturas com aplica\u00e7\u00f5es pr\u00e1ticas e desenvolve aplica\u00e7\u00f5es de Vis\u00e3o Computacional e Processamento de Linguagem Natural. Clique\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  e veja mais detalhes sobre o programa.\n </p>\n <p>\n  Isso conclui a primeira parte deste livro, com uma introdu\u00e7\u00e3o ao universo do Deep Learning. No pr\u00f3ximo cap\u00edtulo come\u00e7aremos a ver as redes neurais em a\u00e7\u00e3o. At\u00e9 l\u00e1.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://deeplearning4j.org/multilayerperceptron.html\" rel=\"noopener\" target=\"_blank\">\n    Multilayer Perceptrons\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://people.idsia.ch/~juergen/rnn.html\" rel=\"noopener\" target=\"_blank\">\n    Long Short-Term Memory\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\" rel=\"noopener\" target=\"_blank\">\n    Long Short-Term Memory Neural Computation\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"noopener\" target=\"_blank\">\n    The Unreasonable Effectiveness of Recurrent Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://mitpress.mit.edu/books/perceptrons\" rel=\"noopener\" target=\"_blank\">\n    Perceptrons, Expanded Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/@james_aka_yale/the-8-neural-network-architectures-machine-learning-researchers-need-to-learn-2f5c4e61aeeb\" rel=\"noopener\" target=\"_blank\">\n    The 8 Neural Network Architectures Machine Learning Researchers Need to Learn\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/abs/1710.09829v1\" rel=\"noopener\" target=\"_blank\">\n    Dynamic Routing Between Capsules\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.pnas.org/content/pnas/79/8/2554.full.pdf\" rel=\"noopener\" target=\"_blank\">\n    Neural networks and physical systems with emergent collective computational abilities\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Deep Boltzmann Machines\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.cs.toronto.edu/~hinton/absps/ruhijournal.pdf\" rel=\"noopener\" target=\"_blank\">\n    Application of Deep Belief Networks for Natural Language Understanding\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\" rel=\"noopener\" target=\"_blank\">\n    A fast learning algorithm for deep belief nets\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://deeplearning4j.org/deepautoencoder\" rel=\"noopener\" target=\"_blank\">\n    A Beginner\u2019s Guide to Deep Autoencoders\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://openreview.net/pdf?id=HJWLfGWRb\" rel=\"noopener\" target=\"_blank\">\n    Matrix Capsules With RM Routing\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf\" rel=\"noopener\" target=\"_blank\">\n    Generative Adversarial Networks (GANs)\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-255\" href=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-255\" href=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-255\" href=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-255\" href=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-255-5e0dd04fd8297\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=255&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-255-5e0dd04fd8297\" id=\"like-post-wrapper-140353593-255-5e0dd04fd8297\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "11": "<h1 class=\"entry-title\" id=\"capitulo-11\">\n Cap\u00edtulo 11 \u2013 Design De Uma Rede Neural Para Reconhecimento de D\u00edgitos\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Na primeira parte deste livro online, durante os 10 primeiros cap\u00edtulos, definimos e estudamos o universo das redes neurais artificias. Neste ponto voc\u00ea j\u00e1 deve ter uma boa compreens\u00e3o sobre que s\u00e3o estes algoritmos e como podem ser usados, al\u00e9m da import\u00e2ncia das redes neurais para a constru\u00e7\u00e3o de sistemas de Intelig\u00eancia Artificial. Estamos prontos para iniciar a constru\u00e7\u00e3o de redes neurais e na sequ\u00eancia estudaremos as arquiteturas mais avan\u00e7adas. Vamos come\u00e7ar\u00a0definindo o\u00a0Design De Uma Rede Neural Para Reconhecimento de D\u00edgitos.\n </p>\n <p style=\"text-align: justify;\">\n  Nossa primeira tarefa ser\u00e1 construir uma rede neural para reconhecer caligrafia, ou seja, d\u00edgitos escritos \u00e0 m\u00e3o que foram digitalizados em imagens no computador. Por que vamos come\u00e7ar com este tipo de tarefa? Porque ela permite percorrer todas as etapas e procedimentos matem\u00e1ticos de uma rede neural, sendo portanto uma excelente introdu\u00e7\u00e3o. Vamos come\u00e7ar?\n </p>\n <p style=\"text-align: justify;\">\n  Se voc\u00ea acompanha os cursos na\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br\" rel=\"noopener\" target=\"_blank\">\n    Data Science Academy\n   </a>\n  </span>\n  j\u00e1 sabe que: antes de pensar em escrever sua primeira linha de c\u00f3digo, \u00e9 preciso definir claramente o problema a ser resolvido. A tecnologia existe para resolver problemas e a defini\u00e7\u00e3o clara do objetivo \u00e9 o ponto de partida de qualquer projeto de sucesso! Neste cap\u00edtulo definiremos o problema a ser resolvido, nesse caso o reconhecimento de d\u00edgitos\u00a0manuscritos.\n </p>\n <p style=\"text-align: justify;\">\n  Podemos dividir o problema de reconhecer os d\u00edgitos manuscritos em dois sub-problemas. Primeiro, precisamos encontrar uma maneira de quebrar uma imagem que contenha muitos d\u00edgitos em uma sequ\u00eancia de imagens separadas, cada uma contendo um \u00fanico d\u00edgito. Por exemplo, gostar\u00edamos de quebrar a imagem:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter wp-image-309 size-medium\" data-attachment-id=\"309\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"digits\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?fit=623%2C128\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?fit=300%2C62\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?fit=623%2C128\" data-orig-size=\"623,128\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/digits-2/\" data-recalc-dims=\"1\" height=\"62\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?resize=300%2C62\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?resize=300%2C62 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?resize=200%2C41 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits.png?w=623 623w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p>\n  em seis imagens separadas:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-medium wp-image-310\" data-attachment-id=\"310\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"digits_separate\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?fit=630%2C99\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?fit=300%2C47\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?fit=630%2C99\" data-orig-size=\"630,99\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/digits_separate/\" data-recalc-dims=\"1\" height=\"47\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?resize=300%2C47\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?resize=300%2C47 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?resize=200%2C31 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate.png?w=630 630w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  N\u00f3s, humanos, resolvemos esse problema de segmenta\u00e7\u00e3o com facilidade, mas \u00e9 um desafio para um programa de computador dividir corretamente a imagem. Uma vez que a imagem foi segmentada, o programa precisa classificar cada d\u00edgito individual. Ent\u00e3o, por exemplo, gostar\u00edamos que nosso programa reconhecesse automaticamente que o primeiro d\u00edgito acima \u00e9 um 5:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-full wp-image-311\" data-attachment-id=\"311\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"mnist_first_digit\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?fit=31%2C35\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?fit=31%2C35\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?fit=31%2C35\" data-orig-size=\"31,35\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_first_digit/\" data-recalc-dims=\"1\" height=\"35\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_first_digit.png?resize=31%2C35\" width=\"31\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Vamos nos concentrar em escrever um programa para resolver o segundo problema, isto \u00e9, classificar d\u00edgitos individuais. O problema da segmenta\u00e7\u00e3o n\u00e3o \u00e9 t\u00e3o dif\u00edcil de resolver, uma vez que voc\u00ea tenha uma boa maneira de classificar os d\u00edgitos individuais. Existem muitas abordagens para resolver o problema de segmenta\u00e7\u00e3o. Uma abordagem \u00e9 testar muitas maneiras diferentes de segmentar a imagem, usando o classificador de d\u00edgitos individuais para marcar cada segmenta\u00e7\u00e3o de teste. Uma segmenta\u00e7\u00e3o de teste obt\u00e9m uma pontua\u00e7\u00e3o alta se o classificador de d\u00edgitos individuais estiver confiante de sua classifica\u00e7\u00e3o em todos os segmentos e uma pontua\u00e7\u00e3o baixa se o classificador tiver muitos problemas em um ou mais segmentos. A ideia \u00e9 que, se o classificador estiver tendo problemas em algum lugar, provavelmente est\u00e1 tendo problemas porque a segmenta\u00e7\u00e3o foi escolhida incorretamente. Essa ideia e outras varia\u00e7\u00f5es podem ser usadas para resolver o problema de segmenta\u00e7\u00e3o. Ent\u00e3o, em vez de se preocupar com a segmenta\u00e7\u00e3o, nos concentraremos no desenvolvimento de uma rede neural que pode resolver o problema mais interessante e dif\u00edcil, ou seja, reconhecer d\u00edgitos individuais manuscritos.\n </p>\n <p>\n  Para reconhecer d\u00edgitos individuais, usaremos uma rede neural de tr\u00eas camadas:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter wp-image-312 size-full\" data-attachment-id=\"312\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tikz12\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?fit=537%2C447\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?fit=300%2C250\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?fit=537%2C447\" data-orig-size=\"537,447\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/tikz12/\" data-recalc-dims=\"1\" height=\"447\" sizes=\"(max-width: 537px) 100vw, 537px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?resize=537%2C447\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?w=537 537w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?resize=300%2C250 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/tikz12.png?resize=200%2C166 200w\" width=\"537\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A camada de entrada da rede cont\u00e9m neur\u00f4nios que codificam os valores dos pixels de entrada. Conforme iremos discutir no pr\u00f3ximo cap\u00edtulo, nossos dados de treinamento para a rede consistir\u00e3o em muitas imagens de 28 por 28 pixels de d\u00edgitos manuscritos digitalizados e, portanto, a camada de entrada cont\u00e9m 28 \u00d7 28 = 784 neur\u00f4nios (Nota: uma imagem nada mais \u00e9 do que uma matriz, nesse caso de dimens\u00f5es 28\u00d728, que iremos converter em um vetor cujo tamanho ser\u00e1 784, onde cada item representa um pixel na imagem). Os pixels de entrada s\u00e3o de escala de cinza, com um valor de 0.0 representando branco e um valor de 1.0 representando preto. Valores intermedi\u00e1rios representam tonalidades gradualmente escurecidas de cinza.\n </p>\n <p style=\"text-align: justify;\">\n  A segunda camada da rede \u00e9 uma camada oculta. Representaremos o n\u00famero de neur\u00f4nios nesta camada oculta por n, e vamos experimentar diferentes valores para n. O exemplo mostrado acima ilustra uma pequena camada oculta, contendo apenas n = 15 neur\u00f4nios.\n </p>\n <p style=\"text-align: justify;\">\n  A camada de sa\u00edda da rede cont\u00e9m 10 neur\u00f4nios. Se o primeiro neur\u00f4nio \u201cdisparar\u201d (for ativado), ou seja, tiver uma sa\u00edda \u2248 1, ent\u00e3o isso indicar\u00e1 que a rede acha que o d\u00edgito \u00e9 0. Se o segundo neur\u00f4nio \u201cdisparar\u201d (for ativado), isso indicar\u00e1 que a rede pensa que o d\u00edgito \u00e9 um 1. E assim por diante. Em resumo, vamos numerar os neur\u00f4nios de sa\u00edda de 0 a 9 e descobrimos qual neur\u00f4nio possui o maior valor de ativa\u00e7\u00e3o. Se esse neur\u00f4nio \u00e9, digamos, neur\u00f4nio n\u00famero 6, ent\u00e3o nossa rede adivinhar\u00e1 que o d\u00edgito de entrada era um 6. E assim por diante para os outros neur\u00f4nios de sa\u00edda.\n </p>\n <p style=\"text-align: justify;\">\n  Voc\u00ea pode se perguntar por que usamos 10 neur\u00f4nios de sa\u00edda. Afinal, o objetivo da rede \u00e9 nos dizer qual d\u00edgito (0,1,2, \u2026, 9) corresponde \u00e0 imagem de entrada. Uma maneira aparentemente natural de fazer isso \u00e9 usar apenas 4 neur\u00f4nios de sa\u00edda, tratando cada neur\u00f4nio como assumindo um valor bin\u00e1rio, dependendo se a sa\u00edda do neur\u00f4nio est\u00e1 mais pr\u00f3xima de 0 ou 1. Quatro neur\u00f4nios s\u00e3o suficientes para codificar a resposta, desde que 2\u02c64 = 16 \u00e9 mais do que os 10 valores poss\u00edveis para o d\u00edgito de entrada. Por que nossa rede deve usar 10 neur\u00f4nios em vez disso? Isso n\u00e3o \u00e9 ineficiente? A justificativa final \u00e9 emp\u00edrica: podemos experimentar ambos os projetos de rede, e verifica-se que, para este problema espec\u00edfico, a rede com 10 neur\u00f4nios de sa\u00edda aprende a reconhecer d\u00edgitos melhor do que a rede com 4 neur\u00f4nios de sa\u00edda. Mas isso ainda deixa a pergunta por que o uso de 10 neur\u00f4nios de sa\u00edda funciona melhor. Existe alguma heur\u00edstica que nos diga com anteced\u00eancia que devemos usar a codifica\u00e7\u00e3o de 10 sa\u00eddas em vez da codifica\u00e7\u00e3o de 4 sa\u00eddas?\n </p>\n <p style=\"text-align: justify;\">\n  Entender porque fazemos isso, ajuda a pensar sobre o que a rede neural est\u00e1 realmente fazendo. Considere primeiro o caso em que usamos 10 neur\u00f4nios de sa\u00edda. Vamos nos concentrar no primeiro neur\u00f4nio de sa\u00edda, aquele que est\u00e1 tentando decidir se o d\u00edgito \u00e9 ou n\u00e3o 0. Ele faz isso pesando evid\u00eancias da camada oculta dos neur\u00f4nios. O que esses neur\u00f4nios ocultos est\u00e3o fazendo? Bem, vamos supor que o primeiro neur\u00f4nio na camada oculta detecta ou n\u00e3o uma imagem como a seguinte:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter wp-image-313 size-thumbnail\" data-attachment-id=\"313\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"mnist_top_left_feature\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?fit=497%2C511\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?fit=292%2C300\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?fit=497%2C511\" data-orig-size=\"497,511\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_top_left_feature/\" data-recalc-dims=\"1\" height=\"150\" sizes=\"(max-width: 150px) 100vw, 150px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?resize=150%2C150\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?resize=150%2C150 150w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?resize=280%2C280 280w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_top_left_feature.png?zoom=3&amp;resize=150%2C150 450w\" width=\"150\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Isso pode ser feito pesando fortemente pixels de entrada que se sobrep\u00f5em \u00e0 imagem e apenas ponderam ligeiramente as outras entradas. De forma semelhante, suponhamos que o segundo, terceiro e quarto neur\u00f4nios na camada oculta detectem se as seguintes imagens est\u00e3o ou n\u00e3o presentes:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-medium wp-image-314\" data-attachment-id=\"314\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"mnist_other_features\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?fit=636%2C203\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?fit=300%2C96\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?fit=636%2C203\" data-orig-size=\"636,203\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_other_features/\" data-recalc-dims=\"1\" height=\"96\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?resize=300%2C96\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?resize=300%2C96 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?resize=200%2C64 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_other_features.png?w=636 636w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Como voc\u00ea pode ter adivinhado, essas quatro imagens juntas comp\u00f5em a imagem 0 que vimos na linha de d\u00edgitos mostrada anteriormente:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-medium wp-image-315\" data-attachment-id=\"315\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"mnist_complete_zero\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?fit=495%2C497\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?fit=300%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?fit=495%2C497\" data-orig-size=\"495,497\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/mnist_complete_zero/\" data-recalc-dims=\"1\" height=\"300\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=300%2C300\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=300%2C300 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=150%2C150 150w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=200%2C201 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?resize=280%2C280 280w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/mnist_complete_zero.png?w=495 495w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Ent\u00e3o, se todos os quatro neur\u00f4nios ocultos est\u00e3o disparando, podemos concluir que o d\u00edgito \u00e9 um 0. Claro, esse n\u00e3o \u00e9 o \u00fanico tipo de evid\u00eancia que podemos usar para concluir que a imagem era um 0 \u2013 podemos legitimamente obter um 0 em muitas outras maneiras (por exemplo, atrav\u00e9s de tradu\u00e7\u00f5es das imagens acima, ou pequenas distor\u00e7\u00f5es). Mas parece seguro dizer que, pelo menos neste caso, concluir\u00edamos que a entrada era um 0.\n </p>\n <p style=\"text-align: justify;\">\n  Supondo que a rede neural funciona assim, podemos dar uma explica\u00e7\u00e3o plaus\u00edvel sobre porque \u00e9 melhor ter 10 sa\u00eddas da rede, em vez de 4. Se tiv\u00e9ssemos 4 sa\u00eddas, o primeiro neur\u00f4nio de sa\u00edda tentaria decidir o que mais um bit significativo do d\u00edgito representa. E n\u00e3o existe uma maneira f\u00e1cil de relacionar esse bit mais significativo com formas simples, como as mostradas acima. As formas componentes do d\u00edgito estar\u00e3o intimamente relacionadas com (digamos) o bit mais significativo na sa\u00edda.\n </p>\n <p style=\"text-align: justify;\">\n  Isso tudo \u00e9 apenas uma heur\u00edstica. Nada diz que a rede neural de tr\u00eas camadas tem que operar da maneira que descrevemos, com os neur\u00f4nios ocultos detectando formas de componentes simples. Talvez um algoritmo de aprendizado inteligente encontre alguma atribui\u00e7\u00e3o de pesos que nos permita usar apenas 4 neur\u00f4nios de sa\u00edda. Mas, usar uma boa heur\u00edstica pode economizar muito tempo na concep\u00e7\u00e3o de boas arquiteturas de redes neurais.\n </p>\n <p style=\"text-align: justify;\">\n  J\u00e1 temos ent\u00e3o um design para a nossa rede neural. Agora precisamos definir como ser\u00e1 o processo de aprendizagem do algoritmo, antes de come\u00e7ar a codificar nossa rede em linguagem Python. Usaremos o treinamento com Gradiente Descendente, assunto do pr\u00f3ximo cap\u00edtulo, que ali\u00e1s eu n\u00e3o perderia por nada, se fosse voc\u00ea, pois a\u00ed est\u00e1 a \u201cmagia\u201d por tr\u00e1s das redes neurais. At\u00e9 l\u00e1!\n </p>\n <p style=\"text-align: justify;\">\n  Para acompanhar os pr\u00f3ximos cap\u00edtulos e reproduzir os exemplos, voc\u00ea deve ter o Anaconda Python instalado no seu computador com Python vers\u00e3o 3.6.x. Acesse o cap\u00edtulo 1 do curso gratuito\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener\" target=\"_blank\">\n    Python Fundamentos Para An\u00e1lise de Dados\n   </a>\n  </span>\n  , para aprender como instalar o Anaconda.\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide\" rel=\"noopener\" target=\"_blank\">\n    Fun\u00e7\u00e3o Sigm\u00f3ide\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n  (alguns trechos extra\u00eddos e traduzidos com autoriza\u00e7\u00e3o do autor\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://michaelnielsen.org/\" rel=\"noopener\" target=\"_blank\">\n    Michael Nielsen\n   </a>\n  </span>\n  )\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-308\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-308\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-308\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-308\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-308-5e0dd051d548e\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=308&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-308-5e0dd051d548e\" id=\"like-post-wrapper-140353593-308-5e0dd051d548e\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "12": "<h1 class=\"entry-title\" id=\"capitulo-12\">\n Cap\u00edtulo 12 \u2013 Aprendizado Com a Descida do Gradiente\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  No cap\u00edtulo anterior definimos o design para a nossa rede neural e agora podemos come\u00e7ar o processo de aprendizado de m\u00e1quina. Neste cap\u00edtulo voc\u00ea vai compreender o que \u00e9 o Aprendizado Com a Descida do Gradiente.\n </p>\n <p style=\"text-align: justify;\">\n  A primeira coisa que precisamos \u00e9 um conjunto de dados para o treinamento da rede. Usaremos o conjunto de dados\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"noopener\" target=\"_blank\">\n    MNIST\n   </a>\n  </span>\n  , que cont\u00e9m dezenas de milhares de imagens digitalizadas de d\u00edgitos manuscritos, juntamente com suas classifica\u00e7\u00f5es corretas. O nome MNIST vem do fato de que \u00e9 um subconjunto modificado de dois conjuntos de dados coletados pelo NIST, o Instituto Nacional de Padr\u00f5es e Tecnologia dos Estados Unidos. Aqui est\u00e3o algumas imagens do MNIST:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-medium wp-image-328\" data-attachment-id=\"328\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"digits_separate\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?fit=630%2C99\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?fit=300%2C47\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?fit=630%2C99\" data-orig-size=\"630,99\" data-permalink=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/digits_separate-2/\" data-recalc-dims=\"1\" height=\"47\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?resize=300%2C47\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?resize=300%2C47 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?resize=200%2C31 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/digits_separate-1.png?w=630 630w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O MNIST tem duas partes. A primeira parte cont\u00e9m 60.000 imagens para serem usadas como dados de treinamento. Essas imagens s\u00e3o amostras de manuscritos escaneados de 250 pessoas, metade dos quais funcion\u00e1rios do Bureau do Censo dos EUA e metade dos estudantes do ensino m\u00e9dio. As imagens est\u00e3o em escala de cinza e 28 por 28 pixels de tamanho. A segunda parte do conjunto de dados MNIST tem 10.000 imagens a serem usadas como dados de teste, tamb\u00e9m 28 por 28 pixels em escala de cinza. Usaremos os dados do teste para avaliar o qu\u00e3o bem a nossa rede neural aprendeu a reconhecer os d\u00edgitos. Para fazer deste um bom teste de desempenho, os dados de teste foram retirados de um conjunto diferente de 250 pessoas em rela\u00e7\u00e3o aos dados de treinamento originais (embora ainda seja um grupo dividido entre funcion\u00e1rios do Census Bureau e alunos do ensino m\u00e9dio). Isso nos ajuda a confiar que nosso sistema pode reconhecer d\u00edgitos de pessoas cuja escrita n\u00e3o viu durante o treinamento.\n </p>\n <p style=\"text-align: justify;\">\n  Usaremos a nota\u00e7\u00e3o x para indicar uma entrada (input) de treinamento. Ser\u00e1 conveniente considerar cada entrada de treinamento x (cada imagem) como um vetor de 784 posi\u00e7\u00f5es (28 x 28 pixels). A imagem abaixo representa como este vetor \u00e9 constru\u00eddo:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-medium wp-image-345\" data-attachment-id=\"345\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"pixels\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?fit=470%2C434\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?fit=300%2C277\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?fit=470%2C434\" data-orig-size=\"470,434\" data-permalink=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/pixels/\" data-recalc-dims=\"1\" height=\"277\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?resize=300%2C277\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?resize=300%2C277 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?resize=200%2C185 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/pixels.png?w=470 470w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Cada entrada no vetor representa o valor de cinza para um \u00fanico pixel na imagem. Vamos indicar a sa\u00edda correspondente desejada por y = y(x), onde y \u00e9 um vetor com dimens\u00e3o 10. Por exemplo, se uma imagem de treinamento particular, x, representa um 3, ent\u00e3o y(x) = (0,0,0,1,0,0,0,0,0,0)T \u00e9 a sa\u00edda desejada da rede . Observe que T aqui \u00e9 a opera\u00e7\u00e3o de transposi\u00e7\u00e3o, transformando um vetor de linha em um vetor comum (coluna). Vamos deixar isso mais claro. Observe a figura abaixo:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter wp-image-346 size-full\" data-attachment-id=\"346\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"label\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?fit=615%2C124\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?fit=300%2C60\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?fit=615%2C124\" data-orig-size=\"615,124\" data-permalink=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/label/\" data-recalc-dims=\"1\" height=\"124\" sizes=\"(max-width: 615px) 100vw, 615px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?resize=615%2C124\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?w=615 615w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?resize=300%2C60 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/label.png?resize=200%2C40 200w\" width=\"615\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Vamos usar os pixels de imagem correspondentes ao fluxo inteiro chamado \u201cfeatures\u201d. Os r\u00f3tulos s\u00e3o One-Hot Encoded 1-hot. O r\u00f3tulo que representa a classe de sa\u00edda da imagem com d\u00edgito 3 torna-se \u201c0001000000\u201d uma vez que temos 10 classes para os 10 d\u00edgitos poss\u00edveis, onde o primeiro \u00edndice corresponde ao d\u00edgito \u201c0\u201d e o \u00faltimo corresponde ao d\u00edgito \u201c9\u201d.\n </p>\n <p style=\"text-align: justify;\">\n  O que queremos \u00e9 um algoritmo que nos permita encontrar pesos e bias para que a sa\u00edda da rede se aproxime de y(x) para todas as entradas de treinamento x. Para quantificar o qu\u00e3o bem estamos alcan\u00e7ando esse objetivo, definimos uma fun\u00e7\u00e3o de custo:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-medium wp-image-329\" data-attachment-id=\"329\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"custo\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?fit=518%2C166\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?fit=300%2C96\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?fit=518%2C166\" data-orig-size=\"518,166\" data-permalink=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/2018-03-14_1123/\" data-recalc-dims=\"1\" height=\"96\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?resize=300%2C96\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?resize=300%2C96 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?resize=200%2C64 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/2018-03-14_1123.png?w=518 518w\" width=\"300\"/>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"font-size: 10pt;\">\n   Fun\u00e7\u00e3o de Custo Quadr\u00e1tico\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Na f\u00f3rmula acima, w indica a coleta de todos os pesos na rede, b todos os bias (vi\u00e9s), n \u00e9 o n\u00famero total de entradas de treinamento, a \u00e9 o vetor de sa\u00eddas da rede (quando x \u00e9 entrada) e a soma \u00e9 sobre todas as entradas de treinamento x. Claro, a sa\u00edda a depende de x, w e b, mas para manter a nota\u00e7\u00e3o simples, eu n\u00e3o indiquei explicitamente essa depend\u00eancia. A nota\u00e7\u00e3o \u2016v\u2016 apenas indica a fun\u00e7\u00e3o de comprimento usual para um vetor v. Chamaremos C a fun\u00e7\u00e3o de custo quadr\u00e1tico, que tamb\u00e9m \u00e9 conhecido como o erro quadr\u00e1tico m\u00e9dio ou apenas o MSE (Mean Squared Error). Inspecionando a forma da fun\u00e7\u00e3o de custo quadr\u00e1tico, vemos que C (w, b) n\u00e3o \u00e9 negativo, pois cada termo na soma n\u00e3o \u00e9 negativo. Al\u00e9m disso, o custo C (w, b) torna-se pequeno, isto \u00e9, C (w, b) \u2248 0, precisamente quando y(x) \u00e9 aproximadamente igual \u00e0 sa\u00edda, a, para todas as entradas de treinamento x.\n </p>\n <p style=\"text-align: justify;\">\n  Portanto, nosso algoritmo de treinamento faz um bom trabalho se ele pode encontrar pesos e bias para que C (w, b) \u2248 0. Isso significa basicamente que nosso modelo fez as previs\u00f5es corretas, ou seja, cada vez que apresentamos ao modelo uma imagem com d\u00edgito 3, ele \u00e9 capaz de reconhecer que se trata do n\u00famero 3.\n </p>\n <p style=\"text-align: justify;\">\n  Em contraste, o algoritmo n\u00e3o ter\u00e1 boa performance, quando C (w, b) for um valor maior que 0 \u2013 isso significaria que nosso algoritmo n\u00e3o est\u00e1 conseguindo fazer as previs\u00f5es, ou seja, quando apresentado a imagem com o d\u00edgito 3, ele n\u00e3o \u00e9 capaz de prever que se trata de um n\u00famero 3. Isso ocorre, porque a diferen\u00e7a entre o valor real da sa\u00edda e o valor previsto pelo modelo, \u00e9 muito alta. Assim, o objetivo do nosso algoritmo de treinamento ser\u00e1 minimizar o custo C(w, b) em fun\u00e7\u00e3o dos pesos e dos bias. Em outras palavras, queremos encontrar um conjunto de pesos e bias que tornem o custo o menor poss\u00edvel. Vamos fazer isso usando um algoritmo conhecido como Descida do Gradiente (Gradient Descent).\n </p>\n <p style=\"text-align: justify;\">\n  Mas antes, uma pergunta. Por que introduzir o custo quadr\u00e1tico? Afinal, n\u00e3o nos interessamos principalmente pelo n\u00famero de imagens corretamente classificadas pela rede? Por que n\u00e3o tentar maximizar esse n\u00famero diretamente, em vez de minimizar uma medida, como o custo quadr\u00e1tico? O problema com isso \u00e9 que o n\u00famero de imagens corretamente classificadas n\u00e3o \u00e9 uma \u201csmooth function\u201d dos pesos e bias na rede. Geralmente, fazer pequenas mudan\u00e7as nos pesos e bias n\u00e3o causar\u00e1 nenhuma altera\u00e7\u00e3o no n\u00famero de imagens de treinamento classificadas corretamente. Isso torna dif\u00edcil descobrir como mudar os pesos e os bias para melhorar o desempenho. Se, em vez disso, usamos uma \u201csmooth cost function\u201d, como o custo quadr\u00e1tico, revela-se f\u00e1cil descobrir como fazer pequenas mudan\u00e7as nos pesos e nos bias para obter uma melhoria no custo. \u00c9 por isso que nos concentramos primeiro na minimiza\u00e7\u00e3o do custo quadr\u00e1tico e somente depois examinaremos a precis\u00e3o da classifica\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Mesmo considerando que queremos usar uma \u201csmooth cost function\u201d, voc\u00ea ainda pode se perguntar por que escolhemos a fun\u00e7\u00e3o quadr\u00e1tica. Talvez se escolh\u00eassemos uma fun\u00e7\u00e3o de custo diferente, obter\u00edamos um conjunto totalmente diferente de pesos e bias? Esta \u00e9 uma preocupa\u00e7\u00e3o v\u00e1lida e, mais tarde, revisitaremos a fun\u00e7\u00e3o de custo e faremos algumas modifica\u00e7\u00f5es. No entanto, a fun\u00e7\u00e3o de custo quadr\u00e1tico mostrada anteriormente funciona perfeitamente para entender os conceitos b\u00e1sicos de aprendizagem em redes neurais, ent\u00e3o ficaremos com isso por enquanto.\n </p>\n <p style=\"text-align: justify;\">\n  Recapitulando, nosso objetivo na constru\u00e7\u00e3o de uma rede neural \u00e9 encontrar pesos e bias que minimizem a fun\u00e7\u00e3o de custo quadr\u00e1tico C (w, b).\n </p>\n <p>\n </p>\n <h3>\n  Descida do Gradiente\n </h3>\n <p style=\"text-align: justify;\">\n  A maioria das tarefas em Machine Learning s\u00e3o na verdade problemas de otimiza\u00e7\u00e3o e um dos algoritmos mais usados para isso \u00e9 o Algoritmo de Descida do Gradiente. Para um iniciante, o nome Algoritmo de Descida do Gradiente pode parecer intimidante, mas espero que depois de ler o que est\u00e1 logo abaixo, isso deixe de ser um mist\u00e9rio para voc\u00ea.\n </p>\n <p style=\"text-align: justify;\">\n  A Descida do Gradiente \u00e9 uma ferramenta padr\u00e3o para otimizar fun\u00e7\u00f5es complexas iterativamente dentro de um programa de computador. Seu objetivo \u00e9: dada alguma fun\u00e7\u00e3o arbitr\u00e1ria, encontrar um m\u00ednimo. Para alguns pequenos subconjuntos de fun\u00e7\u00f5es \u2013 aqueles que s\u00e3o convexos \u2013 h\u00e1 apenas um \u00fanico\n  <em>\n   minumum\n  </em>\n  que tamb\u00e9m acontece de ser global. Para as fun\u00e7\u00f5es mais realistas, pode haver muitos m\u00ednimos, ent\u00e3o a maioria dos m\u00ednimos s\u00e3o locais. Certifique-se de que a otimiza\u00e7\u00e3o encontre o \u201cmelhor\u201d\n  <em>\n   minimum\n  </em>\n  e n\u00e3o fique preso em m\u00ednimos sub-otimistas (um problema comum durante o treinamento do algoritmo).\n </p>\n <p style=\"text-align: justify;\">\n  Para compreender a intui\u00e7\u00e3o da Descida do Gradiente, vamos simplificar um pouco as coisas. Vamos imaginar que simplesmente recebemos uma fun\u00e7\u00e3o de muitas vari\u00e1veis e queremos minimizar essa fun\u00e7\u00e3o. Vamos desenvolver a t\u00e9cnica chamada Descida do Gradiente que pode ser usada para resolver tais problemas de minimiza\u00e7\u00e3o. Ent\u00e3o, voltaremos para a fun\u00e7\u00e3o espec\u00edfica que queremos minimizar para as redes neurais.\n </p>\n <p style=\"text-align: justify;\">\n  Ok, suponhamos que estamos tentando minimizar alguma fun\u00e7\u00e3o, C(v). Esta poderia ser qualquer fun\u00e7\u00e3o de valor real de muitas vari\u00e1veis, onde v = v1, v2, \u2026. Observe que eu substitui a nota\u00e7\u00e3o w e b por v para enfatizar que esta poderia ser qualquer fun\u00e7\u00e3o \u2013 n\u00e3o estamos mais pensando especificamente no contexto das redes neurais apenas. Para minimizar C (v), vamos imaginar C como uma fun\u00e7\u00e3o de apenas duas vari\u00e1veis, que chamaremos v1 e v2, conforme pode ser visto na figura abaixo:\n </p>\n <p>\n  <img alt=\"Descida do Gradiente\" class=\"aligncenter wp-image-330 size-full\" data-attachment-id=\"330\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Descida do Gradiente\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?fit=812%2C612\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?fit=812%2C612\" data-orig-size=\"812,612\" data-permalink=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/valley/\" data-recalc-dims=\"1\" height=\"612\" sizes=\"(max-width: 812px) 100vw, 812px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=812%2C612\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?w=812 812w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=768%2C579 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/valley.png?resize=690%2C520 690w\" width=\"812\"/>\n </p>\n <p style=\"text-align: justify;\">\n  O que queremos \u00e9 encontrar onde C atinge seu m\u00ednimo global. Fica claro, que para a fun\u00e7\u00e3o tra\u00e7ada no gr\u00e1fico acima, podemos observar facilmente o gr\u00e1fico e encontrar o m\u00ednimo. Mas uma fun\u00e7\u00e3o geral, C, pode ser uma fun\u00e7\u00e3o complicada de muitas vari\u00e1veis, e geralmente n\u00e3o ser\u00e1 poss\u00edvel apenas observar o gr\u00e1fico para encontrar o m\u00ednimo.\n </p>\n <p style=\"text-align: justify;\">\n  Uma maneira de atacar o problema \u00e9 usar C\u00e1lculo (especificamente \u00c1lgebra Linear) para tentar encontrar o m\u00ednimo de forma anal\u00edtica. Podemos calcular\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Derivada\" rel=\"noopener\" target=\"_blank\">\n    derivadas\n   </a>\n  </span>\n  e depois tentar us\u00e1-las para encontrar lugares onde C \u00e9 um\n  <em>\n   extremum\n  </em>\n  . Isso pode funcionar quando C \u00e9 uma fun\u00e7\u00e3o de apenas uma ou algumas vari\u00e1veis. Mas vai se transformar em um pesadelo quando tivermos muitas outras vari\u00e1veis. E para as redes neurais, muitas vezes queremos muito mais vari\u00e1veis \u2013 as maiores redes neurais t\u00eam fun\u00e7\u00f5es de custo que dependem de bilh\u00f5es de pesos e bias de uma maneira extremamente complicada. Usando \u201capenas\u201d C\u00e1lculo para minimizar isso, n\u00e3o funcionar\u00e1 e precisamos de algo mais! Precisamos de um algoritmo de otimiza\u00e7\u00e3o capaz de minimizar C (v).\n </p>\n <p style=\"text-align: justify;\">\n  Felizmente, h\u00e1 uma analogia que nos ajuda a compreender como encontrar a solu\u00e7\u00e3o. Come\u00e7amos por pensar em nossa fun\u00e7\u00e3o como uma esp\u00e9cie de vale e imaginamos uma bola rolando pela encosta do vale, conforme pode ser visto na figura abaixo. Nossa experi\u00eancia di\u00e1ria nos diz que a bola acabar\u00e1 rolando para o fundo do vale. Talvez possamos usar essa ideia como forma de encontrar um m\u00ednimo para a fun\u00e7\u00e3o? Escolher\u00edamos aleatoriamente um ponto de partida para uma bola (imagin\u00e1ria), e ent\u00e3o simular\u00edamos o movimento da bola enquanto ela rola at\u00e9 o fundo do vale. Poder\u00edamos fazer essa simula\u00e7\u00e3o simplesmente por derivadas de computa\u00e7\u00e3o da fun\u00e7\u00e3o C \u2013 essas derivadas nos diriam tudo o que precisamos saber sobre a \u201cforma\u201d local do vale, e, portanto, como nossa bola deve rolar.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter wp-image-347 size-large\" data-attachment-id=\"347\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"ball\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?fit=1024%2C555\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?fit=300%2C163\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?fit=1958%2C1061\" data-orig-size=\"1958,1061\" data-permalink=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/ball/\" data-recalc-dims=\"1\" height=\"555\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=1024%2C555\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=1024%2C555 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=300%2C163 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=768%2C416 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=200%2C108 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?resize=690%2C374 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/ball.png?w=1958 1958w\" width=\"1024\"/>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"font-size: 10pt;\">\n   Representa\u00e7\u00e3o da Descida do Gradiente (com o objetivo de minimizar a fun\u00e7\u00e3o de custo)\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Ou seja, a Descida do Gradiente \u00e9 um algoritmo de otimiza\u00e7\u00e3o usado para encontrar os valores de par\u00e2metros (coeficientes ou se preferir w e b \u2013 weight e bias) de uma fun\u00e7\u00e3o que minimizam uma fun\u00e7\u00e3o de custo. A Descida do Gradiente \u00e9 melhor usada quando os par\u00e2metros n\u00e3o podem ser calculados analiticamente (por exemplo, usando \u00e1lgebra linear) e devem ser pesquisados por um algoritmo de otimiza\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  O procedimento come\u00e7a com valores iniciais para o coeficiente ou coeficientes da fun\u00e7\u00e3o. Estes poderiam ser 0.0 ou um pequeno valor aleat\u00f3rio (a inicializa\u00e7\u00e3o dos coeficiente \u00e9 parte cr\u00edtica do processo e diversas t\u00e9cnicas podem ser usadas, ficando a escolha a cargo do\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Cientista de Dados\n   </a>\n  </span>\n  e do problema a ser resolvido com o modelo). Poder\u00edamos iniciar assim nossos coeficientes (valores de w e b):\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   coeficiente = 0,0\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O custo dos coeficientes \u00e9 avaliado ligando-os \u00e0 fun\u00e7\u00e3o e calculando o custo.\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   custo = f (coeficiente)\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  ou\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   custo = avaliar (f (coeficiente))\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A derivada do custo \u00e9 calculada. A derivada \u00e9 um conceito de C\u00e1lculo e refere-se \u00e0 inclina\u00e7\u00e3o da fun\u00e7\u00e3o em um determinado ponto. Precisamos conhecer a inclina\u00e7\u00e3o para que possamos conhecer a dire\u00e7\u00e3o (sinal) para mover os valores dos coeficientes para obter um custo menor na pr\u00f3xima itera\u00e7\u00e3o.\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   delta = derivado (custo)\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Agora que sabemos da derivada em que dire\u00e7\u00e3o est\u00e1 em declive, podemos atualizar os valores dos coeficientes. Um par\u00e2metro de taxa de aprendizagem (alfa) deve ser especificado e controla o quanto os coeficientes podem mudar em cada atualiza\u00e7\u00e3o.\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   coeficiente = coeficiente \u2013 (alfa * delta)\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Este processo \u00e9 repetido at\u00e9 que o\n  <strong>\n   custo dos coeficientes\n  </strong>\n  (\n  <strong>\n   fun\u00e7\u00e3o de custo\n  </strong>\n  ) seja 0,0 ou pr\u00f3ximo o suficiente de zero, indicando que as sa\u00eddas da rede est\u00e3o cada vez mais pr\u00f3ximas dos valores reais (sa\u00eddas desejadas).\n </p>\n <p style=\"text-align: justify;\">\n  A Descida do Gradiente \u00e9 simples, mas exige que seja calculado o gradiente da fun\u00e7\u00e3o de custo ou a fun\u00e7\u00e3o que voc\u00ea est\u00e1 otimizando, mas al\u00e9m disso, \u00e9 muito direto.\u00a0Em resumo:\n </p>\n <blockquote>\n  <p style=\"text-align: justify;\">\n   Voc\u00ea divide seus dados em amostras e a cada amostra (sample), voc\u00ea passa as entradas pela rede, multiplica pelos pesos, soma, e no final voc\u00ea vai ter sua saida (a previs\u00e3o da rede). Voc\u00ea ent\u00e3o compara a sa\u00edda da sua rede com o a resposta certa, calcula o erro, e ent\u00e3o retroage esse erro (backpropagation), ajustando os pesos de cada neur\u00f4nio de cada camada. Quando voc\u00ea acabar de fazer a atualiza\u00e7\u00e3o dos pesos, uma nova amostra \u00e9 introduzida e ela ser\u00e1 multiplicada pelos pesos j\u00e1 atualizados. Esse processo de atualizar os pesos \u00e9 que \u00e9 chamado de \u201caprendizado\u201d.\n  </p>\n  <p style=\"text-align: justify;\">\n   Se voc\u00ea observar os algoritmos mais atuais, todos trabalham dentro de um conceito relativamente novo chamado de mini-lotes (mini-batches). Para otimizar a performance, o que se faz \u00e9 passar pela rede m\u00faltiplas amostras (por exemplo 128 amostras), calcular o erro m\u00e9dio delas e ent\u00e3o realizar o backpropagation e a atualiza\u00e7\u00e3o dos pesos. Do ponto de vista da atualiza\u00e7\u00e3o dos pesos, 1 amostra = 128 amostras. Esse \u00e9 um conceito mais novo, necess\u00e1rio principalmente no treinamento de grandes modelos de Deep Learning.\n  </p>\n </blockquote>\n <p>\n  Em seguida, veremos como podemos usar isso em algoritmos de aprendizado de m\u00e1quina.\n </p>\n <h3>\n </h3>\n <h3 style=\"text-align: justify;\">\n  Batch Gradient Descent em Aprendizado de M\u00e1quina\n </h3>\n <p style=\"text-align: justify;\">\n  O objetivo de todos os algoritmos supervisionados de aprendizagem de m\u00e1quina \u00e9 estimar uma fun\u00e7\u00e3o de destino (f) que mapeia dados de entrada (X) para as vari\u00e1veis \u200b\u200bde sa\u00edda (Y). Isso descreve todos os problemas de classifica\u00e7\u00e3o e regress\u00e3o (aprendizagem supervisionada).\n </p>\n <p style=\"text-align: justify;\">\n  Alguns algoritmos de aprendizagem de m\u00e1quina t\u00eam coeficientes que caracterizam a estimativa de algoritmos para a fun\u00e7\u00e3o alvo (f). Diferentes algoritmos t\u00eam diferentes representa\u00e7\u00f5es e diferentes coeficientes, mas muitos deles requerem um processo de otimiza\u00e7\u00e3o para encontrar o conjunto de coeficientes que resultam na melhor estimativa da fun\u00e7\u00e3o alvo. Os exemplos comuns de algoritmos com coeficientes que podem ser otimizados usando descida do gradiente s\u00e3o Regress\u00e3o linear e Regress\u00e3o log\u00edstica.\n </p>\n <p style=\"text-align: justify;\">\n  A avalia\u00e7\u00e3o de qu\u00e3o pr\u00f3ximo um modelo de aprendizagem de m\u00e1quina estima a fun\u00e7\u00e3o de destino pode ser calculada de v\u00e1rias maneiras, muitas vezes espec\u00edficas para o algoritmo de aprendizagem de m\u00e1quina. A fun\u00e7\u00e3o de custo envolve a avalia\u00e7\u00e3o dos coeficientes no modelo de aprendizagem de m\u00e1quina calculando uma previs\u00e3o para o modelo para cada inst\u00e2ncia de treinamento no conjunto de dados e comparando as previs\u00f5es com os valores de sa\u00edda reais e calculando uma soma ou erro m\u00e9dio (como a Soma de Residuais Quadrados ou SSR no caso de regress\u00e3o linear).\n </p>\n <p style=\"text-align: justify;\">\n  A partir da fun\u00e7\u00e3o de custo, uma derivada pode ser calculada para cada coeficiente para que ele possa ser atualizado usando exatamente a equa\u00e7\u00e3o de atualiza\u00e7\u00e3o descrita acima.\n </p>\n <p style=\"text-align: justify;\">\n  O custo \u00e9 calculado para um algoritmo de aprendizado de m\u00e1quina em todo o conjunto de dados de treinamento para cada itera\u00e7\u00e3o do algoritmo de descida de gradiente. Uma itera\u00e7\u00e3o do algoritmo \u00e9 chamada de um lote e esta forma de descida do gradiente \u00e9 referida como descida do gradiente em lote (Batch Gradient Descent).\n </p>\n <p style=\"text-align: justify;\">\n  A descida do gradiente em lote \u00e9 a forma mais comum de descida do gradiente em Machine Learning.\n </p>\n <h3>\n </h3>\n <h3 style=\"text-align: justify;\">\n  Stochastic Gradient Descent em Aprendizado de M\u00e1quina\n </h3>\n <p style=\"text-align: justify;\">\n  A Descida do Gradiente pode ser lenta para executar em conjuntos de dados muito grandes. Como uma itera\u00e7\u00e3o do algoritmo de descida do gradiente requer uma previs\u00e3o para cada inst\u00e2ncia no conjunto de dados de treinamento, pode demorar muito quando voc\u00ea tem muitos milh\u00f5es de inst\u00e2ncias.\n </p>\n <p style=\"text-align: justify;\">\n  Em situa\u00e7\u00f5es em que voc\u00ea possui grandes quantidades de dados, voc\u00ea pode usar uma varia\u00e7\u00e3o da descida do gradiente chamada Stochastic Gradient Descent.\n </p>\n <p style=\"text-align: justify;\">\n  Nesta varia\u00e7\u00e3o, o procedimento de descida do gradiente descrito acima \u00e9 executado, mas a atualiza\u00e7\u00e3o para os coeficientes \u00e9 realizada para cada inst\u00e2ncia de treinamento, em vez do final do lote de inst\u00e2ncias.\n </p>\n <p style=\"text-align: justify;\">\n  O primeiro passo do procedimento exige que a ordem do conjunto de dados de treinamento seja randomizada. Isto \u00e9, misturar a ordem que as atualiza\u00e7\u00f5es s\u00e3o feitas para os coeficientes. Como os coeficientes s\u00e3o atualizados ap\u00f3s cada inst\u00e2ncia de treinamento, as atualiza\u00e7\u00f5es ser\u00e3o barulhentas saltando por todo o lado, e assim o custo correspondente funcionar\u00e1. Ao misturar a ordem para as atualiza\u00e7\u00f5es dos coeficientes, ela aproveita essa caminhada aleat\u00f3ria e evita que ela fique \u201cdistra\u00edda\u201d ou presa.\n </p>\n <p style=\"text-align: justify;\">\n  O procedimento de atualiza\u00e7\u00e3o para os coeficientes \u00e9 o mesmo que o anterior, exceto que o custo n\u00e3o \u00e9 somado em todos os padr\u00f5es de treinamento, mas sim calculado para um padr\u00e3o de treinamento.\n </p>\n <p style=\"text-align: justify;\">\n  A aprendizagem pode ser muito mais r\u00e1pida com descida de gradiente estoc\u00e1stica para conjuntos de dados de treinamento muito grandes e muitas vezes voc\u00ea s\u00f3 precisa de um pequeno n\u00famero de passagens atrav\u00e9s do conjunto de dados para alcan\u00e7ar um conjunto de coeficientes bom o suficiente.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Ufa, voc\u00ea ainda est\u00e1 a\u00ed? Entende agora porque Cientistas de Dados e Engenheiros de IA devem ser muito bem remunerados? Eles s\u00e3o os \u201cmagos\u201d que est\u00e3o ajudando a transformar o mundo com Machine Learning. E este cap\u00edtulo foi apenas uma breve introdu\u00e7\u00e3o! Voltaremos a este assunto mais a frente no livro, quando estudarmos outros algoritmos. Mas caso voc\u00ea queira aprender em detalhes como tudo isso funciona e criar seus modelos usando linguagens R, Python, Scala ou Java, para aplica\u00e7\u00f5es comerciais, confira:\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n  ,\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-com-linguagem-scala\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning com Scala e Spark\n   </a>\n  </span>\n  ,\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-i\" rel=\"noopener\" target=\"_blank\">\n    Deep Learning\n   </a>\n  </span>\n  e\n  <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-em-java\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    An\u00e1lise Preditiva com Machine Learning em Jav\n   </span>\n   a\n  </a>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Tenho certeza que voc\u00ea est\u00e1 ansioso para criar e treinar sua primeira rede neural. Ent\u00e3o, n\u00e3o perca o pr\u00f3ximo cap\u00edtulo!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-com-linguagem-scala\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning com Scala e Spark\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-em-java\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    An\u00e1lise Preditiva com Machine Learning em Jav\n   </span>\n   a\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"noopener\" target=\"_blank\">\n    MNIST\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Derivada\" rel=\"noopener\" target=\"_blank\">\n    Derivada\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ruder.io/optimizing-gradient-descent/\" rel=\"noopener\" target=\"_blank\">\n    An overview of gradient descent optimization algorithms\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\" rel=\"noopener\" target=\"_blank\">\n    Optimization: Stochastic Gradient Descent\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-327\" href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-327\" href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-327\" href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-327\" href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-327-5e0dd053c2c97\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=327&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-327-5e0dd053c2c97\" id=\"like-post-wrapper-140353593-327-5e0dd053c2c97\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "13": "<h1 class=\"entry-title\" id=\"capitulo-13\">\n Cap\u00edtulo 13 \u2013 Construindo Uma Rede Neural Com Linguagem Python\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Ok. Chegou a hora. Vamos escrever um programa em linguagem Python que aprenda como reconhecer d\u00edgitos manuscritos, usando\u00a0Stochastic Gradient Descent e o dataset de treinamento MNIST. Se voc\u00ea chegou at\u00e9 aqui sem ler os cap\u00edtulos anteriores, ent\u00e3o pare imediatamente, leia os \u00faltimos 12 cap\u00edtulos e depois volte aqui! N\u00e3o tenha pressa! N\u00e3o existe atalho para o aprendizado!\n </p>\n <p>\n </p>\n <p>\n  ******************************** Aten\u00e7\u00e3o\u00a0********************************\n </p>\n <p style=\"text-align: justify;\">\n  Este cap\u00edtulo considera que voc\u00ea j\u00e1 tem o interpretador Python (vers\u00e3o 3.6.x) instalado no seu computador, seja ele com sistema operacional Windows, MacOS ou Linux. Recomendamos que voc\u00ea instale o\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.anaconda.com/download/#macos\" rel=\"noopener\" target=\"_blank\">\n    Anaconda\n   </a>\n  </span>\n  e que j\u00e1 possua conhecimentos em linguagem Python. Se esse n\u00e3o for seu caso, antes de ler este cap\u00edtulo e executar os exemplos aqui fornecidos, acesse o curso gratuito\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener\" target=\"_blank\">\n    Python Fundamentos Para An\u00e1lise de Dados\n   </a>\n  </span>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   Usaremos Python 3 e os scripts podem ser encontrados no reposit\u00f3rio do livro no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener\" target=\"_blank\">\n     GitHub\n    </a>\n   </span>\n   . Vamos come\u00e7ar!\n  </strong>\n </p>\n <p>\n  *************************************************************************\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Quando descrevemos o dataset MNIST anteriormente, dissemos que ele estava dividido em 60.000 imagens de treinamento e 10.000 imagens de teste. Essa \u00e9 a descri\u00e7\u00e3o oficial do MNIST. Mas vamos dividir os dados de forma um pouco diferente. Deixaremos as imagens de teste como est\u00e1, mas dividiremos o conjunto de treinamento MNIST de 60.000 imagens em duas partes: um conjunto de 50.000 imagens, que usaremos para treinar nossa rede neural e um conjunto separado de valida\u00e7\u00e3o de 10.000 imagens. N\u00e3o utilizaremos os dados de valida\u00e7\u00e3o neste cap\u00edtulo, por\u00e9m mais tarde, aqui mesmo no livro, usaremos este dataset quando estivermos configurando certos hiperpar\u00e2metros da rede neural, como a taxa de aprendizado por exemplo. Embora os dados de valida\u00e7\u00e3o n\u00e3o fa\u00e7am parte da especifica\u00e7\u00e3o MNIST original, muitas pessoas usam o MNIST desta forma e o uso de dados de valida\u00e7\u00e3o \u00e9 comum em redes neurais. Quando eu me referir aos \u201cdados de treinamento MNIST\u201d de agora em diante, vou me referir ao nosso conjunto de dados de 50.000 imagens, e n\u00e3o ao conjunto de dados de 60.000 imagens. Fique atento!\n </p>\n <p style=\"text-align: justify;\">\n  Al\u00e9m dos dados MNIST, tamb\u00e9m precisamos de uma biblioteca Python chamada Numpy, para \u00e1lgebra linear. Se voc\u00ea instalou o Anaconda, n\u00e3o precisa se preocupar, pois o Numpy j\u00e1 est\u00e1 instalado. Caso contr\u00e1rio, ser\u00e1 necess\u00e1rio fazer a instala\u00e7\u00e3o do pacote.\n </p>\n <p style=\"text-align: justify;\">\n  Mas antes de carregar e dividir os dados, vamos compreender os principais recursos do nosso c\u00f3digo para constru\u00e7\u00e3o de uma rede neural. A pe\u00e7a central \u00e9 uma classe chamada\n  <strong>\n   Network\n  </strong>\n  , que usamos para representar uma rede neural. Abaixo a classe Network e seu construtor:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-large wp-image-371\" data-attachment-id=\"371\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"classe1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?fit=1024%2C201\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?fit=300%2C59\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?fit=1242%2C244\" data-orig-size=\"1242,244\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/classe1-2/\" data-recalc-dims=\"1\" height=\"201\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=1024%2C201\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=1024%2C201 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=300%2C59 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=768%2C151 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=200%2C39 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?resize=690%2C136 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe1-1.png?w=1242 1242w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Neste c\u00f3digo, o par\u00e2metro\n  <strong>\n   sizes\n  </strong>\n  cont\u00eam o n\u00famero de neur\u00f4nios nas respectivas camadas, sendo um objeto do tipo lista em Python. Ent\u00e3o, por exemplo, se queremos criar um objeto da classe Network com 2 neur\u00f4nios na primeira camada, 3 neur\u00f4nios na segunda camada e 1 neur\u00f4nio na camada final, aqui est\u00e1 o c\u00f3digo que usamos para instanciar um objeto da classe Network::\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"font-size: 18pt;\">\n   <strong>\n    rede1 = Network([2, 3, 1])\n   </strong>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Os bias e pesos no objeto rede1 s\u00e3o todos inicializados aleatoriamente, usando a fun\u00e7\u00e3o Numpy\n  <strong>\n   np.random.randn\n  </strong>\n  para gerar distribui\u00e7\u00f5es gaussianas com 0 de m\u00e9dia e desvio padr\u00e3o 1. Esta inicializa\u00e7\u00e3o aleat\u00f3ria d\u00e1 ao nosso algoritmo de descida do gradiente estoc\u00e1stico um local para come\u00e7ar. Em cap\u00edtulos posteriores, encontraremos melhores maneiras de inicializar os pesos e os bias. Observe que o c\u00f3digo de inicializa\u00e7\u00e3o de rede assume que a primeira camada de neur\u00f4nios \u00e9 uma camada de entrada e omite a defini\u00e7\u00e3o de quaisquer bias para esses neur\u00f4nios, uma vez que os bias s\u00e3o usados apenas para calcular as sa\u00eddas de camadas posteriores.\n </p>\n <p style=\"text-align: justify;\">\n  Observe tamb\u00e9m que os bias e pesos s\u00e3o armazenados como listas de matrizes Numpy. Assim, por exemplo, rede1.weights[1] \u00e9 uma matriz Numpy armazenando os pesos conectando a segunda e terceira camadas de neur\u00f4nios. (N\u00e3o \u00e9 a primeira e segunda camadas, uma vez que a indexa\u00e7\u00e3o da lista em Python come\u00e7a em 0.) Uma vez que rede1.weights[1] \u00e9 bastante detalhado, vamos apenas indicar essa matriz w. \u00c9 uma matriz tal que wjk \u00e9 o peso para a conex\u00e3o entre o neur\u00f4nio kth na segunda camada e o neur\u00f4nio jth na terceira camada. Essa ordena\u00e7\u00e3o dos \u00edndices j e k pode parecer estranha \u2013 certamente teria mais sentido trocar os \u00edndices j e k? A grande vantagem de usar essa ordena\u00e7\u00e3o \u00e9 que isso significa que o vetor de ativa\u00e7\u00f5es da terceira camada de neur\u00f4nios \u00e9:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"Form\" class=\"aligncenter size-full wp-image-367\" data-attachment-id=\"367\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Form\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?fit=137%2C43\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?fit=137%2C43\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?fit=137%2C43\" data-orig-size=\"137,43\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/form-2/\" data-recalc-dims=\"1\" height=\"43\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/form-1.png?resize=137%2C43\" width=\"137\"/>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"font-size: 8pt;\">\n   Equa\u00e7\u00e3o 1\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Onde,\n  <strong>\n   a\n  </strong>\n  \u00e9 o vetor de ativa\u00e7\u00f5es da segunda camada de neur\u00f4nios. Para obter um\n  <strong>\n   a\u2019\n  </strong>\n  multiplicamos\n  <strong>\n   a\n  </strong>\n  pela matriz de peso\n  <strong>\n   w\n  </strong>\n  , e adicionamos o vetor\n  <strong>\n   b\n  </strong>\n  com os bias (se voc\u00ea leu os cap\u00edtulos anteriores, isso n\u00e3o deve ser novidade agora). Em seguida, aplicamos a fun\u00e7\u00e3o\n  <strong>\n   \u03c3\n  </strong>\n  de forma elementar a cada entrada no vetor\n  <strong>\n   wa + b\n  </strong>\n  . (Isto \u00e9 chamado de vetorizar a fun\u00e7\u00e3o \u03c3.)\n </p>\n <p>\n  Com tudo isso em mente, \u00e9 f\u00e1cil escrever c\u00f3digo que computa a sa\u00edda de uma inst\u00e2ncia de rede. Come\u00e7amos definindo a fun\u00e7\u00e3o sigmoide:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-full wp-image-372\" data-attachment-id=\"372\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?fit=492%2C108\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?fit=300%2C66\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?fit=492%2C108\" data-orig-size=\"492,108\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/func/\" data-recalc-dims=\"1\" height=\"108\" sizes=\"(max-width: 492px) 100vw, 492px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?resize=492%2C108\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?w=492 492w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?resize=300%2C66 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func.png?resize=200%2C44 200w\" width=\"492\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Observe que quando a entrada z \u00e9 um vetor ou uma matriz Numpy, Numpy aplica automaticamente a fun\u00e7\u00e3o sigmoid elementwise, ou seja, na forma vetorizada.\n </p>\n <p style=\"text-align: justify;\">\n  Em seguida, adicionamos um m\u00e9todo feedforward \u00e0 classe Network, que, dada a entrada a para a rede, retorna a sa\u00edda corresponente. Basicamente o m\u00e9todo feedforward aplica a Equa\u00e7\u00e3o 1 mostrada acima, para cada camada:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-large wp-image-373\" data-attachment-id=\"373\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func2\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?fit=1024%2C320\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?fit=300%2C94\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?fit=1240%2C388\" data-orig-size=\"1240,388\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/func2/\" data-recalc-dims=\"1\" height=\"320\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=1024%2C320\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=1024%2C320 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=300%2C94 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=768%2C240 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=200%2C63 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?resize=690%2C216 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2.png?w=1240 1240w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A principal atividade que queremos que nossos objetos da classe Network fa\u00e7am \u00e9 aprender. Para esse fim, criaremos um m\u00e9todo SGD (Stochastic Gradient Descent). Aqui est\u00e1 o c\u00f3digo. \u00c9 um pouco misterioso em alguns lugares, mas vamos explicar em detalhes mais abaixo:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"\" class=\"aligncenter size-large wp-image-383\" data-attachment-id=\"383\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?fit=1024%2C682\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?fit=300%2C200\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?fit=1484%2C988\" data-orig-size=\"1484,988\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/func2-2/\" data-recalc-dims=\"1\" height=\"682\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=1024%2C682\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=1024%2C682 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=300%2C200 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=768%2C511 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=200%2C133 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?resize=690%2C459 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/func2-1.png?w=1484 1484w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O\n  <strong>\n   training_data\n  </strong>\n  \u00e9 uma lista de tuplas (x, y) que representam as entradas de treinamento e as correspondentes sa\u00eddas desejadas. As vari\u00e1veis\n  <strong>\n   epochs\n  </strong>\n  e\n  <strong>\n   mini_batch_size\n  </strong>\n  s\u00e3o o que voc\u00ea esperaria \u2013 o n\u00famero de \u00e9pocas para treinar e o tamanho dos mini-lotes a serem usados durante a amostragem, enquanto\n  <strong>\n   eta\n  </strong>\n  \u00e9 a taxa de aprendizagem, \u03b7. Se o argumento opcional test_data for fornecido, o programa avaliar\u00e1 a rede ap\u00f3s cada per\u00edodo de treinamento e imprimir\u00e1 progresso parcial. Isso \u00e9 \u00fatil para rastrear o progresso, mas retarda substancialmente as coisas.\n </p>\n <p style=\"text-align: justify;\">\n  O c\u00f3digo funciona da seguinte forma. Em cada \u00e9poca, ele come\u00e7a arrastando aleatoriamente os dados de treinamento e, em seguida, particiona-os em mini-lotes de tamanho apropriado. Esta \u00e9 uma maneira f\u00e1cil de amostragem aleat\u00f3ria dos dados de treinamento. Ent\u00e3o, para cada mini_batch, aplicamos um \u00fanico passo de descida do gradiente. Isso \u00e9 feito pelo c\u00f3digo self.update_mini_batch (mini_batch, eta), que atualiza os pesos e os bias da rede de acordo com uma \u00fanica itera\u00e7\u00e3o de descida de gradiente, usando apenas os dados de treinamento em mini_batch. Aqui est\u00e1 o c\u00f3digo para o m\u00e9todo update_mini_batch:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"\" class=\"aligncenter size-large wp-image-375\" data-attachment-id=\"375\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"classe4\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?fit=1024%2C976\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?fit=300%2C286\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?fit=1442%2C1374\" data-orig-size=\"1442,1374\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/classe4/\" data-recalc-dims=\"1\" height=\"976\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=1024%2C976\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=1024%2C976 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=300%2C286 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=768%2C732 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=200%2C191 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?resize=690%2C657 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/classe4.png?w=1442 1442w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A maior parte do trabalho \u00e9 feita pela linha delta_nabla_b, delta_nabla_w = self.backprop (x, y). Isso invoca algo chamado algoritmo de backpropagation, que \u00e9 uma maneira r\u00e1pida de calcular o gradiente da fun\u00e7\u00e3o de custo. Portanto, update_mini_batch funciona simplesmente calculando esses gradientes para cada exemplo de treinamento no mini_batch e, em seguida, atualizando self.weights e self.biases adequadamente.\n </p>\n <p style=\"text-align: justify;\">\n  Abaixo voc\u00ea encontra o c\u00f3digo para self.backprop, mas n\u00e3o estudaremos ele agora. Estudaremos em detalhes como funciona o backpropagation no pr\u00f3ximo cap\u00edtulo, incluindo o c\u00f3digo para self.backprop. Por hora, basta assumir que ele se comporta conforme indicado, retornando o gradiente apropriado para o custo associado ao exemplo de treinamento x.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"\" class=\"aligncenter size-large wp-image-376\" data-attachment-id=\"376\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"back\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?fit=1024%2C852\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?fit=300%2C250\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?fit=1192%2C992\" data-orig-size=\"1192,992\" data-permalink=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/back/\" data-recalc-dims=\"1\" height=\"852\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=1024%2C852\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=1024%2C852 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=300%2C250 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=768%2C639 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=200%2C166 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?resize=690%2C574 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/03/back.png?w=1192 1192w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  No programa completo dispon\u00edvel no\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener\" target=\"_blank\">\n    Github\n   </a>\n  </span>\n  voc\u00ea encontra coment\u00e1rios explicando como ocorre todo o processo. Al\u00e9m do self.backprop, o programa \u00e9 auto-explicativo \u2013 todo o levantamento pesado \u00e9 feito em self.SGD e self.update_mini_batch, que j\u00e1 discutimos. O m\u00e9todo self.backprop faz uso de algumas fun\u00e7\u00f5es extras para ajudar no c\u00e1lculo do gradiente, nomeadamente sigmoid_prime, que calcula a derivada da fun\u00e7\u00e3o \u03c3 e self.cost_derivative.\n </p>\n <p style=\"text-align: justify;\">\n  A classe Network \u00e9 em ess\u00eancia nosso algoritmo de rede neural. A partir dela criamos uma inst\u00e2ncia (como rede1), alimentamos com os dados de treinamento e realizamos o treinamento. Avaliamos ent\u00e3o a performance da rede com dados de teste e repetimos todo o processo at\u00e9 alcan\u00e7ar o n\u00edvel de acur\u00e1cia desejado em nosso projeto. Quando o modelo final estiver pronto, usamos para realizar as previs\u00f5es para as quais o modelo foi criado, apresentando a ele novos conjuntos de dados e extraindo as previs\u00f5es. Perceba que este \u00e9 um algoritmo de rede neural bem simples, mas que permite compreender como funcionam as redes neurais e mais tarde, aqui mesmo no livro, as redes neurais profundas ou Deep Learning.\n </p>\n <p style=\"text-align: justify;\">\n  No pr\u00f3ximo cap\u00edtulo vamos continuar trabalhando com este algoritmo e compreender como funciona o Backpropagation. Na sequ\u00eancia, vamos carregar os dados, treinar e testar nossa rede neural e ent\u00e3o us\u00e1-la para reconhecer d\u00edgitos manuscritos. At\u00e9 l\u00e1.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener\" target=\"_blank\">\n    Python Fundamentos Para An\u00e1lise de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-com-linguagem-scala\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning com Scala e Spark\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-em-java\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    An\u00e1lise Preditiva com Machine Learning em Jav\n   </span>\n   a\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"noopener\" target=\"_blank\">\n    MNIST\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Derivada\" rel=\"noopener\" target=\"_blank\">\n    Derivada\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ruder.io/optimizing-gradient-descent/\" rel=\"noopener\" target=\"_blank\">\n    An overview of gradient descent optimization algorithms\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\" rel=\"noopener\" target=\"_blank\">\n    Optimization: Stochastic Gradient Descent\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-363\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-363\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-363\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-363\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-363-5e0dd055b69ab\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=363&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-363-5e0dd055b69ab\" id=\"like-post-wrapper-140353593-363-5e0dd055b69ab\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "14": "<h1 class=\"entry-title\" id=\"capitulo-14\">\n Cap\u00edtulo 14 \u2013 Algoritmo Backpropagation Parte 1 \u2013 Grafos Computacionais e Chain Rule\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  No \u00faltimo cap\u00edtulo, vimos como as redes neurais podem aprender seus pesos e bias usando o algoritmo de gradiente descendente. Houve, no entanto, uma lacuna na nossa explica\u00e7\u00e3o: n\u00e3o discutimos como calcular o gradiente da fun\u00e7\u00e3o de custo. Neste cap\u00edtulo, explicaremos sobre um algoritmo usado para calcular esses gradientes, um algoritmo conhecido como backpropagation. Como esse tema \u00e9 a ess\u00eancia do treinamento de redes neurais, vamos divid\u00ed-lo em dois cap\u00edtulos. Vamos come\u00e7ar com Algoritmo Backpropagation Parte 1 \u2013 Grafos Computacionais e Chain Rule.\n </p>\n <p style=\"text-align: justify;\">\n  O\u00a0backpropagation \u00e9 indiscutivelmente o algoritmo mais importante na hist\u00f3ria das redes neurais \u2013 sem backpropagation, seria quase imposs\u00edvel treinar redes de aprendizagem profunda da forma que vemos hoje. O backpropagation pode ser considerado a pedra angular das redes neurais modernas e consequentemente do Deep Learning.\n </p>\n <p style=\"text-align: justify;\">\n  O algoritmo backpropagation foi originalmente introduzido na d\u00e9cada de 1970, mas sua import\u00e2ncia n\u00e3o foi totalmente apreciada at\u00e9 um famoso\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.nature.com/articles/323533a0\" rel=\"noopener\" target=\"_blank\">\n    artigo de 1986 de David Rumelhart, Geoffrey Hinton e Ronald Williams\n   </a>\n  </span>\n  . Esse artigo descreve v\u00e1rias redes neurais em que o backpropagation funciona muito mais rapidamente do que as abordagens anteriores de aprendizado, possibilitando o uso de redes neurais para resolver problemas que antes eram insol\u00faveis.\n </p>\n <p style=\"text-align: justify;\">\n  O\u00a0backpropagation \u00e9 o algoritmo-chave que faz o treinamento de modelos profundos algo computacionalmente trat\u00e1vel. Para as redes neurais modernas, ele pode tornar o treinamento com gradiente descendente at\u00e9 dez milh\u00f5es de vezes mais r\u00e1pido, em rela\u00e7\u00e3o a uma implementa\u00e7\u00e3o ing\u00eanua. Essa \u00e9 a diferen\u00e7a entre um modelo que leva algumas horas ou dias para treinar e e outro que poderia levar anos (sem exagero).\n </p>\n <p style=\"text-align: justify;\">\n  Al\u00e9m de seu uso em Deep Learning, o backpropagation \u00e9 uma poderosa ferramenta computacional em muitas outras \u00e1reas, desde previs\u00e3o do tempo at\u00e9 a an\u00e1lise da estabilidade num\u00e9rica. De fato, o algoritmo foi reinventado pelo menos dezenas de vezes em diferentes campos. O nome geral, independente da aplica\u00e7\u00e3o, \u00e9 \u201cdiferencia\u00e7\u00e3o no modo reverso\u201d.\n </p>\n <p style=\"text-align: justify;\">\n  Fundamentalmente, backpropagation \u00e9 uma t\u00e9cnica para calcular derivadas rapidamente (n\u00e3o sabe o que \u00e9 derivada? Consulte o link para um excelente v\u00eddeo em portugu\u00eas explicando esse conceito em detalhes nas refer\u00eancias ao final deste cap\u00edtulo). E \u00e9 um truque essencial, n\u00e3o apenas em Deep Learning, mas em uma ampla variedade de situa\u00e7\u00f5es de computa\u00e7\u00e3o num\u00e9rica. E para compreender backpropagation de forma efetiva, vamos primeiro compreender o conceito de grafo computacional e chain rule.\n </p>\n <p>\n </p>\n <h2>\n  Grafo Computacional\n </h2>\n <p style=\"text-align: justify;\">\n  Grafos computacionais s\u00e3o uma boa maneira de pensar em express\u00f5es matem\u00e1ticas. O conceito de grafo foi introduzido por\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/birth-graph-theory-leonhard-euler-and-konigsberg-bridge-problem\" rel=\"noopener\" target=\"_blank\">\n    Leonhard Euler\n   </a>\n  </span>\n  em 1736 para tentar resolver o problema das\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Sete_pontes_de_K%C3%B6nigsberg\" rel=\"noopener\" target=\"_blank\">\n    Pontes de Konigsberg\n   </a>\n  </span>\n  . Grafos s\u00e3o modelos matem\u00e1ticos para resolver problemas pr\u00e1ticos do dia a dia, com v\u00e1rias aplica\u00e7\u00f5es no mundo real tais como: circuitos el\u00e9tricos, redes de distribui\u00e7\u00e3o, rela\u00e7\u00f5es de parentesco entre pessoas, an\u00e1lise de redes sociais, log\u00edstica, redes de estradas, redes de computadores e muito mais. Grafos s\u00e3o muito usados para modelar problemas em computa\u00e7\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Um Grafo \u00e9 um modelo matem\u00e1tico que representa rela\u00e7\u00f5es entre objetos. Um grafo G = (V, E) consiste de um conjunto de v\u00e9rtices\n  <strong>\n   V\n  </strong>\n  (tamb\u00e9m chamados de n\u00f3s), ligados por um conjunto de bordas ou arestas\n  <strong>\n   E\n  </strong>\n  . Para aprender sobre grafos em mais detalhes, clique\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-analise-em-grafos-para-big-data\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  .\n </p>\n <p>\n  Por exemplo, considere a express\u00e3o:\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   e = (a + b) \u2217 (b + 1)\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Existem tr\u00eas opera\u00e7\u00f5es: duas adi\u00e7\u00f5es e uma multiplica\u00e7\u00e3o. Para facilitar a compreens\u00e3o sobre isso, vamos introduzir duas vari\u00e1veis intermedi\u00e1rias c e d para que a sa\u00edda de cada fun\u00e7\u00e3o tenha uma vari\u00e1vel. N\u00f3s agora temos:\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   c = a+b\n  </strong>\n  <br/>\n  <strong>\n   d = b+1\n  </strong>\n  <br/>\n  <strong>\n   e = c\u2217d\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Para criar um grafo computacional, fazemos cada uma dessas opera\u00e7\u00f5es nos\n  <em>\n   <strong>\n    n\u00f3s\n   </strong>\n  </em>\n  , juntamente com as vari\u00e1veis de entrada. Quando o valor de um n\u00f3 \u00e9 a entrada para outro n\u00f3, uma seta vai de um para outro e temos nesse caso um grafo direcionado.\n </p>\n <p>\n </p>\n <div>\n </div>\n <div>\n </div>\n <div>\n  <img alt=\"tree-def\" class=\"aligncenter wp-image-396 size-large\" data-attachment-id=\"396\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tree-def\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?fit=1024%2C592\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?fit=300%2C174\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?fit=1383%2C800\" data-orig-size=\"1383,800\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-def/\" data-recalc-dims=\"1\" height=\"592\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=1024%2C592\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=1024%2C592 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=300%2C174 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=768%2C444 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=200%2C116 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?resize=690%2C399 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-def.png?w=1383 1383w\" width=\"1024\"/>\n </div>\n <div>\n </div>\n <div>\n </div>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Esses tipos de grafos surgem o tempo todo em Ci\u00eancia da Computa\u00e7\u00e3o, especialmente ao falar sobre programas funcionais. Eles est\u00e3o intimamente relacionados com as no\u00e7\u00f5es de grafos de depend\u00eancia e grafos de chamadas. Eles tamb\u00e9m s\u00e3o a principal abstra\u00e7\u00e3o por tr\u00e1s do popular framework de Deep Learning, o\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-frameworks\" rel=\"noopener\" target=\"_blank\">\n    TensorFlow\n   </a>\n  </span>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Podemos avaliar a express\u00e3o definindo as vari\u00e1veis de entrada para determinados valores e computando os n\u00f3s atrav\u00e9s do grafo. Por exemplo, vamos definir a = 2 e b = 1:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"tree-eval\" class=\"aligncenter size-large wp-image-397\" data-attachment-id=\"397\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tree-eval\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?fit=1024%2C592\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?fit=300%2C173\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?fit=1403%2C811\" data-orig-size=\"1403,811\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-eval/\" data-recalc-dims=\"1\" height=\"592\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-1024x592.png?resize=1024%2C592\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=1024%2C592 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=300%2C173 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=768%2C444 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=200%2C116 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?resize=690%2C399 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval.png?w=1403 1403w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p>\n  A express\u00e3o, nesse exemplo, \u00e9 avaliada como 6.\n </p>\n <p>\n </p>\n <h2 style=\"text-align: justify;\">\n  Derivadas em Grafos Computacionais\n </h2>\n <p style=\"text-align: justify;\">\n  Se algu\u00e9m quiser entender derivadas em um grafo computacional, a chave \u00e9 entender as derivadas nas bordas (arestas que conectam os n\u00f3s no grafo). Se\n  <strong>\n   a\n  </strong>\n  afeta diretamente\n  <strong>\n   c\n  </strong>\n  , ent\u00e3o queremos saber como isso afeta\n  <strong>\n   c\n  </strong>\n  . Se\n  <strong>\n   a\n  </strong>\n  muda um pouco, como\n  <strong>\n   c\n  </strong>\n  muda? Chamamos isso de derivada parcial de\n  <strong>\n   c\n  </strong>\n  em rela\u00e7\u00e3o a\n  <strong>\n   a\n  </strong>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Para avaliar as derivadas parciais neste grafo, precisamos da regra da soma e da regra do produto:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"derivada\" class=\"aligncenter wp-image-398 size-full\" data-attachment-id=\"398\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"derivada\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?fit=229%2C119\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?fit=229%2C119\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?fit=229%2C119\" data-orig-size=\"229,119\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/derivada/\" data-recalc-dims=\"1\" height=\"119\" sizes=\"(max-width: 229px) 100vw, 229px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?resize=229%2C119\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?w=229 229w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada.png?resize=200%2C104 200w\" width=\"229\"/>\n </p>\n <p>\n </p>\n <p>\n  Abaixo, o grafo tem a derivada em cada borda (aresta) rotulada.\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"tree-eval-derivs\" class=\"aligncenter size-large wp-image-399\" data-attachment-id=\"399\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tree-eval-derivs\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1024%2C578\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=300%2C169\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1405%2C793\" data-orig-size=\"1405,793\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-eval-derivs/\" data-recalc-dims=\"1\" height=\"578\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=300%2C169 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=768%2C433 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=200%2C113 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=690%2C389 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?w=1405 1405w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  E se quisermos entender como os n\u00f3s que n\u00e3o est\u00e3o diretamente conectados afetam uns aos outros? Vamos considerar como\n  <strong>\n   e\n  </strong>\n  \u00e9 afetado por\n  <strong>\n   a\n  </strong>\n  . Se mudarmos\n  <strong>\n   a\n  </strong>\n  uma velocidade de 1,\n  <strong>\n   c\n  </strong>\n  tamb\u00e9m muda a uma velocidade de 1. Por sua vez,\n  <strong>\n   c\n  </strong>\n  mudando a uma velocidade de 1 faz com que\n  <strong>\n   e\n  </strong>\n  mude a uma velocidade de 2. Ent\u00e3o\n  <strong>\n   e\n  </strong>\n  muda a uma taxa de 1 \u2217 2 em rela\u00e7\u00e3o a\n  <strong>\n   a\n  </strong>\n  (analise o diagrama acima para visualizar isso).\n </p>\n <p style=\"text-align: justify;\">\n  A regra geral \u00e9 somar todos os caminhos poss\u00edveis de um n\u00f3 para o outro, multiplicando as derivadas em cada aresta do caminho. Por exemplo, para obter a derivada de\n  <strong>\n   e\n  </strong>\n  em rela\u00e7\u00e3o a\n  <strong>\n   b\n  </strong>\n  , obtemos:\n </p>\n <p>\n  <img alt=\"form\" class=\"aligncenter size-full wp-image-400\" data-attachment-id=\"400\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?fit=164%2C72\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?fit=164%2C72\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?fit=164%2C72\" data-orig-size=\"164,72\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/form-3/\" data-recalc-dims=\"1\" height=\"72\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form.png?resize=164%2C72\" width=\"164\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Isso explica como\n  <strong>\n   b\n  </strong>\n  afeta\n  <strong>\n   e\n  </strong>\n  atrav\u00e9s de\n  <strong>\n   c\n  </strong>\n  e tamb\u00e9m como isso afeta\n  <strong>\n   d\n  </strong>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Essa regra geral de \u201csoma sobre caminhos\u201d \u00e9 apenas uma maneira diferente de pensar sobre a regra da cadeia multivariada ou\n  <strong>\n   chain rule\n  </strong>\n  .\n </p>\n <p>\n </p>\n <h2>\n </h2>\n <h2>\n  Fatorando os Caminhos\n </h2>\n <p style=\"text-align: justify;\">\n  O problema com apenas \u201csomar os caminhos\u201d \u00e9 que \u00e9 muito f\u00e1cil obter uma explos\u00e3o combinat\u00f3ria no n\u00famero de caminhos poss\u00edveis.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"chain-def-greek\" class=\"aligncenter wp-image-414 size-medium\" data-attachment-id=\"414\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"chain-def-greek\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?fit=1024%2C248\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?fit=300%2C73\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?fit=1660%2C402\" data-orig-size=\"1660,402\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/chain-def-greek/\" data-recalc-dims=\"1\" height=\"73\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=300%2C73\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=300%2C73 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=768%2C186 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=1024%2C248 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=200%2C48 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?resize=690%2C167 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-def-greek.png?w=1660 1660w\" width=\"300\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  No diagrama acima, existem tr\u00eas caminhos de X a Y, e mais tr\u00eas caminhos de Y a Z. Se quisermos obter a derivada \u2202Z/\u2202X somando todos os caminhos, precisamos calcular 3 \u2217 3 = 9 caminhos:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"form2\" class=\"aligncenter size-full wp-image-415\" data-attachment-id=\"415\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?fit=402%2C58\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?fit=300%2C43\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?fit=402%2C58\" data-orig-size=\"402,58\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/form2-3/\" data-recalc-dims=\"1\" height=\"58\" sizes=\"(max-width: 402px) 100vw, 402px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?resize=402%2C58\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?w=402 402w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?resize=300%2C43 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form2.png?resize=200%2C29 200w\" width=\"402\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O exemplo acima s\u00f3 tem nove caminhos, mas seria f\u00e1cil o n\u00famero de caminhos crescer exponencialmente \u00e0 medida que o grafo se torna mais complicado. Em vez de apenas ingenuamente somar os caminhos, seria muito melhor fator\u00e1-los:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"form3\" class=\"aligncenter size-full wp-image-416\" data-attachment-id=\"416\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?fit=228%2C66\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?fit=228%2C66\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?fit=228%2C66\" data-orig-size=\"228,66\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/form3-2/\" data-recalc-dims=\"1\" height=\"66\" sizes=\"(max-width: 228px) 100vw, 228px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?resize=228%2C66\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?w=228 228w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form3.png?resize=200%2C58 200w\" width=\"228\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  \u00c9 a\u00ed que entram a \u201cdiferencia\u00e7\u00e3o de modo de avan\u00e7o\u201d (forward-mode differentiation ou forward pass) e a \u201cdiferencia\u00e7\u00e3o de modo reverso\u201d (reverse-mode differentiation ou backpropagation). Eles s\u00e3o algoritmos para calcular a soma de forma eficiente fatorando os caminhos. Em vez de somar todos os caminhos explicitamente, eles calculam a mesma soma de forma mais eficiente, mesclando os caminhos juntos novamente em cada n\u00f3. De fato, os dois algoritmos tocam cada borda exatamente uma vez!\n </p>\n <p style=\"text-align: justify;\">\n  A diferencia\u00e7\u00e3o do modo de avan\u00e7o inicia em uma entrada para o grafo e se move em dire\u00e7\u00e3o ao final. Em cada n\u00f3, soma todos os caminhos que se alimentam. Cada um desses caminhos representa uma maneira na qual a entrada afeta esse n\u00f3. Ao adicion\u00e1-los, obtemos a maneira total em que o n\u00f3 \u00e9 afetado pela entrada, isso \u00e9 a derivada.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"chain-forward-greek\" class=\"aligncenter wp-image-418 size-large\" data-attachment-id=\"418\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"chain-forward-greek\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?fit=1024%2C345\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?fit=300%2C101\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?fit=1660%2C560\" data-orig-size=\"1660,560\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/chain-forward-greek/\" data-recalc-dims=\"1\" height=\"345\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=1024%2C345\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=1024%2C345 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=300%2C101 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=768%2C259 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=200%2C67 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?resize=690%2C233 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-forward-greek.png?w=1660 1660w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Embora voc\u00ea provavelmente n\u00e3o tenha pensado nisso em termos de grafos, a diferencia\u00e7\u00e3o no modo de avan\u00e7o \u00e9 muito parecida com o que voc\u00ea aprendeu implicitamente caso tenha feito alguma introdu\u00e7\u00e3o a C\u00e1lculo.\n </p>\n <p style=\"text-align: justify;\">\n  A diferencia\u00e7\u00e3o no modo reverso, por outro lado, come\u00e7a na sa\u00edda do grafo e se move em dire\u00e7\u00e3o ao in\u00edcio (ou seja, se retropropaga ou backpropagation). Em cada n\u00f3, ele mescla todos os caminhos originados nesse n\u00f3.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"chain-backward-greek\" class=\"aligncenter size-large wp-image-419\" data-attachment-id=\"419\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"chain-backward-greek\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?fit=1024%2C356\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?fit=300%2C104\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?fit=1660%2C577\" data-orig-size=\"1660,577\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/chain-backward-greek/\" data-recalc-dims=\"1\" height=\"356\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=1024%2C356\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=1024%2C356 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=300%2C104 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=768%2C267 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=200%2C70 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?resize=690%2C240 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/chain-backward-greek.png?w=1660 1660w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A diferencia\u00e7\u00e3o do modo de avan\u00e7o rastreia como uma entrada afeta todos os n\u00f3s. A diferencia\u00e7\u00e3o no modo reverso rastreia como cada n\u00f3 afeta uma sa\u00edda. Ou seja, a diferencia\u00e7\u00e3o de modo de avan\u00e7o aplica o operador \u2202/\u2202X a cada n\u00f3, enquanto a diferencia\u00e7\u00e3o de modo reverso aplica o operador \u2202Z/\u2202 a cada n\u00f3. Se isso parece o conceito de programa\u00e7\u00e3o din\u00e2mica, \u00e9 porque \u00e9 exatamente isso! (acesse um material sobre programa\u00e7\u00e3o din\u00e2mica nas refer\u00eancias ao final do cap\u00edtulo)\n </p>\n <p style=\"text-align: justify;\">\n  Nesse ponto, voc\u00ea pode se perguntar porque algu\u00e9m se importaria com a diferencia\u00e7\u00e3o no modo reverso. Parece uma maneira estranha de fazer a mesma coisa que o modo de avan\u00e7o. Existe alguma vantagem? Vamos considerar nosso exemplo original novamente:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"tree-eval-derivs\" class=\"aligncenter size-large wp-image-399\" data-attachment-id=\"399\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tree-eval-derivs\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1024%2C578\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=300%2C169\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?fit=1405%2C793\" data-orig-size=\"1405,793\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-eval-derivs/\" data-recalc-dims=\"1\" height=\"578\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=1024%2C578 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=300%2C169 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=768%2C433 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=200%2C113 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?resize=690%2C389 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-eval-derivs.png?w=1405 1405w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p>\n  Podemos usar a diferencia\u00e7\u00e3o de modo de avan\u00e7o de\n  <strong>\n   b\n  </strong>\n  para cima. Isso nos d\u00e1 a derivada de cada n\u00f3 em rela\u00e7\u00e3o a\n  <strong>\n   b\n  </strong>\n  .\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"tree-forwradmode\" class=\"aligncenter size-large wp-image-420\" data-attachment-id=\"420\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tree-forwradmode\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?fit=1024%2C583\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?fit=300%2C171\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?fit=1415%2C806\" data-orig-size=\"1415,806\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-forwradmode/\" data-recalc-dims=\"1\" height=\"583\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=1024%2C583\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=1024%2C583 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=300%2C171 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=768%2C437 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=200%2C114 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?resize=690%2C393 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-forwradmode.png?w=1415 1415w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  N\u00f3s calculamos \u2202e/\u2202b, a derivada de nossa sa\u00edda em rela\u00e7\u00e3o a um de nossos inputs. E se fizermos a diferencia\u00e7\u00e3o de modo reverso de\n  <strong>\n   e\n  </strong>\n  para baixo? Isso nos d\u00e1 a derivada de\n  <strong>\n   e\n  </strong>\n  em rela\u00e7\u00e3o a todos os n\u00f3s:\n </p>\n <p>\n  <img alt=\"tree-backprop\" class=\"aligncenter size-large wp-image-421\" data-attachment-id=\"421\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tree-backprop\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?fit=1024%2C606\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?fit=300%2C177\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?fit=1408%2C833\" data-orig-size=\"1408,833\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/tree-backprop/\" data-recalc-dims=\"1\" height=\"606\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=1024%2C606\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=1024%2C606 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=300%2C177 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=768%2C454 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=200%2C118 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?resize=690%2C408 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/tree-backprop.png?w=1408 1408w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Quando digo que a diferencia\u00e7\u00e3o no modo reverso nos d\u00e1 a derivada de\n  <strong>\n   e\n  </strong>\n  em rela\u00e7\u00e3o a cada n\u00f3, eu realmente quero dizer cada n\u00f3. Temos tanto \u2202e/\u2202a quanto \u2202e/\u2202b, as derivadas de\n  <strong>\n   e\n  </strong>\n  em rela\u00e7\u00e3o a ambas as entradas. A diferencia\u00e7\u00e3o no modo de avan\u00e7o nos deu a derivada de nossa sa\u00edda em rela\u00e7\u00e3o a uma \u00fanica entrada, mas a diferencia\u00e7\u00e3o no modo reverso nos d\u00e1 todos eles.\n </p>\n <p style=\"text-align: justify;\">\n  Para este grafo, isso \u00e9 apenas um fator de duas velocidades, mas imagine uma fun\u00e7\u00e3o com um milh\u00e3o de entradas e uma sa\u00edda. A diferencia\u00e7\u00e3o no modo de avan\u00e7o exigiria que pass\u00e1ssemos pelo grafo um milh\u00e3o de vezes para obter as derivadas. Diferencia\u00e7\u00e3o no modo reverso pode fazer isso em uma s\u00f3 passada! Uma acelera\u00e7\u00e3o de um fator de um milh\u00e3o \u00e9 bem legal e explica porque conseguimos treinar um modelo de rede neural profunda em tempo razo\u00e1vel.\n </p>\n <p style=\"text-align: justify;\">\n  Ao treinar redes neurais, pensamos no custo (um valor que descreve o quanto uma rede neural \u00e9 ruim) em fun\u00e7\u00e3o dos par\u00e2metros (n\u00fameros que descrevem como a rede se comporta). Queremos calcular as derivadas do custo em rela\u00e7\u00e3o a todos os par\u00e2metros, para uso em descida do gradiente. Entretanto, muitas vezes, h\u00e1 milh\u00f5es ou at\u00e9 dezenas de milh\u00f5es de par\u00e2metros em uma rede neural. Ent\u00e3o, a diferencia\u00e7\u00e3o no modo reverso, chamada de backpropagation no contexto das redes neurais, nos d\u00e1 uma velocidade enorme!\n </p>\n <p style=\"text-align: justify;\">\n  Existem casos em que a diferencia\u00e7\u00e3o de modo de avan\u00e7o faz mais sentido? Sim, existem! Onde o modo reverso fornece as derivadas de uma sa\u00edda em rela\u00e7\u00e3o a todas as entradas, o modo de avan\u00e7o nos d\u00e1 as derivadas de todas as sa\u00eddas em rela\u00e7\u00e3o a uma entrada. Se tiver uma fun\u00e7\u00e3o com muitas sa\u00eddas, a diferencia\u00e7\u00e3o no modo de avan\u00e7o pode ser muito, muito mais r\u00e1pida.\n </p>\n <p>\n </p>\n <h2>\n  Agora faz sentido?\n </h2>\n <p style=\"text-align: justify;\">\n  Quando aprendemos pela primeira vez o que \u00e9 backpropagation, a rea\u00e7\u00e3o \u00e9: \u201cOh, essa \u00e9 apenas a regra da cadeia (chain rule)! Como demoramos tanto tempo para descobrir?\u201d\n </p>\n <p style=\"text-align: justify;\">\n  Na \u00e9poca em que o backpropagation foi inventado, as pessoas n\u00e3o estavam muito focadas nas redes neurais feedforward. Tamb\u00e9m n\u00e3o era \u00f3bvio que as derivadas eram o caminho certo para trein\u00e1-las. Esses s\u00e3o apenas \u00f3bvios quando voc\u00ea percebe que pode calcular rapidamente derivadas. Houve uma depend\u00eancia circular.\n </p>\n <p style=\"text-align: justify;\">\n  Treinar redes neurais com derivadas? Certamente voc\u00ea ficaria preso em m\u00ednimos locais. E obviamente seria caro computar todas essas derivadas. O fato \u00e9 que s\u00f3 porque sabemos que essa abordagem funciona \u00e9 que n\u00e3o come\u00e7amos imediatamente a listar os motivos que provavelmente n\u00e3o funcionaria. J\u00e1 sabemos que funciona, mas novas abordagens vem sendo propostas no avan\u00e7o das pesquisas em Deep Learning e Intelig\u00eancia Artificial.\n </p>\n <p>\n </p>\n <h2>\n  Conclus\u00e3o da Parte 1\n </h2>\n <p style=\"text-align: justify;\">\n  O backpropagation tamb\u00e9m \u00e9 \u00fatil para entender como as derivadas fluem atrav\u00e9s de um modelo. Isso pode ser extremamente \u00fatil no racioc\u00ednio sobre porque alguns modelos s\u00e3o dif\u00edceis de otimizar. O exemplo cl\u00e1ssico disso \u00e9 o problema do desaparecimento de gradientes em redes neurais recorrentes, que discutiremos mais diante neste livro.\n </p>\n <p style=\"text-align: justify;\">\n  Por fim, h\u00e1 uma li\u00e7\u00e3o algor\u00edtmica ampla a ser retirada dessas t\u00e9cnicas. Backpropagation e forward-mode differentiation usam um poderoso par de truques (lineariza\u00e7\u00e3o e programa\u00e7\u00e3o din\u00e2mica) para computar derivadas de forma mais eficiente do que se poderia imaginar. Se voc\u00ea realmente entende essas t\u00e9cnicas, pode us\u00e1-las para calcular com efici\u00eancia v\u00e1rias outras express\u00f5es interessantes envolvendo derivadas.\n </p>\n <p style=\"text-align: justify;\">\n  Mas este cap\u00edtulo teve como objetivo apenas ajud\u00e1-lo a compreender o algoritmo, j\u00e1 que praticamente n\u00e3o existe documenta\u00e7\u00e3o sobre isso em portugu\u00eas. Falta ainda compreender como o backpropagation \u00e9 aplicado no treinamento das redes neurais. Ansioso por isso? Ent\u00e3o acompanhe o pr\u00f3ximo cap\u00edtulo!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A Data Science Academy oferece um programa completo, onde esses e v\u00e1rios outros conceitos s\u00e3o estudados em detalhes e com v\u00e1rias aplica\u00e7\u00f5es pr\u00e1ticas e usando TensorFlow. A Forma\u00e7\u00e3o Intelig\u00eancia Artificial \u00e9 composta de 9 cursos, tudo 100% online e 100% em portugu\u00eas, que aliam teoria e pr\u00e1tica na medida certa, com aplica\u00e7\u00f5es reais de Intelig\u00eancia Artificial. Confira o programa completo dos cursos:\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial.\n   </a>\n  </span>\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.youtube.com/watch?v=CQxb5ZXeY3E\" rel=\"noopener\" target=\"_blank\">\n    Me Salva! C\u00e1lculo \u2013 O que \u00e9 uma derivada?\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/birth-graph-theory-leonhard-euler-and-konigsberg-bridge-problem\" rel=\"noopener\" target=\"_blank\">\n    The Birth Of Graph Theory: Leonhard Euler And The K\u00f6nigsberg Bridge Problem\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.nature.com/articles/323533a0\" rel=\"noopener\" target=\"_blank\">\n    Learning representations by back-propagating errors\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Chain_rule\" rel=\"noopener\" target=\"_blank\">\n    Chain Rule\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://colah.github.io/posts/2015-08-Backprop/\" rel=\"noopener\" target=\"_blank\">\n    Calculus on Computational Graphs: Backpropagation\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"noopener\" target=\"_blank\">\n    How the backpropagation algorithm works\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Dynamic_programming\" rel=\"noopener\" target=\"_blank\">\n    Dynamic programming\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  Nota: parte das imagens usadas neste cap\u00edtulo foram extra\u00eddas no excelente post (citado nas refer\u00eancias acima) de\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://colah.github.io/about.html\" rel=\"noopener\" target=\"_blank\">\n    Christopher Olah\n   </a>\n  </span>\n  , pesquisador de Machine Learning do Google Brain, e com a devida autoriza\u00e7\u00e3o do autor.\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-390\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-390\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-390\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-390\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-390-5e0dd0578dfc5\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=390&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-390-5e0dd0578dfc5\" id=\"like-post-wrapper-140353593-390-5e0dd0578dfc5\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "15": "<h1 class=\"entry-title\" id=\"capitulo-15\">\n Cap\u00edtulo 15 \u2013 Algoritmo Backpropagation Parte 2 \u2013 Treinamento de Redes Neurais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O backpropagation \u00e9 indiscutivelmente o algoritmo mais importante na hist\u00f3ria das redes neurais \u2013 sem backpropagation (eficiente), seria imposs\u00edvel treinar redes de aprendizagem profunda da forma que vemos hoje. O\u00a0backpropagation pode ser considerado a pedra angular das redes neurais modernas e aprendizagem profunda. Neste cap\u00edtulo, vamos compreender como o backpropagation \u00e9 usado no treinamento das redes neurais: Algoritmo Backpropagation Parte 2 \u2013 Treinamento de Redes Neurais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O algoritmo de backpropagation consiste em duas fases:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   1. O passo para frente (forward pass), onde nossas entradas s\u00e3o passadas atrav\u00e9s da rede e as previs\u00f5es de sa\u00edda obtidas (essa etapa tamb\u00e9m \u00e9 conhecida como fase de propaga\u00e7\u00e3o).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   2. O passo para tr\u00e1s (backward pass), onde calculamos o gradiente da fun\u00e7\u00e3o de perda na camada final (ou seja, camada de previs\u00e3o) da rede e usamos esse gradiente para aplicar recursivamente a regra da cadeia (chain rule) para atualizar os pesos em nossa rede (etapa tamb\u00e9m conhecida como fase de atualiza\u00e7\u00e3o de pesos ou retro-propaga\u00e7\u00e3o).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos analisar cada uma dessas fases e compreender como funciona o backpropagation no treinamento nas redes neurais. No pr\u00f3ximo cap\u00edtulo, voltaremos ao script em Python para compreender como \u00e9 a implementa\u00e7\u00e3o do algoritmo. Let\u2019s begin!\n  </span>\n </p>\n <p>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n  </span>\n </h2>\n <h2>\n  <span style=\"color: #000000;\">\n   Forward Pass\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O prop\u00f3sito do passo para frente \u00e9 propagar nossas entradas (os dados de entrada) atrav\u00e9s da rede aplicando uma s\u00e9rie de\n   <em>\n    dot products\n   </em>\n   (multiplica\u00e7\u00e3o entre os vetores) e ativa\u00e7\u00f5es at\u00e9 chegarmos \u00e0 camada de sa\u00edda da rede (ou seja, nossas previs\u00f5es). Para visualizar esse processo, vamos primeiro considerar a tabela abaixo. Podemos ver que cada entrada X na matriz \u00e9 2-dim (2 dimens\u00f5es), onde cada ponto de dado \u00e9 representado por dois n\u00fameros. Por exemplo, o primeiro ponto de dado \u00e9 representado pelo vetor de recursos (0, 0), o segundo ponto de dado por (0, 1), etc. Em seguida, temos nossos valores de sa\u00edda Y como a coluna da direita. Nossos valores de sa\u00edda s\u00e3o os r\u00f3tulos de classe. Dada uma entrada da matriz, nosso objetivo \u00e9 prever corretamente o valor de sa\u00edda desejado. Em resumo, X representa as entradas e Y a sa\u00edda.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"table\" class=\"aligncenter size-full wp-image-438\" data-attachment-id=\"438\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"table\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?fit=538%2C370\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?fit=300%2C206\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?fit=538%2C370\" data-orig-size=\"538,370\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/table/\" data-recalc-dims=\"1\" height=\"370\" sizes=\"(max-width: 538px) 100vw, 538px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?resize=538%2C370\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?w=538 538w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?resize=300%2C206 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table.png?resize=200%2C138 200w\" width=\"538\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para obter uma precis\u00e3o de classifica\u00e7\u00e3o perfeita nesse problema, precisamos de uma rede neural feedforward com pelo menos uma camada oculta. Podemos ent\u00e3o come\u00e7ar com uma arquitetura de 2-2-1 conforme a imagem abaixo.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"rede1\" class=\"aligncenter size-full wp-image-439\" data-attachment-id=\"439\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?fit=742%2C322\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?fit=300%2C130\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?fit=742%2C322\" data-orig-size=\"742,322\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/rede1/\" data-recalc-dims=\"1\" height=\"322\" sizes=\"(max-width: 742px) 100vw, 742px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=742%2C322\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?w=742 742w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=300%2C130 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=200%2C87 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede1.png?resize=690%2C299 690w\" width=\"742\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Este \u00e9 um bom come\u00e7o, no entanto, estamos esquecendo de incluir o bias. Existem duas maneiras de incluir o bias b em nossa rede. N\u00f3s podemos:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   1. Usar uma vari\u00e1vel separada.\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   2. Tratar o bias como um par\u00e2metro trein\u00e1vel dentro da matriz, inserindo uma coluna de\n   <strong>\n    1s\n   </strong>\n   nos vetores de recursos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Inserir uma coluna de 1s no nosso vetor de recursos \u00e9 feito de forma program\u00e1tica, mas para garantir a did\u00e1tica, vamos atualizar nossa matriz para ver isso explicitamente, conforme tabela abaixo. Como voc\u00ea pode ver, uma coluna de\n   <strong>\n    1s\n   </strong>\n   foi adicionada aos nossos vetores de recursos. Na pr\u00e1tica voc\u00ea pode inserir essa coluna em qualquer lugar que desejar, mas normalmente a colocamos como a primeira entrada no vetor de recursos ou a \u00faltima entrada no vetor de recursos.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"table2\" class=\"aligncenter size-full wp-image-447\" data-attachment-id=\"447\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"table2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?fit=718%2C372\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?fit=300%2C155\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?fit=718%2C372\" data-orig-size=\"718,372\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/table2-2/\" data-recalc-dims=\"1\" height=\"372\" sizes=\"(max-width: 718px) 100vw, 718px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=718%2C372\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?w=718 718w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=300%2C155 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=200%2C104 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/table2-1.png?resize=690%2C357 690w\" width=\"718\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como n\u00f3s mudamos o tamanho do nosso vetor de recursos de entrada (normalmente o que \u00e9 realizado dentro da implementa\u00e7\u00e3o da rede em si, para que n\u00e3o seja necess\u00e1rio modificar explicitamente a nossa matriz), isso muda nossa arquitetura de rede de 2-2-1 para uma arquitetura 3-3-1, conforme imagem abaixo.\u00a0Ainda nos referimos a essa arquitetura de rede como 2-2-1, mas quando se trata de implementa\u00e7\u00e3o, na verdade, \u00e9 3-3-1 devido \u00e0 adi\u00e7\u00e3o do termo de bias incorporado na matriz.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"rede2\" class=\"aligncenter size-full wp-image-442\" data-attachment-id=\"442\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?fit=794%2C502\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?fit=300%2C190\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?fit=794%2C502\" data-orig-size=\"794,502\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/rede2-2/\" data-recalc-dims=\"1\" height=\"502\" sizes=\"(max-width: 794px) 100vw, 794px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=794%2C502\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?w=794 794w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=300%2C190 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=768%2C486 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=200%2C126 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede2.png?resize=690%2C436 690w\" width=\"794\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Finalmente, lembre-se de que tanto nossa camada de entrada quanto todas as camadas ocultas exigem um termo de bias. No entanto, a camada de sa\u00edda final n\u00e3o requer um bias. O bias agora \u00e9 um par\u00e2metro trein\u00e1vel dentro da matriz de peso, tornando o treinamento mais eficiente e substancialmente mais f\u00e1cil de implementar. Para ver o forward pass em a\u00e7\u00e3o, primeiro inicializamos os pesos em nossa rede, conforme figura abaixo. Observe como cada seta na matriz de peso tem um valor associado a ela \u2013 esse \u00e9 o valor de peso atual para um determinado n\u00f3 e significa o valor em que uma determinada entrada \u00e9 amplificada ou diminu\u00edda. Este valor de peso ser\u00e1 ent\u00e3o atualizado durante a fase de backpropgation (lembre-se que ainda estamos no forward pass). Existem v\u00e1rias formas de inicializar o vetor de pesos e isso pode influenciar diretamente no treinamento da rede, como veremos mais abaixo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na extrema esquerda da figura abaixo, apresentamos o vetor de recursos (0, 1, 1) e tamb\u00e9m o valor de sa\u00edda 1 para a rede, pois depois precisamos calcular os erros de previs\u00e3o. Aqui podemos ver que 0,1 e 1 foram atribu\u00eddos aos tr\u00eas n\u00f3s de entrada na rede. Para propagar os valores atrav\u00e9s da rede e obter a classifica\u00e7\u00e3o final, n\u00f3s precisamos do\n   <em>\n    dot product\n   </em>\n   entre as entradas e os valores de peso, seguido pela aplica\u00e7\u00e3o de um fun\u00e7\u00e3o de ativa\u00e7\u00e3o (neste caso, a fun\u00e7\u00e3o\n   <strong>\n    sigm\u00f3ide s\n   </strong>\n   ). Vamos calcular as entradas para os tr\u00eas n\u00f3s nas camadas ocultas:\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   1. s ((0 x 0.351) + (1 x 1.076) + (1 x 1.116)) = 0.899\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   2. s ((0\u0002 x 0.097) + (1\u0002 x 0.165)+(1\u0002 x 0.542)) = 0.593\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   3. s ((0\u0002x 0.457) + (1\u0002 x 0.165)+(1\u0002 x 0.331)) = 0.378\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"rede3\" class=\"aligncenter size-full wp-image-443\" data-attachment-id=\"443\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?fit=833%2C648\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?fit=300%2C233\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?fit=833%2C648\" data-orig-size=\"833,648\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/rede3/\" data-recalc-dims=\"1\" height=\"648\" sizes=\"(max-width: 833px) 100vw, 833px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=833%2C648\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?w=833 833w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=300%2C233 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=768%2C597 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=200%2C156 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/rede3.png?resize=690%2C537 690w\" width=\"833\"/>\n  </span>\n </p>\n <p>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Observando os valores dos n\u00f3s das camadas ocultas (camadas do meio), podemos ver que os n\u00f3s foram atualizados para refletir nossa computa\u00e7\u00e3o. Agora temos nossas entradas para os n\u00f3s da camada oculta. Para calcular a previs\u00e3o de sa\u00edda, uma vez mais usamos o\n   <em>\n    dot product\n   </em>\n   seguido por uma ativa\u00e7\u00e3o sigm\u00f3ide:\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   s ((0.899 x 0.383) + (0.593 x \u2013 0.327) + (0.378 x -0.329)) = 0.506\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   A sa\u00edda da rede \u00e9, portanto, 0.506. Podemos aplicar uma fun\u00e7\u00e3o de etapa (step function) para determinar se a sa\u00edda \u00e9 a classifica\u00e7\u00e3o correta ou n\u00e3o:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"saida\" class=\"aligncenter size-full wp-image-445\" data-attachment-id=\"445\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"saida\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?fit=560%2C170\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?fit=300%2C91\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?fit=560%2C170\" data-orig-size=\"560,170\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/saida-2/\" data-recalc-dims=\"1\" height=\"170\" sizes=\"(max-width: 560px) 100vw, 560px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?resize=560%2C170\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?w=560 560w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?resize=300%2C91 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/saida-1.png?resize=200%2C61 200w\" width=\"560\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aplicando a step function com saida = 0.506, vemos que nossa rede prev\u00ea 1 que \u00e9, de fato, o r\u00f3tulo de classe correto. No entanto, a nossa rede n\u00e3o est\u00e1 muito confiante neste r\u00f3tulo de classe. O valor previsto 0.506 est\u00e1 muito pr\u00f3ximo do limite da etapa. Idealmente, esta previs\u00e3o deve ser mais pr\u00f3xima de 0.98 ou 0.99., implicando que a nossa rede realmente aprendeu o padr\u00e3o no conjunto de dados. Para que nossa rede realmente \u201caprenda\u201d, precisamos aplicar o backpropagation.\n  </span>\n </p>\n <p>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Backpropagation\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para qualquer problema de aprendizagem supervisionada, n\u00f3s selecionamos pesos que fornecem a estimativa \u00f3tima de uma fun\u00e7\u00e3o que modela nossos dados de treinamento. Em outras palavras, queremos encontrar um conjunto de pesos W que minimize a sa\u00edda de J(W), onde J(W) \u00e9 a fun\u00e7\u00e3o de perda, ou o erro da rede. Nos cap\u00edtulos anteriores, discutimos o algoritmo de gradiente descendente, em que atualizamos cada peso por alguma redu\u00e7\u00e3o escalar negativa da derivada do erro em rela\u00e7\u00e3o a esse peso. Se optarmos por usar gradiente descendente (ou quase qualquer outro algoritmo de otimiza\u00e7\u00e3o convexo), precisamos encontrar as derivadas na forma num\u00e9rica.\n  </span>\n </p>\n <blockquote>\n  <p style=\"text-align: center;\">\n   <span style=\"color: #000000;\">\n    O objetivo do backpropagation \u00e9 otimizar os pesos para que a rede neural possa aprender a mapear corretamente as entradas para as sa\u00eddas.\n   </span>\n  </p>\n </blockquote>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para outros algoritmos de aprendizado de m\u00e1quina, como regress\u00e3o log\u00edstica ou regress\u00e3o linear, o c\u00e1lculo das derivadas \u00e9 uma aplica\u00e7\u00e3o elementar de diferencia\u00e7\u00e3o. Isso ocorre porque as sa\u00eddas desses modelos s\u00e3o apenas as entradas multiplicadas por alguns pesos escolhidos e, no m\u00e1ximo, alimentados por uma \u00fanica fun\u00e7\u00e3o de ativa\u00e7\u00e3o (a fun\u00e7\u00e3o sigm\u00f3ide na regress\u00e3o log\u00edstica). O mesmo, no entanto, n\u00e3o pode ser dito para redes neurais. Para demonstrar isso, aqui est\u00e1 um diagrama de uma rede neural de dupla camada:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"diam\" class=\"aligncenter size-full wp-image-454\" data-attachment-id=\"454\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"diam\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?fit=641%2C138\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?fit=300%2C65\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?fit=641%2C138\" data-orig-size=\"641,138\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/diam/\" data-recalc-dims=\"1\" height=\"138\" sizes=\"(max-width: 641px) 100vw, 641px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?resize=641%2C138\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?w=641 641w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?resize=300%2C65 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/diam.jpeg?resize=200%2C43 200w\" width=\"641\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como voc\u00ea pode ver, cada neur\u00f4nio \u00e9 uma fun\u00e7\u00e3o do anterior conectado a ele. Em outras palavras, se algu\u00e9m alterasse o valor de w1, os neur\u00f4nios \u201chidden 1\u201d e \u201chidden 2\u201d (e, finalmente, a sa\u00edda) mudariam. Devido a essa no\u00e7\u00e3o de depend\u00eancias funcionais, podemos formular matematicamente a sa\u00edda como uma fun\u00e7\u00e3o composta extensiva:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"func\" class=\"aligncenter size-full wp-image-460\" data-attachment-id=\"460\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?fit=341%2C124\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?fit=300%2C109\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?fit=341%2C124\" data-orig-size=\"341,124\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func-2/\" data-recalc-dims=\"1\" height=\"124\" sizes=\"(max-width: 341px) 100vw, 341px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?resize=341%2C124\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?w=341 341w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?resize=300%2C109 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func.png?resize=200%2C73 200w\" width=\"341\"/>\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   ou simplesmente:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"func2\" class=\"aligncenter size-full wp-image-461\" data-attachment-id=\"461\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?fit=519%2C33\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?fit=300%2C19\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?fit=519%2C33\" data-orig-size=\"519,33\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func2-3/\" data-recalc-dims=\"1\" height=\"33\" sizes=\"(max-width: 519px) 100vw, 519px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?resize=519%2C33\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?w=519 519w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?resize=300%2C19 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func2.png?resize=200%2C13 200w\" width=\"519\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para aplicar o algoritmo de backpropagation, nossa fun\u00e7\u00e3o de ativa\u00e7\u00e3o deve ser diferenci\u00e1vel, de modo que possamos calcular a derivada parcial do erro em rela\u00e7\u00e3o a um dado peso wi,j, loss(E), sa\u00edda de n\u00f3 oj e sa\u00edda de rede j.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"derivada\" class=\"aligncenter size-full wp-image-449\" data-attachment-id=\"449\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"derivada\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?fit=232%2C68\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?fit=232%2C68\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?fit=232%2C68\" data-orig-size=\"232,68\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/derivada-2/\" data-recalc-dims=\"1\" height=\"68\" sizes=\"(max-width: 232px) 100vw, 232px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?resize=232%2C68\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?w=232 232w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/derivada-1.png?resize=200%2C59 200w\" width=\"232\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui, a sa\u00edda \u00e9 uma fun\u00e7\u00e3o composta dos pesos, entradas e fun\u00e7\u00e3o (ou fun\u00e7\u00f5es) de ativa\u00e7\u00e3o. \u00c9 importante perceber que as unidades / n\u00f3s ocultos s\u00e3o simplesmente c\u00e1lculos intermedi\u00e1rios que, na realidade, podem ser reduzidos a c\u00e1lculos da camada de entrada.\u00a0Se f\u00f4ssemos ent\u00e3o tirar a derivada da fun\u00e7\u00e3o com rela\u00e7\u00e3o a algum peso arbitr\u00e1rio (por exemplo, w1), aplicar\u00edamos iterativamente a regra da cadeia (da qual eu tenho certeza que voc\u00ea se lembra do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/\" rel=\"noopener\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     cap\u00edtulo anterior\n    </a>\n   </span>\n   ). O resultado seria semelhante ao seguinte:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"func3\" class=\"aligncenter size-full wp-image-462\" data-attachment-id=\"462\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?fit=784%2C73\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?fit=300%2C28\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?fit=784%2C73\" data-orig-size=\"784,73\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func3/\" data-recalc-dims=\"1\" height=\"73\" sizes=\"(max-width: 784px) 100vw, 784px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=784%2C73\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?w=784 784w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=300%2C28 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=768%2C72 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=200%2C19 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func3.png?resize=690%2C64 690w\" width=\"784\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora, vamos anexar mais uma opera\u00e7\u00e3o \u00e0 cauda da nossa rede neural. Esta opera\u00e7\u00e3o ir\u00e1 calcular e retornar o erro \u2013 usando a fun\u00e7\u00e3o de custo \u2013 da nossa sa\u00edda:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"func4\" class=\"aligncenter size-full wp-image-463\" data-attachment-id=\"463\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?fit=752%2C144\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?fit=300%2C57\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?fit=752%2C144\" data-orig-size=\"752,144\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func4/\" data-recalc-dims=\"1\" height=\"144\" sizes=\"(max-width: 752px) 100vw, 752px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=752%2C144\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?w=752 752w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=300%2C57 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=200%2C38 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func4.jpeg?resize=690%2C132 690w\" width=\"752\"/>\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Tudo o que fizemos foi adicionar outra depend\u00eancia funcional; nosso erro \u00e9 agora uma fun\u00e7\u00e3o da sa\u00edda e, portanto, uma fun\u00e7\u00e3o da entrada, pesos e fun\u00e7\u00e3o de ativa\u00e7\u00e3o. Se f\u00f4ssemos calcular a derivada do erro com qualquer peso arbitr\u00e1rio (novamente, escolher\u00edamos w1), o resultado seria:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"func5\" class=\"aligncenter size-full wp-image-464\" data-attachment-id=\"464\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"func5\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?fit=649%2C79\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?fit=300%2C37\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?fit=649%2C79\" data-orig-size=\"649,79\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/func5/\" data-recalc-dims=\"1\" height=\"79\" sizes=\"(max-width: 649px) 100vw, 649px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?resize=649%2C79\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?w=649 649w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?resize=300%2C37 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/func5.png?resize=200%2C24 200w\" width=\"649\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Cada uma dessas deriva\u00e7\u00f5es pode ser simplificada, uma vez que escolhemos uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o e erro, de modo que todo o resultado represente um valor num\u00e9rico. Nesse ponto, qualquer abstra\u00e7\u00e3o foi removida e a derivada de erro pode ser usada na descida do gradiente (como discutido anteriormente aqui no livro) para melhorar iterativamente o peso. Calculamos as derivadas de erro w.r.t. para todos os outros pesos na rede e aplicamos gradiente descendente da mesma maneira. Isso \u00e9 backpropagation \u2013 simplesmente o c\u00e1lculo de derivadas que s\u00e3o alimentadas para um algoritmo de otimiza\u00e7\u00e3o convexa. Chamamos isso de \u201cretropropaga\u00e7\u00e3o\u201d porque estamos usando o erro de sa\u00edda para atualizar os pesos, tomando passos iterativos usando a regra da cadeia at\u00e9 que alcancemos o valor de peso ideal.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Depois de compreender o funcionamento do algoritmo backpropagation, voc\u00ea percebe sua simplicidade. Claro, a aritm\u00e9tica/c\u00e1lculos reais podem ser dif\u00edceis, mas esse processo \u00e9 tratado pelos nossos computadores. Na realidade, o\u00a0backpropagation \u00e9 apenas uma aplica\u00e7\u00e3o da regra da cadeia (chain rule). Como as redes neurais s\u00e3o estruturas de modelo de aprendizado de m\u00e1quina multicamadas complicadas, cada peso \u201ccontribui\u201d para o erro geral de uma maneira mais complexa e, portanto, as derivadas reais exigem muito esfor\u00e7o para serem produzidas. No entanto, uma vez que passamos pelo c\u00e1lculo, o backpropagation das redes neurais \u00e9 equivalente \u00e0 descida de gradiente t\u00edpica para regress\u00e3o log\u00edstica / linear.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim, como regra geral de atualiza\u00e7\u00f5es de peso, podemos usar a Regra Delta (Delta Rule):\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Novo Peso = Peso Antigo \u2013 Derivada * Taxa de Aprendizagem\n   </strong>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A taxa de aprendizagem (learning rate) \u00e9 introduzida como uma constante (geralmente muito pequena), a fim de for\u00e7ar o peso a ser atualizado de forma suave e lenta (para evitar grandes passos e comportamento ca\u00f3tico).\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Para validar esta equa\u00e7\u00e3o:\n  </span>\n </p>\n <ul>\n  <li>\n   <span style=\"color: #000000;\">\n    Se a Derivada for positiva, isso significa que um aumento no peso aumentar\u00e1 o erro, portanto, o novo peso dever\u00e1 ser menor.\n   </span>\n  </li>\n  <li>\n   <span style=\"color: #000000;\">\n    Se a Derivada \u00e9 negativa, isso significa que um aumento no peso diminuir\u00e1 o erro, portanto, precisamos aumentar os pesos.\n   </span>\n  </li>\n  <li>\n   <span style=\"color: #000000;\">\n    Se a Derivada \u00e9 0, significa que estamos em um m\u00ednimo est\u00e1vel. Assim, nenhuma atualiza\u00e7\u00e3o nos pesos \u00e9 necess\u00e1ria -&gt; chegamos a um estado est\u00e1vel.\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Existem v\u00e1rios m\u00e9todos de atualiza\u00e7\u00e3o de peso. Esses m\u00e9todos s\u00e3o frequentemente chamados de\n   <em>\n    otimizadores\n   </em>\n   . A regra delta \u00e9 a mais simples e intuitiva, no entanto, possui v\u00e1rias desvantagens. Confira nas refer\u00eancias ao final do cap\u00edtulo, um excelente artigo sobre otimizadores.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como atualizamos os pesos com uma pequena etapa delta de cada vez, ser\u00e3o necess\u00e1rias v\u00e1rias itera\u00e7\u00f5es para ocorrer o aprendizado. Na rede neural, ap\u00f3s cada itera\u00e7\u00e3o, a for\u00e7a de descida do gradiente atualiza os pesos para um valor cada vez menor da fun\u00e7\u00e3o de perda global. A\u00a0atualiza\u00e7\u00e3o de peso na rede neural \u00e9 guiada pela for\u00e7a do gradiente descendente sobre o erro.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Quantas itera\u00e7\u00f5es s\u00e3o necess\u00e1rias para convergir (ou seja, alcan\u00e7ar uma fun\u00e7\u00e3o de perda m\u00ednima global)? Isso vai depender de diversos fatores:\n  </span>\n </p>\n <ul style=\"text-align: justify;\">\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Depende de qu\u00e3o forte \u00e9 a taxa de aprendizado que estamos aplicando. Alta taxa de aprendizado significa aprendizado mais r\u00e1pido, mas com maior chance de instabilidade.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Depende tamb\u00e9m dos hyperpar\u00e2metros da rede (quantas camadas, qu\u00e3o complexas s\u00e3o as fun\u00e7\u00f5es n\u00e3o-lineares, etc..). Quanto mais vari\u00e1veis, mais leva tempo para convergir, mas a precis\u00e3o tende a ser maior.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Depende do uso do m\u00e9todo de otimiza\u00e7\u00e3o, pois algumas regras de atualiza\u00e7\u00e3o de peso s\u00e3o comprovadamente mais r\u00e1pidas do que outras.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Depende da inicializa\u00e7\u00e3o aleat\u00f3ria da rede. Talvez com alguma sorte voc\u00ea inicie a rede com pesos quase ideais e esteja a apenas um passo da solu\u00e7\u00e3o ideal. Mas o contr\u00e1rio tamb\u00e9m pode ocorrer.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Depende da qualidade do conjunto de treinamento. Se a entrada e a sa\u00edda n\u00e3o tiverem correla\u00e7\u00e3o entre si, a rede neural n\u00e3o far\u00e1 m\u00e1gica e n\u00e3o poder\u00e1 aprender uma correla\u00e7\u00e3o aleat\u00f3ria.\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ou seja, treinar uma rede neural n\u00e3o \u00e9 tarefa simples. Imagine agora treinar uma rede profunda, com v\u00e1rias camadas intermedi\u00e1rias e milh\u00f5es ou mesmo bilh\u00f5es de pontos de dados e voc\u00ea compreende o qu\u00e3o trabalhoso isso pode ser e quantas decis\u00f5es devem ser tomadas pelo Cientista de Dados ou Engenheiro de IA. E aprender a trabalhar de forma profissional, requer tempo, dedica\u00e7\u00e3o e preparo e melhor ainda se isso puder ser 100% em portugu\u00eas para acelerar seu aprendizado. Construir aplica\u00e7\u00f5es de IA \u00e9 uma habilidade com demanda cada vez maior no mercado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Pensando nisso, a Data Science Academy oferece um programa completo, onde esses e v\u00e1rios outros conceitos s\u00e3o estudados em detalhes e com v\u00e1rias aplica\u00e7\u00f5es pr\u00e1ticas, usando TensorFlow. A Forma\u00e7\u00e3o Intelig\u00eancia Artificial \u00e9 composta de 9 cursos, tudo 100% online e 100% em portugu\u00eas, que aliam teoria e pr\u00e1tica na medida certa, com aplica\u00e7\u00f5es reais de Intelig\u00eancia Artificial. Confira o programa completo dos cursos:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial.\n    </a>\n   </span>\n   V\u00e1rias empresas em todo Brasil j\u00e1 est\u00e3o treinando seus profissionais conosco! Venha fazer parte da revolu\u00e7\u00e3o da IA.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora que voc\u00ea j\u00e1 compreende como funciona o backpropagation, podemos retornar ao c\u00f3digo Python e ver tudo isso funcionando na pr\u00e1tica. Mas isso \u00e9 assunto para o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Dot_product\" rel=\"noopener\" target=\"_blank\">\n    Dot Product\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"noopener\" target=\"_blank\">\n    How the backpropagation algorithm works\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/3-DeltaRule.pdf\" rel=\"noopener\" target=\"_blank\">\n    Delta Rule\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ruder.io/optimizing-gradient-descent/\" rel=\"noopener\" target=\"_blank\">\n    An overview of gradient descent optimization algorithms\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Derivada\" rel=\"noopener\" target=\"_blank\">\n    Derivada\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ruder.io/optimizing-gradient-descent/\" rel=\"noopener\" target=\"_blank\">\n    An overview of gradient descent optimization algorithms\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\" rel=\"noopener\" target=\"_blank\">\n    Optimization: Stochastic Gradient Descent\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-436\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-436\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-436\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-436\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-436-5e0dd059b429e\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=436&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-436-5e0dd059b429e\" id=\"like-post-wrapper-140353593-436-5e0dd059b429e\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "16": "<h1 class=\"entry-title\" id=\"capitulo-16\">\n Cap\u00edtulo 16 \u2013 Algoritmo Backpropagation em Python\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Depois de compreender como funciona o backpropagation, podemos agora entender o c\u00f3digo usado em alguns cap\u00edtulos anteriores para implementar o algoritmo (o qual vamos reproduzir aqui). O arquivo com o c\u00f3digo completo pode ser encontrado no reposit\u00f3rio do livro no\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener\" target=\"_blank\">\n    Github\n   </a>\n  </span>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Em nosso c\u00f3digo n\u00f3s temos os m\u00e9todos\n  <strong>\n   update_mini_batch\n  </strong>\n  e\n  <strong>\n   backprop\n  </strong>\n  da classe Network. Em particular, o m\u00e9todo\n  <strong>\n   update_mini_batch\n  </strong>\n  atualiza os pesos e bias da rede calculando o gradiente para o mini_batch atual de exemplos (dados) de treinamento:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"metodo1\" class=\"aligncenter wp-image-489 size-large\" data-attachment-id=\"489\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"metodo1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?fit=1024%2C378\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?fit=300%2C111\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?fit=1318%2C486\" data-orig-size=\"1318,486\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/metodo1/\" data-recalc-dims=\"1\" height=\"378\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=1024%2C378\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=1024%2C378 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=300%2C111 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=768%2C283 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=200%2C74 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?resize=690%2C254 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/metodo1.png?w=1318 1318w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A maior parte do trabalho \u00e9 feita pela linha:\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   delta_nabla_b, delta_nabla_w = self.backprop (x, y)\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  que usa o m\u00e9todo backprop para descobrir as derivadas parciais \u2202Cx / \u2202blj e \u2202Cx / \u2202wljk. Isso invoca o algoritmo de backpropagation, que \u00e9 uma maneira r\u00e1pida de calcular o gradiente da fun\u00e7\u00e3o de custo. Portanto, update_mini_batch funciona simplesmente calculando esses gradientes para cada exemplo de treinamento no mini_batch e, em seguida, atualizando self.weights e self.biases adequadamente. H\u00e1 uma pequena mudan\u00e7a \u2013 usamos uma abordagem ligeiramente diferente para indexar as camadas. Essa altera\u00e7\u00e3o \u00e9 feita para aproveitar um recurso do Python, ou seja, o uso de \u00edndices de lista negativa para contar para tr\u00e1s a partir do final de uma lista, por exemplo, lst[-3] \u00e9 a terceira \u00faltima entrada em uma lista chamada lst. O c\u00f3digo para backprop est\u00e1 abaixo, junto com algumas fun\u00e7\u00f5es auxiliares, que s\u00e3o usadas para calcular a fun\u00e7\u00e3o \u03c3, a derivada \u03c3\u2032 e a derivada da fun\u00e7\u00e3o de custo. Com essas inclus\u00f5es, voc\u00ea deve ser capaz de entender o c\u00f3digo de maneira independente:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"backprop\" class=\"aligncenter size-large wp-image-490\" data-attachment-id=\"490\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"backprop\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?fit=1024%2C916\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?fit=300%2C268\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?fit=1292%2C1156\" data-orig-size=\"1292,1156\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/backprop/\" data-recalc-dims=\"1\" height=\"916\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=1024%2C916\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=1024%2C916 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=300%2C268 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=768%2C687 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=200%2C179 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?resize=690%2C617 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/backprop.png?w=1292 1292w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Observe o m\u00e9todo backprop. Come\u00e7amos inicalizando as matrizes de pesos (nabla_w) e bias (nabla_b) com zeros. Essas\u00a0 matrizes ser\u00e3o alimentadas com valores durante o processo de treinamento. Isso \u00e9 o que a rede neural artificial efetivamente aprende. Depois de inicializar alguns objetos, temos um loop for para cada valor de b e w (que a esta altura voc\u00ea j\u00e1 sabe se trata de bias e pesos, respectivamente). Neste loop, usamos a fun\u00e7\u00e3o np.dot do Numpy para a multiplica\u00e7\u00e3o entre matrizes e adi\u00e7\u00e3o do bias, colocamos o resultado na lista z e fazemos uma chamada \u00e0 fun\u00e7\u00e3o de ativa\u00e7\u00e3o Sigm\u00f3ide. Ao final deste loop, teremos a lista com todas as ativa\u00e7\u00f5es e finalizamos a passada para a frente.\n </p>\n <p style=\"text-align: justify;\">\n  Na passada para tr\u00e1s (Backward Pass) calculamos as derivadas e fazemos as multiplica\u00e7\u00f5es de matrizes mais uma vez (o funcionamento de redes neurais artificiais \u00e9 baseado em um conceito elementar da \u00c1lgebra Linear, a multiplica\u00e7\u00e3o de matrizes). Repare que chamamos o m\u00e9todo Transpose() para gerar a transposta da matriz e assim ajustar as dimens\u00f5es antes de efetuar os c\u00e1lculo. Por fim, retornamos bias e pesos.\n </p>\n <p>\n </p>\n <h3 style=\"text-align: justify;\">\n  Em que sentido backpropagation \u00e9 um algoritmo r\u00e1pido?\n </h3>\n <p style=\"text-align: justify;\">\n  Para responder a essa pergunta, vamos considerar outra abordagem para calcular o gradiente. Imagine que \u00e9 o in\u00edcio da pesquisa de redes neurais. Talvez seja a d\u00e9cada de 1950 ou 1960, e voc\u00ea \u00e9 a primeira pessoa no mundo a pensar em usar gradiente descendente para o aprendizado! Mas, para que a ideia funcione, voc\u00ea precisa de uma maneira de calcular o gradiente da fun\u00e7\u00e3o de custo. Voc\u00ea volta ao seu conhecimento de c\u00e1lculo e decide se pode usar a regra da cadeia (chain rule) para calcular o gradiente. Mas depois de brincar um pouco, a \u00e1lgebra parece complicada e voc\u00ea fica desanimado. Ent\u00e3o voc\u00ea tenta encontrar outra abordagem. Voc\u00ea decide considerar o custo como uma fun\u00e7\u00e3o apenas dos pesos C = C(w) (voltaremos ao bias em um momento). Voc\u00ea numera os pesos w1, w2,\u2026 e deseja computar \u2202C / \u2202wj para um peso espec\u00edfico wj. Uma maneira \u00f3bvia de fazer isso \u00e9 usar a aproxima\u00e7\u00e3o\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form\" class=\"aligncenter size-full wp-image-484\" data-attachment-id=\"484\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?fit=488%2C154\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?fit=300%2C95\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?fit=488%2C154\" data-orig-size=\"488,154\" data-permalink=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/form-4/\" data-recalc-dims=\"1\" height=\"154\" sizes=\"(max-width: 488px) 100vw, 488px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?resize=488%2C154\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?w=488 488w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?resize=300%2C95 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/04/form-1.png?resize=200%2C63 200w\" width=\"488\"/>\n </p>\n <p style=\"text-align: justify;\">\n  onde \u03f5&gt; 0 \u00e9 um pequeno n\u00famero positivo e ej \u00e9 o vetor unit\u00e1rio na dire\u00e7\u00e3o j. Em outras palavras, podemos estimar \u2202C / \u2202wj calculando o custo C para dois valores ligeiramente diferentes de wj e, em seguida, aplicando a equa\u00e7\u00e3o. A mesma ideia nos permitir\u00e1 calcular as derivadas parciais \u2202C / \u2202b em rela\u00e7\u00e3o aos vieses (bias).\n </p>\n <p style=\"text-align: justify;\">\n  Essa abordagem parece muito promissora. \u00c9 simples conceitualmente e extremamente f\u00e1cil de implementar, usando apenas algumas linhas de c\u00f3digo. Certamente, parece muito mais promissor do que a ideia de usar a regra da cadeia para calcular o gradiente!\n </p>\n <p style=\"text-align: justify;\">\n  Infelizmente, embora essa abordagem pare\u00e7a promissora, quando voc\u00ea implementa o c\u00f3digo, ele fica extremamente lento. Para entender porque, imagine que temos um milh\u00e3o de pesos em nossa rede. Ent\u00e3o, para cada peso distinto wj, precisamos computar C (w + \u03f5ej) para calcular \u2202C / \u2202wj. Isso significa que, para calcular o gradiente, precisamos computar a fun\u00e7\u00e3o de custo um milh\u00e3o de vezes diferentes, exigindo um milh\u00e3o de passos para frente pela rede (por exemplo, treinamento). Precisamos calcular C(w) tamb\u00e9m, em um total de um milh\u00e3o de vezes e em uma \u00fanica passada pela rede.\n </p>\n <p style=\"text-align: justify;\">\n  O que h\u00e1 de inteligente no backpropagation \u00e9 que ele nos permite calcular simultaneamente todas as derivadas parciais \u2202C / \u2202wj usando apenas uma passagem direta pela rede, seguida por uma passagem para tr\u00e1s pela rede. Grosso modo, o custo computacional do passe para tr\u00e1s \u00e9 quase o mesmo que o do forward. Isso deve ser plaus\u00edvel, mas requer algumas an\u00e1lises para fazer uma declara\u00e7\u00e3o cuidadosa. \u00c9 plaus\u00edvel porque o custo computacional dominante no passe para frente \u00e9 multiplicado pelas matrizes de peso, enquanto no passo para tr\u00e1s \u00e9 multiplicado pelas transpostas das matrizes de peso. Obviamente, essas opera\u00e7\u00f5es t\u00eam um custo computacional similar. E assim, o custo total da retropropaga\u00e7\u00e3o (backpropagation) \u00e9 aproximadamente o mesmo que fazer apenas duas passagens pela rede. Compare isso com o milh\u00e3o e um passe para frente que precis\u00e1vamos para a abordagem que descrevi anteriormente. E assim, embora a retropropaga\u00e7\u00e3o pare\u00e7a superficialmente mais complexa do que a abordagem anterior, \u00e9 na verdade muito, muito mais r\u00e1pida.\n </p>\n <p style=\"text-align: justify;\">\n  Essa acelera\u00e7\u00e3o foi amplamente apreciada em 1986 e expandiu enormemente a gama de problemas que as redes neurais poderiam resolver. Isso, por sua vez, causou uma onda de pessoas usando redes neurais. Claro, a retropropaga\u00e7\u00e3o n\u00e3o \u00e9 uma panac\u00e9ia. Mesmo no final da d\u00e9cada de 1980, as pessoas enfrentavam limites, especialmente quando tentavam usar a retropropaga\u00e7\u00e3o para treinar redes neurais profundas, ou seja, redes com muitas camadas ocultas. Mais adiante, no livro, veremos como os computadores modernos e algumas novas ideias inteligentes tornam poss\u00edvel usar a retropropaga\u00e7\u00e3o para treinar redes neurais bem profundas.\n </p>\n <p style=\"text-align: justify;\">\n  Seu trabalho agora \u00e9 estudar e compreender cada linha de c\u00f3digo usada em nossa rede de amostra. Esse c\u00f3digo \u00e9 bem simples e o objetivo \u00e9 mostrar a voc\u00ea como as coisas funcionam programaticamente. Ainda vamos treinar nossa rede, avaliar seu desempenho, otimizar algumas opera\u00e7\u00f5es e compreender outros conceitos b\u00e1sicos. Temos muito mais vindo por a\u00ed! At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Dot_product\" rel=\"noopener\" target=\"_blank\">\n    Dot Product\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"noopener\" target=\"_blank\">\n    How the backpropagation algorithm works\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/3-DeltaRule.pdf\" rel=\"noopener\" target=\"_blank\">\n    Delta Rule\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ruder.io/optimizing-gradient-descent/\" rel=\"noopener\" target=\"_blank\">\n    An overview of gradient descent optimization algorithms\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Derivada\" rel=\"noopener\" target=\"_blank\">\n    Derivada\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ruder.io/optimizing-gradient-descent/\" rel=\"noopener\" target=\"_blank\">\n    An overview of gradient descent optimization algorithms\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\" rel=\"noopener\" target=\"_blank\">\n    Optimization: Stochastic Gradient Descent\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning\n   </a>\n  </span>\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-480\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-480\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-480\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-480\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-em-python/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-480-5e0dd05cd603d\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=480&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-480-5e0dd05cd603d\" id=\"like-post-wrapper-140353593-480-5e0dd05cd603d\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "17": "<h1 class=\"entry-title\" id=\"capitulo-17\">\n Cap\u00edtulo 17 \u2013 Cross-Entropy Cost Function\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Quando um jogador de t\u00eanis est\u00e1 aprendendo a praticar o esporte, ele geralmente passa a maior parte do tempo desenvolvendo o movimento do corpo. Apenas gradualmente ele desenvolve as tacadas, aprende a movimentar a bola com precis\u00e3o para a quadra advers\u00e1ria e com isso vai construindo sua t\u00e9cnica, que se aprimora \u00e0 medida que ele pratica. De maneira semelhante, at\u00e9 agora nos concentramos em entender o algoritmo de retropropaga\u00e7\u00e3o (backpropagation), a base para aprender a maioria das atividades em redes neurais. A partir de agora, estudaremos um conjunto de t\u00e9cnicas que podem ser usadas para melhorar nossa implementa\u00e7\u00e3o do backpropagation e, assim, melhorar a maneira como nossas redes aprendem.\n </p>\n <p style=\"text-align: justify;\">\n  As t\u00e9cnicas que desenvolveremos incluem: uma melhor escolha de fun\u00e7\u00e3o de custo, conhecida como fun\u00e7\u00e3o de custo de entropia cruzada (ou Cross-Entropy Cost Function); quatro m\u00e9todos de \u201cregulariza\u00e7\u00e3o\u201d (regulariza\u00e7\u00e3o de L1 e L2, dropout e expans\u00e3o artificial dos dados de treinamento), que melhoram nossas redes para generalizar al\u00e9m dos dados de treinamento; um m\u00e9todo melhor para inicializar os pesos na rede; e um conjunto de heur\u00edsticas para ajudar a escolher bons hyperpar\u00e2metros para a rede. Tamb\u00e9m vamos analisar v\u00e1rias outras t\u00e9cnicas com menos profundidade. As discuss\u00f5es s\u00e3o em grande parte independentes umas das outras e, portanto, voc\u00ea pode avan\u00e7ar se quiser. Tamb\u00e9m implementaremos muitas das t\u00e9cnicas em nosso c\u00f3digo e usaremos para melhorar os resultados obtidos no problema de classifica\u00e7\u00e3o de d\u00edgitos manuscritos estudado nos\n  <a href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-para-reconhecimento-de-digitos/\" rel=\"noopener\" target=\"_blank\">\n   cap\u00edtulos anteriores\n  </a>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Naturalmente, estamos cobrindo apenas algumas das muitas t\u00e9cnicas que foram desenvolvidas para uso em redes neurais. A filosofia \u00e9 que o melhor acesso \u00e0 multiplicidade de t\u00e9cnicas dispon\u00edveis \u00e9 o estudo aprofundado de algumas das mais importantes. Dominar essas t\u00e9cnicas importantes n\u00e3o \u00e9 apenas \u00fatil por si s\u00f3, mas tamb\u00e9m ir\u00e1 aprofundar sua compreens\u00e3o sobre quais problemas podem surgir quando voc\u00ea usa redes neurais. Isso deixar\u00e1 voc\u00ea bem preparado para aprender rapidamente outras t\u00e9cnicas, conforme necess\u00e1rio.\n </p>\n <p>\n </p>\n <h3>\n  A Fun\u00e7\u00e3o de Custo\n </h3>\n <p style=\"text-align: justify;\">\n  A maioria de n\u00f3s acha desagrad\u00e1vel estar errado. Logo depois de come\u00e7ar a aprender piano, minha filha fez sua primeira apresenta\u00e7\u00e3o diante de uma plat\u00e9ia. Ela estava nervosa e come\u00e7ou a tocar a pe\u00e7a com uma oitava muito baixa. Ela ficou confusa e n\u00e3o p\u00f4de continuar at\u00e9 que algu\u00e9m apontasse o erro. Ela ficou muito envergonhada. Ainda que desagrad\u00e1vel, tamb\u00e9m aprendemos rapidamente quando estamos decididamente errados. Voc\u00ea pode apostar que a pr\u00f3xima vez que ela se apresentou diante de uma plat\u00e9ia, ela come\u00e7ou na oitava correta! Em contraste, aprendemos mais lentamente quando nossos erros s\u00e3o menos bem definidos.\n </p>\n <p style=\"text-align: justify;\">\n  Idealmente, esperamos que nossas redes neurais aprendam rapidamente com seus erros. Mas \u00e9 isso que acontece na pr\u00e1tica? Para responder a essa pergunta, vamos dar uma olhada em um exemplo simples. O exemplo envolve um neur\u00f4nio com apenas uma entrada:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"neuron\" class=\"aligncenter size-full wp-image-509\" data-attachment-id=\"509\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"neuron\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?fit=297%2C85\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?fit=297%2C85\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?fit=297%2C85\" data-orig-size=\"297,85\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/neuron/\" data-recalc-dims=\"1\" height=\"85\" sizes=\"(max-width: 297px) 100vw, 297px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?resize=297%2C85\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?w=297 297w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron.png?resize=200%2C57 200w\" width=\"297\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  N\u00f3s vamos treinar esse neur\u00f4nio para fazer algo ridiculamente f\u00e1cil: obter a entrada 1 e gerar a sa\u00edda 0. Claro, essa \u00e9 uma tarefa t\u00e3o trivial que poder\u00edamos facilmente descobrir um peso apropriado e um vi\u00e9s (bias) de forma manual, sem usar um algoritmo de aprendizado. No entanto, vai nos ajudar a compreender melhor o processo de usar gradiente descendente para tentar aprender um peso e vi\u00e9s. Ent\u00e3o, vamos dar uma olhada em como o neur\u00f4nio aprende.\n </p>\n <p style=\"text-align: justify;\">\n  Para tornar as coisas definitivas, escolhemos o peso inicial como 0.6 e o \u200b\u200bvi\u00e9s inicial como 0.9. Estas s\u00e3o escolhas gen\u00e9ricas usadas como um lugar para come\u00e7ar a aprender, eu n\u00e3o as escolhi para serem especiais de alguma forma. A sa\u00edda inicial do neur\u00f4nio \u00e9 0.82, ent\u00e3o um pouco de aprendizado ser\u00e1 necess\u00e1rio antes que nosso neur\u00f4nio se aproxime da sa\u00edda desejada 0,0.\n </p>\n <p style=\"text-align: justify;\">\n  No gr\u00e1fico abaixo, podemos ver como o neur\u00f4nio aprende uma sa\u00edda muito mais pr\u00f3xima de 0.0. Durante o treinamento, o modelo est\u00e1 realmente computando o gradiente, e usando o gradiente para atualizar o peso e o vi\u00e9s, e exibir o resultado. A taxa de aprendizado \u00e9 \u03b7 = 0.15, o que acaba sendo lento o suficiente para que possamos acompanhar o que est\u00e1 acontecendo, mas r\u00e1pido o suficiente para que possamos obter um aprendizado substancial em apenas alguns segundos. O custo \u00e9 a fun\u00e7\u00e3o de custo quadr\u00e1tico, C, apresentada nos cap\u00edtulos anteriores. Vou lembr\u00e1-lo da forma exata da fun\u00e7\u00e3o de custo em breve.\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"train\" class=\"aligncenter size-full wp-image-510\" data-attachment-id=\"510\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"train\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?fit=524%2C292\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?fit=300%2C167\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?fit=524%2C292\" data-orig-size=\"524,292\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/train/\" data-recalc-dims=\"1\" height=\"292\" sizes=\"(max-width: 524px) 100vw, 524px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?resize=524%2C292\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?w=524 524w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?resize=300%2C167 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train.png?resize=200%2C111 200w\" width=\"524\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Como voc\u00ea pode ver, o neur\u00f4nio aprende um peso e um vi\u00e9s que diminui o custo e d\u00e1 uma sa\u00edda do neur\u00f4nio de cerca de 0.09 (Epoch, ou \u00c9poca em portugu\u00eas, \u00e9 o n\u00famero de passadas que nosso modelo faz pelos dados. A cada passada, os pesos s\u00e3o atualizados, o aprendizado ocorre e o custo, ou a taxa de erros, diminui). Isso n\u00e3o \u00e9 exatamente o resultado desejado, 0.0, mas \u00e9 muito bom.\n </p>\n <p style=\"text-align: justify;\">\n  Suponha, no entanto, que, em vez disso, escolhamos o peso inicial e o vi\u00e9s inicial como 2.0. Nesse caso, a sa\u00edda inicial \u00e9 0.98, o que \u00e9 muito ruim. Vamos ver como o neur\u00f4nio aprende a gerar 0 neste caso:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"train2\" class=\"aligncenter size-full wp-image-511\" data-attachment-id=\"511\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"train2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?fit=517%2C288\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?fit=300%2C167\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?fit=517%2C288\" data-orig-size=\"517,288\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/train2/\" data-recalc-dims=\"1\" height=\"288\" sizes=\"(max-width: 517px) 100vw, 517px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?resize=517%2C288\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?w=517 517w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?resize=300%2C167 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train2.png?resize=200%2C111 200w\" width=\"517\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Embora este exemplo use a mesma taxa de aprendizado (\u03b7 = 0.15), podemos ver que a aprendizagem come\u00e7a muito mais devagar. De fato, nas primeiras 150 \u00e9pocas de aprendizado, os pesos e vieses n\u00e3o mudam muito. Ent\u00e3o o aprendizado entra em a\u00e7\u00e3o e, como em nosso primeiro exemplo, a sa\u00edda do neur\u00f4nio se aproxima rapidamente de 0.0.\n </p>\n <p style=\"text-align: justify;\">\n  Esse comportamento \u00e9 estranho quando comparado ao aprendizado humano. Como eu disse no come\u00e7o deste cap\u00edtulo, muitas vezes aprendemos mais r\u00e1pido quando estamos muito errados sobre algo. Mas acabamos de ver que nosso neur\u00f4nio artificial tem muita dificuldade em aprender quando est\u00e1 muito errado \u2013 muito mais dificuldade do que quando est\u00e1 apenas um pouco errado. Al\u00e9m do mais, verifica-se que esse comportamento ocorre n\u00e3o apenas neste exemplo, mas em redes mais gerais. Por que aprender t\u00e3o devagar? E podemos encontrar uma maneira de evitar essa desacelera\u00e7\u00e3o?\n </p>\n <p style=\"text-align: justify;\">\n  Para entender a origem do problema, considere que nosso neur\u00f4nio aprende mudando o peso e o vi\u00e9s a uma taxa determinada pelas derivadas parciais da fun\u00e7\u00e3o custo, \u2202C/\u2202w e \u2202C/\u2202b. Ent\u00e3o, dizer \u201caprender \u00e9 lento\u201d \u00e9 realmente o mesmo que dizer que essas derivadas parciais s\u00e3o pequenas. O desafio \u00e9 entender por que eles s\u00e3o pequenas. Para entender isso, vamos calcular as derivadas parciais. Lembre-se de que estamos usando a fun\u00e7\u00e3o de custo quadr\u00e1tico, que \u00e9 dada por:\n </p>\n <p>\n  <img alt=\"cost\" class=\"aligncenter size-full wp-image-512\" data-attachment-id=\"512\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"cost\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?fit=123%2C69\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?fit=123%2C69\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?fit=123%2C69\" data-orig-size=\"123,69\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/cost/\" data-recalc-dims=\"1\" height=\"69\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost.png?resize=123%2C69\" width=\"123\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  onde\n  <strong>\n   a\n  </strong>\n  \u00e9 a sa\u00edda do neur\u00f4nio quando a entrada de treinamento x = 1 \u00e9 usada, e y = 0 \u00e9 a sa\u00edda desejada correspondente. Para escrever isso mais explicitamente em termos de peso e vi\u00e9s, lembre-se que a = \u03c3(z), onde z = wx + b. Usando a regra da cadeia para diferenciar em rela\u00e7\u00e3o ao peso e vi\u00e9s, obtemos:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"cost2\" class=\"aligncenter size-full wp-image-513\" data-attachment-id=\"513\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"cost2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?fit=268%2C122\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?fit=268%2C122\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?fit=268%2C122\" data-orig-size=\"268,122\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/cost2/\" data-recalc-dims=\"1\" height=\"122\" sizes=\"(max-width: 268px) 100vw, 268px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?resize=268%2C122\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?w=268 268w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost2.png?resize=200%2C91 200w\" width=\"268\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  onde substitui x = 1 e y = 0. Para entender o comportamento dessas express\u00f5es, vamos olhar mais de perto o termo \u03c3 \u2032 (z) no lado direito. Lembre-se da forma da fun\u00e7\u00e3o \u03c3:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"sig\" class=\"aligncenter size-full wp-image-514\" data-attachment-id=\"514\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"sig\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?fit=436%2C300\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?fit=300%2C206\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?fit=436%2C300\" data-orig-size=\"436,300\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/sig/\" data-recalc-dims=\"1\" height=\"300\" sizes=\"(max-width: 436px) 100vw, 436px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?resize=436%2C300\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?w=436 436w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?resize=300%2C206 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/sig.png?resize=200%2C138 200w\" width=\"436\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Podemos ver neste gr\u00e1fico que quando a sa\u00edda do neur\u00f4nio \u00e9 pr\u00f3xima de 1, a curva fica muito plana, e ent\u00e3o \u03c3 \u2032 (z) fica muito pequeno. As equa\u00e7\u00f5es acima ent\u00e3o nos dizem que \u2202C/\u2202w e \u2202C/\u2202b ficam muito pequenos. Esta \u00e9 a origem da desacelera\u00e7\u00e3o da aprendizagem. Al\u00e9m do mais, como veremos mais adiante, a desacelera\u00e7\u00e3o do aprendizado ocorre basicamente pelo mesmo motivo em redes neurais mais gen\u00e9ricas, n\u00e3o apenas neste exemplo simples.\n </p>\n <p>\n </p>\n <h3>\n  A Fun\u00e7\u00e3o de Custo de Entropia Cruzada\n </h3>\n <p style=\"text-align: justify;\">\n  Como podemos abordar a desacelera\u00e7\u00e3o da aprendizagem? Acontece que podemos resolver o problema substituindo o custo quadr\u00e1tico por uma fun\u00e7\u00e3o de custo diferente, conhecida como entropia cruzada. Para entender a entropia cruzada, vamos nos afastar um pouco do nosso modelo super-simples. Vamos supor que estamos tentando treinar um neur\u00f4nio com diversas vari\u00e1veis de entrada, x1, x2,\u2026, pesos correspondentes w1, w2,\u2026 e um vi\u00e9s, b:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"neuron2\" class=\"aligncenter size-full wp-image-515\" data-attachment-id=\"515\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"neuron2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?fit=288%2C138\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?fit=288%2C138\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?fit=288%2C138\" data-orig-size=\"288,138\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/neuron2/\" data-recalc-dims=\"1\" height=\"138\" sizes=\"(max-width: 288px) 100vw, 288px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?resize=288%2C138\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?w=288 288w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/neuron2.png?resize=200%2C96 200w\" width=\"288\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A sa\u00edda do neur\u00f4nio \u00e9, naturalmente,\n  <strong>\n   a = \u03c3(z)\n  </strong>\n  , onde\n  <strong>\n   z = \u2211jwjxj + b\n  </strong>\n  \u00e9 a soma ponderada das entradas. N\u00f3s definimos a fun\u00e7\u00e3o de custo de entropia cruzada para este neur\u00f4nio assim:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"entropy\" class=\"aligncenter size-full wp-image-516\" data-attachment-id=\"516\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"entropy\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?fit=327%2C74\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?fit=300%2C68\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?fit=327%2C74\" data-orig-size=\"327,74\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/entropy/\" data-recalc-dims=\"1\" height=\"74\" sizes=\"(max-width: 327px) 100vw, 327px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?resize=327%2C74\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?w=327 327w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?resize=300%2C68 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/entropy.png?resize=200%2C45 200w\" width=\"327\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  onde\n  <strong>\n   n\n  </strong>\n  \u00e9 o n\u00famero total de itens de dados de treinamento, a soma \u00e9 sobre todas as entradas de treinamento\n  <strong>\n   x,\n  </strong>\n  e\n  <strong>\n   y\n  </strong>\n  \u00e9 a sa\u00edda desejada correspondente. N\u00e3o \u00e9 \u00f3bvio que a express\u00e3o anterior resolva o problema de desacelera\u00e7\u00e3o do aprendizado. De fato, francamente, nem \u00e9 \u00f3bvio que faz sentido chamar isso de uma fun\u00e7\u00e3o de custo! Antes de abordar a desacelera\u00e7\u00e3o da aprendizagem, vamos ver em que sentido a entropia cruzada pode ser interpretada como uma fun\u00e7\u00e3o de custo.\n </p>\n <p style=\"text-align: justify;\">\n  Duas propriedades em particular tornam razo\u00e1vel interpretar a entropia cruzada como uma fun\u00e7\u00e3o de custo. Primeiro, n\u00e3o \u00e9 negativo, isto \u00e9, C &gt; 0. Para visualizar isso, observe na f\u00f3rmula anterior que: (a) todos os termos individuais na soma s\u00e3o negativos, j\u00e1 que ambos os logaritmos s\u00e3o de n\u00fameros no intervalo de 0 a 1; e (b) h\u00e1 um sinal de menos na frente da soma.\n </p>\n <p style=\"text-align: justify;\">\n  Segundo, se a sa\u00edda real do neur\u00f4nio estiver pr\u00f3xima da sa\u00edda desejada para todas as entradas de treinamento x, ent\u00e3o a entropia cruzada ser\u00e1 pr\u00f3xima de zero. Para ver isso, suponha, por exemplo, que y = 0 e a \u2248 0 para alguma entrada x. Este \u00e9 um caso quando o neur\u00f4nio est\u00e1 fazendo um bom trabalho nessa entrada. Vemos que o primeiro termo (na f\u00f3rmula acima) para o custo, desaparece, desde que y = 0, enquanto o segundo termo \u00e9 apenas \u2212ln (1 \u2212 a) \u2248 0. Uma an\u00e1lise semelhante \u00e9 v\u00e1lida quando y = 1 e a \u2248 1. E assim, a contribui\u00e7\u00e3o para o custo ser\u00e1 baixa, desde que a sa\u00edda real esteja pr\u00f3xima da sa\u00edda desejada.\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   Em suma, a entropia cruzada \u00e9 positiva e tende a zero, \u00e0 medida que o neur\u00f4nio melhora a computa\u00e7\u00e3o da sa\u00edda desejada, y, para todas as entradas de treinamento, x\n  </strong>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  Essas s\u00e3o as duas propriedades que esperamos intuitivamente para uma fun\u00e7\u00e3o de custo. De fato, ambas as propriedades tamb\u00e9m s\u00e3o satisfeitas pelo custo quadr\u00e1tico. Portanto, isso \u00e9 uma boa not\u00edcia para a entropia cruzada. Mas a fun\u00e7\u00e3o custo de entropia cruzada tem o benef\u00edcio de que, ao contr\u00e1rio do custo quadr\u00e1tico, evita o problema de desacelera\u00e7\u00e3o do aprendizado. Para ver isso, vamos calcular a derivada parcial do custo de entropia cruzada em rela\u00e7\u00e3o aos pesos. Substitu\u00edmos a = \u03c3 (z) na f\u00f3rmula acima e aplicamos a regra da cadeia duas vezes, obtendo:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"form1\" class=\"aligncenter size-full wp-image-518\" data-attachment-id=\"518\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?fit=406%2C134\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?fit=300%2C99\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?fit=406%2C134\" data-orig-size=\"406,134\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/form1/\" data-recalc-dims=\"1\" height=\"134\" sizes=\"(max-width: 406px) 100vw, 406px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?resize=406%2C134\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?w=406 406w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?resize=300%2C99 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form1.png?resize=200%2C66 200w\" width=\"406\"/>\n </p>\n <p>\n </p>\n <p>\n  Colocando tudo em um denominador comum e simplificando, isso se torna:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"form2\" class=\"aligncenter size-full wp-image-519\" data-attachment-id=\"519\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?fit=328%2C91\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?fit=300%2C83\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?fit=328%2C91\" data-orig-size=\"328,91\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/form2-4/\" data-recalc-dims=\"1\" height=\"91\" sizes=\"(max-width: 328px) 100vw, 328px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?resize=328%2C91\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?w=328 328w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?resize=300%2C83 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form2.png?resize=200%2C55 200w\" width=\"328\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Usando a defini\u00e7\u00e3o da fun\u00e7\u00e3o sigm\u00f3ide, \u03c3 (z) = 1 / (1 + ez), e um pouco de \u00e1lgebra, podemos mostrar que \u03c3 (z) = \u03c3 (z) (1 \u2212 \u03c3 (z)). Vemos que os termos \u03c3\u2032 (z) e \u03c3 (z) (1 \u2212 \u03c3 (z)) se cancelam na equa\u00e7\u00e3o acima, e simplificando torna-se:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"form3\" class=\"aligncenter size-full wp-image-520\" data-attachment-id=\"520\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?fit=217%2C79\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?fit=217%2C79\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?fit=217%2C79\" data-orig-size=\"217,79\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/form3-3/\" data-recalc-dims=\"1\" height=\"79\" sizes=\"(max-width: 217px) 100vw, 217px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?resize=217%2C79\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?w=217 217w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form3.png?resize=200%2C73 200w\" width=\"217\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Esta \u00e9 uma bela express\u00e3o. Ela nos diz que a taxa na qual o peso aprende \u00e9 controlada por \u03c3 (z) \u2212y, ou seja, pelo erro na sa\u00edda. Quanto maior o erro, mais r\u00e1pido o neur\u00f4nio aprender\u00e1. Isso \u00e9 exatamente o que n\u00f3s esperamos intuitivamente. Em particular, evita a lentid\u00e3o de aprendizado causada pelo termo \u03c3\u2032 (z) na equa\u00e7\u00e3o an\u00e1loga para o custo quadr\u00e1tico. Quando usamos a entropia cruzada, o termo \u03c3\u2032 (z) \u00e9 cancelado e n\u00e3o precisamos mais nos preocupar em ser pequeno. Este cancelamento \u00e9 o milagre especial assegurado pela fun\u00e7\u00e3o de custo de entropia cruzada. Na verdade, n\u00e3o \u00e9 realmente um milagre. Como veremos mais adiante, a entropia cruzada foi especialmente escolhida por ter apenas essa propriedade.\n </p>\n <p style=\"text-align: justify;\">\n  De maneira semelhante, podemos calcular a derivada parcial para o vi\u00e9s. Eu n\u00e3o vou passar por todos os detalhes novamente, mas voc\u00ea pode facilmente verificar que:\n </p>\n <p>\n  <img alt=\"form4\" class=\"aligncenter size-full wp-image-521\" data-attachment-id=\"521\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?fit=197%2C75\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?fit=197%2C75\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?fit=197%2C75\" data-orig-size=\"197,75\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/form4-2/\" data-recalc-dims=\"1\" height=\"75\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/form4.png?resize=197%2C75\" width=\"197\"/>\n </p>\n <p>\n </p>\n <p>\n  Novamente, isso evita a lentid\u00e3o de aprendizado causada pelo termo \u03c3\u2032 (z) na equa\u00e7\u00e3o an\u00e1loga para o custo quadr\u00e1tico.\n </p>\n <p>\n  Agora vamos retornar ao exemplo do in\u00edcio deste cap\u00edtulo, e explorar o que acontece quando usamos a entropia cruzada em vez do custo quadr\u00e1tico. Para nos reorientarmos, come\u00e7aremos com o caso em que o custo quadr\u00e1tico foi bom, com peso inicial de 0.6 e vi\u00e9s inicial de 0.9. Veja o que acontece quando substitu\u00edmos o custo quadr\u00e1tico pela entropia cruzada:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"train3\" class=\"aligncenter size-full wp-image-522\" data-attachment-id=\"522\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"train3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?fit=518%2C286\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?fit=300%2C166\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?fit=518%2C286\" data-orig-size=\"518,286\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/train3/\" data-recalc-dims=\"1\" height=\"286\" sizes=\"(max-width: 518px) 100vw, 518px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?resize=518%2C286\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?w=518 518w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?resize=300%2C166 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train3.png?resize=200%2C110 200w\" width=\"518\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Como era de se esperar, o neur\u00f4nio aprende perfeitamente bem neste caso, assim como fez anteriormente. E agora vamos olhar para o caso em que nosso neur\u00f4nio ficou preso antes, com o peso e o vi\u00e9s ambos come\u00e7ando em 2.0:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"train4\" class=\"aligncenter size-full wp-image-523\" data-attachment-id=\"523\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"train4\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?fit=533%2C284\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?fit=300%2C160\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?fit=533%2C284\" data-orig-size=\"533,284\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/train4/\" data-recalc-dims=\"1\" height=\"284\" sizes=\"(max-width: 533px) 100vw, 533px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?resize=533%2C284\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?w=533 533w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?resize=300%2C160 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/train4.png?resize=200%2C107 200w\" width=\"533\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   Sucesso!\n  </strong>\n  Desta vez, o neur\u00f4nio aprendeu rapidamente, exatamente como esper\u00e1vamos. Se voc\u00ea observar atentamente, pode ver que a inclina\u00e7\u00e3o da curva de custo era muito mais \u00edngreme inicialmente do que a regi\u00e3o plana inicial na curva correspondente para o custo quadr\u00e1tico. \u00c9 essa inclina\u00e7\u00e3o que a entropia cruzada nos ajuda a resolver, impedindo-nos de ficar presos exatamente quando esperamos que nosso neur\u00f4nio aprenda mais depressa, ou seja, quando o neur\u00f4nio come\u00e7a errado.\n </p>\n <p style=\"text-align: justify;\">\n  Eu n\u00e3o disse qual taxa de aprendizado foi usada nos exemplos que acabei de ilustrar. Anteriormente, com o custo quadr\u00e1tico, usamos \u03b7 = 0.15. Dever\u00edamos ter usado a mesma taxa de aprendizado nos novos exemplos? De fato, com a mudan\u00e7a na fun\u00e7\u00e3o de custo, n\u00e3o \u00e9 poss\u00edvel dizer precisamente o que significa usar a \u201cmesma\u201d taxa de aprendizado; \u00e9 uma compara\u00e7\u00e3o de ma\u00e7\u00e3s e laranjas. Para ambas as fun\u00e7\u00f5es de custo, simplesmente experimentei encontrar uma taxa de aprendizado que possibilitasse ver o que est\u00e1 acontecendo. Se voc\u00ea ainda estiver curioso, aqui est\u00e1 o resumo: usei \u03b7 = 0.005 nos exemplos que acabei de fornecer.\n </p>\n <p style=\"text-align: justify;\">\n  Voc\u00ea pode contestar que a mudan\u00e7a na taxa de aprendizado torna os gr\u00e1ficos acima sem sentido. Quem se importa com a rapidez com que o neur\u00f4nio aprende, quando a nossa escolha de taxa de aprendizado foi arbitr\u00e1ria, para come\u00e7ar ?! Mas essa obje\u00e7\u00e3o n\u00e3o procede. O ponto dos gr\u00e1ficos n\u00e3o \u00e9 sobre a velocidade absoluta de aprendizagem. \u00c9 sobre como a velocidade do aprendizado muda. Em particular, quando usamos o custo quadr\u00e1tico, a aprendizagem \u00e9 mais lenta quando o neur\u00f4nio est\u00e1 inequivocamente errado do que \u00e9 mais tarde durante o treinamento, \u00e0 medida que o neur\u00f4nio se aproxima da sa\u00edda correta; enquanto o aprendizado de entropia cruzada \u00e9 mais r\u00e1pido quando o neur\u00f4nio est\u00e1 inequivocamente errado. Essas declara\u00e7\u00f5es n\u00e3o dependem de como a taxa de aprendizado \u00e9 definida.\n </p>\n <p style=\"text-align: justify;\">\n  Estamos estudando a entropia cruzada para um \u00fanico neur\u00f4nio. No entanto, \u00e9 f\u00e1cil generalizar a entropia cruzada para redes multicamadas de muitos neur\u00f4nios. Em particular, suponha que y = y1, y2,\u2026 s\u00e3o os valores desejados nos neur\u00f4nios de sa\u00edda, ou seja, os neur\u00f4nios na camada final, enquanto aL1, aL2,\u2026 s\u00e3o os valores reais de sa\u00edda. Ent\u00e3o n\u00f3s definimos a entropia cruzada por:\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"cost\" class=\"aligncenter size-full wp-image-524\" data-attachment-id=\"524\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"cost\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?fit=399%2C68\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?fit=300%2C51\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?fit=399%2C68\" data-orig-size=\"399,68\" data-permalink=\"http://deeplearningbook.com.br/cross-entropy-cost-function/cost-2/\" data-recalc-dims=\"1\" height=\"68\" sizes=\"(max-width: 399px) 100vw, 399px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?resize=399%2C68\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?w=399 399w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?resize=300%2C51 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/05/cost-1.png?resize=200%2C34 200w\" width=\"399\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Isso \u00e9 o mesmo que nossa express\u00e3o anterior, exceto que agora n\u00f3s temos o \u2211j somando todos os neur\u00f4nios de sa\u00edda. N\u00e3o vou explicitamente trabalhar com uma deriva\u00e7\u00e3o, mas deve ser plaus\u00edvel que o uso da express\u00e3o anterior evite uma desacelera\u00e7\u00e3o na aprendizagem em muitas redes de neur\u00f4nios.\n </p>\n <p style=\"text-align: justify;\">\n  A prop\u00f3sito, estou usando o termo \u201centropia cruzada\u201d de uma maneira que confundiu alguns dos primeiros leitores, j\u00e1 que parece superficialmente entrar em conflito com outras fontes. Em particular, \u00e9 comum definir a entropia cruzada para duas distribui\u00e7\u00f5es de probabilidade, pj e qj, como \u2211jpjlnqj. Esta defini\u00e7\u00e3o pode ser conectada a f\u00f3rmula da entropia para um neur\u00f4nio mostrada anteriormente, se tratarmos um \u00fanico neur\u00f4nio sigm\u00f3ide como sa\u00edda de uma distribui\u00e7\u00e3o de probabilidade que consiste na ativa\u00e7\u00e3o a do neur\u00f4nio ae seu complemento 1 \u2212 a.\n </p>\n <p style=\"text-align: justify;\">\n  No entanto, quando temos muitos neur\u00f4nios sigmoides na camada final, o vetor aLj de ativa\u00e7\u00f5es n\u00e3o costuma formar uma distribui\u00e7\u00e3o de probabilidade. Como resultado, uma defini\u00e7\u00e3o como \u2211jpjlnqj n\u00e3o faz sentido, j\u00e1 que n\u00e3o estamos trabalhando com distribui\u00e7\u00f5es de probabilidade. Em vez disso, voc\u00ea pode pensar na f\u00f3rmula da entropia para m\u00faltiplos neur\u00f4nios como um conjunto somado de entropias cruzadas por neur\u00f4nio, com a ativa\u00e7\u00e3o de cada neur\u00f4nio sendo interpretada como parte de uma distribui\u00e7\u00e3o de probabilidade de dois elementos. Sim, eu sei que isso n\u00e3o \u00e9 simples.\n </p>\n <p style=\"text-align: justify;\">\n  Nesse sentido, a f\u00f3rmula da entropia para m\u00faltiplos neur\u00f4nios \u00e9 uma generaliza\u00e7\u00e3o da entropia cruzada para distribui\u00e7\u00f5es de probabilidade.\n </p>\n <p style=\"text-align: justify;\">\n  Quando devemos usar a entropia cruzada em vez do custo quadr\u00e1tico? De fato, a entropia cruzada \u00e9 quase sempre a melhor escolha, desde que os neur\u00f4nios de sa\u00edda sejam neur\u00f4nios sigm\u00f3ides. Para entender por que, considere que, quando estamos configurando a rede, normalmente inicializamos os pesos e vieses usando algum tipo de aleatoriedade. Pode acontecer que essas escolhas iniciais resultem na rede sendo decisivamente errada para alguma entrada de treinamento \u2013 isto \u00e9, um neur\u00f4nio de sa\u00edda ter\u00e1 saturado pr\u00f3ximo de 1, quando deveria ser 0, ou vice-versa. Se estamos usando o custo quadr\u00e1tico que ir\u00e1 desacelerar a aprendizagem, ele n\u00e3o vai parar de aprender completamente, j\u00e1 que os pesos continuar\u00e3o aprendendo com outras entradas de treinamento, mas \u00e9 obviamente indesej\u00e1vel.\n </p>\n <p style=\"text-align: justify;\">\n  Construir aplica\u00e7\u00f5es de IA \u00e9 uma habilidade com demanda cada vez maior no mercado.\n </p>\n <p style=\"text-align: justify;\">\n  Pensando nisso, a Data Science Academy oferece um programa completo, onde esses e v\u00e1rios outros conceitos s\u00e3o estudados em detalhes e com v\u00e1rias aplica\u00e7\u00f5es pr\u00e1ticas, usando TensorFlow. A\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n  \u00e9 composta de 9 cursos, tudo 100% online e 100% em portugu\u00eas, que aliam teoria e pr\u00e1tica na medida certa, com aplica\u00e7\u00f5es reais de Intelig\u00eancia Artificial. Confira o programa completo dos cursos:\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n  .\u00a0V\u00e1rias empresas em todo Brasil j\u00e1 est\u00e3o treinando seus profissionais conosco! Venha fazer parte da revolu\u00e7\u00e3o da IA.\n </p>\n <p style=\"text-align: justify;\">\n  At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Dot_product\" rel=\"noopener\" target=\"_blank\">\n    Dot Product\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Derivada\" rel=\"noopener\" target=\"_blank\">\n    Derivada\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-506\" href=\"http://deeplearningbook.com.br/cross-entropy-cost-function/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-506\" href=\"http://deeplearningbook.com.br/cross-entropy-cost-function/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-506\" href=\"http://deeplearningbook.com.br/cross-entropy-cost-function/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-506\" href=\"http://deeplearningbook.com.br/cross-entropy-cost-function/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/cross-entropy-cost-function/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/cross-entropy-cost-function/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-506-5e0dd114eaaf9\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=506&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-506-5e0dd114eaaf9\" id=\"like-post-wrapper-140353593-506-5e0dd114eaaf9\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "18": "<h1 class=\"entry-title\" id=\"capitulo-18\">\n Cap\u00edtulo 18 \u2013 Entropia Cruzada Para Quantificar a Diferen\u00e7a Entre Duas Distribui\u00e7\u00f5es de Probabilidade\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  A Cross-Entropy (ou entropia cruzada, se voc\u00ea preferir o termo em portugu\u00eas) \u00e9 f\u00e1cil de implementar como parte de um programa que aprende usando gradiente descendente e backpropagation. Faremos isso nos pr\u00f3ximos cap\u00edtulos quando treinarmos uma rede completa, desenvolvendo uma vers\u00e3o melhorada do nosso programa anterior para classificar os d\u00edgitos manuscritos do dataset MNIST. O novo programa \u00e9 chamado de network2.py e incorpora n\u00e3o apenas a entropia cruzada, mas tamb\u00e9m v\u00e1rias outras t\u00e9cnicas que estudaremos mais adiante. Agora, vejamos como usar a\u00a0Entropia Cruzada Para Quantificar a Diferen\u00e7a Entre Duas Distribui\u00e7\u00f5es de Probabilidade.\n </p>\n <p style=\"text-align: justify;\">\n  Por enquanto, vamos ver como nosso novo programa classifica os d\u00edgitos MNIST. Usaremos uma rede com 30 neur\u00f4nios ocultos, e usaremos um tamanho de mini-lote de 10. Definimos a taxa de aprendizado para \u03b7 = 0,5\u00a0e n\u00f3s treinamos por 30 \u00e9pocas. A interface para o network2.py ser\u00e1 um pouco diferente do network.py, mas ainda deve estar claro o que est\u00e1 acontecendo. Nos pr\u00f3ximos cap\u00edtulos apresentamos o c\u00f3digo completo no reposit\u00f3rio do livro no\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener\" target=\"_blank\">\n    Github\n   </a>\n  </span>\n  .\n </p>\n <p>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n   <img alt=\"corss-entropy\" class=\"aligncenter wp-image-560 size-full\" data-attachment-id=\"560\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"corss-entropy\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?fit=553%2C154\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?fit=300%2C84\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?fit=553%2C154\" data-orig-size=\"553,154\" data-permalink=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/corss-entropy/\" data-recalc-dims=\"1\" height=\"154\" sizes=\"(max-width: 553px) 100vw, 553px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?resize=553%2C154\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?w=553 553w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?resize=300%2C84 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/corss-entropy.png?resize=200%2C56 200w\" title=\"Entropia Curzada\" width=\"553\"/>\n  </a>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Perceba que o comando net.large_weight_initializer() \u00e9 usado para inicializar os pesos e vieses da mesma maneira que j\u00e1 descrevemos anteriormente. Precisamos executar este comando porque mais adiante vamos alterar o peso padr\u00e3o para inicializa\u00e7\u00e3o em nossas redes. O resultado da execu\u00e7\u00e3o da sequ\u00eancia de comandos acima \u00e9 uma rede com 95,49% de precis\u00e3o.\n </p>\n <p style=\"text-align: justify;\">\n  Vejamos tamb\u00e9m o caso em que usamos 100 neur\u00f4nios ocultos, a entropia cruzada, e mantemos os par\u00e2metros da mesma forma. Neste caso, obtemos uma precis\u00e3o de 96,82%. Essa \u00e9 uma melhoria substancial em rela\u00e7\u00e3o aos resultados que obtivemos nos\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/\" rel=\"noopener\" target=\"_blank\">\n    cap\u00edtulos anteriores\n   </a>\n  </span>\n  , onde a precis\u00e3o de classifica\u00e7\u00e3o foi de 96,59%, usando o custo quadr\u00e1tico. Isso pode parecer uma pequena mudan\u00e7a, mas considere que a taxa de erro caiu de 3,41% para 3,18%. Ou seja, eliminamos cerca de um em quatorze dos erros originais. Isso \u00e9 uma melhoria bastante \u00fatil.\n </p>\n <p style=\"text-align: justify;\">\n  \u00c9 encorajador que o custo de entropia cruzada nos d\u00ea resultados semelhantes ou melhores do que o custo quadr\u00e1tico. No entanto, esses resultados n\u00e3o provam conclusivamente que a entropia cruzada \u00e9 uma escolha melhor. A raz\u00e3o \u00e9 que n\u00f3s colocamos apenas um pequeno esfor\u00e7o na escolha de hyperpar\u00e2metros como taxa de aprendizado, tamanho de mini-lote e assim por diante. Para que a melhoria seja realmente convincente, precisar\u00edamos fazer um trabalho completo de otimiza\u00e7\u00e3o desses hyperpar\u00e2metros. Ainda assim, os resultados s\u00e3o encorajadores e refor\u00e7am nosso argumento te\u00f3rico anterior de que a entropia cruzada \u00e9 uma escolha melhor do que o custo quadr\u00e1tico.\n </p>\n <p style=\"text-align: justify;\">\n  Isso, a prop\u00f3sito, \u00e9 parte de um padr\u00e3o geral que veremos nos pr\u00f3ximos cap\u00edtulos e, na verdade, em grande parte do restante do livro. Vamos desenvolver uma nova t\u00e9cnica, vamos experiment\u00e1-la e obteremos resultados \u201caprimorados\u201d. \u00c9 claro que \u00e9 bom vermos essas melhorias, mas a interpreta\u00e7\u00e3o de tais melhorias \u00e9 sempre problem\u00e1tica. Elas s\u00f3 s\u00e3o verdadeiramente convincentes se virmos uma melhoria depois de nos esfor\u00e7armos para otimizar todos os outros hyperpar\u00e2metros. Isso \u00e9 uma grande quantidade de trabalho, exigindo muito poder de computa\u00e7\u00e3o, e normalmente n\u00e3o vamos fazer uma investiga\u00e7\u00e3o t\u00e3o exaustiva. Em vez disso, procederemos com base em testes informais como os realizados at\u00e9 aqui.\n </p>\n <p style=\"text-align: justify;\">\n  At\u00e9 agora, discutimos a entropia cruzada de forma bem detalhada. Por que tanto esfor\u00e7o quando a entropia cruzada nos d\u00e1 apenas uma pequena melhora em nossos resultados com o dataset MNIST? Mais adiante veremos outras t\u00e9cnicas, notadamente a regulariza\u00e7\u00e3o, que trazem melhorias muito maiores. Ent\u00e3o, por que tanto foco na entropia cruzada? Parte da raz\u00e3o \u00e9 que a entropia cruzada \u00e9 uma fun\u00e7\u00e3o de custo amplamente utilizada e, portanto, vale a pena compreend\u00ea-la bem. Mas a raz\u00e3o mais importante \u00e9 que a satura\u00e7\u00e3o dos neur\u00f4nios \u00e9 um problema importante nas redes neurais, um problema ao qual voltaremos repetidamente ao longo do livro. Por isso discutimos a entropia cruzada em extens\u00e3o pois \u00e9 um bom laborat\u00f3rio para come\u00e7ar a entender a satura\u00e7\u00e3o dos neur\u00f4nios e como ela pode ser abordada.\n </p>\n <p>\n </p>\n <h3 style=\"text-align: justify;\">\n  O que significa a entropia cruzada? De onde isso vem?\n </h3>\n <p style=\"text-align: justify;\">\n  Nossa discuss\u00e3o sobre a entropia cruzada se concentrou na an\u00e1lise alg\u00e9brica e na implementa\u00e7\u00e3o pr\u00e1tica. Isso \u00e9 \u00fatil, mas deixa quest\u00f5es conceituais mais amplas n\u00e3o respondidas, como: o que significa a entropia cruzada? Existe alguma maneira intuitiva de pensar sobre a entropia cruzada? E quanto ao significado intuitivo da entropia cruzada? Como devemos pensar sobre isso?\n </p>\n <p style=\"text-align: justify;\">\n  Explicar isso em profundidade nos levaria mais longe do que queremos ir neste livro. No entanto, vale ressaltar que existe uma maneira padr\u00e3o de interpretar a entropia cruzada que vem do campo da teoria da informa\u00e7\u00e3o. Vejamos.\n </p>\n <p style=\"text-align: justify;\">\n  J\u00e1 sabemos que para treinar uma rede neural, voc\u00ea precisa encontrar o erro entre as sa\u00eddas calculadas e as sa\u00eddas alvo desejadas. A medida de erro mais comum \u00e9 chamada de erro quadr\u00e1tico m\u00e9dio (ou Mean Square Error). No entanto, existem alguns resultados de pesquisa que sugerem o uso de uma medida diferente, denominada erro de entropia cruzada, como m\u00e9todo prefer\u00edvel em rela\u00e7\u00e3o ao erro quadr\u00e1tico m\u00e9dio.\n </p>\n <p style=\"text-align: justify;\">\n  A medida de entropia cruzada tem sido utilizada como alternativa ao erro quadr\u00e1tico m\u00e9dio. A entropia cruzada pode ser usada como uma medida de erro quando as sa\u00eddas de uma rede podem ser pensadas como representando hip\u00f3teses independentes (por exemplo, cada n\u00f3 significa um conceito diferente) e as ativa\u00e7\u00f5es dos n\u00f3s podem ser entendidas como representando a probabilidade (ou a confian\u00e7a) que cada uma das hip\u00f3teses pode ser verdadeira. Nesse caso, o vetor de sa\u00edda representa uma distribui\u00e7\u00e3o de probabilidade, e nossa medida de erro \u2013 entropia cruzada \u2013 indica a dist\u00e2ncia entre o que a rede acredita que essa distribui\u00e7\u00e3o deve ser e o que realmente deveria ser. Existe tamb\u00e9m uma raz\u00e3o pr\u00e1tica para usar a entropia cruzada. Pode ser mais \u00fatil em problemas nos quais os alvos s\u00e3o 0 e 1. A entropia cruzada tende a permitir que erros alterem pesos mesmo quando houver n\u00f3s saturados (o que significa que suas derivadas s\u00e3o pr\u00f3ximas de 0). Vamos compreender melhor isso:\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   A entropia cruzada \u00e9 comumente usada para quantificar a diferen\u00e7a entre duas distribui\u00e7\u00f5es de probabilidade.\n  </strong>\n  Geralmente, a distribui\u00e7\u00e3o \u201cverdadeira\u201d (dos dados usados para treinamento) \u00e9 expressa em termos de uma distribui\u00e7\u00e3o One-Hot.\n </p>\n <p style=\"text-align: justify;\">\n  Por exemplo, suponha que para uma inst\u00e2ncia de treinamento espec\u00edfica (uma \u00fanica linha no seu dataset), a classe seja B (de 3 poss\u00edveis possibilidades: A, B e C). A distribui\u00e7\u00e3o \u00fanica para esta inst\u00e2ncia de treinamento \u00e9, portanto:\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   Pr(Class A)\u00a0 Pr(Class B)\u00a0 Pr(Class C)\n  </strong>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   0.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.0\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Voc\u00ea pode interpretar a distribui\u00e7\u00e3o acima da seguinte forma: a inst\u00e2ncia de treinamento tem 0% de probabilidade de ser classe A, 100% de probabilidade de ser classe B e 0% de probabilidade de ser a classe C.\n </p>\n <p style=\"text-align: justify;\">\n  Agora, suponha que seu algoritmo de aprendizado de m\u00e1quina tenha previsto a seguinte distribui\u00e7\u00e3o de probabilidade:\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   Pr(Class A)\u00a0 Pr(Class B)\u00a0 Pr(Class C)\n  </strong>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   0.228\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0.619\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00.153\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Qu\u00e3o pr\u00f3xima \u00e9 a distribui\u00e7\u00e3o prevista da distribui\u00e7\u00e3o verdadeira?\n  <span style=\"text-decoration: underline;\">\n   <strong>\n    \u00c9 isso que determina o erro de entropia cruzada\n   </strong>\n  </span>\n  . A entropia cruzada \u00e9 representada por esta f\u00f3rmula:\n </p>\n <p>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n   <img alt=\"cross-entropy\" class=\"aligncenter wp-image-561 size-full\" data-attachment-id=\"561\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"cross-entropy\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?fit=625%2C116\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?fit=300%2C56\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?fit=625%2C116\" data-orig-size=\"625,116\" data-permalink=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/cross-entropy/\" data-recalc-dims=\"1\" height=\"116\" sizes=\"(max-width: 625px) 100vw, 625px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?resize=625%2C116\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?w=625 625w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?resize=300%2C56 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/cross-entropy.png?resize=200%2C37 200w\" title=\"Entropia Cruzada\" width=\"625\"/>\n  </a>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A soma \u00e9 sobre as tr\u00eas classes A, B e C. Se voc\u00ea completar o c\u00e1lculo, voc\u00ea achar\u00e1 que a perda \u00e9 0.479. Ent\u00e3o, \u00e9 assim que \u201clonge\u201d est\u00e1 a sua previs\u00e3o da distribui\u00e7\u00e3o verdadeira.\n </p>\n <p style=\"text-align: justify;\">\n  A entropia cruzada \u00e9 uma das muitas fun\u00e7\u00f5es de perda poss\u00edveis. Essas fun\u00e7\u00f5es de perda s\u00e3o tipicamente escritas como J(theta) e podem ser usadas dentro da descida do gradiente, que \u00e9 uma estrutura iterativa para mover os par\u00e2metros (ou coeficientes) para os valores \u00f3timos.\n  <strong>\n   A entropia cruzada descreve a perda entre duas distribui\u00e7\u00f5es de probabilidade.\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  Ao usar uma rede neural para realizar classifica\u00e7\u00e3o e predi\u00e7\u00e3o, geralmente \u00e9 melhor usar o erro de entropia cruzada do que o erro de classifica\u00e7\u00e3o e um pouco melhor usar o erro de entropia cruzada do que o erro quadr\u00e1tico m\u00e9dio para avaliar a qualidade da rede neural. \u00c9 importante deixar claro que estamos lidando apenas com uma rede neural que \u00e9 usada para classificar os dados, como a previs\u00e3o da concess\u00e3o de cr\u00e9dito (sim ou n\u00e3o), ou ainda outras classifica\u00e7\u00f5es como idade, sexo ou d\u00edgitos no dataset MNIST e assim por diante. N\u00e3o estamos lidando com uma rede neural que faz regress\u00e3o, onde o valor a ser previsto \u00e9 num\u00e9rico.\n </p>\n <p style=\"text-align: justify;\">\n  At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Dot_product\" rel=\"noopener\" target=\"_blank\">\n    Dot Product\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://pt.wikipedia.org/wiki/Derivada\" rel=\"noopener\" target=\"_blank\">\n    Derivada\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-558\" href=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-558\" href=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-558\" href=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-558\" href=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/entropia-cruzada-para-quantificar-a-diferenca-entre-duas-distribuicoes-de-probabilidade/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-558-5e0dd116e5454\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=558&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-558-5e0dd116e5454\" id=\"like-post-wrapper-140353593-558-5e0dd116e5454\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "19": "<h1 class=\"entry-title\" id=\"capitulo-19\">\n Cap\u00edtulo 19 \u2013 Overfitting e Regulariza\u00e7\u00e3o \u2013 Parte 1\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  O f\u00edsico\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.nobelprize.org/nobel_prizes/physics/laureates/1938/fermi-bio.html\" rel=\"noopener\" target=\"_blank\">\n    Enrico Fermi\n   </a>\n  </span>\n  , ganhador do Pr\u00eamio Nobel de F\u00edsica em 1938, foi questionado sobre sua opini\u00e3o em rela\u00e7\u00e3o a um modelo matem\u00e1tico que alguns colegas haviam proposto como a solu\u00e7\u00e3o para um importante problema de f\u00edsica n\u00e3o resolvido. O modelo teve excelente performance no experimento, mas Fermi estava c\u00e9tico. Ele perguntou quantos par\u00e2metros livres poderiam ser definidos no modelo. \u201cQuatro\u201d foi a resposta. Fermi respondeu: \u201cEu lembro que meu amigo Johnny Von Neumann costumava dizer: com quatro par\u00e2metros eu posso encaixar um elefante, e com cinco eu posso faz\u00ea-lo mexer seu tronco\u201d *.\u00a0Com isso, ele quis dizer que n\u00e3o se deve ficar impressionado quando um modelo complexo se ajusta bem a um conjunto de dados. Com par\u00e2metros suficientes, voc\u00ea pode ajustar qualquer conjunto de dados.\n </p>\n <p style=\"text-align: justify;\">\n  (* A cita\u00e7\u00e3o vem de um artigo de Freeman Dyson, que \u00e9 uma das pessoas que prop\u00f4s o modelo. O artigo \u201cUm elefante de quatro par\u00e2metros\u201d ou \u201cA four-parameter elephant\u201d pode ser encontrado\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/\" rel=\"noopener\" target=\"_blank\">\n    aqui\n   </a>\n  </span>\n  .)\n </p>\n <p style=\"text-align: justify;\">\n  O ponto, claro, \u00e9 que modelos com um grande n\u00famero de par\u00e2metros podem descrever uma variedade incrivelmente ampla de fen\u00f4menos. Mesmo que tal modelo esteja de acordo com os dados dispon\u00edveis, isso n\u00e3o o torna um bom modelo. Isso pode significar apenas que h\u00e1 liberdade suficiente no modelo que pode descrever quase qualquer conjunto de dados de tamanho determinado, sem capturar nenhuma percep\u00e7\u00e3o genu\u00edna do fen\u00f4meno em quest\u00e3o. Quando isso acontece, o modelo funcionar\u00e1 bem para os dados existentes, mas n\u00e3o conseguir\u00e1 generalizar para novas situa\u00e7\u00f5es. O verdadeiro teste de um modelo \u00e9 sua capacidade de fazer previs\u00f5es em situa\u00e7\u00f5es que n\u00e3o foram expostas antes.\n </p>\n <p style=\"text-align: justify;\">\n  Fermi e von Neumann suspeitavam de modelos com quatro par\u00e2metros. Nossa rede de 30 neur\u00f4nios ocultos para classifica\u00e7\u00e3o de d\u00edgitos MNIST possui quase 24.000 par\u00e2metros! Nossa rede de 100 neur\u00f4nios ocultos tem cerca de 80.000 par\u00e2metros e redes neurais profundas de \u00faltima gera\u00e7\u00e3o \u00e0s vezes cont\u00eam milh\u00f5es ou at\u00e9 bilh\u00f5es de par\u00e2metros. Devemos confiar nos resultados?\n </p>\n <p style=\"text-align: justify;\">\n  Vamos agu\u00e7ar este problema construindo uma situa\u00e7\u00e3o em que a nossa rede faz um mau trabalho ao generalizar para novas situa\u00e7\u00f5es. Usaremos\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/\" rel=\"noopener\" target=\"_blank\">\n    nossa rede de 30 neur\u00f4nios ocultos\n   </a>\n  </span>\n  , com seus 23.860 par\u00e2metros. Mas n\u00e3o treinamos a rede usando todas as imagens de treinamento de 50.000 d\u00edgitos MNIST. Em vez disso, usaremos apenas as primeiras 1.000 imagens de treinamento. Usar esse conjunto restrito tornar\u00e1 o problema com a generaliza\u00e7\u00e3o muito mais evidente. Vamos treinar usando a fun\u00e7\u00e3o de custo de entropia cruzada, com uma taxa de aprendizado de \u03b7 = 0,5 e um tamanho de mini-lote de 10. No entanto, vamos treinar por 400 \u00e9pocas, pois n\u00e3o estamos usando muitos exemplos de treinamento. Vamos usar network2 para ver como a fun\u00e7\u00e3o de custo muda (o c\u00f3digo voc\u00ea encontra no reposit\u00f3rio do curso no\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener\" target=\"_blank\">\n    Github)\n   </a>\n  </span>\n  :\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"network\" class=\"aligncenter size-full wp-image-580\" data-attachment-id=\"580\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"network\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?fit=574%2C172\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?fit=300%2C90\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?fit=574%2C172\" data-orig-size=\"574,172\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/network/\" data-recalc-dims=\"1\" height=\"172\" sizes=\"(max-width: 574px) 100vw, 574px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?resize=574%2C172\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?w=574 574w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?resize=300%2C90 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/network.png?resize=200%2C60 200w\" width=\"574\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Usando os resultados, podemos tra\u00e7ar a maneira como o custo muda \u00e0 medida que a rede aprende (o script overfitting.py cont\u00e9m o c\u00f3digo que gera esse resultado):\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"overfitting1\" class=\"aligncenter size-full wp-image-581\" data-attachment-id=\"581\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"overfitting1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?fit=815%2C615\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?fit=300%2C226\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting1/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=815%2C615\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?w=815 815w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=300%2C226 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=768%2C580 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=200%2C151 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting1.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Isso parece encorajador, mostrando uma redu\u00e7\u00e3o suave no custo, exatamente como esperamos. Note que eu s\u00f3 mostrei as \u00e9pocas de treinamento de 200 a 399. Isso nos d\u00e1 uma boa vis\u00e3o dos \u00faltimos est\u00e1gios do aprendizado, que, como veremos, \u00e9 onde est\u00e1 a a\u00e7\u00e3o interessante.\n </p>\n <p style=\"text-align: justify;\">\n  Vamos agora ver como a precis\u00e3o da classifica\u00e7\u00e3o nos dados de teste muda com o tempo:\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"overfitting2\" class=\"aligncenter size-full wp-image-582\" data-attachment-id=\"582\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"overfitting2\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?fit=815%2C615\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?fit=300%2C226\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting2/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=815%2C615\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?w=815 815w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=300%2C226 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=768%2C580 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=200%2C151 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting2.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Mais uma vez, eu ampliei um pouco. Nas primeiras 200 \u00e9pocas (n\u00e3o mostradas), a precis\u00e3o sobe para pouco menos de 82%. O aprendizado ent\u00e3o diminui gradualmente. Finalmente, por volta da \u00e9poca 280, a precis\u00e3o da classifica\u00e7\u00e3o praticamente p\u00e1ra de melhorar. As \u00e9pocas posteriores meramente v\u00eaem pequenas flutua\u00e7\u00f5es estoc\u00e1sticas perto do valor da precis\u00e3o na \u00e9poca 280. Compare isso com o gr\u00e1fico anterior, em que o custo associado aos dados de treinamento continua a cair suavemente. Se olharmos apenas para esse custo, parece que nosso modelo ainda est\u00e1 ficando \u201cmelhor\u201d. Mas os resultados da precis\u00e3o do teste mostram que a melhoria \u00e9 uma ilus\u00e3o. Assim como o modelo que Fermi n\u00e3o gostava, o que nossa rede aprende ap\u00f3s a \u00e9poca 280 n\u00e3o mais se generaliza para os dados de teste. E assim n\u00e3o \u00e9 um aprendizado \u00fatil. Dizemos que a rede est\u00e1 super adaptando ou com sobreajuste ou ainda com overfitting, a partir da \u00e9poca 280.\n </p>\n <p style=\"text-align: justify;\">\n  Voc\u00ea pode se perguntar se o problema aqui \u00e9 que eu estou olhando para o custo dos dados de treinamento, ao contr\u00e1rio da precis\u00e3o da classifica\u00e7\u00e3o nos dados de teste. Em outras palavras, talvez o problema seja que estamos fazendo uma compara\u00e7\u00e3o de ma\u00e7\u00e3s e laranjas. O que aconteceria se compar\u00e1ssemos o custo dos dados de treinamento com o custo dos dados de teste, estar\u00edamos comparando medidas semelhantes? Ou talvez pud\u00e9ssemos comparar a precis\u00e3o da classifica\u00e7\u00e3o tanto nos dados de treinamento quanto nos dados de teste? Na verdade, essencialmente o mesmo fen\u00f4meno aparece, n\u00e3o importa como fazemos a compara\u00e7\u00e3o. Os detalhes mudam, no entanto. Por exemplo, vamos analisar o custo nos dados de teste:\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"overfitting3\" class=\"aligncenter size-full wp-image-583\" data-attachment-id=\"583\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"overfitting3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting3/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting3.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Podemos ver que o custo nos dados de teste melhora at\u00e9 a \u00e9poca 15, mas depois disso ele realmente come\u00e7a a piorar, mesmo que o custo nos dados de treinamento continue melhorando. Este \u00e9 outro sinal de que nosso modelo est\u00e1 super adaptando (overfitting). No entanto, coloca um enigma, que \u00e9 se devemos considerar a \u00e9poca 15 ou a \u00e9poca 280 como o ponto em que o overfitting est\u00e1 dominando a aprendizagem? Do ponto de vista pr\u00e1tico, o que realmente nos importa \u00e9 melhorar a precis\u00e3o da classifica\u00e7\u00e3o nos dados de teste, enquanto o custo dos dados de teste n\u00e3o \u00e9 mais do que um proxy para a precis\u00e3o da classifica\u00e7\u00e3o. E assim faz mais sentido considerar a \u00e9poca 280 como o ponto al\u00e9m do qual o overfitting est\u00e1 dominando o aprendizado em nossa rede neural.\n </p>\n <p style=\"text-align: justify;\">\n  Outro sinal de overfitting pode ser visto na precis\u00e3o da classifica\u00e7\u00e3o nos dados de treinamento:\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"overfitting4\" class=\"aligncenter size-full wp-image-584\" data-attachment-id=\"584\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"overfitting4\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting4/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting4.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p style=\"text-align: justify;\">\n  A precis\u00e3o aumenta at\u00e9 100%. Ou seja, nossa rede classifica corretamente todas as 1.000 imagens de treinamento! Enquanto isso, nossa precis\u00e3o de teste atinge apenas 82,27%. Portanto, nossa rede realmente est\u00e1 aprendendo sobre as peculiaridades do conjunto de treinamento, n\u00e3o apenas reconhecendo os d\u00edgitos em geral. \u00c9 quase como se nossa rede estivesse apenas memorizando o conjunto de treinamento, sem entender os d\u00edgitos suficientemente bem para generalizar o conjunto de testes.\n </p>\n <p style=\"text-align: justify;\">\n  Overfitting \u00e9 um grande problema em redes neurais. Isso \u00e9 especialmente verdadeiro em redes modernas, que geralmente t\u00eam um grande n\u00famero de pesos e vieses. Para treinar de forma eficaz, precisamos de uma maneira de detectar quando o overfitting est\u00e1 acontecendo. E precisamos aplicar t\u00e9cnicas para reduzir os efeitos do overfitting (por todo esse trabalho e conhecimento necess\u00e1rio,\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Cientistas de Dados\n   </a>\n  </span>\n  devem ser muito bem remunerados).\n </p>\n <p style=\"text-align: justify;\">\n  A maneira \u00f3bvia de detectar overfitting \u00e9 usar a abordagem acima, mantendo o controle da precis\u00e3o nos dados de teste conforme nossos treinos da rede. Se percebermos que a precis\u00e3o nos dados de teste n\u00e3o est\u00e1 mais melhorando, devemos parar de treinar. \u00c9 claro que, estritamente falando, isso n\u00e3o \u00e9 necessariamente um sinal de overfitting. Pode ser que a precis\u00e3o nos dados de teste e os dados de treinamento parem de melhorar ao mesmo tempo. Ainda assim, a ado\u00e7\u00e3o dessa estrat\u00e9gia impedir\u00e1 o overfitting.\n </p>\n <p style=\"text-align: justify;\">\n  Na verdade, usaremos uma varia\u00e7\u00e3o dessa estrat\u00e9gia. Lembre-se de que, quando carregamos os dados MNIST, carregamos em tr\u00eas conjuntos de dados:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"code\" class=\"aligncenter size-full wp-image-585\" data-attachment-id=\"585\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"code\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?fit=459%2C69\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?fit=300%2C45\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?fit=459%2C69\" data-orig-size=\"459,69\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/code/\" data-recalc-dims=\"1\" height=\"69\" sizes=\"(max-width: 459px) 100vw, 459px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?resize=459%2C69\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?w=459 459w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?resize=300%2C45 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/code.png?resize=200%2C30 200w\" width=\"459\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  At\u00e9 agora, usamos o\n  <strong>\n   training_data\n  </strong>\n  e\n  <strong>\n   test_data\n  </strong>\n  e ignoramos o\n  <strong>\n   validation_data\n  </strong>\n  . O validation_data cont\u00e9m 10.000 imagens de d\u00edgitos, imagens que s\u00e3o diferentes das 50.000 imagens no conjunto de treinamento MNIST e das 10.000 imagens no conjunto de teste MNIST. Em vez de usar o test_data para evitar overfitting, usaremos o validation_data. Para fazer isso, usaremos praticamente a mesma estrat\u00e9gia descrita acima para o test_data. Ou seja, calcularemos a precis\u00e3o da classifica\u00e7\u00e3o nos dados de valida\u00e7\u00e3o no final de cada \u00e9poca. Quando a precis\u00e3o da classifica\u00e7\u00e3o nos dados de valida\u00e7\u00e3o estiver saturada, paramos de treinar. Essa estrat\u00e9gia \u00e9 chamada de parada antecipada (Early-Stopping). \u00c9 claro que, na pr\u00e1tica, n\u00e3o sabemos imediatamente quando a precis\u00e3o est\u00e1 saturada. Em vez disso, continuamos treinando at\u00e9 termos certeza de que a precis\u00e3o est\u00e1 saturada.\n </p>\n <p style=\"text-align: justify;\">\n  Por que usar o validation_data para evitar overfitting, em vez de test_data? Na verdade, isso faz parte de uma estrat\u00e9gia mais geral, que \u00e9 usar o validation_data para avaliar diferentes op\u00e7\u00f5es de avalia\u00e7\u00e3o de hiperpar\u00e2metros, como o n\u00famero de \u00e9pocas para treinamento, a taxa de aprendizado, a melhor arquitetura de rede e assim por diante. Usamos essas avalia\u00e7\u00f5es para encontrar e definir bons valores para os hiperpar\u00e2metros. De fato, embora eu n\u00e3o tenha mencionado isso at\u00e9 agora, isto \u00e9, em parte, como chegamos \u00e0s escolhas de hiperpar\u00e2metros feitas anteriormente neste livro. (Mais sobre isso depois.)\n </p>\n <p style=\"text-align: justify;\">\n  Claro, isso n\u00e3o responde de forma alguma \u00e0 pergunta de por que estamos usando o validation_data para evitar overfitting, em vez de test_data. Para entender o porqu\u00ea, considere que, ao definir os hiperpar\u00e2metros, \u00e9 prov\u00e1vel que tentemos muitas op\u00e7\u00f5es diferentes para os hiperpar\u00e2metros. Se definirmos os hiperpar\u00e2metros com base nas avalia\u00e7\u00f5es do test_data, ser\u00e1 poss\u00edvel acabarmos super adequando nossos hiperpar\u00e2metros ao test_data. Ou seja, podemos acabar encontrando hiperpar\u00e2metros que se encaixam em peculiaridades particulares dos dados de teste, mas onde o desempenho da rede n\u00e3o se generalizar\u00e1 para outros conjuntos de dados. Protegemos contra isso descobrindo os hiperpar\u00e2metros usando o validation_data. Ent\u00e3o, uma vez que tenhamos os hiperpar\u00e2metros que queremos, fazemos uma avalia\u00e7\u00e3o final da precis\u00e3o usando o test_data. Isso nos d\u00e1 confian\u00e7a de que nossos resultados nos dados de teste s\u00e3o uma medida real de qu\u00e3o bem nossa rede neural se generaliza. Para colocar de outra forma, voc\u00ea pode pensar nos dados de valida\u00e7\u00e3o como um tipo de dados de treinamento que nos ajuda a aprender bons par\u00e2metros. Essa abordagem para encontrar bons hiperpar\u00e2metros \u00e9 \u00e0s vezes conhecida como o m\u00e9todo \u201chold out\u201d, uma vez que os dados de valida\u00e7\u00e3o s\u00e3o mantidos separados ou \u201cmantidos\u201d a partir dos dados de treinamento.\n </p>\n <p style=\"text-align: justify;\">\n  Agora, na pr\u00e1tica, mesmo depois de avaliar o desempenho nos dados de teste, podemos mudar nossa opini\u00e3o e tentar outra abordagem \u2013 talvez uma arquitetura de rede diferente \u2013 que envolva a descoberta de um novo conjunto de hiperpar\u00e2metros. Se fizermos isso, n\u00e3o h\u00e1 perigo de acabarmos com o test_data tamb\u00e9m? Precisamos de uma regress\u00e3o potencialmente infinita de conjuntos de dados, para que possamos ter certeza de que nossos resultados ser\u00e3o generalizados? Abordar essa preocupa\u00e7\u00e3o \u00e9 um problema profundo e dif\u00edcil. Mas para nossos objetivos pr\u00e1ticos, n\u00e3o vamos nos preocupar muito com essa quest\u00e3o. Em vez disso, vamos nos concentrar no m\u00e9todo b\u00e1sico de reten\u00e7\u00e3o, com base nos dados training_data, validation_data e test_data, conforme descrito acima.\n </p>\n <p style=\"text-align: justify;\">\n  Vimos que o overfitting ocorre quando estamos usando apenas 1.000 imagens de treinamento. O que acontece quando usamos o conjunto completo de treinamento de 50.000 imagens? Manteremos todos os outros par\u00e2metros iguais (30 neur\u00f4nios ocultos, taxa de aprendizado de 0,5, tamanho de mini-lote de 10), mas treinamos usando todas as 50.000 imagens por 30 \u00e9pocas. Aqui est\u00e1 um gr\u00e1fico mostrando os resultados da precis\u00e3o de classifica\u00e7\u00e3o nos dados de treinamento e nos dados de teste. Observe que usei os dados de teste aqui, em vez dos dados de valida\u00e7\u00e3o, para tornar os resultados mais diretamente compar\u00e1veis aos gr\u00e1ficos anteriores.\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"overfitting_full\" class=\"aligncenter size-full wp-image-586\" data-attachment-id=\"586\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"overfitting_full\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?fit=815%2C615\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?fit=300%2C226\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/overfitting_full/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=815%2C615\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?w=815 815w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=300%2C226 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=768%2C580 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=200%2C151 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/06/overfitting_full.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Como voc\u00ea pode ver, a precis\u00e3o nos dados de teste e treinamento permanece muito mais pr\u00f3xima do que quando est\u00e1vamos usando 1.000 exemplos de treinamento. Em particular, a melhor precis\u00e3o de classifica\u00e7\u00e3o de 97,86% nos dados de treinamento \u00e9 apenas 2,53% maior do que os 95,33% nos dados de teste. Isso \u00e9 comparado com a diferen\u00e7a de 17,73% que tivemos anteriormente! Overfitting ainda est\u00e1 acontecendo, mas foi bastante reduzido. Nossa rede est\u00e1 se generalizando muito melhor dos dados de treinamento para os dados de teste. Em geral, uma das melhores maneiras de reduzir o overfitting \u00e9 aumentar o volume (tamanho) dos dados de treinamento (fica claro agora porque\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=big-data-fundamentos\" rel=\"noopener\" target=\"_blank\">\n    Big Data\n   </a>\n  </span>\n  est\u00e1 revolucionando a Ci\u00eancia de Dados?). Com dados de treinamento suficientes, \u00e9 dif\u00edcil at\u00e9 mesmo uma rede muito grande sofrer de overfitting. Infelizmente, os dados de treinamento podem ser caros ou dif\u00edceis de adquirir, por isso nem sempre \u00e9 uma op\u00e7\u00e3o pr\u00e1tica.\n </p>\n <p style=\"text-align: justify;\">\n  Aumentar a quantidade de dados de treinamento \u00e9 uma maneira de reduzir o overfitting. Mas existem outras maneiras de reduzir a extens\u00e3o do overfitting? Uma abordagem poss\u00edvel \u00e9 reduzir o tamanho da nossa rede. No entanto, redes grandes t\u00eam o potencial de serem mais poderosas do que redes pequenas e, portanto, essa \u00e9 uma op\u00e7\u00e3o que s\u00f3 adotamos em \u00faltimo caso.\n </p>\n <p style=\"text-align: justify;\">\n  Felizmente, existem outras t\u00e9cnicas que podem reduzir o overfitting, mesmo quando temos uma rede fixa e dados de treinamento fixos. Estas t\u00e9cnicas s\u00e3o conhecidas como t\u00e9cnicas de regulariza\u00e7\u00e3o e ser\u00e3o assunto do pr\u00f3ximo cap\u00edtulo.\n </p>\n <p style=\"text-align: justify;\">\n  At\u00e9 l\u00e1!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Dot_product\" rel=\"noopener\" target=\"_blank\">\n    Dot Product\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-579\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-579\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-579\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-579\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-579-5e0dd119035da\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=579&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-579-5e0dd119035da\" id=\"like-post-wrapper-140353593-579-5e0dd119035da\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "20": "<h1 class=\"entry-title\" id=\"capitulo-20\">\n Cap\u00edtulo 20 \u2013 Overfitting e Regulariza\u00e7\u00e3o \u2013 Parte 2\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Aumentar a quantidade de dados de treinamento \u00e9 uma maneira de reduzir o overfitting. Mas existem outras maneiras de reduzir a extens\u00e3o de ocorr\u00eancia do overfitting? Uma abordagem poss\u00edvel \u00e9 reduzir o tamanho da nossa rede. No entanto, redes grandes t\u00eam o potencial de serem mais poderosas do que redes pequenas e essa \u00e9 uma op\u00e7\u00e3o que s\u00f3 adotar\u00edamos com relut\u00e2ncia.\n </p>\n <p style=\"text-align: justify;\">\n  Felizmente, existem outras t\u00e9cnicas que podem reduzir o overfitting, mesmo quando temos uma rede de tamanho fixo e dados de treinamento em quantidade limitada. Essas t\u00e9cnicas s\u00e3o conhecidos como\n  <strong>\n   t\u00e9cnicas de regulariza\u00e7\u00e3o\n  </strong>\n  . Neste cap\u00edtulo descrevemos uma das t\u00e9cnicas de regulariza\u00e7\u00e3o mais comumente usadas, uma t\u00e9cnica \u00e0s vezes conhecida como decaimento de peso (weight decay) ou Regulariza\u00e7\u00e3o L2. A ideia da Regulariza\u00e7\u00e3o L2 \u00e9 adicionar um termo extra \u00e0 fun\u00e7\u00e3o de custo, um termo chamado termo de regulariza\u00e7\u00e3o. Aqui est\u00e1 a entropia cruzada regularizada:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form\" class=\"aligncenter size-full wp-image-596\" data-attachment-id=\"596\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?fit=952%2C154\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?fit=300%2C49\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?fit=952%2C154\" data-orig-size=\"952,154\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form-5/\" data-recalc-dims=\"1\" height=\"154\" sizes=\"(max-width: 952px) 100vw, 952px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=952%2C154\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?w=952 952w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=300%2C49 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=768%2C124 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=200%2C32 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form.png?resize=690%2C112 690w\" width=\"952\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 1\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  O primeiro termo \u00e9 apenas a express\u00e3o usual para a entropia cruzada. Mas adicionamos um segundo termo, a soma dos quadrados de todos os pesos da rede. Isto \u00e9 escalonado por um fator \u03bb / 2n, onde \u03bb &gt; 0 \u00e9 conhecido como o par\u00e2metro de regulariza\u00e7\u00e3o e\n  <strong>\n   n\n  </strong>\n  \u00e9, como de costume, o tamanho do nosso conjunto de treinamento. Vou discutir mais tarde como \u03bb \u00e9 escolhido. \u00c9 importante notar tamb\u00e9m que o termo de regulariza\u00e7\u00e3o n\u00e3o inclui os vieses. Eu tamb\u00e9m voltarei a isso mais frente.\n </p>\n <p style=\"text-align: justify;\">\n  Claro, \u00e9 poss\u00edvel regularizar outras fun\u00e7\u00f5es de custo, como o custo quadr\u00e1tico. Isso pode ser feito de maneira semelhante:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form2\" class=\"aligncenter size-full wp-image-597\" data-attachment-id=\"597\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?fit=622%2C160\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?fit=300%2C77\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?fit=622%2C160\" data-orig-size=\"622,160\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form2-5/\" data-recalc-dims=\"1\" height=\"160\" sizes=\"(max-width: 622px) 100vw, 622px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?resize=622%2C160\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?w=622 622w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?resize=300%2C77 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2.png?resize=200%2C51 200w\" width=\"622\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 2\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Em ambos os casos, podemos escrever a fun\u00e7\u00e3o de custo regularizada como:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form3\" class=\"aligncenter size-full wp-image-598\" data-attachment-id=\"598\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?fit=366%2C138\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?fit=300%2C113\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?fit=366%2C138\" data-orig-size=\"366,138\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form3-4/\" data-recalc-dims=\"1\" height=\"138\" sizes=\"(max-width: 366px) 100vw, 366px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?resize=366%2C138\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?w=366 366w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?resize=300%2C113 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3.png?resize=200%2C75 200w\" width=\"366\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 3\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  onde C0 \u00e9 a fun\u00e7\u00e3o de custo original e n\u00e3o regularizada.\n </p>\n <p style=\"text-align: justify;\">\n  Intuitivamente, o efeito da regulariza\u00e7\u00e3o \u00e9 fazer com que a rede prefira aprender pequenos pesos, sendo todas as outras coisas iguais. Pesos grandes s\u00f3 ser\u00e3o permitidos se melhorarem consideravelmente a primeira parte da fun\u00e7\u00e3o de custo. Dito de outra forma, a regulariza\u00e7\u00e3o pode ser vista como uma forma de se comprometer entre encontrar pequenos pesos e minimizar a fun\u00e7\u00e3o de custo original. A import\u00e2ncia relativa dos dois elementos do compromisso depende do valor de \u03bb: quando \u03bb \u00e9 pequeno, preferimos minimizar a fun\u00e7\u00e3o de custo original, mas quando \u03bb \u00e9 grande, preferimos pesos pequenos.\n </p>\n <p style=\"text-align: justify;\">\n  Agora, n\u00e3o \u00e9 de todo \u00f3bvio porque fazer este tipo de compromisso deve ajudar a reduzir o overfitting! Mas acontece que sim, reduz. Abordaremos a quest\u00e3o de porque isso ajuda na redu\u00e7\u00e3o do overfitting no pr\u00f3ximo cap\u00edtulo, mas primeiro vamos trabalhar em um exemplo mostrando como a regulariza\u00e7\u00e3o reduz o overfitting.\n </p>\n <p style=\"text-align: justify;\">\n  Para construir um exemplo, primeiro precisamos descobrir como aplicar nosso algoritmo de aprendizado de\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/\" rel=\"noopener\" target=\"_blank\">\n    descida de gradiente estoc\u00e1stico\n   </a>\n  </span>\n  em uma rede neural regularizada. Em particular, precisamos saber como calcular as derivadas parciais \u2202C/\u2202w e \u2202C/\u2202b para todos os pesos e vieses na rede. Tomando as derivadas parciais da Equa\u00e7\u00e3o 3 acima, temos:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form4\" class=\"aligncenter size-full wp-image-599\" data-attachment-id=\"599\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?fit=352%2C244\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?fit=300%2C208\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?fit=352%2C244\" data-orig-size=\"352,244\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form4-3/\" data-recalc-dims=\"1\" height=\"244\" sizes=\"(max-width: 352px) 100vw, 352px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?resize=352%2C244\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?w=352 352w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?resize=300%2C208 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4.png?resize=200%2C139 200w\" width=\"352\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 4\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Os termos \u2202C0/\u2202w e \u2202C0/\u2202b podem ser calculados usando backpropagation, conforme descrito nos cap\u00edtulos anteriores. E assim vemos que \u00e9 f\u00e1cil calcular o gradiente da fun\u00e7\u00e3o de custo regularizada, pois basta usar backpropagation, como de costume, e depois adicionar\n  <strong>\n   (\u03bb/n).w\n  </strong>\n  \u00e0 derivada parcial de todos os termos de peso. As derivadas parciais em rela\u00e7\u00e3o aos vieses s\u00e3o inalteradas e, portanto, a regra de aprendizado de descida de gradiente para os vieses n\u00e3o muda da regra usual:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form5\" class=\"aligncenter size-full wp-image-600\" data-attachment-id=\"600\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?fit=276%2C166\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?fit=276%2C166\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?fit=276%2C166\" data-orig-size=\"276,166\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form5-2/\" data-recalc-dims=\"1\" height=\"166\" sizes=\"(max-width: 276px) 100vw, 276px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?resize=276%2C166\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?w=276 276w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form5.png?resize=200%2C120 200w\" width=\"276\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 5\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  A regra de aprendizado para os pesos se torna:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form6\" class=\"aligncenter size-full wp-image-601\" data-attachment-id=\"601\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form6\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?fit=478%2C234\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?fit=300%2C147\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?fit=478%2C234\" data-orig-size=\"478,234\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form6/\" data-recalc-dims=\"1\" height=\"234\" sizes=\"(max-width: 478px) 100vw, 478px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?resize=478%2C234\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?w=478 478w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?resize=300%2C147 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form6.png?resize=200%2C98 200w\" width=\"478\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 6\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Isto \u00e9 exatamente o mesmo que a regra usual de aprendizado de descida de gradiente, exceto pelo fato de primeiro redimensionarmos o peso w por um fator 1 \u2212 (\u03b7\u03bb/n). Esse\n  <em>\n   reescalonamento\n  </em>\n  \u00e9, \u00e0s vezes, chamado de redu\u00e7\u00e3o de peso, uma vez que diminui os pesos. \u00c0 primeira vista, parece que isso significa que os pesos est\u00e3o sendo direcionados para zero, mas isso n\u00e3o \u00e9 bem isso, uma vez que o outro termo pode levar os pesos a aumentar, se isso causar uma diminui\u00e7\u00e3o na fun\u00e7\u00e3o de custo n\u00e3o regularizada.\n </p>\n <p style=\"text-align: justify;\">\n  Ok, \u00e9 assim que a descida de gradiente funciona. E quanto \u00e0 descida de gradiente estoc\u00e1stica? Bem, assim como na descida de gradiente estoc\u00e1stica n\u00e3o-regularizada, podemos estimar \u2202C0/\u2202w pela m\u00e9dia de um mini-lote de m exemplos de treinamento. Assim, a regra de aprendizagem regularizada para a descida de gradiente estoc\u00e1stica torna-se:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form7\" class=\"aligncenter size-full wp-image-602\" data-attachment-id=\"602\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form7\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?fit=556%2C164\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?fit=300%2C88\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?fit=556%2C164\" data-orig-size=\"556,164\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form7/\" data-recalc-dims=\"1\" height=\"164\" sizes=\"(max-width: 556px) 100vw, 556px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?resize=556%2C164\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?w=556 556w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?resize=300%2C88 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form7.png?resize=200%2C59 200w\" width=\"556\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 7\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  onde a soma \u00e9 sobre exemplos de treinamento x no mini-lote, e Cx \u00e9 o custo (n\u00e3o-regularizado) para cada exemplo de treinamento. Isto \u00e9 exatamente o mesmo que a regra usual para descida de gradiente estoc\u00e1stico, exceto pelo fator de decaimento de peso de 1 \u2212 (\u03b7\u03bb/n). Finalmente, e por completo, deixe-me declarar a regra de aprendizagem regularizada para os vieses. Isto \u00e9, naturalmente, exatamente o mesmo que no caso n\u00e3o regularizado:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form8\" class=\"aligncenter size-full wp-image-603\" data-attachment-id=\"603\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form8\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?fit=370%2C134\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?fit=300%2C109\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?fit=370%2C134\" data-orig-size=\"370,134\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/form8/\" data-recalc-dims=\"1\" height=\"134\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?resize=370%2C134\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?w=370 370w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?resize=300%2C109 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form8.png?resize=200%2C72 200w\" width=\"370\"/>\n </p>\n <p style=\"text-align: center;\">\n  Equa\u00e7\u00e3o 8\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  onde a soma \u00e9 sobre exemplos de treinamento x no mini-lote.\n </p>\n <p style=\"text-align: justify;\">\n  Vamos ver como a regulariza\u00e7\u00e3o altera o desempenho da nossa rede neural. Usaremos uma rede com 30 neur\u00f4nios ocultos, um tamanho de mini-lote de 10, uma taxa de aprendizado de 0,5 e a fun\u00e7\u00e3o de custo de entropia cruzada. No entanto, desta vez vamos usar um par\u00e2metro de regulariza\u00e7\u00e3o de \u03bb = 0,1. Note que no c\u00f3digo, usamos o nome da vari\u00e1vel\n  <strong>\n   lmbda\n  </strong>\n  , porque\n  <strong>\n   lambda\n  </strong>\n  \u00e9 uma palavra reservada em Python, com um significado n\u00e3o relacionado ao que estamos fazendo aqui (caso tenha d\u00favidas sobre as palavras reservadas em Python, acesse o curso gratuito\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener\" target=\"_blank\">\n    Python Fundamentos Para An\u00e1lise de Dados \u2013 Cap\u00edtulo 2\n   </a>\n  </span>\n  ).\n </p>\n <p style=\"text-align: justify;\">\n  Eu tamb\u00e9m usei o test_data novamente, n\u00e3o o validation_data. Estritamente falando, devemos usar o validation_data, por todas as raz\u00f5es que discutimos anteriormente. Mas decidi usar o test_data porque ele torna os resultados mais diretamente compar\u00e1veis com nossos resultados anteriores e n\u00e3o regularizados. Voc\u00ea pode facilmente alterar o c\u00f3digo para usar o validation_data e voc\u00ea ver\u00e1 que ele ter\u00e1 resultados semelhantes.\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"python\" class=\"aligncenter wp-image-604 size-large\" data-attachment-id=\"604\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"python\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?fit=1024%2C397\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?fit=300%2C116\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?fit=1228%2C476\" data-orig-size=\"1228,476\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/python/\" data-recalc-dims=\"1\" height=\"397\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=1024%2C397\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=1024%2C397 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=300%2C116 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=768%2C298 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=200%2C78 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?resize=690%2C267 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python.png?w=1228 1228w\" width=\"1024\"/>\n </p>\n <p style=\"text-align: justify;\">\n  O custo com os dados de treinamento diminui durante todo o tempo, da mesma forma que no caso anterior, n\u00e3o regularizado no\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-1/\" rel=\"noopener\" target=\"_blank\">\n    cap\u00edtulo anterior\n   </a>\n  </span>\n  :\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"regularized1\" class=\"aligncenter size-full wp-image-605\" data-attachment-id=\"605\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"regularized1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?fit=815%2C615\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?fit=300%2C226\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/regularized1/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=815%2C615\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?w=815 815w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=300%2C226 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=768%2C580 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=200%2C151 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized1.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Mas desta vez a precis\u00e3o no test_data continua a aumentar durante as 400 \u00e9pocas:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"regularized2\" class=\"aligncenter size-full wp-image-606\" data-attachment-id=\"606\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"regularized2\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?fit=815%2C615\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?fit=300%2C226\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/regularized2/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=815%2C615\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?w=815 815w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=300%2C226 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=768%2C580 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=200%2C151 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized2.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Claramente, o uso da regulariza\u00e7\u00e3o suprimiu o overfitting. Al\u00e9m do mais, a precis\u00e3o \u00e9 consideravelmente maior, com uma precis\u00e3o de classifica\u00e7\u00e3o de pico de 87.1%, em compara\u00e7\u00e3o com o pico de 82.27% obtido no caso n\u00e3o regularizado. De fato, quase certamente poder\u00edamos obter resultados consideravelmente melhores, continuando a treinar mais de 400 \u00e9pocas. Parece que, empiricamente, a regulariza\u00e7\u00e3o est\u00e1 fazendo com que nossa rede generalize melhor e reduza consideravelmente os efeitos do overfitting.\n </p>\n <p style=\"text-align: justify;\">\n  O que acontece se sairmos do ambiente artificial de ter apenas 1.000 imagens de treinamento e retornar ao conjunto completo de treinamento de 50.000 imagens? \u00c9 claro, j\u00e1 vimos que o overfitting \u00e9 muito menos problem\u00e1tico com as 50.000 imagens. A regulariza\u00e7\u00e3o ajuda ainda mais? Vamos manter os hiperpar\u00e2metros iguais ao exemplo anterior \u2013 30 \u00e9pocas, taxa de aprendizado de 0,5, tamanho de mini-lote de 10. No entanto, precisamos modificar o par\u00e2metro de regulariza\u00e7\u00e3o. A raz\u00e3o \u00e9 porque o tamanho n do conjunto de treinamento mudou de n = 1.000 para n = 50.000, e isso muda o fator de decaimento de peso 1 \u2212 (\u03b7\u03bb/n). Se continu\u00e1ssemos a usar \u03bb = 0,1, isso significaria muito menos perda de peso e, portanto, muito menos efeito de regulariza\u00e7\u00e3o. N\u00f3s compensamos mudando para \u03bb = 5.0.\n </p>\n <p style=\"text-align: justify;\">\n  Ok, vamos treinar nossa rede, parando primeiro para reinicializar os pesos:\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"python2\" class=\"aligncenter size-large wp-image-607\" data-attachment-id=\"607\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"python2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?fit=1024%2C185\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?fit=300%2C54\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?fit=1226%2C222\" data-orig-size=\"1226,222\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/python2/\" data-recalc-dims=\"1\" height=\"185\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=1024%2C185\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=1024%2C185 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=300%2C54 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=768%2C139 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=200%2C36 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?resize=690%2C125 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python2.png?w=1226 1226w\" width=\"1024\"/>\n </p>\n <p style=\"text-align: justify;\">\n  Obtemos os resultados:\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"regularized_full\" class=\"aligncenter size-full wp-image-608\" data-attachment-id=\"608\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"regularized_full\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/regularized_full/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/regularized_full.png?resize=690%2C521 690w\" width=\"815\"/>\n </p>\n <p style=\"text-align: justify;\">\n  H\u00e1 muitas boas not\u00edcias aqui. Primeiro, nossa precis\u00e3o de classifica\u00e7\u00e3o nos dados de teste aumentou de 95.49%, quando n\u00e3o foi regularizada, para 96.49%. Isso \u00e9 uma grande melhoria. Em segundo lugar, podemos ver que a diferen\u00e7a entre os resultados nos dados de treinamento e teste \u00e9 muito menor do que antes, com um percentual abaixo de zero. Essa ainda \u00e9 uma lacuna significativa, mas obviamente fizemos um progresso substancial para reduzir o overfitting.\n </p>\n <p style=\"text-align: justify;\">\n  Finalmente, vamos ver qual a precis\u00e3o da classifica\u00e7\u00e3o de teste que obtemos quando usamos 100 neur\u00f4nios ocultos e um par\u00e2metro de regulariza\u00e7\u00e3o de \u03bb = 5.0. Eu n\u00e3o vou passar por uma an\u00e1lise detalhada de overfitting aqui, isso \u00e9 puramente por divers\u00e3o, s\u00f3 para ver a precis\u00e3o que podemos obter quando usamos nossos novos truques: a fun\u00e7\u00e3o de custo de entropia cruzada e a Regulariza\u00e7\u00e3o L2.\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"python3\" class=\"aligncenter size-large wp-image-609\" data-attachment-id=\"609\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"python3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?fit=1024%2C232\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?fit=300%2C68\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?fit=1236%2C280\" data-orig-size=\"1236,280\" data-permalink=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/python3/\" data-recalc-dims=\"1\" height=\"232\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=1024%2C232\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=1024%2C232 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=300%2C68 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=768%2C174 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=200%2C45 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?resize=690%2C156 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/python3.png?w=1236 1236w\" width=\"1024\"/>\n </p>\n <p style=\"text-align: justify;\">\n  O resultado final \u00e9 uma precis\u00e3o de classifica\u00e7\u00e3o de 97.92% nos dados de valida\u00e7\u00e3o. \u00c9 um grande salto do caso dos 30 neur\u00f4nios ocultos. Na verdade, ajustando um pouco mais, para executar por 60 \u00e9pocas com \u03b7 = 0.1 e \u03bb = 5.0, quebramos a barreira de 98%, alcan\u00e7ando uma precis\u00e3o de classifica\u00e7\u00e3o de 98.04% nos dados de valida\u00e7\u00e3o. Nada mal para o que acaba sendo 152 linhas de c\u00f3digo!\n </p>\n <p style=\"text-align: justify;\">\n  Descrevi a regulariza\u00e7\u00e3o como uma forma de reduzir o overfitting e aumentar as precis\u00f5es de classifica\u00e7\u00e3o. Na verdade, esse n\u00e3o \u00e9 o \u00fanico benef\u00edcio. Empiricamente, ao executar v\u00e1rias execu\u00e7\u00f5es de nossas redes com o dataset MNIST, mas com diferentes inicializa\u00e7\u00f5es de peso (aleat\u00f3rias), descobrimos que as execu\u00e7\u00f5es n\u00e3o-regularizadas ocasionalmente ficar\u00e3o \u201cpresas\u201d, aparentemente capturadas em m\u00ednimos locais da fun\u00e7\u00e3o de custo. O resultado \u00e9 que diferentes execu\u00e7\u00f5es \u00e0s vezes fornecem resultados bastante diferentes. Por outro lado, as execu\u00e7\u00f5es regularizadas forneceram resultados muito mais facilmente replic\u00e1veis.\n </p>\n <p style=\"text-align: justify;\">\n  Por que isso est\u00e1 acontecendo? Heuristicamente, se a fun\u00e7\u00e3o de custo for\n  <em>\n   desregularizada\n  </em>\n  , o comprimento do vetor de peso provavelmente crescer\u00e1, todas as outras coisas sendo iguais. Com o tempo, isso pode levar o vetor de peso a ser realmente muito grande. Isso pode fazer com que o vetor de peso fique preso apontando mais ou menos na mesma dire\u00e7\u00e3o, j\u00e1 que as mudan\u00e7as devido a descida do gradiente fazem apenas pequenas altera\u00e7\u00f5es na dire\u00e7\u00e3o, quando o comprimento \u00e9 longo. Acredito que esse fen\u00f4meno esteja dificultando o nosso algoritmo de aprendizado para explorar adequadamente o espa\u00e7o de pesos e, consequentemente, mais dif\u00edcil encontrar bons m\u00ednimos da fun\u00e7\u00e3o de custo.\n </p>\n <p>\n  Ainda n\u00e3o acabamos sobre regulariza\u00e7\u00e3o. Mais sobre isso no pr\u00f3ximo cap\u00edtulo! At\u00e9 l\u00e1!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://en.wikipedia.org/wiki/Dot_product\" rel=\"noopener\" target=\"_blank\">\n    Dot Product\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-594\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-594\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-594\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-594\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-594-5e0dd11b5ace2\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=594&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-594-5e0dd11b5ace2\" id=\"like-post-wrapper-140353593-594-5e0dd11b5ace2\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "21": "<h1 class=\"entry-title\" id=\"capitulo-21\">\n Cap\u00edtulo 21 \u2013 Afinal, Por Que a Regulariza\u00e7\u00e3o Ajuda a Reduzir o Overfitting?\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  Vimos no\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/\" rel=\"noopener\" target=\"_blank\">\n    cap\u00edtulo anterior\n   </a>\n  </span>\n  que a regulariza\u00e7\u00e3o ajuda a reduzir o overfitting. Isso \u00e9 encorajador, mas, infelizmente, n\u00e3o \u00e9 \u00f3bvio porque a regulariza\u00e7\u00e3o ajuda a resolver o overfitting! Uma hist\u00f3ria padr\u00e3o que as pessoas contam para explicar o que est\u00e1 acontecendo segue mais ou menos esse racioc\u00ednio: pesos menores s\u00e3o, em certo sentido, de menor complexidade e, portanto, fornecem uma explica\u00e7\u00e3o mais simples e mais poderosa para os dados e devem, normalmente, ser preferidos. \u00c9 uma hist\u00f3ria bastante concisa e cont\u00e9m v\u00e1rios elementos que talvez pare\u00e7am d\u00fabios ou mistificadores. Vamos descompactar essa explica\u00e7\u00e3o e examin\u00e1-la criticamente.\u00a0Afinal, Por Que a Regulariza\u00e7\u00e3o Ajuda a Reduzir o Overfitting?\n </p>\n <p style=\"text-align: justify;\">\n  Para fazer isso, vamos supor que temos um conjunto de dados simples para o qual desejamos construir um modelo:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"graph\" class=\"aligncenter size-large wp-image-612\" data-attachment-id=\"612\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graph\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?fit=1024%2C672\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?fit=300%2C197\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?fit=1118%2C734\" data-orig-size=\"1118,734\" data-permalink=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/graph/\" data-recalc-dims=\"1\" height=\"672\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=1024%2C672\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=1024%2C672 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=300%2C197 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=768%2C504 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=200%2C131 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?resize=690%2C453 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph.png?w=1118 1118w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Implicitamente, estamos estudando algum fen\u00f4meno do mundo real aqui, com x e y representando dados desse fen\u00f4meno. Nosso objetivo \u00e9 construir um modelo que nos permita prever y como uma fun\u00e7\u00e3o de x (isso \u00e9 o que fazemos em\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n  ). Poder\u00edamos tentar usar redes neurais para construir esse modelo, mas vou fazer algo ainda mais simples: vou tentar modelar y como um polin\u00f4mio em x. Estou fazendo isso em vez de usar redes neurais porque usar polin\u00f4mios tornar\u00e1 as coisas particularmente transparentes. Uma vez que tenhamos entendido o caso polinomial, vamos traduzir para redes neurais.\n </p>\n <p style=\"text-align: justify;\">\n  H\u00e1 dez pontos no gr\u00e1fico acima, o que significa que podemos encontrar um \u00fanico polin\u00f4mio de 9\u00aa ordem y = a0x9 + a1x8 +\u2026 + a9 que se ajusta exatamente aos dados. Aqui est\u00e1 o gr\u00e1fico desse polin\u00f4mio:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"graph2\" class=\"aligncenter size-large wp-image-613\" data-attachment-id=\"613\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graph2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?fit=1024%2C698\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?fit=300%2C204\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?fit=1124%2C766\" data-orig-size=\"1124,766\" data-permalink=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/graph2/\" data-recalc-dims=\"1\" height=\"698\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=1024%2C698\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=1024%2C698 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=300%2C204 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=768%2C523 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=200%2C136 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?resize=690%2C470 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph2.png?w=1124 1124w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Isso fornece um ajuste exato. Mas tamb\u00e9m podemos obter um bom ajuste usando o modelo linear y = 2x:\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"graph3\" class=\"aligncenter size-large wp-image-614\" data-attachment-id=\"614\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graph3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?fit=1024%2C648\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?fit=300%2C190\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?fit=1128%2C714\" data-orig-size=\"1128,714\" data-permalink=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/graph3/\" data-recalc-dims=\"1\" height=\"648\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=1024%2C648\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=1024%2C648 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=300%2C190 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=768%2C486 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=200%2C127 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?resize=690%2C437 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/graph3.png?w=1128 1128w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Qual destes \u00e9 o melhor modelo? E qual modelo \u00e9 mais prov\u00e1vel de generalizar bem a outros exemplos do mesmo fen\u00f4meno do mundo real?\n </p>\n <p style=\"text-align: justify;\">\n  Essas s\u00e3o quest\u00f5es dif\u00edceis. De fato, n\u00e3o podemos determinar com certeza a resposta para qualquer uma das perguntas acima, sem muito mais informa\u00e7\u00f5es sobre o fen\u00f4meno do mundo real que estamos analisando (\u00e9 onde entra a experi\u00eancia do\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Cientista de Dados\n   </a>\n  </span>\n  sobre \u00e1reas de neg\u00f3cio). Mas vamos considerar duas possibilidades: (1) o polin\u00f4mio de 9\u00aa ordem \u00e9, de fato, o modelo que realmente descreve o fen\u00f4meno do mundo real, e o modelo, portanto, generalizar\u00e1 perfeitamente; (2) o modelo correto \u00e9 y = 2x, mas h\u00e1 um pequeno ru\u00eddo adicional devido a, digamos, erros de medi\u00e7\u00e3o, e \u00e9 por isso que o modelo n\u00e3o \u00e9 um ajuste exato.\n </p>\n <p style=\"text-align: justify;\">\n  N\u00e3o \u00e9 poss\u00edvel a\u00a0priori dizer qual dessas duas possibilidades est\u00e1 correta (ou, na verdade, se alguma terceira possibilidade \u00e9 v\u00e1lida). Logicamente, qualquer uma poderia ser verdade. E n\u00e3o \u00e9 uma diferen\u00e7a trivial. \u00c9 verdade que nos dados fornecidos h\u00e1 apenas uma pequena diferen\u00e7a entre os dois modelos. Mas suponha que queremos predizer o valor de y correspondendo a um grande valor de x, muito maior do que qualquer um mostrado nos gr\u00e1ficos acima. Se tentarmos fazer isso, haver\u00e1 uma diferen\u00e7a dram\u00e1tica entre as previs\u00f5es dos dois modelos, j\u00e1 que o modelo polinomial de 9\u00aa ordem passa a ser dominado pelo termo x9, enquanto o modelo linear permanece, bem, linear.\n </p>\n <p style=\"text-align: justify;\">\n  Um ponto de vista \u00e9 dizer que, na ci\u00eancia, devemos seguir a explica\u00e7\u00e3o mais simples, a menos que sejamos obrigados a fazer o contr\u00e1rio. Quando encontramos um modelo simples que parece explicar muitos dados, somos tentados a gritar \u201cEureka!\u201d Afinal, parece improv\u00e1vel que uma explica\u00e7\u00e3o simples ocorra apenas por coincid\u00eancia. Em vez disso, suspeitamos que o modelo deve estar expressando alguma verdade subjacente sobre o fen\u00f4meno. No caso em quest\u00e3o, o modelo y = 2x + ru\u00eddo parece muito mais simples que y = a0x9 + a1x8 +\u2026. Seria surpreendente se essa simplicidade tivesse ocorrido por acaso, e ent\u00e3o suspeitamos que y = 2x + ru\u00eddo expressa alguma verdade subjacente. Nesse ponto de vista, o modelo de 9\u00aa ordem est\u00e1 realmente aprendendo apenas os efeitos do ru\u00eddo local. E assim, enquanto o modelo de 9\u00aa ordem funciona perfeitamente para esses pontos de dados particulares, o modelo n\u00e3o conseguir\u00e1 generalizar para outros pontos de dados, e o modelo linear ter\u00e1 maior poder preditivo.\n </p>\n <p style=\"text-align: justify;\">\n  Vamos ver o que esse ponto de vista significa para redes neurais. Suponha que nossa rede tenha, na maioria das vezes, pequenos pesos, como tender\u00e1 a acontecer em uma rede regularizada. O tamanho menor dos pesos significa que o comportamento da rede n\u00e3o mudar\u00e1 muito se alterarmos algumas entradas aleat\u00f3rias aqui e ali. Isso dificulta que uma rede regularizada aprenda os efeitos do ru\u00eddo local nos dados. Pense nisso como uma maneira de fazer com que as evid\u00eancias n\u00e3o importem muito para a sa\u00edda da rede. Em vez disso, uma rede regularizada aprende a responder a tipos de evid\u00eancias que s\u00e3o vistas com frequ\u00eancia em todo o conjunto de treinamento. Por outro lado, uma rede com grandes pesos pode alterar bastante seu comportamento em resposta a pequenas altera\u00e7\u00f5es na entrada. Assim, uma rede n\u00e3o regularizada pode usar grandes pesos para aprender um modelo complexo que cont\u00e9m muitas informa\u00e7\u00f5es sobre o ru\u00eddo nos dados de treinamento. Em suma, as redes regularizadas s\u00e3o levadas a construir modelos relativamente simples baseados em padr\u00f5es vistos frequentemente nos dados de treinamento e s\u00e3o resistentes \u00e0s peculiaridades de aprendizagem do ru\u00eddo nos dados de treinamento. A esperan\u00e7a \u00e9 que isso for\u00e7ar\u00e1 nossas redes a aprender de verdade sobre o fen\u00f4meno em quest\u00e3o e a generalizar melhor o que aprendem.\n </p>\n <p style=\"text-align: justify;\">\n  Com isso dito, e mantendo a necessidade de cautela em mente, \u00e9 um fato emp\u00edrico que as redes neurais regularizadas geralmente generalizam melhor do que as redes n\u00e3o regularizadas. A verdade \u00e9 que ningu\u00e9m ainda desenvolveu uma explica\u00e7\u00e3o te\u00f3rica inteiramente convincente para explicar porque a regulariza\u00e7\u00e3o ajuda a generalizar as redes. De fato, os pesquisadores continuam a escrever artigos nos quais tentam abordagens diferentes \u00e0 regulariza\u00e7\u00e3o, comparam-nas para ver qual funciona melhor e tentam entender por que diferentes abordagens funcionam melhor ou pior. Embora muitas vezes ajude, n\u00e3o temos uma compreens\u00e3o sistem\u00e1tica inteiramente satisfat\u00f3ria do que est\u00e1 acontecendo, apenas heur\u00edsticas incompletas e regras gerais.\n </p>\n <p style=\"text-align: justify;\">\n  H\u00e1 um conjunto mais profundo de quest\u00f5es aqui, quest\u00f5es que v\u00e3o para o cora\u00e7\u00e3o da ci\u00eancia. \u00c9 a quest\u00e3o de como generalizamos. A regulariza\u00e7\u00e3o pode nos dar uma varinha m\u00e1gica computacional que ajuda nossas redes a generalizar melhor, mas n\u00e3o nos d\u00e1 uma compreens\u00e3o baseada em princ\u00edpios de como a generaliza\u00e7\u00e3o funciona, nem de qual \u00e9 a melhor abordagem.\n </p>\n <p style=\"text-align: justify;\">\n  Isso \u00e9 particularmente irritante porque na vida cotidiana, n\u00f3s humanos generalizamos bem. Mostradas apenas algumas imagens de um elefante, uma crian\u00e7a aprender\u00e1 rapidamente a reconhecer outros elefantes. \u00c9 claro que eles podem ocasionalmente cometer erros, talvez confundindo um rinoceronte com um elefante, mas em geral esse processo funciona notavelmente com precis\u00e3o. Ent\u00e3o n\u00f3s temos um sistema \u2013 o c\u00e9rebro humano \u2013 com um grande n\u00famero de par\u00e2metros livres. E depois de ser mostrado apenas uma ou algumas imagens de treinamento, o sistema aprende a generalizar para outras imagens. Nossos c\u00e9rebros est\u00e3o, em certo sentido, se regularizando incrivelmente bem! Como fazemos isso? Neste ponto n\u00e3o sabemos. Espero que nos pr\u00f3ximos anos desenvolvamos t\u00e9cnicas mais poderosas de regulariza\u00e7\u00e3o em redes neurais artificiais, t\u00e9cnicas que permitir\u00e3o que as redes neurais generalizem bem, mesmo a partir de pequenos conjuntos de dados.\n </p>\n <p style=\"text-align: justify;\">\n  De fato, nossas redes j\u00e1 generalizam melhor do que se poderia esperar a priori. Uma rede com 100 neur\u00f4nios ocultos tem quase 80.000 par\u00e2metros. Temos apenas 50.000 imagens em nossos dados de treinamento. \u00c9 como tentar encaixar um polin\u00f4mio de grau 80.000 em 50.000 pontos de dados. Consequentemente, nossa rede deve se ajustar muito bem. E, no entanto, como vimos anteriormente, essa rede realmente faz um \u00f3timo trabalho generalizando. Por que esse \u00e9 o caso? N\u00e3o \u00e9 bem entendido. Foi conjecturado que \u201ca din\u00e2mica do aprendizado de gradiente descendente em redes multicamadas tem um efeito de \u2018autorregula\u00e7\u00e3o'\u201d. Isso \u00e9 excepcionalmente bom, mas tamb\u00e9m \u00e9 um tanto inquietante que n\u00e3o entendemos porque exatamente isso ocorre e por isso muitas vezes modelos de redes neurais profundas s\u00e3o chamados de \u201ccaixa preta\u201d. Enquanto isso, adotaremos a abordagem pragm\u00e1tica e usaremos a regulariza\u00e7\u00e3o sempre que pudermos. Nossas redes neurais ser\u00e3o melhores assim.\n </p>\n <p style=\"text-align: justify;\">\n  Deixe-me concluir esta se\u00e7\u00e3o voltando a um detalhe que deixei inexplicado antes: o fato de que a regulariza\u00e7\u00e3o L2 n\u00e3o restringe os vieses. \u00c9 claro que seria f\u00e1cil modificar o procedimento de regulariza\u00e7\u00e3o para regularizar os vieses. Empiricamente, fazendo isso muitas vezes n\u00e3o muda muito os resultados, ent\u00e3o, em certa medida, \u00e9 apenas uma conven\u00e7\u00e3o se regularizar os vieses ou n\u00e3o. No entanto, vale a pena notar que ter um grande vi\u00e9s n\u00e3o torna um neur\u00f4nio sens\u00edvel \u00e0s suas entradas da mesma maneira que ter pesos grandes. Portanto, n\u00e3o precisamos nos preocupar com grandes vieses que permitem que nossa rede aprenda o ru\u00eddo em nossos dados de treinamento. Ao mesmo tempo, permitir grandes vieses d\u00e1 \u00e0s nossas redes mais flexibilidade no comportamento \u2013 em particular, grandes vieses facilitam a satura\u00e7\u00e3o dos neur\u00f4nios, o que \u00e0s vezes \u00e9 desej\u00e1vel. Por essas raz\u00f5es, geralmente n\u00e3o inclu\u00edmos termos de vi\u00e9s quando regularizamos a rede neural.\n </p>\n <p style=\"text-align: justify;\">\n  Voc\u00ea j\u00e1 percebeu que regulariza\u00e7\u00e3o \u00e9 um assunto importante quando tratamos de redes neurais. Nos pr\u00f3ximos cap\u00edtulos estudaremos mais duas t\u00e9cnicas de regulariza\u00e7\u00e3o: Regulariza\u00e7\u00e3o L1 e Dropout! N\u00e3o perca!\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  Refer\u00eancias:\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-611\" href=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-611\" href=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-611\" href=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-611\" href=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-611-5e0dd11de5af2\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=611&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-611-5e0dd11de5af2\" id=\"like-post-wrapper-140353593-611-5e0dd11de5af2\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "22": "<h1 class=\"entry-title\" id=\"capitulo-22\">\n Cap\u00edtulo 22 \u2013 Regulariza\u00e7\u00e3o L1\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Existem muitas t\u00e9cnicas de regulariza\u00e7\u00e3o al\u00e9m da Regulariza\u00e7\u00e3o L2 que vimos no\n  </span>\n  <a href=\"http://deeplearningbook.com.br/afinal-por-que-a-regularizacao-ajuda-a-reduzir-o-overfitting/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    cap\u00edtulo anterior\n   </span>\n  </a>\n  .\n  <span style=\"color: #000000;\">\n   De fato, tantas t\u00e9cnicas foram desenvolvidas que \u00e9 dif\u00edcil resumir todas elas. Neste e nos pr\u00f3ximos dois cap\u00edtulos, vamos descrever brevemente tr\u00eas outras abordagens para reduzir o overfitting: Regulariza\u00e7\u00e3o L1, Dropout e aumento artificial do tamanho do conjunto de treinamento. N\u00e3o aprofundaremos tanto nessas t\u00e9cnicas como fizemos com a Regulariza\u00e7\u00e3o L2. Em vez disso, o objetivo \u00e9 familiarizar voc\u00ea com as ideias principais e apreciar a diversidade de t\u00e9cnicas de regulariza\u00e7\u00e3o dispon\u00edveis.\n  </span>\n </p>\n <p>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Regulariza\u00e7\u00e3o L1\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nesta abordagem, modificamos a fun\u00e7\u00e3o de custo n\u00e3o regularizada, adicionando a soma dos valores absolutos dos pesos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form1\" class=\"aligncenter size-full wp-image-649\" data-attachment-id=\"649\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?fit=350%2C166\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?fit=300%2C142\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?fit=350%2C166\" data-orig-size=\"350,166\" data-permalink=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form1-2/\" data-recalc-dims=\"1\" height=\"166\" sizes=\"(max-width: 350px) 100vw, 350px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?resize=350%2C166\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?w=350 350w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?resize=300%2C142 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form1.png?resize=200%2C95 200w\" width=\"350\"/>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Equa\u00e7\u00e3o 1\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Intuitivamente, isso \u00e9 semelhante \u00e0 Regulariza\u00e7\u00e3o L2, penalizando grandes pesos e tendendo a fazer com que a rede prefira pequenos pesos. Naturalmente, o termo de Regulariza\u00e7\u00e3o L1 n\u00e3o \u00e9 o mesmo que o termo de Regulariza\u00e7\u00e3o L2 e, portanto, n\u00e3o devemos esperar obter exatamente o mesmo comportamento. Vamos tentar entender como o comportamento de uma rede treinada usando a Regulariza\u00e7\u00e3o L1 difere de uma rede treinada usando a Regulariza\u00e7\u00e3o L2.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para fazer isso, vejamos as derivadas parciais da fun\u00e7\u00e3o de custo. A partir da f\u00f3rmula anterior obtemos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form2\" class=\"aligncenter size-full wp-image-650\" data-attachment-id=\"650\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?fit=404%2C154\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?fit=300%2C114\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?fit=404%2C154\" data-orig-size=\"404,154\" data-permalink=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form2-6/\" data-recalc-dims=\"1\" height=\"154\" sizes=\"(max-width: 404px) 100vw, 404px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?resize=404%2C154\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?w=404 404w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?resize=300%2C114 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form2-1.png?resize=200%2C76 200w\" width=\"404\"/>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Equa\u00e7\u00e3o 2\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   onde sgn(w) \u00e9 o sinal de w, isto \u00e9, +1 se w \u00e9 positivo e \u22121 se w \u00e9 negativo. Usando essa express\u00e3o, podemos facilmente modificar a retropropaga\u00e7\u00e3o (backpropagation) para fazer a descida de gradiente estoc\u00e1stica usando a Regulariza\u00e7\u00e3o L1. A regra de atualiza\u00e7\u00e3o resultante para uma rede regularizada L1 \u00e9:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form3\" class=\"aligncenter size-full wp-image-651\" data-attachment-id=\"651\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?fit=580%2C158\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?fit=300%2C82\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?fit=580%2C158\" data-orig-size=\"580,158\" data-permalink=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form3-5/\" data-recalc-dims=\"1\" height=\"158\" sizes=\"(max-width: 580px) 100vw, 580px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?resize=580%2C158\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?w=580 580w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?resize=300%2C82 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form3-1.png?resize=200%2C54 200w\" width=\"580\"/>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Equa\u00e7\u00e3o 3 \u2013 Regra de atualiza\u00e7\u00e3o L1\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   onde, como de costume, podemos estimar \u2202C0/\u2202w usando uma m\u00e9dia de mini-lote, se desejarmos. Compare isso com a regra de atualiza\u00e7\u00e3o para a Regulariza\u00e7\u00e3o L2:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form4\" class=\"aligncenter size-full wp-image-652\" data-attachment-id=\"652\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?fit=554%2C148\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?fit=300%2C80\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?fit=554%2C148\" data-orig-size=\"554,148\" data-permalink=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/form4-4/\" data-recalc-dims=\"1\" height=\"148\" sizes=\"(max-width: 554px) 100vw, 554px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?resize=554%2C148\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?w=554 554w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?resize=300%2C80 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/form4-1.png?resize=200%2C53 200w\" width=\"554\"/>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Equa\u00e7\u00e3o 4 \u2013 Regra de atualiza\u00e7\u00e3o L2\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em ambas as express\u00f5es, o efeito da regulariza\u00e7\u00e3o \u00e9 diminuir os pesos. Isso est\u00e1 de acordo com a nossa intui\u00e7\u00e3o de que ambos os tipos de regulariza\u00e7\u00e3o penalizam grandes pesos. Mas a maneira como os pesos diminuem \u00e9 diferente. Na Regulariza\u00e7\u00e3o L1, os pesos diminuem em uma quantidade constante para 0. Na Regulariza\u00e7\u00e3o L2, os pesos diminuem em um valor proporcional a w. E assim, quando um peso espec\u00edfico tem uma grande magnitude, a Regulariza\u00e7\u00e3o L1 reduz o peso muito menos do que a Regulariza\u00e7\u00e3o L2. Em contraste, quando |w| \u00e9 pequena, a Regulariza\u00e7\u00e3o L1 reduz o peso muito mais do que a Regulariza\u00e7\u00e3o L2. O resultado \u00e9 que a Regulariza\u00e7\u00e3o L1 tende a concentrar o peso da rede em um n\u00famero relativamente pequeno de conex\u00f5es de alta import\u00e2ncia, enquanto os outros pesos s\u00e3o direcionados para zero.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas h\u00e1 ainda um pequeno detalhe na discuss\u00e3o acima. A derivada parcial \u2202C/\u2202w n\u00e3o \u00e9 definida quando w = 0. A raz\u00e3o \u00e9 que a fun\u00e7\u00e3o |w| tem um \u201ccanto\u201d agudo em w = 0 e, portanto, n\u00e3o \u00e9 diferenci\u00e1vel nesse ponto. Tudo bem, no entanto. O que faremos \u00e9 aplicar a regra usual (n\u00e3o regularizada) para descida de gradiente estoc\u00e1stica quando w = 0. Isso ajuda a resolver a quest\u00e3o \u2013 intuitivamente, o efeito da regulariza\u00e7\u00e3o \u00e9 diminuir os pesos e, obviamente, n\u00e3o pode reduzir um peso que j\u00e1 \u00e9 0. Para coloc\u00e1-lo com mais precis\u00e3o, usaremos as Equa\u00e7\u00f5es (2) e (3) com a conven\u00e7\u00e3o que sgn(0) = 0. Isso d\u00e1 uma regra legal e compacta para se fazer uma descida gradiente estoc\u00e1stica com Regulariza\u00e7\u00e3o L1.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Agora vamos para o Dropout, no pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Neural Networks and Deep Learning\n   </span>\n  </a>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-647\" href=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-647\" href=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-647\" href=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-647\" href=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-647-5e0dd11fed5ec\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=647&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-647-5e0dd11fed5ec\" id=\"like-post-wrapper-140353593-647-5e0dd11fed5ec\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "23": "<h1 class=\"entry-title\" id=\"capitulo-23\">\n Cap\u00edtulo 23 \u2013 Como Funciona o Dropout?\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Dropout \u00e9 uma t\u00e9cnica radicalmente diferente para regulariza\u00e7\u00e3o. Ao contr\u00e1rio da Regulariza\u00e7\u00e3o L1 e L2, o Dropout n\u00e3o depende da modifica\u00e7\u00e3o da fun\u00e7\u00e3o de custo. Em vez disso, no Dropout, modificamos a pr\u00f3pria rede. Deixe-me descrever a mec\u00e2nica b\u00e1sica de\n   <strong>\n    Como Funciona o Dropout?\n   </strong>\n   antes de entender porque ele funciona e quais s\u00e3o os resultados.\u00a0Suponha que estamos tentando treinar uma rede neural:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"rede\" class=\"aligncenter size-full wp-image-655\" data-attachment-id=\"655\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?fit=310%2C324\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?fit=287%2C300\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?fit=310%2C324\" data-orig-size=\"310,324\" data-permalink=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/rede-3/\" data-recalc-dims=\"1\" height=\"324\" sizes=\"(max-width: 310px) 100vw, 310px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?resize=310%2C324\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?w=310 310w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?resize=287%2C300 287w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede.png?resize=200%2C209 200w\" width=\"310\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em particular, suponha que tenhamos uma entrada de treinamento x e a sa\u00edda desejada correspondente y. Normalmente, n\u00f3s treinamos pela propaga\u00e7\u00e3o direta de x atrav\u00e9s da rede, e depois retrocedemos (retropropaga\u00e7\u00e3o) para determinar a contribui\u00e7\u00e3o do erro para o gradiente. Com o Dropout, esse processo \u00e9 modificado. Come\u00e7amos por eliminar aleatoriamente (e temporariamente) alguns dos neur\u00f4nios ocultos na rede, deixando os neur\u00f4nios de entrada e sa\u00edda intocados. Depois de fazer isso, terminaremos com uma rede da seguinte forma (observe as linhas tracejadas na figura abaixo). Note os neur\u00f4nios que foram temporariamente eliminados:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"rede2\" class=\"aligncenter size-full wp-image-656\" data-attachment-id=\"656\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?fit=310%2C324\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?fit=287%2C300\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?fit=310%2C324\" data-orig-size=\"310,324\" data-permalink=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/rede2-3/\" data-recalc-dims=\"1\" height=\"324\" sizes=\"(max-width: 310px) 100vw, 310px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?resize=310%2C324\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?w=310 310w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?resize=287%2C300 287w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/07/rede2.png?resize=200%2C209 200w\" width=\"310\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   N\u00f3s encaminhamos para frente a entrada x atrav\u00e9s da rede modificada, e depois retropropagamos o resultado, tamb\u00e9m atrav\u00e9s da rede modificada. Depois de fazer isso em um mini-lote de exemplos, atualizamos os pesos e vieses apropriados. Em seguida, repetimos o processo, primeiro restaurando os neur\u00f4nios removidos, depois escolhendo um novo subconjunto aleat\u00f3rio de neur\u00f4nios ocultos para excluir, estimando o gradiente para um mini-lote diferente e atualizando os pesos e vieses na rede.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao repetir esse processo v\u00e1rias vezes, nossa rede aprender\u00e1 um conjunto de pesos e vieses. Naturalmente, esses pesos e vieses ter\u00e3o sido aprendidos sob condi\u00e7\u00f5es em que parte dos neur\u00f4nios ocultos foram descartados. Quando realmente executamos a rede completa, isso significa que mais neur\u00f4nios ocultos estar\u00e3o ativos. Para compensar isso, reduzimos pela metade os pesos que saem dos neur\u00f4nios ocultos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esse procedimento de desist\u00eancia pode parecer estranho e ad-hoc. Por que esperamos que ajude com a regulariza\u00e7\u00e3o? Para explicar o que est\u00e1 acontecendo, gostaria que voc\u00ea parasse brevemente de pensar sobre o Dropout e, em vez disso, imagine o treinamento de redes neurais no modo padr\u00e3o (sem Dropout). Em particular, imagine que treinamos v\u00e1rias redes neurais diferentes, todas usando os mesmos dados de treinamento. \u00c9 claro que as redes podem n\u00e3o come\u00e7ar id\u00eanticas e, como resultado, ap\u00f3s o treinamento, elas podem, \u00e0s vezes, dar resultados diferentes. Quando isso acontece, podemos usar algum tipo de esquema de m\u00e9dia ou vota\u00e7\u00e3o para decidir qual sa\u00edda aceitar. Por exemplo, se n\u00f3s treinamos cinco redes, e tr\u00eas delas est\u00e3o classificando um d\u00edgito como um \u201c3\u201d, ent\u00e3o provavelmente \u00e9 um \u201c3\u201d. As outras duas redes provavelmente est\u00e3o cometendo um erro. Este tipo de esquema de m\u00e9dia \u00e9 frequentemente encontrado como uma maneira poderosa (embora requeira mais capacidade computacional) de reduzir o overfitting. A raz\u00e3o \u00e9 que as diferentes redes podem se sobrepor de diferentes maneiras e a m\u00e9dia pode ajudar a eliminar esse tipo de overfitting.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O que isso tem a ver com o Dropout? Heuristicamente, quando abandonamos diferentes conjuntos de neur\u00f4nios, \u00e9 como se estiv\u00e9ssemos treinando redes neurais diferentes. E assim, o procedimento de elimina\u00e7\u00e3o \u00e9 como calcular a m\u00e9dia dos efeitos de um grande n\u00famero de redes diferentes. As diferentes redes se adaptar\u00e3o de diferentes maneiras, e assim, esperan\u00e7osamente, o efeito l\u00edquido do Dropout ser\u00e1 reduzir o overfitting.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma explica\u00e7\u00e3o heur\u00edstica relacionada ao Dropout \u00e9 dada em um dos primeiros artigos a usar a t\u00e9cnica: \u201cEsta t\u00e9cnica reduz co-adapta\u00e7\u00f5es complexas de neur\u00f4nios, j\u00e1 que um neur\u00f4nio n\u00e3o pode confiar na presen\u00e7a de outros neur\u00f4nios em particular. \u00c9, portanto, for\u00e7ado a aprenda recursos mais robustos que s\u00e3o \u00fateis em conjunto com muitos subconjuntos aleat\u00f3rios diferentes dos outros neur\u00f4nios\u201d. Em outras palavras, se pensarmos em nossa rede como um modelo que est\u00e1 fazendo previs\u00f5es, ent\u00e3o podemos pensar no Dropout como uma forma de garantir que o modelo seja robusto para a perda de qualquer evid\u00eancia individual. Nesse ponto, \u00e9 um pouco semelhante \u00e0 Regulariza\u00e7\u00e3o L1 e L2, que tendem a reduzir os pesos e, assim, tornar a rede mais robusta para perder qualquer conex\u00e3o individual na rede.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Naturalmente, a verdadeira medida do Dropout \u00e9 que ele foi muito bem sucedido em melhorar o desempenho das redes neurais.\n  </span>\n  <span style=\"color: #000000;\">\n   O\n  </span>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1207.0580.pdf\" rel=\"noopener\" target=\"_blank\">\n    artigo original\n   </a>\n  </span>\n  ,\n  <span style=\"color: #000000;\">\n   introduzindo a t\u00e9cnica, aplicou-a a muitas tarefas diferentes. Para n\u00f3s, \u00e9 de particular interesse que eles aplicaram o Dropout na classifica\u00e7\u00e3o de d\u00edgitos MNIST, usando uma rede neural feedforward \u201cvanilla\u201d ao longo de linhas similares \u00e0quelas que estamos considerando. O documento observou que o melhor resultado que algu\u00e9m alcan\u00e7ou at\u00e9 aquele ponto usando tal arquitetura foi a precis\u00e3o de classifica\u00e7\u00e3o de 98,4% no conjunto de testes. Eles melhoraram isso para 98,7% de precis\u00e3o usando uma combina\u00e7\u00e3o de Dropout e uma forma modificada de Regulariza\u00e7\u00e3o L2. Da mesma forma, resultados impressionantes foram obtidos para muitas outras tarefas, incluindo problemas de reconhecimento de imagem e fala e processamento de linguagem natural. O Dropout tem sido especialmente \u00fatil no treinamento de redes grandes e profundas, nas quais o problema do overfitting \u00e9 frequentemente agudo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Dropout \u00e9 estudado em detalhes e na pr\u00e1tica no curso\n  </span>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n    Deep Learning II\n   </a>\n  </span>\n  <span style=\"color: #000000;\">\n   da Data Science Academy.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"noopener\" target=\"_blank\">\n    ImageNet Classification with Deep Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1207.0580.pdf\" rel=\"noopener\" target=\"_blank\">\n    Improving neural networks by preventing co-adaptation of feature detectors\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-654\" href=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-654\" href=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-654\" href=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-654\" href=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-23-como-funciona-o-dropout/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-654-5e0dd1220aed3\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=654&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-654-5e0dd1220aed3\" id=\"like-post-wrapper-140353593-654-5e0dd1220aed3\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "24": "<h1 class=\"entry-title\" id=\"capitulo-24\">\n Cap\u00edtulo 24 \u2013 Expandir Artificialmente os Dados de Treinamento\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vimos\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/\" rel=\"noopener\" target=\"_blank\">\n     anteriormente\n    </a>\n   </span>\n   que a precis\u00e3o da classifica\u00e7\u00e3o com o dataset MNIST caiu para porcentagens em torno de 80%, quando usamos apenas 1.000 imagens de treinamento. N\u00e3o \u00e9 de surpreender que isso aconte\u00e7a, uma vez que menos dados de treinamento significam que nossa rede ser\u00e1 exposta a menos varia\u00e7\u00f5es na forma como os seres humanos escrevem d\u00edgitos. Vamos tentar treinar nossa rede de 30 neur\u00f4nios ocultos com uma variedade de diferentes tamanhos de conjuntos de dados de treinamento, para ver como o desempenho varia. N\u00f3s treinaremos usando um tamanho de mini-lote de 10, uma taxa de aprendizado \u03b7 = 0,5, um par\u00e2metro de regulariza\u00e7\u00e3o \u03bb = 5.0 e a fun\u00e7\u00e3o de custo de entropia cruzada. Treinaremos por 30 \u00e9pocas quando o conjunto completo de dados de treinamento for usado e aumentaremos o n\u00famero de \u00e9pocas proporcionalmente quando conjuntos de treinamento menores forem usados. Para garantir que o fator de decaimento do peso (weight decay factor) permane\u00e7a o mesmo nos conjuntos de treinamento, usaremos um par\u00e2metro de regulariza\u00e7\u00e3o de \u03bb = 5.0 quando o conjunto de dados de treinamento completo for usado, e reduziremos proporcionalmente quando conjuntos de treinamento menores forem usados. Observe esse gr\u00e1fico:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"more_data\" class=\"aligncenter size-full wp-image-670\" data-attachment-id=\"670\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"more_data\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data.png?resize=690%2C521 690w\" width=\"815\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como voc\u00ea pode ver analisando o gr\u00e1fico acima, as precis\u00f5es de classifica\u00e7\u00e3o melhoram consideravelmente \u00e0 medida que usamos mais dados de treinamento. Presumivelmente, essa melhoria continuaria se houvesse mais dados dispon\u00edveis. \u00c9 claro que, olhando para o gr\u00e1fico acima, parece que estamos chegando perto da satura\u00e7\u00e3o. Suponha, no entanto, que refizemos o gr\u00e1fico com o tamanho do conjunto de treinamento plotado logaritmicamente:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"more_data_log\" class=\"aligncenter size-full wp-image-671\" data-attachment-id=\"671\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"more_data_log\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_log/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log.png?resize=690%2C521 690w\" width=\"815\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Parece claro que o gr\u00e1fico ainda est\u00e1 subindo em dire\u00e7\u00e3o aos 100% de precis\u00e3o. Isso sugere que, se us\u00e1ssemos muito mais dados de treinamento \u2013 digamos, milh\u00f5es ou at\u00e9 bilh\u00f5es de amostras de d\u00edgitos manuscritos, em vez de apenas 50.000, provavelmente ter\u00edamos um desempenho consideravelmente melhor, mesmo nessa rede muito pequena.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Obter mais dados de treinamento \u00e9 uma \u00f3tima ideia. Infelizmente, pode ser caro e nem sempre \u00e9 poss\u00edvel na pr\u00e1tica. No entanto, h\u00e1 outra t\u00e9cnica que pode funcionar quase t\u00e3o bem, que \u00e9 expandir artificialmente os dados de treinamento. Suponha, por exemplo, que tomemos uma imagem de treinamento MNIST, o d\u00edgito 5:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"more_data_5\" class=\"aligncenter wp-image-672 size-thumbnail\" data-attachment-id=\"672\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"more_data_5\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?fit=815%2C615\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?fit=300%2C226\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_5/\" data-recalc-dims=\"1\" height=\"150\" sizes=\"(max-width: 150px) 100vw, 150px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?resize=150%2C150\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?resize=150%2C150 150w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?resize=280%2C280 280w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_5.png?zoom=3&amp;resize=150%2C150 450w\" width=\"150\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   e rotacionamos por um pequeno \u00e2ngulo, digamos 15 graus:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"more_data_rotated_5\" class=\"aligncenter size-thumbnail wp-image-673\" data-attachment-id=\"673\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"more_data_rotated_5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_rotated_5/\" data-recalc-dims=\"1\" height=\"150\" sizes=\"(max-width: 150px) 100vw, 150px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?resize=150%2C150\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?resize=150%2C150 150w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?resize=280%2C280 280w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_rotated_5.png?zoom=3&amp;resize=150%2C150 450w\" width=\"150\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ainda \u00e9 reconhecivelmente o mesmo d\u00edgito. E ainda no n\u00edvel do pixel \u00e9 bem diferente de qualquer imagem atualmente nos dados de treinamento MNIST. \u00c9 poss\u00edvel que adicionar essa imagem aos dados de treinamento possa ajudar nossa rede a aprender mais sobre como classificar os d\u00edgitos. Al\u00e9m do mais, obviamente, n\u00e3o estamos limitados a adicionar apenas uma imagem. Podemos expandir nossos dados de treinamento fazendo muitas rota\u00e7\u00f5es pequenas de todas as imagens de treinamento MNIST e, em seguida, usando os dados de treinamento expandidos para melhorar o desempenho de nossa rede.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa t\u00e9cnica \u00e9 muito poderosa e tem sido amplamente usada. Vejamos alguns dos resultados de um\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://ieeexplore.ieee.org/document/1227801/\" rel=\"noopener\" target=\"_blank\">\n     artigo\n    </a>\n   </span>\n   que aplicou diversas varia\u00e7\u00f5es da t\u00e9cnica ao MNIST. Uma das arquiteturas de redes neurais que eles consideraram foi similar \u00e0s que estamos usando, uma rede feedforward com 800 neur\u00f4nios ocultos e usando a fun\u00e7\u00e3o de custo de entropia cruzada. Executando a rede com os dados de treinamento MNIST padr\u00e3o, eles obtiveram uma precis\u00e3o de classifica\u00e7\u00e3o de 98,4% em seu conjunto de testes. Eles ent\u00e3o expandiram os dados de treinamento, usando n\u00e3o apenas rota\u00e7\u00f5es, como descrevi acima, mas tamb\u00e9m traduzindo e distorcendo as imagens. Ao treinar no conjunto de dados expandido, aumentaram a precis\u00e3o de sua rede para 98,9%. Eles tamb\u00e9m experimentaram o que chamaram de \u201cdistor\u00e7\u00f5es el\u00e1sticas\u201d, um tipo especial de distor\u00e7\u00e3o de imagem destinada a emular as oscila\u00e7\u00f5es aleat\u00f3rias encontradas nos m\u00fasculos da m\u00e3o. Usando as distor\u00e7\u00f5es el\u00e1sticas para expandir os dados, eles alcan\u00e7aram uma precis\u00e3o ainda maior, 99,3%. Efetivamente, eles estavam ampliando a experi\u00eancia de sua rede, expondo-a ao tipo de varia\u00e7\u00f5es encontradas na caligrafia real. Caso queira aprender sobre estas t\u00e9cnicas, elas s\u00e3o estudadas em detalhes em\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/curso-visao-computacional-e-reconhecimento-de-imagens\" rel=\"noopener\" target=\"_blank\">\n     Vis\u00e3o Computacional e Reconhecimento de Imagens\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Varia\u00e7\u00f5es sobre essa t\u00e9cnica podem ser usadas para melhorar o desempenho em muitas tarefas de aprendizado, n\u00e3o apenas no reconhecimento de manuscrito. O princ\u00edpio geral \u00e9 expandir os dados de treinamento aplicando opera\u00e7\u00f5es que reflitam a varia\u00e7\u00e3o do mundo real. N\u00e3o \u00e9 dif\u00edcil pensar em maneiras de fazer isso. Suponha, por exemplo, que voc\u00ea esteja construindo uma rede neural para fazer o reconhecimento de fala. N\u00f3s humanos podemos reconhecer a fala mesmo na presen\u00e7a de distor\u00e7\u00f5es como ru\u00eddo de fundo e assim voc\u00ea pode expandir seus dados adicionando ru\u00eddo de fundo. Tamb\u00e9m podemos reconhecer a fala se ela estiver acelerada ou desacelerada. Ent\u00e3o, essa \u00e9 outra maneira de expandir os dados de treinamento. Essas t\u00e9cnicas nem sempre s\u00e3o usadas \u2013 por exemplo, em vez de expandir os dados de treinamento adicionando ru\u00eddo, pode ser mais eficiente limpar a entrada para a rede aplicando primeiro um filtro de redu\u00e7\u00e3o de ru\u00eddo. Ainda assim, vale a pena manter a ideia de expandir os dados de treinamento e buscar oportunidades para aplicar a t\u00e9cnica.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora voc\u00ea compreende melhor o poder do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=big-data-fundamentos\" rel=\"noopener\" target=\"_blank\">\n     Big Data\n    </a>\n   </span>\n   , pois com mais dados, em maior variedade e gerados em alta velocidade, conseguimos chegar a resultados nunca antes vistos em Intelig\u00eancia Artificial. Vamos ver novamente como a precis\u00e3o da nossa rede neural varia com o tamanho do conjunto de treinamento:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"more_data_log2\" class=\"aligncenter wp-image-674 size-full\" data-attachment-id=\"674\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"more_data_log2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_log2/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_log2.png?resize=690%2C521 690w\" width=\"815\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Suponha que, em vez de usar uma rede neural, usemos alguma outra t\u00e9cnica de aprendizado de m\u00e1quina para classificar os d\u00edgitos. Por exemplo, vamos tentar usar as m\u00e1quinas de vetores de suporte (SVMs). N\u00e3o se preocupe se voc\u00ea n\u00e3o estiver familiarizado com SVMs, n\u00e3o precisamos entender seus detalhes (caso queira aprender sobre SVMs, elas s\u00e3o estudadas em detalhes em\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/curso-machine-learning\" rel=\"noopener\" target=\"_blank\">\n     Machine Learning\n    </a>\n   </span>\n   ). Vamos usar o SVM fornecido pela biblioteca scikit-learn. Veja como o desempenho do SVM varia em fun\u00e7\u00e3o do tamanho do conjunto de treinamento. Eu tracei os resultados da rede neural tamb\u00e9m, para facilitar a compara\u00e7\u00e3o:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"more_data_comparison\" class=\"aligncenter wp-image-675 size-full\" data-attachment-id=\"675\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"more_data_comparison\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/more_data_comparison/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/more_data_comparison.png?resize=690%2C521 690w\" width=\"815\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Provavelmente, a primeira coisa que chama a aten\u00e7\u00e3o sobre esse gr\u00e1fico \u00e9 que nossa rede neural supera o SVM para cada tamanho de conjunto de treinamento. Isso \u00e9 bom, embora tenhamos usado as configura\u00e7\u00f5es prontas do SVM do scikit-learn, enquanto fizemos um bom trabalho customizando nossa rede neural. Um fato sutil, por\u00e9m interessante, sobre o gr\u00e1fico \u00e9 que, se treinarmos o SVM usando 50.000 imagens, ele ter\u00e1 melhor desempenho (94,48% de precis\u00e3o) do que a nossa rede neural quando treinado usando 5.000 imagens (precis\u00e3o de 93,24%).\n   <strong>\n    Em outras palavras, mais dados de treinamento podem, \u00e0s vezes, compensar diferen\u00e7as no algoritmo de aprendizado de m\u00e1quina usado\n   </strong>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Algo ainda mais interessante pode ocorrer. Suponha que estamos tentando resolver um problema usando dois algoritmos de aprendizado de m\u00e1quina, algoritmo A e algoritmo B. \u00c0s vezes acontece que o algoritmo A superar\u00e1 o algoritmo B com um conjunto de dados de treinamento, enquanto o algoritmo B superar\u00e1 o algoritmo A com um conjunto diferente de dados de treinamento. N\u00e3o vemos isso acima \u2013 seria necess\u00e1rio que os dois gr\u00e1ficos se cruzassem \u2013 mas a resposta correta \u00e0 pergunta \u201cO algoritmo A \u00e9 melhor que o algoritmo B?\u201d seria: \u201cQual o tamanho do conjunto de dados de treinamento que voc\u00ea est\u00e1 usando?\u201d\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Tudo isso \u00e9 uma precau\u00e7\u00e3o a ter em mente, tanto ao fazer o desenvolvimento quanto ao ler artigos de pesquisa. Muitos artigos concentram-se em encontrar novos truques para obter melhor desempenho em conjuntos de dados de refer\u00eancia padr\u00e3o. \u201cNossa t\u00e9cnica XPTO nos deu uma melhoria de X por cento no benchmark padr\u00e3o Y\u201d \u00e9 uma forma can\u00f4nica de alega\u00e7\u00e3o de pesquisa. Tais alega\u00e7\u00f5es s\u00e3o, com frequ\u00eancia, genuinamente interessantes, mas devem ser entendidas como aplic\u00e1veis \u200b\u200bapenas no contexto do conjunto de dados de treinamento espec\u00edfico usado. Imagine uma hist\u00f3ria alternativa na qual as pessoas que originalmente criaram o conjunto de dados de refer\u00eancia tinham uma concess\u00e3o de pesquisa maior. Eles podem ter usado o dinheiro extra para coletar mais dados de treinamento. \u00c9 perfeitamente poss\u00edvel que o \u201caprimoramento\u201d devido \u00e0 t\u00e9cnica de \u201cXPTO\u201d desapare\u00e7a em um conjunto maior de dados. Em outras palavras, a suposta melhoria pode ser apenas um acidente da hist\u00f3ria. A mensagem a ser retirada, especialmente em aplica\u00e7\u00f5es pr\u00e1ticas, \u00e9 que o que queremos \u00e9 melhores algoritmos e melhores dados de treinamento. N\u00e3o h\u00e1 problema em procurar algoritmos melhores, mas certifique-se de n\u00e3o estar se concentrando apenas em melhores algoritmos, excluindo a busca por mais ou melhores dados de treinamento.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com isso conclu\u00edmos nosso mergulho no overfitting e na regulariza\u00e7\u00e3o. Claro, voltaremos novamente ao assunto. Como j\u00e1 mencionamos v\u00e1rias vezes, o overfitting \u00e9 um grande problema nas redes neurais, especialmente \u00e0 medida que os computadores se tornam mais poderosos e temos a capacidade de treinar redes maiores. Como resultado, h\u00e1 uma necessidade premente de desenvolver t\u00e9cnicas poderosas de regulariza\u00e7\u00e3o para reduzir o overfitting, e esta \u00e9 uma \u00e1rea extremamente ativa de pesquisa.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   No pr\u00f3ximo cap\u00edtulo vamos tratar de um outro importante assunto: a inicializa\u00e7\u00e3o de pesos. At\u00e9 l\u00e1.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ieeexplore.ieee.org/document/1227801/\" rel=\"noopener\" target=\"_blank\">\n    Best practices for convolutional neural networks applied to visual document analysis\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://dl.acm.org/citation.cfm?doid=1073012.1073017\" rel=\"noopener\" target=\"_blank\">\n    Scaling to very very large corpora for natural language disambiguation\n   </a>\n  </span>\n </p>\n <p>\n  <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n   Gradient-Based Learning Applied to Document Recognition\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\" rel=\"noopener\" target=\"_blank\">\n    Understanding Activation Functions in Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Redes-Neurais-Princ%C3%ADpios-e-Pr%C3%A1tica-ebook/dp/B073QSG69Y/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=1516302804&amp;sr=1-1\" rel=\"noopener\" target=\"_blank\">\n    Redes Neurais, princ\u00edpios e pr\u00e1ticas\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"noopener\" target=\"_blank\">\n    ImageNet Classification with Deep Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1207.0580.pdf\" rel=\"noopener\" target=\"_blank\">\n    Improving neural networks by preventing co-adaptation of feature detectors\n   </a>\n  </span>\n </p>\n <p>\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-669\" href=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-669\" href=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-669\" href=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-669\" href=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-24-expandir-artificialmente-os-dados-de-treinamento/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-669-5e0dd1243870c\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=669&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-669-5e0dd1243870c\" id=\"like-post-wrapper-140353593-669-5e0dd1243870c\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "25": "<h1 class=\"entry-title\" id=\"capitulo-25\">\n Cap\u00edtulo 25 \u2013 Inicializa\u00e7\u00e3o de Pesos em Redes Neurais Artificiais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Quando criamos nossas redes neurais, temos que fazer escolhas para os valores iniciais de pesos e vieses (bias). At\u00e9 agora, n\u00f3s os escolhemos de acordo com uma prescri\u00e7\u00e3o que discutimos nos\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/\" rel=\"noopener\" target=\"_blank\">\n     cap\u00edtulos anteriores\n    </a>\n   </span>\n   . S\u00f3 para lembrar, a prescri\u00e7\u00e3o era escolher tanto os pesos quanto os vieses usando vari\u00e1veis aleat\u00f3rias Gaussianas independentes, normalizadas para ter a m\u00e9dia 0 e desvio padr\u00e3o 1 (esse \u00e9 um conceito fundamental em Estat\u00edstica e caso queira adquirir conhecimento em Estat\u00edstica, confira nossa mais nova Forma\u00e7\u00e3o:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n   ).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Embora esta abordagem tenha funcionado bem, foi bastante\n   <em>\n    ad-hoc\n   </em>\n   , e vale a pena revisitar para ver se podemos encontrar uma maneira melhor de definir nossos pesos e vieses iniciais, e talvez ajudar nossas redes neurais a aprender mais r\u00e1pido. \u00c9 o que iremos estudar neste cap\u00edtulo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para come\u00e7ar, vamos compreender porque podemos fazer um pouco melhor do que inicializar pesos e vieses com valores Gaussianos normalizados. Para ver porque, suponha que estamos trabalhando com uma rede com um grande n\u00famero \u2013 digamos 1.000 \u2013 de neur\u00f4nios de entrada. E vamos supor que usamos valores Gaussianos normalizados para inicializar os pesos conectados \u00e0 primeira camada oculta. Por enquanto, vou me concentrar especificamente nos pesos que conectam os neur\u00f4nios de entrada ao primeiro neur\u00f4nio na camada oculta e ignorar o restante da rede:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede\" class=\"aligncenter wp-image-692 size-full\" data-attachment-id=\"692\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?fit=268%2C294\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?fit=268%2C294\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?fit=268%2C294\" data-orig-size=\"268,294\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede-5/\" data-recalc-dims=\"1\" height=\"294\" sizes=\"(max-width: 268px) 100vw, 268px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?resize=268%2C294\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?w=268 268w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede-1.png?resize=200%2C219 200w\" width=\"268\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos supor, por simplicidade, que estamos tentando treinar usando uma entrada de treinamento x na qual metade dos neur\u00f4nios de entrada est\u00e3o ativados, isto \u00e9, configurados para 1, e metade dos neur\u00f4nios de entrada est\u00e3o desligados, ou seja, ajustados para 0. O argumento a seguir aplica-se de forma mais geral, mas voc\u00ea obter\u00e1 a ess\u00eancia deste caso especial. Vamos considerar a soma ponderada\n   <strong>\n    z = \u2211jwjxj + b\n   </strong>\n   de entradas para nosso neur\u00f4nio oculto. Ocorre que 500 termos nesta soma desaparecem, porque a entrada correspondente xj \u00e9 zero e, assim, z \u00e9 uma soma sobre um total de 501 vari\u00e1veis aleat\u00f3rias Gaussianas normalizadas, representando os 500 termos de peso e o termo extra de vi\u00e9s (bias). Logo, z \u00e9 ele pr\u00f3prio uma distribui\u00e7\u00e3o Gaussiana com m\u00e9dia zero e desvio padr\u00e3o \u2248 22.4 (raiz quadrada de 501). Ou seja, z tem uma distribui\u00e7\u00e3o Gaussiana muito ampla, sem um pico agudo, conforme a figura abaixo:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede2\" class=\"aligncenter wp-image-693 size-full\" data-attachment-id=\"693\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?fit=593%2C144\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?fit=300%2C73\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?fit=593%2C144\" data-orig-size=\"593,144\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede2-4/\" data-recalc-dims=\"1\" height=\"144\" sizes=\"(max-width: 593px) 100vw, 593px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?resize=593%2C144\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?w=593 593w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?resize=300%2C73 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede2.png?resize=200%2C49 200w\" width=\"593\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em particular, podemos ver neste gr\u00e1fico que \u00e9 bem prov\u00e1vel que | z | ser\u00e1 bastante grande, isto \u00e9, z &gt; 1 ou z &lt; -1. Se for esse o caso, a sa\u00edda \u03c3(z) do neur\u00f4nio oculto estar\u00e1 muito pr\u00f3xima de 1 ou 0. Isso significa que nosso neur\u00f4nio oculto ter\u00e1 saturado. E quando isso acontece, como sabemos, fazer pequenas mudan\u00e7as nos pesos far\u00e1 apenas mudan\u00e7as absolutamente min\u00fasculas na ativa\u00e7\u00e3o de nosso neur\u00f4nio oculto. Essa mudan\u00e7a min\u00fascula na ativa\u00e7\u00e3o do neur\u00f4nio oculto, por sua vez, dificilmente afetar\u00e1 o resto dos neur\u00f4nios na rede, e veremos uma mudan\u00e7a min\u00fascula correspondente na fun\u00e7\u00e3o de custo. Como resultado, esses pesos s\u00f3 aprender\u00e3o muito lentamente quando usarmos o algoritmo de descida do gradiente. \u00c9 semelhante ao problema que discutimos anteriormente em outros cap\u00edtulos, no qual os neur\u00f4nios de sa\u00edda que saturaram o valor errado fizeram com que o aprendizado diminu\u00edsse. Abordamos esse problema anterior com uma escolha inteligente de fun\u00e7\u00e3o de custo. Infelizmente, enquanto isso ajudou com os neur\u00f4nios de\n   <strong>\n    sa\u00edda\n   </strong>\n   saturados, ele n\u00e3o faz nada pelo problema dos neur\u00f4nios\n   <strong>\n    ocultos\n   </strong>\n   saturados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Temos falado sobre a entrada de pesos para a primeira camada oculta. Naturalmente, argumentos semelhantes aplicam-se tamb\u00e9m a camadas ocultas posteriores: se os pesos em camadas ocultas posteriores forem inicializados usando Gaussianos normalizados, ent\u00e3o as ativa\u00e7\u00f5es estar\u00e3o frequentemente muito pr\u00f3ximas de 0 ou 1, e o aprendizado prosseguir\u00e1 muito lentamente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Existe alguma maneira de escolhermos melhores inicializa\u00e7\u00f5es para os pesos e vieses, para que n\u00e3o tenhamos esse tipo de satura\u00e7\u00e3o e, assim, evitar uma desacelera\u00e7\u00e3o na aprendizagem? Suponha que tenhamos um neur\u00f4nio com pesos de entrada\n   <strong>\n    nin\n   </strong>\n   . Ent\u00e3o, inicializaremos esses pesos como vari\u00e1veis \u200b\u200baleat\u00f3rias gaussianas com m\u00e9dia 0 e desvio padr\u00e3o:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form\" class=\"aligncenter wp-image-704 size-full\" data-recalc-dims=\"1\" height=\"31\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?resize=62%2C31\" width=\"62\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isto \u00e9, vamos \u201cesmagar os gaussianos\u201d, tornando menos prov\u00e1vel que nosso neur\u00f4nio seja saturado. Continuaremos a escolher o vi\u00e9s como um Gaussiano com m\u00e9dia 0 e desvio padr\u00e3o 1, por motivos pelos quais voltaremos daqui a pouco. Com essas escolhas, a soma ponderada\n   <strong>\n    z = \u2211jwjxj + b\n   </strong>\n   ser\u00e1 novamente uma vari\u00e1vel aleat\u00f3ria Gaussiana com m\u00e9dia 0, mas ser\u00e1 muito mais aguda que antes. Suponha, como fizemos anteriormente, que 500 das entradas s\u00e3o zero e 500 s\u00e3o 1. Ent\u00e3o \u00e9 f\u00e1cil mostrar (veja o gr\u00e1fico abaixo) que z tem uma distribui\u00e7\u00e3o Gaussiana com m\u00e9dia 0 e desvio padr\u00e3o igual a 1,22\u2026(raiz quadrada de 3/2). Isso \u00e9 muito mais agudo do que antes, tanto que at\u00e9 o gr\u00e1fico abaixo subestima a situa\u00e7\u00e3o, j\u00e1 que precisamos redimensionar o eixo vertical, quando comparado ao gr\u00e1fico anterior:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede3\" class=\"aligncenter size-full wp-image-694\" data-attachment-id=\"694\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?fit=592%2C381\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?fit=300%2C193\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?fit=592%2C381\" data-orig-size=\"592,381\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede3-2/\" data-recalc-dims=\"1\" height=\"381\" sizes=\"(max-width: 592px) 100vw, 592px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?resize=592%2C381\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?w=592 592w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?resize=300%2C193 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede3.png?resize=200%2C129 200w\" width=\"592\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c9 muito menos prov\u00e1vel que tal neur\u00f4nio sature e, correspondentemente, \u00e9 muito menos prov\u00e1vel que tenha problemas com a lentid\u00e3o do aprendizado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Eu afirmei acima que n\u00f3s continuaremos a inicializar os vieses como antes, como vari\u00e1veis \u200b\u200baleat\u00f3rias Gaussianas com uma m\u00e9dia de 0 e um desvio padr\u00e3o de 1. Isto n\u00e3o tem problema, pois \u00e9 pouco prov\u00e1vel que nossos neur\u00f4nios v\u00e3o saturar. Na verdade, n\u00e3o importa muito como inicializamos os vieses, desde que evitemos o problema com a satura\u00e7\u00e3o dos neur\u00f4nios. Algumas pessoas v\u00e3o t\u00e3o longe a ponto de inicializar todos os vieses com 0, e dependem da descida de gradiente para aprender vieses apropriados. Mas como \u00e9 improv\u00e1vel que fa\u00e7a muita diferen\u00e7a, continuaremos com o mesmo procedimento de inicializa\u00e7\u00e3o de antes.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos comparar os resultados para as nossas abordagens antiga e nova para inicializa\u00e7\u00e3o de peso, usando a tarefa de classifica\u00e7\u00e3o de d\u00edgitos MNIST. Como antes, usaremos 30 neur\u00f4nios ocultos, um tamanho de mini-lote de 10, um par\u00e2metro de regulariza\u00e7\u00e3o \u03bb = 5.0 e a fun\u00e7\u00e3o de custo de entropia cruzada. Diminuiremos ligeiramente a taxa de aprendizado de \u03b7 = 0,5 para 0,1, pois isso torna os resultados um pouco mais vis\u00edveis nos gr\u00e1ficos. Podemos treinar usando o antigo m\u00e9todo de inicializa\u00e7\u00e3o de peso (o c\u00f3digo pode ser encontrado no reposit\u00f3rio deste livro no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr\" rel=\"noopener\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   ):\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede4\" class=\"aligncenter size-full wp-image-695\" data-attachment-id=\"695\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?fit=607%2C185\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?fit=300%2C91\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?fit=607%2C185\" data-orig-size=\"607,185\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede4/\" data-recalc-dims=\"1\" height=\"185\" sizes=\"(max-width: 607px) 100vw, 607px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?resize=607%2C185\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?w=607 607w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?resize=300%2C91 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede4.png?resize=200%2C61 200w\" width=\"607\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Tamb\u00e9m podemos treinar usando a nova abordagem para inicializar o peso. Na verdade, isso \u00e9 ainda mais f\u00e1cil, j\u00e1 que a maneira padr\u00e3o de inicializar os pesos da rede2 \u00e9 usar essa nova abordagem. Isso significa que podemos omitir a chamada net.large_weight_initializer () acima:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede5\" class=\"aligncenter size-full wp-image-696\" data-attachment-id=\"696\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?fit=609%2C86\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?fit=300%2C42\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?fit=609%2C86\" data-orig-size=\"609,86\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede5/\" data-recalc-dims=\"1\" height=\"86\" sizes=\"(max-width: 609px) 100vw, 609px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?resize=609%2C86\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?w=609 609w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?resize=300%2C42 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede5.png?resize=200%2C28 200w\" width=\"609\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Plotando os resultados, obtemos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede6\" class=\"aligncenter size-full wp-image-697\" data-attachment-id=\"697\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede6\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?fit=454%2C379\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?fit=300%2C250\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?fit=454%2C379\" data-orig-size=\"454,379\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede6/\" data-recalc-dims=\"1\" height=\"379\" sizes=\"(max-width: 454px) 100vw, 454px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?resize=454%2C379\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?w=454 454w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?resize=300%2C250 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede6.png?resize=200%2C167 200w\" width=\"454\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em ambos os casos, acabamos com uma precis\u00e3o de classifica\u00e7\u00e3o um pouco acima de 96%. A precis\u00e3o final da classifica\u00e7\u00e3o \u00e9 quase exatamente a mesma nos dois casos, mas a nova t\u00e9cnica de inicializa\u00e7\u00e3o \u00e9 muito, muito mais r\u00e1pida. No final da primeira \u00e9poca de treinamento, a antiga abordagem de inicializa\u00e7\u00e3o de peso tem uma precis\u00e3o de classifica\u00e7\u00e3o abaixo de 87%, enquanto a nova abordagem j\u00e1 chega a quase 93%. O que parece estar acontecendo \u00e9 que nossa nova abordagem para a inicializa\u00e7\u00e3o do peso nos leva a um processo muito melhor, o que nos permite obter bons resultados muito mais rapidamente. O mesmo fen\u00f4meno tamb\u00e9m \u00e9 visto se tra\u00e7armos resultados com 100 neur\u00f4nios ocultos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede7\" class=\"aligncenter size-full wp-image-698\" data-attachment-id=\"698\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede7\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?fit=461%2C388\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?fit=300%2C252\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?fit=461%2C388\" data-orig-size=\"461,388\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/rede7/\" data-recalc-dims=\"1\" height=\"388\" sizes=\"(max-width: 461px) 100vw, 461px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?resize=461%2C388\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?w=461 461w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?resize=300%2C252 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/rede7.png?resize=200%2C168 200w\" width=\"461\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste caso, as duas curvas n\u00e3o se encontram. No entanto, nossas experi\u00eancias sugerem que, com apenas mais algumas \u00e9pocas de treinamento (n\u00e3o mostradas), as precis\u00f5es se tornam quase exatamente as mesmas. Portanto, com base nesses experimentos, parece que a inicializa\u00e7\u00e3o do peso aprimorado apenas acelera o aprendizado, n\u00e3o altera o desempenho final de nossas redes. No entanto, veremos mais a frente alguns exemplos de redes neurais em que o comportamento de longo prazo \u00e9 significativamente melhor com a inicializa\u00e7\u00e3o de peso usando:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form\" class=\"aligncenter size-full wp-image-704\" data-attachment-id=\"704\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?fit=62%2C31\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?fit=62%2C31\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?fit=62%2C31\" data-orig-size=\"62,31\" data-permalink=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/form-6/\" data-recalc-dims=\"1\" height=\"31\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/form.png?resize=62%2C31\" width=\"62\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim, n\u00e3o \u00e9 apenas a velocidade de aprendizado que \u00e9 melhorada, mas tamb\u00e9m o desempenho final.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A abordagem acima para a inicializa\u00e7\u00e3o do peso ajuda a melhorar a maneira como nossas redes neurais aprendem. Outras t\u00e9cnicas para inicializa\u00e7\u00e3o de peso tamb\u00e9m foram propostas, muitas baseadas nessa ideia b\u00e1sica. N\u00e3o vamos rever as outras abordagens aqui, j\u00e1 que a descrita anteriormente funciona bem o suficiente para nossos prop\u00f3sitos. Se voc\u00ea estiver interessado em pesquisar mais, recomendamos a leitura das p\u00e1ginas 14 e 15 de um artigo de 2012 de Yoshua Bengio (um dos padrinhos do Deep Learning), bem como as refer\u00eancias nele contidas:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n   . Nos cursos\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-i\" rel=\"noopener\" target=\"_blank\">\n     Deep Learning I\n    </a>\n   </span>\n   e\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   esse tema tamb\u00e9m \u00e9 estudado em detalhes.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-690\" href=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-690\" href=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-690\" href=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-690\" href=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/inicializacao-de-pesos-em-redes-neurais-artificiais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-690-5e0dd1264b84b\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=690&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-690-5e0dd1264b84b\" id=\"like-post-wrapper-140353593-690-5e0dd1264b84b\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "26": "<h1 class=\"entry-title\" id=\"capitulo-26\">\n Cap\u00edtulo 26 \u2013 Como Escolher os Hiperpar\u00e2metros de Uma Rede Neural\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 agora n\u00e3o explicamos como foram escolhidos os valores dos hiperpar\u00e2metros como a taxa de aprendizado, \u03b7, o par\u00e2metro de regulariza\u00e7\u00e3o, \u03bb e assim por diante. Fornecemos valores que funcionaram muito bem, mas, na pr\u00e1tica, quando voc\u00ea est\u00e1 usando redes neurais para resolver um problema, pode ser dif\u00edcil encontrar bons par\u00e2metros. Neste cap\u00edtulo, come\u00e7amos nosso estudo sobre\u00a0Como Escolher os Hiperpar\u00e2metros de Uma Rede Neural. Vamos come\u00e7ar?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Imagine, por exemplo, que acabamos de ser apresentados ao dataset MNIST e come\u00e7amos a trabalhar nele, sem saber nada sobre quais hiperpar\u00e2metros usar. Vamos supor que, por sorte, em nossos primeiros experimentos, escolhemos muitos dos hiperpar\u00e2metros da mesma forma como foi feito nos\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/\" rel=\"noopener\" target=\"_blank\">\n     cap\u00edtulos anteriores\n    </a>\n   </span>\n   : 30 neur\u00f4nios ocultos, um tamanho de mini-lote de 10, treinando por 30 \u00e9pocas usando a entropia cruzada. Mas escolhemos uma taxa de aprendizado \u03b7 = 10.0 e o par\u00e2metro de regulariza\u00e7\u00e3o \u03bb = 1000.0. Aqui est\u00e1 um exemplo de execu\u00e7\u00e3o da rede (o script est\u00e1 dispon\u00edvel no reposit\u00f3rio do livro no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr\" rel=\"noopener\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   )\n  </span>\n  <span style=\"color: #000000;\">\n   :\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"net1\" class=\"aligncenter size-full wp-image-720\" data-attachment-id=\"720\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"net1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?fit=611%2C531\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?fit=300%2C261\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?fit=611%2C531\" data-orig-size=\"611,531\" data-permalink=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net1/\" data-recalc-dims=\"1\" height=\"531\" sizes=\"(max-width: 611px) 100vw, 611px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?resize=611%2C531\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?w=611 611w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?resize=300%2C261 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net1.png?resize=200%2C174 200w\" width=\"611\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nossas precis\u00f5es de classifica\u00e7\u00e3o n\u00e3o s\u00e3o melhores do que o acaso! Nossa rede est\u00e1 agindo como um gerador de ru\u00eddo aleat\u00f3rio!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u201cBem, isso \u00e9 f\u00e1cil de consertar\u201d, voc\u00ea pode dizer, \u201capenas diminua a taxa de aprendizado e os hiperpar\u00e2metros de regulariza\u00e7\u00e3o\u201d. Infelizmente, voc\u00ea n\u00e3o sabe a priori quais s\u00e3o os hiperpar\u00e2metros que voc\u00ea precisa ajustar. Talvez o verdadeiro problema seja que nossa rede de neur\u00f4nios ocultos nunca funcionar\u00e1 bem, n\u00e3o importa como os outros hiperpar\u00e2metros sejam escolhidos? Talvez realmente precisemos de pelo menos 100 neur\u00f4nios ocultos? Ou 300 neur\u00f4nios ocultos? Ou v\u00e1rias camadas ocultas? Ou uma abordagem diferente para codificar a sa\u00edda? Talvez nossa rede esteja aprendendo, mas precisamos treinar em mais \u00e9pocas? Talvez os mini-lotes sejam pequenos demais? Talvez seja melhor voltarmos para a fun\u00e7\u00e3o de custo quadr\u00e1tico? Talvez precisemos tentar uma abordagem diferente para inicializar o peso? E assim por diante. Se fosse f\u00e1cil, n\u00e3o precisar\u00edamos de um\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Cientista de Dados\n    </a>\n   </span>\n   , n\u00e3o \u00e9 verdade?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c9 f\u00e1cil sentir-se perdido com tantas escolhas e combina\u00e7\u00f5es poss\u00edveis para os hiperpar\u00e2metros. Isso pode ser particularmente frustrante se sua rede for muito grande ou usar muitos dados de treinamento, pois voc\u00ea pode treinar por horas, dias ou semanas, apenas para n\u00e3o obter resultados. Se a situa\u00e7\u00e3o persistir, prejudicar\u00e1 sua confian\u00e7a. Talvez as redes neurais sejam a abordagem errada para o seu problema? Talvez voc\u00ea devesse largar o emprego e trabalhar com a apicultura?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nos pr\u00f3ximos cap\u00edtulos, explicaremos algumas heur\u00edsticas que podem ser usadas para definir os hiperpar\u00e2metros em uma rede neural. O objetivo \u00e9 ajud\u00e1-lo a desenvolver um fluxo que permita que voc\u00ea fa\u00e7a um bom trabalho definindo hiperpar\u00e2metros. Claro, n\u00e3o vamos cobrir tudo sobre otimiza\u00e7\u00e3o de hiperpar\u00e2metros. Esse \u00e9 um assunto enorme, e n\u00e3o \u00e9, de qualquer forma, um problema que j\u00e1 est\u00e1 completamente resolvido, nem existe um acordo universal entre os profissionais sobre as estrat\u00e9gias corretas a serem usadas. H\u00e1 sempre mais um truque que voc\u00ea pode tentar para obter um pouco mais de desempenho da sua rede. Mas temos algumas heur\u00edsticas com as quais podemos come\u00e7ar.\n  </span>\n </p>\n <p>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Compreendendo a Situa\u00e7\u00e3o \u2013 Estrat\u00e9gia Geral\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao usar redes neurais para atacar um novo problema, o primeiro desafio \u00e9 obter qualquer aprendizado n\u00e3o-trivial, ou seja, para que a rede obtenha resultados melhores que o acaso. Isso pode ser surpreendentemente dif\u00edcil, especialmente ao confrontar uma nova classe de problemas. Vejamos algumas estrat\u00e9gias que voc\u00ea pode usar se tiver esse tipo de problema.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Suponha, por exemplo, que voc\u00ea esteja atacando o MNIST pela primeira vez. Voc\u00ea come\u00e7a entusiasmado, mas fica um pouco desanimado quando sua primeira rede falha completamente, como no exemplo acima. O caminho a percorrer \u00e9 reduzir o tamanho do problema. Livre-se de todas as imagens de treinamento e valida\u00e7\u00e3o, exceto imagens de 0s ou 1s. Em seguida, tente treinar uma rede para distinguir 0s de 1s. N\u00e3o s\u00f3 isso \u00e9 um problema inerentemente mais f\u00e1cil do que distinguir todos os dez d\u00edgitos, como tamb\u00e9m reduz a quantidade de dados de treinamento em 80%, acelerando o treinamento por um fator de 5. Isso permite experimenta\u00e7\u00f5es muito mais r\u00e1pidas e, portanto, fornece uma vis\u00e3o mais r\u00e1pida sobre como construir uma boa rede.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea pode acelerar ainda mais a experimenta\u00e7\u00e3o, desmembrando sua rede na rede mais simples, provavelmente fazendo aprendizado significativo. Se voc\u00ea acredita que uma rede [784, 10] provavelmente faz uma classifica\u00e7\u00e3o melhor que o acaso com o dataset de d\u00edgitos MNIST, ent\u00e3o comece sua experimenta\u00e7\u00e3o com essa rede. Vai ser muito mais r\u00e1pido do que treinar uma rede [784, 30, 10], e voc\u00ea pode \u201cfalhar\u201d mais r\u00e1pido (este \u00e9 um conceito muito comum nos EUA: \u201cfail fast\u201d, ou seja, cometa falhas o mais r\u00e1pido poss\u00edvel e aprenda com elas. N\u00e3o se preocupe em tentar atingir a perfei\u00e7\u00e3o, pois voc\u00ea n\u00e3o vai conseguir de qualquer forma).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea pode acelerar mais na experimenta\u00e7\u00e3o aumentando a frequ\u00eancia de monitoramento. No network2.py, monitoramos o desempenho no final de cada \u00e9poca de treinamento. Com 50.000 imagens por \u00e9poca, isso significa esperar um pouco \u2013 cerca de dez segundos por \u00e9poca, no meu laptop, ao treinar uma rede [784, 30, 10] \u2013 antes de obter feedback sobre o quanto a rede est\u00e1 aprendendo. \u00c9 claro que dez segundos n\u00e3o s\u00e3o muito longos, mas se voc\u00ea quiser testar dezenas de op\u00e7\u00f5es de hiperpar\u00e2metros, \u00e9 irritante, e se voc\u00ea quiser testar centenas ou milhares de op\u00e7\u00f5es, isso come\u00e7a a ficar debilitante. Podemos obter feedback mais rapidamente, monitorando a precis\u00e3o da valida\u00e7\u00e3o com mais frequ\u00eancia, digamos, a cada 1.000 imagens de treinamento. Al\u00e9m disso, em vez de usar o conjunto completo de 10.000\u00a0imagens de valida\u00e7\u00e3o para monitorar o desempenho, podemos obter uma estimativa muito mais r\u00e1pida usando apenas 100 imagens de valida\u00e7\u00e3o. Tudo o que importa \u00e9 que a rede veja imagens suficientes para aprender de verdade e obter uma boa estimativa aproximada de desempenho. Claro, nosso programa network2.py atualmente n\u00e3o faz esse tipo de monitoramento. Mas, como um cl\u00edmax para obter um efeito semelhante para fins de ilustra\u00e7\u00e3o, vamos reduzir nossos dados de treinamento para apenas as primeiras 1.000 imagens de treinamento MNIST. Vamos tentar e ver o que acontece. (Para manter o c\u00f3digo abaixo simples, n\u00e3o implementei a ideia de usar apenas imagens 0 e 1. Claro, isso pode ser feito com um pouco mais de trabalho).\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"net2\" class=\"aligncenter size-full wp-image-721\" data-attachment-id=\"721\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"net2\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?fit=615%2C268\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?fit=300%2C131\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?fit=615%2C268\" data-orig-size=\"615,268\" data-permalink=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net2/\" data-recalc-dims=\"1\" height=\"268\" sizes=\"(max-width: 615px) 100vw, 615px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?resize=615%2C268\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?w=615 615w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?resize=300%2C131 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net2.png?resize=200%2C87 200w\" width=\"615\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ainda estamos recebendo puro ru\u00eddo! Mas h\u00e1 uma grande vit\u00f3ria: agora estamos obtendo feedback em uma fra\u00e7\u00e3o de segundo, em vez de uma vez a cada dez segundos. Isso significa que voc\u00ea pode experimentar mais rapidamente outras op\u00e7\u00f5es de hiperpar\u00e2metros, ou at\u00e9 mesmo conduzir experimentos testando muitas op\u00e7\u00f5es diferentes de hiperpar\u00e2metros quase simultaneamente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No exemplo acima, eu deixamos \u03bb como \u03bb = 1000.0, como usamos anteriormente. Mas como mudamos o n\u00famero de exemplos de treinamento, dever\u00edamos realmente mudar \u03bb para manter\n   <em>\n    weight decay\n   </em>\n   o mesmo. Isso significa mudar \u03bb para 20.0. Se fizermos isso, ent\u00e3o \u00e9 o que acontece:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"net3\" class=\"aligncenter size-full wp-image-722\" data-attachment-id=\"722\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"net3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?fit=618%2C335\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?fit=300%2C163\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?fit=618%2C335\" data-orig-size=\"618,335\" data-permalink=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net3/\" data-recalc-dims=\"1\" height=\"335\" sizes=\"(max-width: 618px) 100vw, 618px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?resize=618%2C335\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?w=618 618w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?resize=300%2C163 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net3.png?resize=200%2C108 200w\" width=\"618\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ah! N\u00f3s temos um sinal. N\u00e3o \u00e9 um sinal muito bom, mas um sinal, no entanto. Isso \u00e9 algo que podemos construir, modificando os hiperpar\u00e2metros para tentar melhorar ainda mais. Talvez n\u00f3s achemos que nossa taxa de aprendizado precisa ser maior. (Como voc\u00ea talvez perceba, \u00e9 um palpite bobo, por raz\u00f5es que discutiremos em breve, mas chegaremos l\u00e1. N\u00e3o existe atalho para o aprendizado). Ent\u00e3o, para testar nosso palpite, tentamos alterar \u03b7 at\u00e9 100.0:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"net4\" class=\"aligncenter size-full wp-image-723\" data-attachment-id=\"723\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"net4\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?fit=611%2C349\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?fit=300%2C171\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?fit=611%2C349\" data-orig-size=\"611,349\" data-permalink=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net4/\" data-recalc-dims=\"1\" height=\"349\" sizes=\"(max-width: 611px) 100vw, 611px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?resize=611%2C349\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?w=611 611w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?resize=300%2C171 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net4.png?resize=200%2C114 200w\" width=\"611\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso n\u00e3o \u00e9 bom, pois sugere que nosso palpite estava errado e o problema n\u00e3o era que a taxa de aprendizado fosse muito baixa. Ent\u00e3o, em vez disso, tentamos alterar \u03b7 para \u03b7 = 1.0:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"net5\" class=\"aligncenter size-full wp-image-724\" data-attachment-id=\"724\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"net5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?fit=611%2C349\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?fit=300%2C171\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?fit=611%2C349\" data-orig-size=\"611,349\" data-permalink=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/net5/\" data-recalc-dims=\"1\" height=\"349\" sizes=\"(max-width: 611px) 100vw, 611px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?resize=611%2C349\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?w=611 611w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?resize=300%2C171 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/08/net5.png?resize=200%2C114 200w\" width=\"611\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora ficou melhor! E assim podemos continuar, ajustando individualmente cada hiperpar\u00e2metro, melhorando gradualmente o desempenho. Uma vez feita a explora\u00e7\u00e3o para encontrar um valor melhor para \u03b7, seguimos para encontrar um bom valor para \u03bb. Em seguida, experimente uma arquitetura mais complexa, digamos uma rede com 10 neur\u00f4nios ocultos e ajuste os valores para \u03b7 e \u03bb novamente. Depois, aumente para 20 neur\u00f4nios ocultos e ent\u00e3o, ajuste outros hiperpar\u00e2metros um pouco mais e assim por diante, em cada est\u00e1gio avaliando o desempenho usando nossos dados de valida\u00e7\u00e3o e usando essas avalia\u00e7\u00f5es para encontrar melhores hiperpar\u00e2metros. Ao fazer isso, normalmente leva mais tempo para testemunhar o impacto devido a modifica\u00e7\u00f5es dos hiperpar\u00e2metros, e assim podemos diminuir gradualmente a frequ\u00eancia de monitoramento.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Tudo isso parece muito promissor como uma estrat\u00e9gia ampla. No entanto, quero voltar a esse est\u00e1gio inicial de encontrar hiperpar\u00e2metros que permitem que uma rede aprenda qualquer coisa. De fato, mesmo a discuss\u00e3o acima transmite uma perspectiva muito positiva. Pode ser extremamente frustrante trabalhar com uma rede que n\u00e3o est\u00e1 aprendendo nada. Voc\u00ea pode ajustar os hiperpar\u00e2metros por dias e ainda n\u00e3o obter uma resposta significativa. Por isso, gostaria de enfatizar novamente que, durante os primeiros est\u00e1gios, voc\u00ea deve se certificar de que pode obter um feedback r\u00e1pido dos experimentos. Intuitivamente, pode parecer que simplificar o problema e a arquitetura apenas ir\u00e1 atras\u00e1-lo. Na verdade, isso acelera as coisas, pois voc\u00ea encontra muito mais rapidamente uma rede com um sinal significativo. Uma vez que voc\u00ea tenha recebido tal sinal, muitas vezes voc\u00ea pode obter melhorias r\u00e1pidas aprimorando os hiperpar\u00e2metros. Assim como em tudo na vida, come\u00e7ar pode ser a coisa mais dif\u00edcil a se fazer.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ok, essa \u00e9 a estrat\u00e9gia geral. Vamos agora olhar algumas recomenda\u00e7\u00f5es espec\u00edficas para definir hiperpar\u00e2metros. Vou me concentrar na taxa de aprendizado, \u03b7, no par\u00e2metro de regulariza\u00e7\u00e3o L2, \u03bb e no tamanho do mini-lote. No entanto, muitas das observa\u00e7\u00f5es tamb\u00e9m se aplicam a outros hiperpar\u00e2metros, incluindo aqueles associados \u00e0 arquitetura de rede, outras formas de regulariza\u00e7\u00e3o e alguns hiperpar\u00e2metros que encontraremos mais adiante aqui no Deep Learning Book, como o coeficiente momentum.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Oh n\u00e3o! O cap\u00edtulo acabou! Fique tranquilo, continuamos no pr\u00f3ximo. At\u00e9 l\u00e1!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n </div>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-719\" href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-719\" href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-719\" href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-719\" href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-719-5e0dd12850e2c\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=719&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-719-5e0dd12850e2c\" id=\"like-post-wrapper-140353593-719-5e0dd12850e2c\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "27": "<h1 class=\"entry-title\" id=\"capitulo-27\">\n Cap\u00edtulo 27 \u2013 A Taxa de Aprendizado de Uma Rede Neural\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos continuar a discuss\u00e3o do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/\" rel=\"noopener\" target=\"_blank\">\n     cap\u00edtulo anterior\n    </a>\n   </span>\n   sobre a escolha dos hiperpar\u00e2metros de um modelo de rede neural, estudando um dos mais importantes, a taxa de aprendizado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Suponha que executemos tr\u00eas redes neurais artificiais sendo treinadas com o dataset MNIST com tr\u00eas taxas de aprendizado diferentes, \u03b7 = 0.025, \u03b7 = 0.25 e \u03b7 = 2.5, respectivamente. Vamos definir os outros hiperpar\u00e2metros de acordo com as experi\u00eancias nos cap\u00edtulos anteriores, executando mais de 30 epochs, com um tamanho de mini-lote de 10 e com \u03bb = 5.0. Tamb\u00e9m voltaremos a usar todas as 50.000 imagens de treinamento. Aqui est\u00e1 um gr\u00e1fico mostrando o comportamento do custo de treinamento enquanto treinamos:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"multiple_eta\" class=\"aligncenter size-full wp-image-742\" data-attachment-id=\"742\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"multiple_eta\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?fit=815%2C615\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?fit=300%2C226\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?fit=815%2C615\" data-orig-size=\"815,615\" data-permalink=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/multiple_eta/\" data-recalc-dims=\"1\" height=\"615\" sizes=\"(max-width: 815px) 100vw, 815px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=815%2C615\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?w=815 815w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=300%2C226 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=768%2C580 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=200%2C151 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/multiple_eta.png?resize=690%2C521 690w\" width=\"815\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com \u03b7 = 0.025, o custo diminui suavemente at\u00e9 a \u00e9poca final. Com \u03b7 = 0.25 o custo inicialmente diminui, mas ap\u00f3s cerca de 20 \u00e9pocas ele est\u00e1 pr\u00f3ximo da satura\u00e7\u00e3o, e da\u00ed em diante a maioria das mudan\u00e7as s\u00e3o meramente pequenas e aparentemente oscila\u00e7\u00f5es aleat\u00f3rias. Finalmente, com \u03b7 = 2.5, o custo faz grandes oscila\u00e7\u00f5es desde o in\u00edcio. Para entender o motivo das oscila\u00e7\u00f5es, lembre-se de que a descida estoc\u00e1stica\u00a0do gradiente supostamente nos levar\u00e1 gradualmente a um vale da fun\u00e7\u00e3o de custo (conforme explicado\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/\" rel=\"noopener\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   ):\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"taxa de aprendizado\" class=\"aligncenter size-full wp-image-743\" data-attachment-id=\"743\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"taxa de aprendizado\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?fit=555%2C409\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?fit=300%2C221\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?fit=555%2C409\" data-orig-size=\"555,409\" data-permalink=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/tikz33/\" data-recalc-dims=\"1\" height=\"409\" sizes=\"(max-width: 555px) 100vw, 555px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?resize=555%2C409\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?w=555 555w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?resize=300%2C221 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/tikz33.png?resize=200%2C147 200w\" width=\"555\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No entanto, se \u03b7 for muito grande, os passos ser\u00e3o t\u00e3o grandes que poder\u00e3o, na verdade, ultrapassar o m\u00ednimo, fazendo com que o algoritmo simplesmente fique perdido durante o treinamento. Isso \u00e9 provavelmente o que est\u00e1 causando a oscila\u00e7\u00e3o do custo quando \u03b7 = 2.5. Quando escolhemos \u03b7 = 0.25, os passos iniciais nos levam a um m\u00ednimo da fun\u00e7\u00e3o de custo, e \u00e9 s\u00f3 quando chegamos perto desse m\u00ednimo que come\u00e7amos a sofrer com o problema de\n   <em>\n    overshooting\n   </em>\n   . E quando escolhemos \u03b7 = 0.025, n\u00e3o sofremos este problema durante as primeiras 30 \u00e9pocas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Claro, escolher \u03b7 t\u00e3o pequeno cria outro problema, que reduz a velocidade da descida estoc\u00e1stica do gradiente, aumentando o tempo total de treinamento. Uma abordagem ainda melhor seria come\u00e7ar com \u03b7 = 0.25, treinar por 20 \u00e9pocas e ent\u00e3o mudar para \u03b7 = 0.025. Discutiremos essas tabelas de taxas de aprendizado vari\u00e1veis posteriormente. Por enquanto, por\u00e9m, vamos nos ater a descobrir como encontrar um \u00fanico valor bom para a taxa de aprendizado, \u03b7.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com esta imagem em mente, podemos definir \u03b7 da seguinte maneira. Primeiro, estimamos o valor limite para \u03b7 no qual o custo nos dados de treinamento come\u00e7a imediatamente a diminuir, em vez de oscilar ou aumentar. Essa estimativa n\u00e3o tem que ser muito precisa. Voc\u00ea pode estimar a ordem de magnitude come\u00e7ando com \u03b7 = 0.01. Se o custo diminuir durante as primeiras \u00e9pocas, ent\u00e3o voc\u00ea deve sucessivamente tentar \u03b7 = 0.1, 1.0,\u2026 at\u00e9 encontrar um valor para \u03b7 onde o custo oscile ou aumente durante as primeiras poucas \u00e9pocas (isso faz parte do trabalho de um\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Cientista de Dados\n    </a>\n   </span>\n   ).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Alternativamente, se o custo oscilar ou aumentar durante as primeiras \u00e9pocas, quando \u03b7 = 0.01, ent\u00e3o tente \u03b7 = 0.001 ,0.0001,\u2026 at\u00e9 encontrar um valor para \u03b7 onde o custo diminui durante as primeiras poucas \u00e9pocas. Seguindo este procedimento, obteremos uma estimativa da ordem de magnitude para o valor limite de \u03b7. Voc\u00ea pode, opcionalmente, refinar sua estimativa, para escolher o maior valor de \u03b7 no qual o custo diminui durante as primeiras poucas \u00e9pocas, digamos \u03b7 = 0.5 ou \u03b7 = 0.2 (n\u00e3o h\u00e1 necessidade de que isso seja super-preciso). Isso nos d\u00e1 uma estimativa para o valor limite de \u03b7. E claro, documente tudo!!!!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Obviamente, o valor real de \u03b7 que voc\u00ea usa n\u00e3o deve ser maior que o valor limite. De fato, se o valor de \u03b7 permanecer utiliz\u00e1vel ao longo de muitas \u00e9pocas, ent\u00e3o voc\u00ea provavelmente desejar\u00e1 usar um valor para \u03b7 que seja menor, digamos, um fator de dois abaixo do limite. Essa escolha normalmente permitir\u00e1 que voc\u00ea treine por muitas \u00e9pocas, sem causar muita lentid\u00e3o no aprendizado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No caso dos dados MNIST, seguir esta estrat\u00e9gia leva a uma estimativa de 0.1 para a ordem de magnitude do valor limite de \u03b7. Depois de um pouco mais de refinamento, obtemos um valor limite \u03b7 = 0.5. Seguindo a prescri\u00e7\u00e3o acima, isso sugere usar \u03b7 = 0.25 como nosso valor para a taxa de aprendizado. De fato, eu descobri que usar \u03b7 = 0.5 funcionava bem o suficiente em 30 \u00e9pocas que, na maioria das vezes, eu n\u00e3o me preocupava em usar um valor menor de \u03b7.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Tudo isso parece bastante simples. No entanto, usar o custo de treinamento para escolher \u03b7 parece contradizer o que dissemos anteriormente, que escolher\u00edamos os hiperpar\u00e2metros avaliando o desempenho usando nossos dados de valida\u00e7\u00e3o. Na verdade, usaremos a precis\u00e3o de valida\u00e7\u00e3o para escolher o hiperpar\u00e2metro de regulariza\u00e7\u00e3o, o tamanho do mini-lote e os par\u00e2metros de rede, como o n\u00famero de camadas e neur\u00f4nios ocultos, e assim por diante (estudaremos isso nos pr\u00f3ximos cap\u00edtulos).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por que as coisas diferem para a taxa de aprendizado? Francamente, essa escolha \u00e9 uma prefer\u00eancia est\u00e9tica pessoal e talvez seja um tanto idiossincr\u00e1tica. O racioc\u00ednio \u00e9 que os outros hiperpar\u00e2metros s\u00e3o destinados a melhorar a precis\u00e3o final da classifica\u00e7\u00e3o no conjunto de testes, e por isso faz sentido selecion\u00e1-los com base na precis\u00e3o da valida\u00e7\u00e3o. No entanto, a taxa de aprendizado \u00e9 apenas para influenciar a precis\u00e3o final da classifica\u00e7\u00e3o. Sua finalidade principal \u00e9 realmente controlar o tamanho da etapa na descida do gradiente e monitorar o custo do treinamento \u00e9 a melhor maneira de detectar se o tamanho da etapa \u00e9 muito grande. Com isso dito, essa \u00e9 uma prefer\u00eancia pessoal. No in\u00edcio, durante o aprendizado, o custo do treinamento geralmente diminui apenas se a precis\u00e3o da valida\u00e7\u00e3o melhorar e assim, na pr\u00e1tica, \u00e9 improv\u00e1vel que fa\u00e7a muita diferen\u00e7a em qual crit\u00e9rio voc\u00ea usa.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   No pr\u00f3ximo cap\u00edtulo tem mais. At\u00e9 l\u00e1!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-741\" href=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-741\" href=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-741\" href=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-741\" href=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-taxa-de-aprendizado-de-uma-rede-neural/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-741-5e0dd12a595c6\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=741&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-741-5e0dd12a595c6\" id=\"like-post-wrapper-140353593-741-5e0dd12a595c6\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "28": "<h1 class=\"entry-title\" id=\"capitulo-28\">\n Cap\u00edtulo 28 \u2013 Usando Early Stopping Para Definir o N\u00famero de \u00c9pocas de Treinamento\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao treinar redes neurais, v\u00e1rias decis\u00f5es precisam ser tomadas em rela\u00e7\u00e3o \u00e0s configura\u00e7\u00f5es (\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/capitulo-26-como-escolher-os-hiperparametros-de-uma-rede-neural/\" rel=\"noopener\" target=\"_blank\">\n     hiperpar\u00e2metros\n    </a>\n   </span>\n   ) usadas, a fim de obter um bom desempenho. Um desses hiperpar\u00e2metros \u00e9 o n\u00famero de \u00e9pocas de treinamento: ou seja, quantas passagens completas do conjunto de dados (\u00e9pocas) devem ser usadas? Se usarmos poucas \u00e9pocas, poderemos ter problemas de underfitting (ou seja, n\u00e3o aprender tudo o que pudermos com os dados de treinamento); se usarmos muitas \u00e9pocas, podemos ter o problema oposto, overfitting (\u201caprender demais\u201d, ou seja, ajustar o \u201cru\u00eddo\u201d nos dados de treinamento, e n\u00e3o o sinal).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Usamos o\u00a0Early Stopping (\u201cParada Antecipada\u201d ou \u201cParada Precoce\u201d) exatamente para tentar definir manualmente esse valor. Tamb\u00e9m pode ser considerado um tipo de m\u00e9todo de regulariza\u00e7\u00e3o (como L1/L2 weight decay e dropout estudados\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/capitulo-22-regularizacao-l1/\" rel=\"noopener\" target=\"_blank\">\n     anteriormente\n    </a>\n   </span>\n   aqui no livro), pois pode impedir o overfitting da rede neural. A imagem abaixo ajuda a definir claramente o que \u00e9 o\u00a0Early Stopping:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"earlystopping\" class=\"aligncenter size-full wp-image-759\" data-attachment-id=\"759\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"earlystopping\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?fit=800%2C450\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?fit=300%2C169\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?fit=800%2C450\" data-orig-size=\"800,450\" data-permalink=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/earlystopping/\" data-recalc-dims=\"1\" height=\"450\" sizes=\"(max-width: 800px) 100vw, 800px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=800%2C450\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=300%2C169 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=768%2C432 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=200%2C113 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/earlystopping.png?resize=690%2C388 690w\" width=\"800\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao treinar uma rede neural, geralmente se est\u00e1 interessado em obter uma rede com desempenho ideal de generaliza\u00e7\u00e3o. No entanto, todas as arquiteturas de rede neural padr\u00e3o, como o perceptron multicamada totalmente conectado, s\u00e3o propensas a overfitting. Enquanto a rede parece melhorar, isto \u00e9, o erro no conjunto de treinamento diminui, em algum momento durante o treinamento na verdade come\u00e7a a piorar novamente, ou seja, o erro em exemplos invis\u00edveis aumenta.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Normalmente, o erro de generaliza\u00e7\u00e3o \u00e9 estimado pelo erro de valida\u00e7\u00e3o, isto \u00e9, o erro m\u00e9dio em um conjunto de valida\u00e7\u00e3o, um conjunto fixo de exemplos que n\u00e3o s\u00e3o do conjunto de treino. Existem basicamente duas maneiras de combater o overfitting: reduzindo o n\u00famero de dimens\u00f5es do espa\u00e7o de par\u00e2metros ou reduzindo o tamanho efetivo de cada dimens\u00e3o. T\u00e9cnicas para reduzir o n\u00famero de par\u00e2metros s\u00e3o aprendizagem construtiva gananciosa, poda ou compartilhamento de peso. T\u00e9cnicas para reduzir o tamanho de cada dimens\u00e3o de par\u00e2metro s\u00e3o a regulariza\u00e7\u00e3o, como weight decay ou dropout, ou a Parada Precoce (Early Stopping). A parada precoce \u00e9 amplamente usada porque \u00e9 simples de entender e implementar e foi relatada como sendo superior aos m\u00e9todos de regulariza\u00e7\u00e3o em muitos casos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Usar Early Stopping significa que, no final de cada \u00e9poca, devemos calcular a precis\u00e3o da classifica\u00e7\u00e3o nos dados de valida\u00e7\u00e3o. Quando a precis\u00e3o parar de melhorar, terminamos o treinamento. Isso torna a configura\u00e7\u00e3o do n\u00famero de \u00e9pocas muito simples. Em particular, isso significa que n\u00e3o precisamos nos preocupar em descobrir explicitamente como o n\u00famero de \u00e9pocas depende dos outros hiperpar\u00e2metros, pois isso \u00e9 feito automaticamente. Al\u00e9m disso, a Parada Antecipada tamb\u00e9m impede automaticamente o overfitting. Isto \u00e9, obviamente, uma coisa boa, embora nos est\u00e1gios iniciais da experimenta\u00e7\u00e3o possa ser \u00fatil desligar a Parada Antecipada, para que voc\u00ea possa ver quaisquer sinais de overfitting e us\u00e1-los para definir sua abordagem de regulariza\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para implementar a Parada Antecipada, precisamos dizer com mais precis\u00e3o o que significa que a precis\u00e3o da classifica\u00e7\u00e3o parou de melhorar. Como j\u00e1 vimos, a precis\u00e3o pode se mover um pouco, mesmo quando a tend\u00eancia geral \u00e9 melhorar. Se pararmos pela primeira vez, a precis\u00e3o diminui, ent\u00e3o quase certamente pararemos quando houver mais melhorias a serem feitas. Uma regra melhor \u00e9 terminar se a melhor precis\u00e3o de classifica\u00e7\u00e3o n\u00e3o melhorar por algum tempo. Suponha, por exemplo, que estamos trabalhando com o dataset MNIST. Poder\u00edamos optar por terminar se a precis\u00e3o da classifica\u00e7\u00e3o n\u00e3o melhorou durante as \u00faltimas dez \u00e9pocas. Isso garante que n\u00e3o paremos cedo demais, em resposta \u00e0 m\u00e1 sorte no treinamento, mas tamb\u00e9m que n\u00e3o estamos esperando para sempre uma melhoria que nunca acontece.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esta regra de \u201cparar o treinamento se n\u00e3o melhorar em dez \u00e9pocas\u201d \u00e9 boa para a explora\u00e7\u00e3o inicial do MNIST. No entanto, as redes podem \u00e0s vezes estabilizar-se perto de uma determinada precis\u00e3o de classifica\u00e7\u00e3o por algum tempo, apenas para come\u00e7ar a melhorar novamente. Se voc\u00ea est\u00e1 tentando obter um desempenho realmente bom, a regra de \u201cparar o treinamento se n\u00e3o melhorar em dez \u00e9pocas\u201d pode ser muito agressiva. Nesse caso, sugerimos usar essa regra para a experimenta\u00e7\u00e3o inicial e, gradualmente, adotar regras mais brandas, conforme entender melhor a maneira como sua rede treina: sem melhoria em vinte \u00e9pocas, sem melhoria em cinquenta \u00e9pocas e assim por diante. Claro, isso introduz um novo hiperpar\u00e2metro para otimizar! Na pr\u00e1tica, no entanto, geralmente \u00e9 f\u00e1cil definir esse hiperpar\u00e2metro para obter bons resultados. Da mesma forma, para problemas diferentes do MNIST, a regra de n\u00e3o-melhoria-em-dez pode ser agressiva demais ou n\u00e3o ser agressiva o suficiente, dependendo dos detalhes do problema. No entanto, com um pouco de experimenta\u00e7\u00e3o, geralmente \u00e9 f\u00e1cil encontrar uma boa estrat\u00e9gia para o Early Stopping. Isso faz parte do trabalho do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Cientista de Dados\n    </a>\n   </span>\n   ou do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n     Engenheiro de Intelig\u00eancia Artificial\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 aqui, n\u00f3s n\u00e3o usamos o\u00a0Early Stopping em nossos experimentos MNIST. A raz\u00e3o \u00e9 que temos feito muitas compara\u00e7\u00f5es entre diferentes abordagens de aprendizado. Para tais compara\u00e7\u00f5es, \u00e9 \u00fatil usar o mesmo n\u00famero de \u00e9pocas em cada caso. No entanto, vale a pena modificar o network2.py (dispon\u00edvel no reposit\u00f3rio do curso no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   ) para implementar o\u00a0Early Stopping, e deixaremos isso como tarefa para voc\u00ea. Se precisar de ajuda, o Early Stopping \u00e9 estudado em detalhes e com atividades pr\u00e1ticas em Deep Learning na DSA, no curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-758\" href=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-758\" href=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-758\" href=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-758\" href=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-758-5e0dd12c51434\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=758&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-758-5e0dd12c51434\" id=\"like-post-wrapper-140353593-758-5e0dd12c51434\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "29": "<h1 class=\"entry-title\" id=\"capitulo-29\">\n Cap\u00edtulo 29 \u2013 Definindo o Tamanho do Mini-Batch\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Quando os dados de treinamento s\u00e3o divididos em pequenos lotes, cada lote recebe o nome de Mini-Batch (ou Mini-Lote). Suponha que os dados de treinamento tenham 32.000 inst\u00e2ncias e que o tamanho de um Mini-Batch esteja definido como 32. Ent\u00e3o, haver\u00e1 1.000 Mini-Batches. Mas qual deve ser o tamanho do Mini-Batch? Isso \u00e9 o que veremos neste cap\u00edtulo\n  </span>\n  <span style=\"color: #000000;\">\n   :\u00a0Definindo o Tamanho do Mini-Batch.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas porque usamos Mini-Batches? Digamos que voc\u00ea tenha cerca de 1 bilh\u00e3o de dados de treinamento. Se voc\u00ea decidir usar o conjunto completo de treinamento em cada \u00e9poca, voc\u00ea precisar\u00e1 de muita mem\u00f3ria RAM e armazenamento para processar esses dados, sendo bem prov\u00e1vel que sua m\u00e1quina (ou mesmo um\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-engenheiro-de-dados\" rel=\"noopener\" target=\"_blank\">\n     cluster de computadores\n    </a>\n   </span>\n   ) n\u00e3o tenha mem\u00f3ria suficiente. Se voc\u00ea decidir usar um exemplo de treinamento em cada \u00e9poca, de um bilh\u00e3o de dados, de uma s\u00f3 vez, voc\u00ea est\u00e1 ignorando a filosofia de vetoriza\u00e7\u00e3o e isso tornar\u00e1 o processo de treinamento muito mais lento.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Portanto, usamos um subconjunto de dados de treinamento (chamamos de \u201cMini-Batch\u201d) de cada vez em cada \u00e9poca. Isso nos permitir\u00e1 manter os dois objetivos: ajustar dados suficientes na mem\u00f3ria do computador e manter a filosofia de vetoriza\u00e7\u00e3o ao mesmo tempo. Uma coisa importante sobre o Mini-Batch \u00e9 que, \u00e9 melhor escolher o tamanho do Mini-Batch como m\u00faltiplo de 2 e os valores comuns s\u00e3o: 64, 128, 256 e 512. Sinta-se \u00e0 vontade para usar outros valores e discutiremos mais sobre isso mais a frente aqui mesmo neste cap\u00edtulo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Veja um exemplo: digamos que voc\u00ea tenha 1 bilh\u00e3o de dados de treinamento. Voc\u00ea define seu tamanho de Mini-Batch para, digamos, 512. Portanto, em cada \u00e9poca voc\u00ea tem 512 dados de treinamento para processar. Esta configura\u00e7\u00e3o levar\u00e1 aproximadamente: (1.000.000.000 / 512) = 1.953.125 \u00e9pocas para ser conclu\u00edda. Portanto, o tamanho do Mini-Batch \u00e9 a quantidade de dados que voc\u00ea deseja processar em cada \u00e9poca.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Se atualizarmos os par\u00e2metros do modelo ap\u00f3s o processamento de todos os dados de treinamento (ou seja, \u00e9poca), levaria muito tempo para obter uma atualiza\u00e7\u00e3o do modelo no treinamento, e os dados de treinamento inteiros provavelmente n\u00e3o caberiam na mem\u00f3ria. Se atualizarmos os par\u00e2metros do modelo ap\u00f3s o processamento de cada inst\u00e2ncia (por exemplo, descida de gradiente estoc\u00e1stico), as atualiza\u00e7\u00f5es do modelo seriam demasiado ruidosas e o processo n\u00e3o seria computacionalmente eficiente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Portanto, a utiliza\u00e7\u00e3o do Mini-Batch (principalmente na descida do gradiente) \u00e9 introduzida como um trade-off entre {atualiza\u00e7\u00f5es r\u00e1pidas do modelo, efici\u00eancia de mem\u00f3ria} e {atualiza\u00e7\u00f5es precisas do modelo, efici\u00eancia computacional}. \u00c9 trabalho do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Cientista de Dados\n    </a>\n   </span>\n   ajustar mais esse par\u00e2metro no processo de treinamento.\n  </span>\n </p>\n <h2>\n </h2>\n <h2>\n </h2>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas Como Devemos Definir o Tamanho do Mini-Batch?\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para responder a essa pergunta, vamos primeiro supor que estamos fazendo aprendizado on-line, ou seja, que estamos usando um tamanho de Mini-Batch igual a 1.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A preocupa\u00e7\u00e3o \u00f3bvia sobre o aprendizado online \u00e9 que o uso de Mini-Lotes que cont\u00eam apenas um \u00fanico exemplo de treinamento causar\u00e1 erros significativos em nossa estimativa do gradiente. A raz\u00e3o \u00e9 que as estimativas graduais individuais n\u00e3o tem que ser super precisas. Tudo o que precisamos \u00e9 de uma estimativa precisa o suficiente para que nossa fun\u00e7\u00e3o de custo continue diminuindo. \u00c9 como se voc\u00ea estivesse tentando chegar ao P\u00f3lo Norte, mas tivesse uma b\u00fassola informando 10 a 20 graus cada vez que voc\u00ea olhasse para ela. Desde que voc\u00ea pare para checar a b\u00fassola com frequ\u00eancia, e a b\u00fassola acerte na dire\u00e7\u00e3o, voc\u00ea acabar\u00e1 chegando ao P\u00f3lo Norte.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com base nesse argumento, parece que devemos usar o aprendizado on-line. De fato, a situa\u00e7\u00e3o acaba sendo mais complicada do que isso. Em um problema do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/\" rel=\"noopener\" target=\"_blank\">\n     \u00faltimo\n    </a>\n   </span>\n   cap\u00edtulo, mostramos que \u00e9 poss\u00edvel usar t\u00e9cnicas de matriz para calcular a atualiza\u00e7\u00e3o de gradiente para todos os exemplos em um Mini-Lote simultaneamente, em vez de fazer um loop sobre eles. Dependendo dos detalhes de seu hardware e da biblioteca de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener\" target=\"_blank\">\n     \u00e1lgebra linear\n    </a>\n   </span>\n   , pode ser um pouco mais r\u00e1pido calcular a estimativa de gradiente para um Mini-Lote de (por exemplo) tamanho 100, em vez de computar a estimativa de gradiente Mini-Lote fazendo um loop sobre os 100 exemplos de treinamento separadamente. Pode levar (digamos) apenas 50 vezes mais tempo, em vez de 100 vezes mais tempo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora, a princ\u00edpio, parece que isso n\u00e3o nos ajuda muito. Com nosso Mini-Lote de tamanho 100, a regra de aprendizado para os pesos se parece com:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form1\" class=\"aligncenter size-full wp-image-779\" data-attachment-id=\"779\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?fit=504%2C154\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?fit=300%2C92\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?fit=504%2C154\" data-orig-size=\"504,154\" data-permalink=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/form1-3/\" data-recalc-dims=\"1\" height=\"154\" sizes=\"(max-width: 504px) 100vw, 504px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?resize=504%2C154\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?w=504 504w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?resize=300%2C92 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form1.png?resize=200%2C61 200w\" width=\"504\"/>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   onde a soma \u00e9 sobre exemplos de treinamento no Mini-Lote. Isso \u00e9 equivalente a:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form2\" class=\"aligncenter size-full wp-image-780\" data-attachment-id=\"780\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?fit=386%2C104\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?fit=300%2C81\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?fit=386%2C104\" data-orig-size=\"386,104\" data-permalink=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/form2-7/\" data-recalc-dims=\"1\" height=\"104\" sizes=\"(max-width: 386px) 100vw, 386px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?resize=386%2C104\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?w=386 386w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?resize=300%2C81 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form2.png?resize=200%2C54 200w\" width=\"386\"/>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   para aprendizagem online. Mesmo que demore 50 vezes mais para fazer a atualiza\u00e7\u00e3o do Mini-Batch, ainda parece ser melhor fazer o aprendizado online, porque estar\u00edamos atualizando com muito mais frequ\u00eancia. Suponha, no entanto, que no caso do Mini-Lote n\u00f3s aumentemos a taxa de aprendizado por um fator 100, ent\u00e3o a regra de atualiza\u00e7\u00e3o se torna:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <img alt=\"form3\" class=\"aligncenter size-full wp-image-781\" data-attachment-id=\"781\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?fit=432%2C140\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?fit=300%2C97\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?fit=432%2C140\" data-orig-size=\"432,140\" data-permalink=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/form3-6/\" data-recalc-dims=\"1\" height=\"140\" sizes=\"(max-width: 432px) 100vw, 432px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?resize=432%2C140\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?w=432 432w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?resize=300%2C97 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/09/form3.png?resize=200%2C65 200w\" width=\"432\"/>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso \u00e9 muito parecido com 100 inst\u00e2ncias separadas de aprendizado online com uma taxa de aprendizado de \u03b7. Mas leva apenas 50 vezes mais tempo do que fazer uma \u00fanica inst\u00e2ncia de aprendizado online. Naturalmente, n\u00e3o \u00e9 exatamente o mesmo que 100 inst\u00e2ncias de aprendizado online, j\u00e1 que no Mini-Lote os \u2207Cxs s\u00e3o todos avaliados para o mesmo conjunto de pesos, ao contr\u00e1rio do aprendizado cumulativo que ocorre no caso online. Ainda assim, parece claramente poss\u00edvel que o uso do Mini-Lote maior acelere as coisas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com esses fatores em mente, escolher o melhor tamanho de Mini-Lote \u00e9 um trade-off (escolha). Muito pequeno, e voc\u00ea n\u00e3o consegue aproveitar ao m\u00e1ximo os benef\u00edcios de boas bibliotecas de matrizes otimizadas para hardware veloz. Demasiado grande e voc\u00ea simplesmente n\u00e3o est\u00e1 atualizando seus pesos com frequ\u00eancia suficiente. O que voc\u00ea precisa \u00e9 escolher um valor que maximize a velocidade de aprendizado. Felizmente, a escolha do tamanho do Mini-Lote no qual a velocidade \u00e9 maximizada \u00e9 relativamente independente dos outros hiperpar\u00e2metros (al\u00e9m da arquitetura geral), portanto, voc\u00ea n\u00e3o precisa ter otimizado esses hiperpar\u00e2metros para encontrar um bom tamanho Mini-Lote. O caminho a percorrer \u00e9, portanto, usar alguns valores aceit\u00e1veis \u200b\u200b(mas n\u00e3o necessariamente ideais) para os outros hiperpar\u00e2metros, e ent\u00e3o testar v\u00e1rios tamanhos diferentes de Mini-Lotes, escalando \u03b7 como fizemos no exemplo acima. Plote a precis\u00e3o da valida\u00e7\u00e3o em rela\u00e7\u00e3o ao tempo (como em tempo real decorrido, n\u00e3o em \u00e9poca!) e escolha o tamanho do Mini-Lote que forne\u00e7a a melhoria mais r\u00e1pida no desempenho. Com o tamanho do Mini-Lote escolhido, voc\u00ea pode continuar a otimizar os outros hiperpar\u00e2metros. Entendeu agora porque\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Cientistas de Dados\n    </a>\n   </span>\n   devem ser muito bem remunerados?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Claro, como voc\u00ea, sem d\u00favida, percebeu, n\u00e3o fizemos essa otimiza\u00e7\u00e3o em nossa rede de exemplo (que voc\u00ea encontra no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr\" rel=\"noopener\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   ). De fato, nossa implementa\u00e7\u00e3o n\u00e3o usa a abordagem mais r\u00e1pida para atualiza\u00e7\u00f5es de Mini-Batch. N\u00f3s simplesmente usamos um tamanho de Mini-Lote de 10 sem coment\u00e1rios ou explica\u00e7\u00f5es em quase todos os exemplos. Por causa disso, poder\u00edamos ter acelerado o aprendizado reduzindo o tamanho do Mini-Lote. N\u00e3o fizemos isso, em parte porque quer\u00edamos ilustrar o uso de Mini-Lotes al\u00e9m do tamanho 1, e em parte porque nossos experimentos preliminares sugeriam que a acelera\u00e7\u00e3o seria bastante modesta, uma vez que nossa rede de exemplo \u00e9 bem simples. Em implementa\u00e7\u00f5es pr\u00e1ticas, no entanto, certamente implementar\u00edamos a abordagem mais r\u00e1pida para atualiza\u00e7\u00f5es de Mini-Batch e, em seguida, far\u00edamos um esfor\u00e7o para otimizar o tamanho do Mini-Lote, a fim de maximizar nossa velocidade geral.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nos cursos da\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n    ,\n   </span>\n   os alunos trabalho com Mini-Batches pois os datasets usados s\u00e3o muito grandes e precisamos otimizar o tempo de treinamento. Todos os alunos da Forma\u00e7\u00e3o tem acesso remoto gratuito ao super servidor da DSA com duas GPUs e ensinamos como otimizar o treinamento e usar os recursos computacionais de forma eficiente. A defini\u00e7\u00e3o dos Mini-Batches \u00e9 uma das atividades principais. Acesse o programa completo dos cursos aqui:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n   e comece sua capacita\u00e7\u00e3o hoje mesmo.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Matem\u00e1tica Para Machine Learning\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-772\" href=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-772\" href=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-772\" href=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-772\" href=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-772-5e0dd12e47708\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=772&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-772-5e0dd12e47708\" id=\"like-post-wrapper-140353593-772-5e0dd12e47708\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "30": "<h1 class=\"entry-title\" id=\"capitulo-30\">\n Cap\u00edtulo 30 \u2013 Varia\u00e7\u00f5es do Stochastic Gradient Descent \u2013 Hessian Optimization e Momentum\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Cada t\u00e9cnica mostrada at\u00e9\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/definindo-o-tamanho-do-mini-batch/\" rel=\"noopener\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   \u00e9 valiosa e deve ser dominada por aqueles que pretendem trabalhar com redes neurais artificiais e aplica\u00e7\u00f5es de Intelig\u00eancia Artificial, mas essa n\u00e3o \u00e9 a \u00fanica raz\u00e3o pela qual n\u00f3s as explicamos. O ponto principal \u00e9 familiarizar voc\u00ea com alguns dos problemas que podem ocorrer nas redes neurais e com um estilo de an\u00e1lise que pode ajudar a superar esses problemas. De certo modo, aprendemos a pensar sobre redes neurais. Agora neste cap\u00edtulo, esquematizamos brevemente algumas outras t\u00e9cnicas. Esses esbo\u00e7os s\u00e3o menos aprofundados do que as discuss\u00f5es anteriores, mas devem transmitir algum sentimento pela diversidade de t\u00e9cnicas dispon\u00edveis para uso em redes neurais. Lembrando que voc\u00ea sempre pode estudar todas essas t\u00e9cnicas em detalhes nos cursos da\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <h2>\n </h2>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Varia\u00e7\u00f5es do Stochastic Gradient Descent\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A descida de gradiente estoc\u00e1stico pela retropropaga\u00e7\u00e3o tem nos servido bem no ataque ao problema de classifica\u00e7\u00e3o de d\u00edgitos do dataset MNIST. No entanto, existem muitas outras abordagens para otimizar a fun\u00e7\u00e3o de custo e, \u00e0s vezes, essas outras abordagens oferecem desempenho superior ao gradiente estoc\u00e1stico em mini-lote. Neste cap\u00edtulo discutiremos duas dessas abordagens, Hessian Optimization e Momentum.\n  </span>\n </p>\n <h3>\n </h3>\n <h3 style=\"text-align: justify;\">\n  Hessian Optimization\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para iniciar nossa discuss\u00e3o, ajuda a colocar as redes neurais de lado por um tempo. Em vez disso, vamos apenas considerar o problema abstrato de minimizar uma fun\u00e7\u00e3o de custo C que \u00e9 uma fun\u00e7\u00e3o de muitas vari\u00e1veis, w = w1, w2,\u2026, ent\u00e3o C = C(w). Pelo teorema de Taylor, a fun\u00e7\u00e3o custo pode ser aproximada perto de um ponto w por:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter size-full wp-image-799\" data-attachment-id=\"799\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?fit=794%2C290\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?fit=300%2C110\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?fit=794%2C290\" data-orig-size=\"794,290\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form1-4/\" data-recalc-dims=\"1\" height=\"290\" sizes=\"(max-width: 794px) 100vw, 794px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=794%2C290\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?w=794 794w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=300%2C110 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=768%2C281 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=200%2C73 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form1.png?resize=690%2C252 690w\" width=\"794\"/>\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  F\u00f3rmula 1\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos reescrever isso de forma mais compacta:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-800\" data-attachment-id=\"800\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?fit=876%2C122\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?fit=300%2C42\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?fit=876%2C122\" data-orig-size=\"876,122\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form2-8/\" data-recalc-dims=\"1\" height=\"122\" sizes=\"(max-width: 876px) 100vw, 876px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=876%2C122\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?w=876 876w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=300%2C42 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=768%2C107 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=200%2C28 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form2.png?resize=690%2C96 690w\" width=\"876\"/>\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  F\u00f3rmula 2\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   onde \u2207C \u00e9 o vetor gradiente usual e H \u00e9 uma matriz conhecida como Matriz Hessiana. Suponha que n\u00f3s aproximemos C descartando os termos de ordem superior representados por \u2026 acima:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"\" class=\"aligncenter size-full wp-image-801\" data-attachment-id=\"801\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?fit=778%2C124\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?fit=300%2C48\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?fit=778%2C124\" data-orig-size=\"778,124\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form3-7/\" data-recalc-dims=\"1\" height=\"124\" sizes=\"(max-width: 778px) 100vw, 778px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=778%2C124\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?w=778 778w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=300%2C48 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=768%2C122 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=200%2C32 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form3.png?resize=690%2C110 690w\" width=\"778\"/>\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  F\u00f3rmula 3\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Usando o c\u00e1lculo, podemos mostrar que a express\u00e3o do lado direito pode ser minimizada escolhendo:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form4\" class=\"aligncenter size-full wp-image-802\" data-attachment-id=\"802\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?fit=282%2C72\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?fit=282%2C72\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?fit=282%2C72\" data-orig-size=\"282,72\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form4-5/\" data-recalc-dims=\"1\" height=\"72\" sizes=\"(max-width: 282px) 100vw, 282px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?resize=282%2C72\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?w=282 282w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?resize=200%2C51 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form4.png?resize=280%2C72 280w\" width=\"282\"/>\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  F\u00f3rmula 4\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Considerando que a F\u00f3rmula 3 \u00e9 uma boa express\u00e3o aproximada para a fun\u00e7\u00e3o custo, ent\u00e3o esperamos que a mudan\u00e7a do ponto w para\n   <img alt=\"form6\" class=\"aligncenter size-full wp-image-812\" data-attachment-id=\"812\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form6\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?fit=400%2C64\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?fit=300%2C48\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?fit=400%2C64\" data-orig-size=\"400,64\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form6-2/\" data-recalc-dims=\"1\" height=\"64\" sizes=\"(max-width: 400px) 100vw, 400px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?resize=400%2C64\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?w=400 400w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?resize=300%2C48 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form6.png?resize=200%2C32 200w\" width=\"400\"/>\n   deva diminuir significativamente a fun\u00e7\u00e3o custo. Isso sugere um algoritmo poss\u00edvel para minimizar o custo:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Escolha um ponto de partida, w.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Atualize w para um novo ponto w \u2032 = w \u2212 H ^ \u2212 1 \u2207C, onde o Hessian H e \u2207C s\u00e3o calculados em w.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Atualize w\u2032 para um novo ponto w\u2032\u2032 = w\u2032 \u2212 H\u2032 ^ \u2212 1 \u2207\u2032C, onde o Hessian H\u2032 e \u2207\u2032C s\u00e3o calculados em w\u2032.\n   </span>\n   <br/>\n   <span style=\"color: #000000;\">\n    \u2026\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na pr\u00e1tica, a\u00a0F\u00f3rmula 3 \u00e9 apenas uma aproxima\u00e7\u00e3o e \u00e9 melhor dar passos menores. Fazemos isso alterando repetidamente w por uma quantidade\n   <img alt=\"form8\" class=\"aligncenter size-full wp-image-814\" data-attachment-id=\"814\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form8\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?fit=310%2C70\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?fit=300%2C68\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?fit=310%2C70\" data-orig-size=\"310,70\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form8-3/\" data-recalc-dims=\"1\" height=\"70\" sizes=\"(max-width: 310px) 100vw, 310px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?resize=310%2C70\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?w=310 310w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?resize=300%2C68 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form8-1.png?resize=200%2C45 200w\" width=\"310\"/>\n   onde \u03b7 \u00e9 conhecido como taxa de aprendizado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa abordagem para minimizar uma fun\u00e7\u00e3o de custo \u00e9 conhecida como Hessian Technique ou Hessian Optimization. Existem resultados te\u00f3ricos e emp\u00edricos mostrando que os m\u00e9todos de Hessian convergem em um m\u00ednimo em menos etapas do que a descida de gradiente padr\u00e3o. Em particular, ao incorporar informa\u00e7\u00f5es sobre mudan\u00e7as de segunda ordem na fun\u00e7\u00e3o de custo, \u00e9 poss\u00edvel que a abordagem Hessiana evite muitas patologias que podem ocorrer na descida de gradiente. Al\u00e9m disso, h\u00e1 vers\u00f5es do algoritmo de retropropaga\u00e7\u00e3o que podem ser usadas para computar o Hessian.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Se a Hessian Optimization \u00e9 t\u00e3o bom, por que n\u00e3o a estamos usando em nossas redes neurais? Infelizmente, embora tenha muitas propriedades desej\u00e1veis, tem uma propriedade muito indesej\u00e1vel: \u00e9 muito dif\u00edcil de aplicar na pr\u00e1tica. Parte do problema \u00e9 o tamanho da matriz Hessiana. Suponha que voc\u00ea tenha uma rede neural com 107 pesos e vieses. Em seguida, a matriz Hessiana correspondente conter\u00e1 107 \u00d7 107 = 1014 entradas. Isso \u00e9 um n\u00famero grande de entradas! E isso torna a computa\u00e7\u00e3o H ^ \u2212 1 \u2207C extremamente dif\u00edcil na pr\u00e1tica. No entanto, isso n\u00e3o significa que n\u00e3o seja \u00fatil entender. De fato, h\u00e1 muitas varia\u00e7\u00f5es na descida de gradiente que s\u00e3o inspiradas pela Hessian Optimization, mas que evitam o problema com matrizes excessivamente grandes. Vamos dar uma olhada em uma dessas t\u00e9cnicas, a descida do gradiente baseada em Momentum.\n  </span>\n </p>\n <h3>\n </h3>\n <h3 style=\"text-align: justify;\">\n </h3>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Momentum\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Intuitivamente, a vantagem da Hessian Optimization \u00e9 que ela incorpora n\u00e3o apenas informa\u00e7\u00f5es sobre o gradiente, mas tamb\u00e9m informa\u00e7\u00f5es sobre como o gradiente est\u00e1 mudando. A descida do gradiente baseada no Momentum baseia-se em uma intui\u00e7\u00e3o similar, mas evita grandes matrizes de derivadas secund\u00e1rias. Para entender a t\u00e9cnica de Momentum, pense em nossa imagem original de descida do gradiente, na qual consideramos uma bola rolando em um vale (veja figura abaixo). Observamos que a descida do gradiente \u00e9, apesar de seu nome, apenas vagamente semelhante a uma bola caindo no fundo de um vale.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A t\u00e9cnica de Momentum modifica a descida do gradiente de duas maneiras que a tornam mais semelhante \u00e0 imagem f\u00edsica. Primeiro, introduz uma no\u00e7\u00e3o de \u201cvelocidade\u201d para os par\u00e2metros que estamos tentando otimizar. O gradiente atua para alterar a velocidade, n\u00e3o (diretamente) a \u201cposi\u00e7\u00e3o\u201d, da mesma maneira que as for\u00e7as f\u00edsicas alteram a velocidade, afetando apenas indiretamente a posi\u00e7\u00e3o. Em segundo lugar, o m\u00e9todo Momentum introduz um tipo de termo de fric\u00e7\u00e3o, que tende a reduzir gradualmente a velocidade.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos dar uma descri\u00e7\u00e3o matem\u00e1tica mais precisa. Introduzimos vari\u00e1veis de velocidade v = v1, v2,\u2026, uma para cada vari\u00e1vel wj correspondente. Ent\u00e3o n\u00f3s substitu\u00edmos a regra de atualiza\u00e7\u00e3o de descida de gradiente w \u2192 w\u2032 = w \u2212 \u03b7\u2207C por:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"form5\" class=\"aligncenter size-full wp-image-808\" data-attachment-id=\"808\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?fit=374%2C152\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?fit=300%2C122\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?fit=374%2C152\" data-orig-size=\"374,152\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/form5-3/\" data-recalc-dims=\"1\" height=\"152\" sizes=\"(max-width: 374px) 100vw, 374px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?resize=374%2C152\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?w=374 374w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?resize=300%2C122 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/form5.png?resize=200%2C81 200w\" width=\"374\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nessas equa\u00e7\u00f5es, \u03bc \u00e9 um hiperpar\u00e2metro que controla a quantidade de amortecimento ou atrito no sistema. Para entender o significado das equa\u00e7\u00f5es, \u00e9 \u00fatil considerar primeiro o caso onde \u03bc = 1, o que corresponde a nenhum atrito. Quando esse \u00e9 o caso, a inspe\u00e7\u00e3o das equa\u00e7\u00f5es mostra que a \u201cfor\u00e7a\u201d \u2207C est\u00e1 agora modificando a velocidade, v, e a velocidade est\u00e1 controlando a taxa de varia\u00e7\u00e3o de w. Intuitivamente, n\u00f3s aumentamos a velocidade adicionando repetidamente termos de gradiente a ela. Isso significa que se o gradiente estiver na (aproximadamente) mesma dire\u00e7\u00e3o atrav\u00e9s de v\u00e1rias rodadas de aprendizado, poderemos desenvolver um pouco de vapor movendo-se nessa dire\u00e7\u00e3o. Pense, por exemplo, no que acontece se estivermos nos movendo diretamente por um declive:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"gradient\" class=\"aligncenter size-full wp-image-803\" data-attachment-id=\"803\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gradient\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?fit=555%2C409\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?fit=300%2C221\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?fit=555%2C409\" data-orig-size=\"555,409\" data-permalink=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/gradient/\" data-recalc-dims=\"1\" height=\"409\" sizes=\"(max-width: 555px) 100vw, 555px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?resize=555%2C409\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?w=555 555w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?resize=300%2C221 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/gradient.png?resize=200%2C147 200w\" width=\"555\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A cada passo a velocidade se torna maior no declive, ent\u00e3o nos movemos mais e mais rapidamente para o fundo do vale. Isso pode permitir que a t\u00e9cnica de Momentum funcione muito mais rapidamente do que a descida de gradiente padr\u00e3o. Claro, um problema \u00e9 que, uma vez que chegarmos ao fundo do vale, vamos ultrapassar. Ou, se o gradiente deve mudar rapidamente, ent\u00e3o podemos nos encontrar indo na dire\u00e7\u00e3o errada. Essa \u00e9 a raz\u00e3o para o hiperpar\u00e2metro \u00b5 nas equa\u00e7\u00f5es acima.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Eu disse anteriormente que \u03bc controla a quantidade de atrito no sistema; para ser um pouco mais preciso, voc\u00ea deve pensar em 1 \u2212 \u03bc como a quantidade de atrito no sistema. Quando \u03bc = 1, como vimos, n\u00e3o h\u00e1 atrito e a velocidade \u00e9 completamente controlada pelo gradiente \u2207C. Em contraste, quando \u03bc = 0 h\u00e1 muito atrito, a velocidade n\u00e3o pode se acumular e as equa\u00e7\u00f5es acima reduzem \u00e0 equa\u00e7\u00e3o usual para o gradiente descendente, w \u2192 w \u2032 = w \u2212 \u03b7\u2207C. Na pr\u00e1tica, usar um valor intermedi\u00e1rio entre 0 e 1 pode nos dar muito do benef\u00edcio de ser capaz de aumentar a velocidade, mas sem causar overshooting. Podemos escolher um valor para \u03bc usando os dados de valida\u00e7\u00e3o retidos, da mesma maneira que selecionamos \u03b7 e \u03bb. Essa t\u00e9cnica \u00e9 estudada em detalhes\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/curso-deep-learning-ii\" rel=\"noopener\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Evitei nomear o hiperpar\u00e2metro \u03bc at\u00e9 agora. A raz\u00e3o \u00e9 que o nome padr\u00e3o para \u03bc \u00e9 mal escolhido: \u00e9 chamado de coeficiente de momentum. Isso \u00e9 potencialmente confuso, j\u00e1 que \u03bc n\u00e3o \u00e9 de maneira alguma a no\u00e7\u00e3o de momento da f\u00edsica. Pelo contr\u00e1rio, est\u00e1 muito mais relacionado ao atrito. No entanto, o termo coeficiente de momentum \u00e9 amplamente utilizado, por isso continuaremos a us\u00e1-lo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma coisa boa sobre a t\u00e9cnica do Momentum \u00e9 que n\u00e3o \u00e9 preciso quase nenhum trabalho para modificar uma implementa\u00e7\u00e3o de descida de gradiente para incorporar o Momentum. Ainda podemos usar a retropropaga\u00e7\u00e3o para calcular os gradientes, assim como antes, e usar ideias como a amostragem de mini-lotes estocasticamente escolhidos. Desta forma, podemos obter algumas das vantagens da Hessian Optimization, usando informa\u00e7\u00f5es sobre como o gradiente est\u00e1 mudando, mas sem as desvantagens e com apenas pequenas modifica\u00e7\u00f5es no nosso c\u00f3digo. Na pr\u00e1tica, a t\u00e9cnica do Momentum \u00e9 comumente usada e, muitas vezes, acelera o aprendizado.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Vejo voc\u00ea no pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-794\" href=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-794\" href=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-794\" href=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-794\" href=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/variacoes-do-stochastic-gradient-descent-hessian-optimization-e-momentum/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-794-5e0dd1306aea2\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=794&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-794-5e0dd1306aea2\" id=\"like-post-wrapper-140353593-794-5e0dd1306aea2\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "31": "<h1 class=\"entry-title\" id=\"capitulo-31\">\n Cap\u00edtulo 31 \u2013 As Redes Neurais Artificiais Podem Computar Qualquer Fun\u00e7\u00e3o?\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Um dos fatos mais impressionantes sobre redes neurais \u00e9 que elas podem computar qualquer fun\u00e7\u00e3o. Isto \u00e9, suponha que algu\u00e9m lhe d\u00ea alguma fun\u00e7\u00e3o complicada, f(x):\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"image1\" class=\"aligncenter size-full wp-image-822\" data-attachment-id=\"822\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"image1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?fit=300%2C300\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?fit=300%2C300\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?fit=300%2C300\" data-orig-size=\"300,300\" data-permalink=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/image1/\" data-recalc-dims=\"1\" height=\"300\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=300%2C300\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?w=300 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=150%2C150 150w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=200%2C200 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image1.png?resize=280%2C280 280w\" width=\"300\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   N\u00e3o importa qual seja a fun\u00e7\u00e3o, \u00e9 garantido que existe uma rede neural de modo que, para cada entrada poss\u00edvel, x, o valor f(x) (ou alguma aproxima\u00e7\u00e3o) seja transmitido da rede, por exemplo:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"image2\" class=\"aligncenter size-full wp-image-823\" data-attachment-id=\"823\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"image2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?fit=350%2C220\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?fit=300%2C189\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?fit=350%2C220\" data-orig-size=\"350,220\" data-permalink=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/image2/\" data-recalc-dims=\"1\" height=\"220\" sizes=\"(max-width: 350px) 100vw, 350px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?resize=350%2C220\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?w=350 350w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?resize=300%2C189 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image2.png?resize=200%2C126 200w\" width=\"350\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Este resultado \u00e9 v\u00e1lido mesmo se a fun\u00e7\u00e3o tiver muitas entradas, f = f(x1,\u2026, xm) e muitas sa\u00eddas. Por exemplo, aqui est\u00e1 uma rede computando uma fun\u00e7\u00e3o com m = 3 entradas e n = 2 sa\u00eddas:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"image3\" class=\"aligncenter size-full wp-image-824\" data-attachment-id=\"824\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"image3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?fit=450%2C370\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?fit=300%2C247\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?fit=450%2C370\" data-orig-size=\"450,370\" data-permalink=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/image3/\" data-recalc-dims=\"1\" height=\"370\" sizes=\"(max-width: 450px) 100vw, 450px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?resize=450%2C370\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?w=450 450w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?resize=300%2C247 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/10/image3.png?resize=200%2C164 200w\" width=\"450\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Este resultado nos diz que as redes neurais t\u00eam um tipo de universalidade. N\u00e3o importa qual fun\u00e7\u00e3o queremos computar, sabemos que existe uma rede neural que pode fazer o trabalho.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Al\u00e9m do mais, esse teorema da universalidade \u00e9 v\u00e1lido mesmo se restringirmos nossas redes a ter apenas uma \u00fanica camada intermedi\u00e1ria entre os neur\u00f4nios de entrada e de sa\u00edda \u2013 uma chamada camada oculta \u00fanica. Portanto, mesmo arquiteturas de rede muito simples podem ser extremamente poderosas e isso ajuda a explicar porque as redes neurais vem sendo usadas em aplica\u00e7\u00f5es avan\u00e7adas de Intelig\u00eancia Artificial.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O teorema da universalidade \u00e9 bem conhecido por pessoas que usam redes neurais. Mas porque \u00e9 verdade n\u00e3o \u00e9 t\u00e3o amplamente compreendido. A maioria das explica\u00e7\u00f5es dispon\u00edveis \u00e9 bastante t\u00e9cnica. Por exemplo, um dos artigos originais que comprovou o resultado utilizou o teorema de Hahn-Banach, o teorema da representa\u00e7\u00e3o de Riesz e alguma an\u00e1lise de Fourier. Se voc\u00ea \u00e9 um matem\u00e1tico, o argumento n\u00e3o \u00e9 dif\u00edcil de seguir, mas n\u00e3o \u00e9 t\u00e3o f\u00e1cil para a maioria das pessoas. \u00c9 uma pena, j\u00e1 que as raz\u00f5es subjacentes \u00e0 universalidade s\u00e3o simples e belas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nos pr\u00f3ximos cap\u00edtulos, faremos uma explica\u00e7\u00e3o simples e principalmente visual do teorema da universalidade. N\u00f3s vamos passo a passo atrav\u00e9s das id\u00e9ias principais. Voc\u00ea entender\u00e1 porque \u00e9 verdade que as redes neurais podem computar qualquer fun\u00e7\u00e3o. Voc\u00ea entender\u00e1 algumas das limita\u00e7\u00f5es do resultado. E voc\u00ea entender\u00e1 como o resultado se relaciona com redes neurais profundas (Deep Learning).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os cap\u00edtulos ser\u00e3o estruturados para ser agrad\u00e1veis e objetivos. Desde que voc\u00ea tenha apenas um pouco de familiaridade b\u00e1sica com redes neurais, voc\u00ea deve ser capaz de seguir a explica\u00e7\u00e3o. No entanto, iremos fornecer links ocasionais para materiais anteriores, para ajudar a preencher quaisquer lacunas em seu conhecimento.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os teoremas da universalidade s\u00e3o um lugar comum na ci\u00eancia da computa\u00e7\u00e3o, tanto que \u00e0s vezes nos esquecemos do qu\u00e3o surpreendentes eles s\u00e3o. Mas vale a pena lembrar-nos: a capacidade de calcular uma fun\u00e7\u00e3o arbitr\u00e1ria \u00e9 verdadeiramente not\u00e1vel. Quase qualquer processo que voc\u00ea possa imaginar pode ser considerado como computa\u00e7\u00e3o de fun\u00e7\u00e3o. Considere o problema de nomear uma pe\u00e7a musical com base em uma pequena amostra da pe\u00e7a. Isso pode ser pensado como computa\u00e7\u00e3o de uma fun\u00e7\u00e3o. Ou considere o problema de traduzir um texto chin\u00eas para o ingl\u00eas. Mais uma vez, isso pode ser pensado como computa\u00e7\u00e3o de uma fun\u00e7\u00e3o. Ou considere o problema de analisar um arquivo de filme mp4 e gerar uma descri\u00e7\u00e3o do enredo do filme e uma discuss\u00e3o sobre a qualidade da atua\u00e7\u00e3o dos atores. Novamente, isso pode ser pensado como um tipo de computa\u00e7\u00e3o de fun\u00e7\u00e3o. Universalidade significa que, em princ\u00edpio, as redes neurais podem fazer tudo isso e muito mais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c9 claro, s\u00f3 porque sabemos que existe uma rede neural que pode (por exemplo) traduzir o texto chin\u00eas para o ingl\u00eas, isso n\u00e3o significa que temos boas t\u00e9cnicas para construir ou mesmo reconhecer tal rede. Essa limita\u00e7\u00e3o se aplica tamb\u00e9m aos teoremas da universalidade tradicionais para modelos como circuitos booleanos. Mas, como vimos anteriormente no livro, as redes neurais possuem algoritmos poderosos para fun\u00e7\u00f5es de aprendizado. Essa combina\u00e7\u00e3o de algoritmos de aprendizado + universalidade \u00e9 uma mistura atraente. At\u00e9 agora, o livro se concentrou nos algoritmos de aprendizado. Nos pr\u00f3ximos cap\u00edtulos, nos concentramos na universalidade e no que ela significa.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   A compreens\u00e3o desse conceito \u00e9 a chave para as arquiteturas mais avan\u00e7adas de Deep Learning, que est\u00e3o por vir mais a frente, neste livro!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias\n  </span>\n  :\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-821\" href=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-821\" href=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-821\" href=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-821\" href=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-821-5e0dd1327788d\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=821&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-821-5e0dd1327788d\" id=\"like-post-wrapper-140353593-821-5e0dd1327788d\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "32": "<h1 class=\"entry-title\" id=\"capitulo-32\">\n Cap\u00edtulo 32 \u2013 Como Uma Rede Neural Artificial Encontra a Aproxima\u00e7\u00e3o de Uma Fun\u00e7\u00e3o\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Este \u00e9 um cap\u00edtulo muito importante para compreender como as redes neurais realmente funcionam e\n   <strong>\n    Como Uma Rede Neural Artificial Encontra a Aproxima\u00e7\u00e3o de Uma Fun\u00e7\u00e3o\n   </strong>\n   . Acompanhe a explica\u00e7\u00e3o passo a passo analisando cada um dos gr\u00e1ficos apresentados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas antes de explicar porque o teorema da universalidade \u00e9 verdadeiro, quero mencionar duas advert\u00eancias a esta declara\u00e7\u00e3o informal: \u201cuma rede neural pode computar qualquer fun\u00e7\u00e3o\u201d, que vimos no cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/as-redes-neurais-artificiais-podem-computar-qualquer-funcao/\" rel=\"noopener\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Primeiro, isso n\u00e3o significa que uma rede possa ser usada para calcular exatamente qualquer fun\u00e7\u00e3o. Em vez disso, podemos obter uma aproxima\u00e7\u00e3o que seja t\u00e3o boa quanto desejamos. Aumentando o n\u00famero de neur\u00f4nios ocultos, podemos melhorar a aproxima\u00e7\u00e3o. Por exemplo, anteriormente ilustramos uma rede computando alguma fun\u00e7\u00e3o f(x) usando tr\u00eas neur\u00f4nios ocultos. Para a maioria das fun\u00e7\u00f5es, apenas uma aproxima\u00e7\u00e3o de baixa qualidade ser\u00e1 poss\u00edvel usando tr\u00eas neur\u00f4nios ocultos. Ao aumentar o n\u00famero de neur\u00f4nios ocultos (digamos, para cinco), podemos obter uma melhor aproxima\u00e7\u00e3o:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede\" class=\"aligncenter size-full wp-image-834\" data-attachment-id=\"834\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?fit=350%2C380\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?fit=276%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?fit=350%2C380\" data-orig-size=\"350,380\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-6/\" data-recalc-dims=\"1\" height=\"380\" sizes=\"(max-width: 350px) 100vw, 350px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?resize=350%2C380\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?w=350 350w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?resize=276%2C300 276w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede.png?resize=200%2C217 200w\" width=\"350\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E podemos melhorar ainda mais aumentando o n\u00famero de neur\u00f4nios ocultos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para tornar esta afirma\u00e7\u00e3o mais precisa, suponha que tenhamos uma fun\u00e7\u00e3o f(x) que gostar\u00edamos de computar com alguma precis\u00e3o desejada \u03f5 &gt; 0. A garantia \u00e9 que usando neur\u00f4nios ocultos suficientes sempre podemos encontrar uma rede neural cuja sa\u00edda g(x) satisfa\u00e7a | g(x) \u2212 f(x) | &lt; \u03f5, para todas as entradas x. Em outras palavras, a aproxima\u00e7\u00e3o ser\u00e1 boa dentro da precis\u00e3o desejada para cada entrada poss\u00edvel.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A segunda ressalva \u00e9 que a classe de fun\u00e7\u00f5es que podem ser aproximadas da maneira descrita s\u00e3o as fun\u00e7\u00f5es cont\u00ednuas. Se uma fun\u00e7\u00e3o \u00e9 descont\u00ednua, isto \u00e9, faz saltos bruscos e repentinos, ent\u00e3o, em geral, n\u00e3o ser\u00e1 poss\u00edvel aproximar usando uma rede neural. Isso n\u00e3o \u00e9 surpreendente, j\u00e1 que nossas redes neurais calculam fun\u00e7\u00f5es cont\u00ednuas de sua entrada. No entanto, mesmo que a fun\u00e7\u00e3o que realmente gostar\u00edamos de computar fosse descont\u00ednua, muitas vezes a aproxima\u00e7\u00e3o cont\u00ednua \u00e9 boa o suficiente. Se \u00e9 assim, ent\u00e3o podemos usar uma rede neural. Na pr\u00e1tica, isso geralmente n\u00e3o \u00e9 uma limita\u00e7\u00e3o importante.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em suma, uma afirma\u00e7\u00e3o mais precisa do teorema da universalidade \u00e9 que redes neurais com uma \u00fanica camada oculta podem ser usadas para aproximar qualquer fun\u00e7\u00e3o cont\u00ednua a qualquer precis\u00e3o desejada. Neste e no pr\u00f3ximo cap\u00edtulo, vamos provar uma vers\u00e3o desse resultado.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Universalidade Com Uma Entrada e Uma Sa\u00edda\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para entender por que o teorema da universalidade \u00e9 verdadeiro, vamos come\u00e7ar entendendo como construir uma rede neural que se aproxima de uma fun\u00e7\u00e3o com apenas uma entrada e uma sa\u00edda:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede2\" class=\"aligncenter size-full wp-image-835\" data-attachment-id=\"835\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?fit=300%2C300\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?fit=300%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?fit=300%2C300\" data-orig-size=\"300,300\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede2-5/\" data-recalc-dims=\"1\" height=\"300\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=300%2C300\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?w=300 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=150%2C150 150w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=200%2C200 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede2.png?resize=280%2C280 280w\" width=\"300\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Este \u00e9 o cerne do problema da universalidade. Uma vez que entendemos esse caso especial, \u00e9 realmente f\u00e1cil estender para fun\u00e7\u00f5es com muitas entradas e muitas sa\u00eddas (tema do pr\u00f3ximo cap\u00edtulo).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para construir um insight sobre como construir uma rede para calcular f, vamos come\u00e7ar com uma rede contendo apenas uma camada oculta, com dois neur\u00f4nios ocultos e uma camada de sa\u00edda contendo um \u00fanico neur\u00f4nio de sa\u00edda:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede\" class=\"aligncenter size-full wp-image-837\" data-attachment-id=\"837\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?fit=350%2C220\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?fit=300%2C189\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?fit=350%2C220\" data-orig-size=\"350,220\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-7/\" data-recalc-dims=\"1\" height=\"220\" sizes=\"(max-width: 350px) 100vw, 350px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?resize=350%2C220\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?w=350 350w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?resize=300%2C189 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-1.png?resize=200%2C126 200w\" width=\"350\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para ter uma ideia de como funcionam os componentes da rede, vamos nos concentrar no neur\u00f4nio oculto superior. No diagrama abaixo, aumentando o valor de w, podemos ver imediatamente como a fun\u00e7\u00e3o computada pelo neur\u00f4nio oculto superior muda:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede\" class=\"aligncenter wp-image-857 size-large\" data-attachment-id=\"857\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?fit=1024%2C517\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?fit=300%2C152\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?fit=1164%2C588\" data-orig-size=\"1164,588\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-9/\" data-recalc-dims=\"1\" height=\"517\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=1024%2C517\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=1024%2C517 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=300%2C152 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=768%2C388 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=200%2C101 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?resize=690%2C349 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-3.png?w=1164 1164w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como aprendemos anteriormente no livro, o que est\u00e1 sendo computado pelo neur\u00f4nio oculto \u00e9 \u03c3(wx + b), onde \u03c3(z) \u2261 1 / (1 + e^-z) \u00e9 a fun\u00e7\u00e3o sigm\u00f3ide. At\u00e9 agora, fizemos uso frequente dessa forma alg\u00e9brica. Mas, para a prova da universalidade, obteremos mais discernimento ignorando inteiramente a \u00e1lgebra e, em vez disso, manipulando e observando a forma mostrada no gr\u00e1fico. Isso n\u00e3o apenas nos dar\u00e1 uma ideia melhor do que est\u00e1 acontecendo, mas tamb\u00e9m nos dar\u00e1 uma prova de universalidade que se aplica a outras fun\u00e7\u00f5es de ativa\u00e7\u00e3o que n\u00e3o a fun\u00e7\u00e3o sigm\u00f3ide.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para come\u00e7ar esta prova, podemos aumentar o bias, b, no diagrama acima. Voc\u00ea ver\u00e1 que, conforme o bias aumenta, o gr\u00e1fico se move para a esquerda, mas sua forma n\u00e3o muda.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em seguida, podemos diminuir o vi\u00e9s (bias). Voc\u00ea ver\u00e1 que conforme o vi\u00e9s diminui, o gr\u00e1fico se move para a direita, mas, novamente, sua forma n\u00e3o muda. Em seguida, diminu\u00edmos o peso para cerca de 2 ou 3. Voc\u00ea ver\u00e1 que \u00e0 medida que diminui o peso, a curva se alarga. Talvez seja necess\u00e1rio alterar o bias tamb\u00e9m, para manter a curva no quadro.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Finalmente, aumentamos o peso acima de w = 100. A curva fica mais \u00edngreme, at\u00e9 que, eventualmente, ela come\u00e7a a parecer uma fun\u00e7\u00e3o de passo (Step Function). A imagem a seguir mostra como deve ser resultado:\n  </span>\n </p>\n <p>\n  <img alt=\"rede3\" class=\"aligncenter size-full wp-image-851\" data-attachment-id=\"851\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?fit=1024%2C523\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?fit=300%2C153\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?fit=1136%2C580\" data-orig-size=\"1136,580\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede3-4/\" data-recalc-dims=\"1\" height=\"580\" sizes=\"(max-width: 1136px) 100vw, 1136px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=1136%2C580\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?w=1136 1136w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=300%2C153 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=768%2C392 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=1024%2C523 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=200%2C102 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede3-1.png?resize=690%2C352 690w\" width=\"1136\"/>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos simplificar um pouco nossa an\u00e1lise aumentando o peso para que a sa\u00edda realmente seja uma Step Function, para uma aproxima\u00e7\u00e3o muito boa. Abaixo eu plotei a sa\u00edda do neur\u00f4nio oculto superior quando o peso \u00e9 w = 999.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede4\" class=\"aligncenter size-full wp-image-840\" data-attachment-id=\"840\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?fit=1024%2C509\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?fit=300%2C149\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?fit=1154%2C574\" data-orig-size=\"1154,574\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede4-2/\" data-recalc-dims=\"1\" height=\"574\" sizes=\"(max-width: 1154px) 100vw, 1154px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=1154%2C574\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?w=1154 1154w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=300%2C149 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=768%2C382 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=1024%2C509 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=200%2C99 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede4.png?resize=690%2C343 690w\" width=\"1154\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na verdade, \u00e9 um pouco mais f\u00e1cil trabalhar com fun\u00e7\u00f5es step do que com fun\u00e7\u00f5es gerais sigm\u00f3ides. A raz\u00e3o \u00e9 que, na camada de sa\u00edda, somamos contribui\u00e7\u00f5es de todos os neur\u00f4nios ocultos. \u00c9 f\u00e1cil analisar a soma de v\u00e1rias fun\u00e7\u00f5es step, mas \u00e9 mais dif\u00edcil pensar sobre o que acontece quando voc\u00ea adiciona um monte de curvas em forma de sigm\u00f3ide. E assim torna as coisas muito mais f\u00e1ceis de assumir que nossos neur\u00f4nios ocultos est\u00e3o emitindo fun\u00e7\u00f5es step. Mais concretamente, fazemos isso fixando o peso w como sendo um valor muito grande e, em seguida, definindo a posi\u00e7\u00e3o da etapa modificando o bias. \u00c9 claro que tratar a sa\u00edda como uma fun\u00e7\u00e3o step \u00e9 uma aproxima\u00e7\u00e3o, mas \u00e9 uma aproxima\u00e7\u00e3o muito boa e, por enquanto, vamos trat\u00e1-la como exata. Voltarei mais tarde para discutir o impacto dos desvios dessa aproxima\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em que valor de x a etapa ocorre? Em outras palavras, como a posi\u00e7\u00e3o da etapa depende do peso e do vi\u00e9s?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para responder a essa pergunta, podemos modificar o peso e o vi\u00e9s no diagrama acima. Voc\u00ea consegue descobrir como a posi\u00e7\u00e3o da etapa depende de w e b. Com um pouco de trabalho, voc\u00ea deve ser capaz de se convencer de que a posi\u00e7\u00e3o da etapa \u00e9 proporcional a b e inversamente proporcional a w.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na verdade, a etapa est\u00e1 na posi\u00e7\u00e3o s = \u2212b / w, como voc\u00ea pode ver modificando o peso e o bias no diagrama a seguir:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede5\" class=\"aligncenter size-full wp-image-841\" data-attachment-id=\"841\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?fit=1024%2C510\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?fit=300%2C149\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?fit=1168%2C582\" data-orig-size=\"1168,582\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede5-2/\" data-recalc-dims=\"1\" height=\"582\" sizes=\"(max-width: 1168px) 100vw, 1168px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=1168%2C582\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?w=1168 1168w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=300%2C149 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=768%2C383 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=1024%2C510 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=200%2C100 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede5.png?resize=690%2C344 690w\" width=\"1168\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso simplificar\u00e1 muito nossas vidas para descrever os neur\u00f4nios ocultos usando apenas um \u00fanico par\u00e2metro, s, que \u00e9 a posi\u00e7\u00e3o do passo, s = \u2212b / w.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede6\" class=\"aligncenter size-full wp-image-842\" data-attachment-id=\"842\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede6\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?fit=1024%2C533\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?fit=300%2C156\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?fit=1156%2C602\" data-orig-size=\"1156,602\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/redd6/\" data-recalc-dims=\"1\" height=\"602\" sizes=\"(max-width: 1156px) 100vw, 1156px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=1156%2C602\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?w=1156 1156w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=300%2C156 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=768%2C400 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=1024%2C533 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=200%2C104 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/redd6.png?resize=690%2C359 690w\" width=\"1156\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como mencionado acima, n\u00f3s implicitamente definimos o peso w na entrada como um valor grande \u2013 grande o suficiente para que a fun\u00e7\u00e3o de passo seja uma boa aproxima\u00e7\u00e3o. Podemos facilmente converter um neur\u00f4nio parametrizado dessa maneira de volta ao modelo convencional, escolhendo o vi\u00e9s b = \u2212ws.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 agora, nos concentramos na sa\u00edda apenas do neur\u00f4nio oculto superior. Vamos dar uma olhada no comportamento de toda a rede. Em particular, vamos supor que os neur\u00f4nios ocultos estejam computando fun\u00e7\u00f5es de passos parametrizadas pelos pontos de degrau s1 (neur\u00f4nio superior) e s2 (neur\u00f4nio de baixo). E eles ter\u00e3o os respectivos pesos de sa\u00edda w1 e w2. Aqui est\u00e1 a rede:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede7\" class=\"aligncenter size-full wp-image-843\" data-attachment-id=\"843\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede7\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?fit=1024%2C550\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?fit=300%2C161\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?fit=1154%2C620\" data-orig-size=\"1154,620\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede7-2/\" data-recalc-dims=\"1\" height=\"620\" sizes=\"(max-width: 1154px) 100vw, 1154px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=1154%2C620\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?w=1154 1154w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=300%2C161 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=768%2C413 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=1024%2C550 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=200%2C107 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede7.png?resize=690%2C371 690w\" width=\"1154\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O que est\u00e1 sendo plotado \u00e0 direita \u00e9 a sa\u00edda ponderada w1a1 + w2a2 da camada oculta. Aqui, a1 e a2 s\u00e3o as sa\u00eddas dos neur\u00f4nios ocultos superior e inferior, respectivamente. Essas sa\u00eddas s\u00e3o frequentemente conhecidas como ativa\u00e7\u00f5es dos neur\u00f4nios.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos aumentar ou diminuir o ponto de passo s1 do neur\u00f4nio oculto superior e isso nos d\u00e1 uma ideia de como isso altera a sa\u00edda ponderada da camada oculta. Vale a pena entender o que acontece quando o s1 passa do s2. Voc\u00ea ver\u00e1 que o gr\u00e1fico muda de forma quando isso acontece, j\u00e1 que nos movemos de uma situa\u00e7\u00e3o em que o neur\u00f4nio oculto superior \u00e9 o primeiro a ser ativado para uma situa\u00e7\u00e3o em que o neur\u00f4nio oculto na parte inferior \u00e9 o primeiro a ser ativado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Da mesma forma, podemos manipular o ponto de passo s2 do neur\u00f4nio oculto na parte inferior e ter uma ideia de como isso altera a sa\u00edda combinada dos neur\u00f4nios ocultos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Finalmente, podemos definir w1 como 0.8 e w2 como \u22120.8. Voc\u00ea recebe uma fun\u00e7\u00e3o \u201cbump\u201d, que come\u00e7a no ponto s1, termina no ponto s2 e tem a altura 0.8. Por exemplo, a sa\u00edda ponderada pode ser assim:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede8\" class=\"aligncenter size-full wp-image-844\" data-attachment-id=\"844\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede8\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?fit=1024%2C582\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?fit=300%2C170\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?fit=1148%2C652\" data-orig-size=\"1148,652\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede8/\" data-recalc-dims=\"1\" height=\"652\" sizes=\"(max-width: 1148px) 100vw, 1148px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=1148%2C652\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?w=1148 1148w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=300%2C170 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=768%2C436 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=1024%2C582 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=200%2C114 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede8.png?resize=690%2C392 690w\" width=\"1148\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Claro, podemos redimensionar o bump para ter qualquer altura. Vamos usar um \u00fanico par\u00e2metro, h, para indicar a altura. Para reduzir a confus\u00e3o, tamb\u00e9m removerei as nota\u00e7\u00f5es \u201cs1 = \u2026\u201d e \u201cw1 = \u2026\u201d.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede9\" class=\"aligncenter size-full wp-image-845\" data-attachment-id=\"845\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede9\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?fit=1024%2C552\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?fit=300%2C162\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?fit=1154%2C622\" data-orig-size=\"1154,622\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede9/\" data-recalc-dims=\"1\" height=\"622\" sizes=\"(max-width: 1154px) 100vw, 1154px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=1154%2C622\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?w=1154 1154w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=300%2C162 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=768%2C414 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=1024%2C552 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=200%2C108 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede9.png?resize=690%2C372 690w\" width=\"1154\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos alterar o valor de h para cima e para baixo, para ver como a altura do bump muda.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea notar\u00e1, a prop\u00f3sito, que estamos usando nossos neur\u00f4nios de uma forma que pode ser pensada n\u00e3o apenas em termos gr\u00e1ficos, mas em termos de programa\u00e7\u00e3o mais convencionais, como uma esp\u00e9cie de declara\u00e7\u00e3o if-then-else, por exemplo:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"code\" class=\"aligncenter size-full wp-image-846\" data-attachment-id=\"846\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"code\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?fit=754%2C158\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?fit=300%2C63\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?fit=754%2C158\" data-orig-size=\"754,158\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/code-2/\" data-recalc-dims=\"1\" height=\"158\" sizes=\"(max-width: 754px) 100vw, 754px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=754%2C158\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?w=754 754w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=300%2C63 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=200%2C42 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/code.png?resize=690%2C145 690w\" width=\"754\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na maior parte eu vou ficar com o ponto de vista gr\u00e1fico. Mas, no que se segue, \u00e0s vezes voc\u00ea pode achar \u00fatil trocar pontos de vista e pensar sobre as coisas em termos de se-ent\u00e3o-sen\u00e3o (uma das bases da programa\u00e7\u00e3o convencional).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos usar o nosso truque de fazer bump para obter dois solavancos, colando dois pares de neur\u00f4nios ocultos na mesma rede:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede11\" class=\"aligncenter size-full wp-image-847\" data-attachment-id=\"847\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede11\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?fit=1024%2C528\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?fit=300%2C155\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?fit=1168%2C602\" data-orig-size=\"1168,602\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede11/\" data-recalc-dims=\"1\" height=\"602\" sizes=\"(max-width: 1168px) 100vw, 1168px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=1168%2C602\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?w=1168 1168w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=300%2C155 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=768%2C396 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=1024%2C528 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=200%2C103 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede11.png?resize=690%2C356 690w\" width=\"1168\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Eu suprimi os pesos aqui, simplesmente escrevendo os valores h para cada par de neur\u00f4nios ocultos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   De maneira mais geral, podemos usar essa ideia para obter o m\u00e1ximo de picos que quisermos, de qualquer altura. Em particular, podemos dividir o intervalo [0,1] em um n\u00famero grande, N, de subintervalos, e usar N pares de neur\u00f4nios ocultos para configurar picos de qualquer altura desejada. Vamos ver como isso funciona para N = 5. Desculpa pela a complexidade do diagrama abaixo (eu poderia esconder a complexidade abstraindo mais, mas acho que vale a pena colocar um pouco de complexidade, para obter uma ideia mais concreta de como essas redes funciona):\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede\" class=\"aligncenter size-full wp-image-848\" data-attachment-id=\"848\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?fit=939%2C1024\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?fit=275%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?fit=1168%2C1274\" data-orig-size=\"1168,1274\" data-permalink=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/rede-8/\" data-recalc-dims=\"1\" height=\"1274\" sizes=\"(max-width: 1168px) 100vw, 1168px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=1168%2C1274\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?w=1168 1168w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=768%2C838 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=939%2C1024 939w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=200%2C218 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/11/rede-2.png?resize=690%2C753 690w\" width=\"1168\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea pode ver que existem cinco pares de neur\u00f4nios ocultos. Os pontos escalonados para os respectivos pares de neur\u00f4nios s\u00e3o 0,1 / 5, depois 1 / 5,2 / 5 e assim por diante, para 4 / 5,5 / 5. Esses valores s\u00e3o fixos \u2013 eles fazem com que tenhamos cinco sali\u00eancias uniformemente espa\u00e7adas no gr\u00e1fico.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Cada par de neur\u00f4nios tem um valor de h associado a ele. Lembre-se, as conex\u00f5es sa\u00eddas dos neur\u00f4nios t\u00eam pesos h e \u2212h (n\u00e3o marcados). Ao alterar os pesos de sa\u00edda, estamos realmente projetando a fun\u00e7\u00e3o!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Conforme alteramos as alturas, \u00e9 poss\u00edvel ver a mudan\u00e7a correspondente nos valores h. E h\u00e1 tamb\u00e9m uma mudan\u00e7a nos pesos de sa\u00edda correspondentes, que s\u00e3o + h e \u2212h.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em outras palavras, podemos manipular diretamente a fun\u00e7\u00e3o que aparece no gr\u00e1fico \u00e0 direita e ver isso refletido nos valores h \u00e0 esquerda.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas aqui consideramos uma entrada e uma sa\u00edda, o que \u00e9 bem simples. Com m\u00faltiplas entradas o conceito \u00e9 basicamente o mesmo, mas iremos discutir as particularidades nos pr\u00f3ximos cap\u00edtulos, quando mergulharmos nas redes neurais profundas. At\u00e9 l\u00e1.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning (material utilizado com autoriza\u00e7\u00e3o do autor)\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n </div>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-833\" href=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-833\" href=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-833\" href=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-833\" href=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/como-uma-rede-neural-artificial-encontra-a-aproximacao-de-uma-funcao/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-833-5e0dd134c0ab2\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=833&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-833-5e0dd134c0ab2\" id=\"like-post-wrapper-140353593-833-5e0dd134c0ab2\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "33": "<h1 class=\"entry-title\" id=\"capitulo-33\">\n Cap\u00edtulo 33 \u2013 Por que as Redes Neurais Profundas S\u00e3o Dif\u00edceis de Treinar?\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Iniciamos agora a terceira e \u00faltima parte deste livro, em que estudaremos como funciona Deep Learning e os principais modelos e arquiteturas de redes neurais profundas, com diversos exemplos e aplica\u00e7\u00f5es. Mas primeiro temos que responder a seguinte pergunta:\u00a0Por que as Redes Neurais Profundas S\u00e3o Dif\u00edceis de Treinar?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Imagine que voc\u00ea \u00e9 um engenheiro que foi solicitado a projetar um computador do zero. Um dia, voc\u00ea est\u00e1 trabalhando em seu escrit\u00f3rio, projetando circuitos l\u00f3gicos, estabelecendo portas AND e OU, e assim por diante, quando seu chefe chega com m\u00e1s not\u00edcias. O cliente acaba de adicionar um requisito de design surpreendente: o circuito para o computador inteiro deve ter apenas duas camadas de profundidade:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"shallow_circuit\" class=\"aligncenter size-full wp-image-894\" data-attachment-id=\"894\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"shallow_circuit\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?fit=541%2C305\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?fit=300%2C169\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?fit=541%2C305\" data-orig-size=\"541,305\" data-permalink=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/shallow_circuit/\" data-recalc-dims=\"1\" height=\"305\" sizes=\"(max-width: 541px) 100vw, 541px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?resize=541%2C305\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?w=541 541w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?resize=300%2C169 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/shallow_circuit.png?resize=200%2C113 200w\" width=\"541\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea fica estupefato e diz ao seu chefe: \u201cO cliente est\u00e1 louco!\u201d\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Seu chefe responde: \u201cEu acho que eles s\u00e3o loucos tamb\u00e9m. Mas precisamos atender este requisito.\u201d\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na verdade, h\u00e1 um sentido limitado em que o cliente n\u00e3o \u00e9 louco. Suponha que voc\u00ea tenha permiss\u00e3o para usar uma porta l\u00f3gica especial que permite a voc\u00ea aplicar o AND (o \u201ce\u201d da l\u00f3gica) e juntar quantas entradas desejar. E voc\u00ea tamb\u00e9m tem permiss\u00e3o para uma porta NAND com muitas entradas, ou seja, uma porta que pode aplicar o AND a v\u00e1rias entradas e depois nega a sa\u00edda. Com essas portas especiais, \u00e9 poss\u00edvel calcular qualquer fun\u00e7\u00e3o usando um circuito com apenas duas camadas de profundidade.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas s\u00f3 porque algo \u00e9 poss\u00edvel, n\u00e3o \u00e9 uma boa ideia. Na pr\u00e1tica, quando resolvemos problemas de projeto de circuitos (ou quase todos os tipos de problemas algor\u00edtmicos), geralmente come\u00e7amos descobrindo como resolver sub-problemas, e ent\u00e3o gradualmente integramos as solu\u00e7\u00f5es. Em outras palavras, criamos uma solu\u00e7\u00e3o atrav\u00e9s de v\u00e1rias camadas de abstra\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por exemplo, suponha que estamos projetando um circuito l\u00f3gico para multiplicar dois n\u00fameros. Provavelmente, queremos constru\u00ed-lo a partir de sub-circuitos, fazendo opera\u00e7\u00f5es como adicionar dois n\u00fameros. Os sub-circuitos para adicionar dois n\u00fameros ser\u00e3o, por sua vez, constru\u00eddos a partir de sub-sub-circuitos para adicionar dois bits. Muito grosso modo, nosso circuito ser\u00e1 parecido com:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"circuit_multiplication\" class=\"aligncenter size-full wp-image-895\" data-attachment-id=\"895\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"circuit_multiplication\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?fit=541%2C351\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?fit=300%2C195\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?fit=541%2C351\" data-orig-size=\"541,351\" data-permalink=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/circuit_multiplication/\" data-recalc-dims=\"1\" height=\"351\" sizes=\"(max-width: 541px) 100vw, 541px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?resize=541%2C351\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?w=541 541w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?resize=300%2C195 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/circuit_multiplication.png?resize=200%2C130 200w\" width=\"541\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ou seja, nosso circuito final cont\u00e9m pelo menos tr\u00eas camadas de elementos de circuito. Na verdade, provavelmente conter\u00e1 mais de tr\u00eas camadas, pois dividimos as sub-tarefas em unidades menores do que as descritas anteriormente. Mas voc\u00ea compreendeu a ideia geral.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ent\u00e3o circuitos profundos facilitam o processo de design. Mas eles n\u00e3o s\u00e3o apenas \u00fateis para o design. Existem, de fato, provas matem\u00e1ticas mostrando que, para algumas fun\u00e7\u00f5es, circuitos muito superficiais requerem exponencialmente mais elementos de circuitos para serem computados do que circuitos profundos. Por exemplo, uma famosa s\u00e9rie de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://eccc.weizmann.ac.il//report/2012/137/\" rel=\"noopener\" target=\"_blank\">\n     artigos\n    </a>\n   </span>\n   no in\u00edcio dos anos 1980 mostrou que calcular a paridade de um conjunto de bits requer muitos port\u00f5es exponencialmente, se feito com um circuito superficial. Por outro lado, se voc\u00ea usa circuitos mais profundos, \u00e9 f\u00e1cil calcular a paridade usando um pequeno circuito: basta calcular a paridade de pares de bits, depois usar esses resultados para calcular a paridade de pares de pares de bits e assim por diante. construindo rapidamente a paridade geral. Os circuitos profundos, portanto, podem ser intrinsecamente muito mais poderosos que os circuitos superficiais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 agora, este livro abordou redes neurais como o cliente louco. Quase todas as redes com as quais trabalhamos t\u00eam apenas uma camada oculta de neur\u00f4nios (mais as camadas de entrada e sa\u00edda):\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede neural profunda\" class=\"aligncenter size-full wp-image-896\" data-attachment-id=\"896\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede neural profunda\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?fit=333%2C274\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?fit=300%2C247\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?fit=333%2C274\" data-orig-size=\"333,274\" data-permalink=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/tikz35/\" data-recalc-dims=\"1\" height=\"274\" sizes=\"(max-width: 333px) 100vw, 333px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?resize=333%2C274\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?w=333 333w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?resize=300%2C247 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz35.png?resize=200%2C165 200w\" width=\"333\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essas redes simples t\u00eam sido extraordinariamente \u00fateis: nos cap\u00edtulos anteriores, usamos redes como essa para classificar d\u00edgitos manuscritos com precis\u00e3o superior a 98%! No entanto, intuitivamente, esperamos que as redes com muito mais camadas ocultas sejam mais poderosas:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede neural profunda\" class=\"aligncenter size-full wp-image-897\" data-attachment-id=\"897\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede neural profunda\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?fit=560%2C279\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?fit=300%2C149\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?fit=560%2C279\" data-orig-size=\"560,279\" data-permalink=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/tikz36/\" data-recalc-dims=\"1\" height=\"279\" sizes=\"(max-width: 560px) 100vw, 560px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?resize=560%2C279\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?w=560 560w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?resize=300%2C149 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/tikz36.png?resize=200%2C100 200w\" width=\"560\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Tais redes poderiam usar as camadas intermedi\u00e1rias para construir m\u00faltiplas camadas de abstra\u00e7\u00e3o, assim como fazemos em circuitos booleanos. Por exemplo, se estamos fazendo reconhecimento de padr\u00f5es visuais, ent\u00e3o os neur\u00f4nios da primeira camada podem aprender a reconhecer bordas, os neur\u00f4nios da segunda camada podem aprender a reconhecer formas mais complexas, digamos, tri\u00e2ngulo ou ret\u00e2ngulos, constru\u00eddos a partir de bordas. A terceira camada reconheceria formas ainda mais complexas. E assim por diante.\n   <strong>\n    Essas m\u00faltiplas camadas de abstra\u00e7\u00e3o parecem propiciar \u00e0s redes profundas uma vantagem convincente em aprender a resolver problemas complexos de reconhecimento de padr\u00f5es.\n   </strong>\n   Al\u00e9m disso, assim como no caso dos circuitos, existem resultados te\u00f3ricos sugerindo que as redes profundas s\u00e3o intrinsecamente mais poderosas do que as redes superficiais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como podemos treinar essas redes profundas? Nos pr\u00f3ximos cap\u00edtulos, tentaremos treinar redes profundas usando nosso algoritmo de aprendizado: descendente de gradiente estoc\u00e1stico por retropropaga\u00e7\u00e3o (que j\u00e1 estudamos em detalhes nos cap\u00edtulos anteriores, mas que agora aplicaremos em redes neurais profundas). Mas vamos nos deparar com problemas, com nossas redes profundas n\u00e3o realizando muito (se for o caso) melhor do que redes rasas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa falha parece surpreendente \u00e0 luz da discuss\u00e3o acima. Em vez de desistir de redes profundas, vamos nos aprofundar e tentar entender o que est\u00e1 dificultando o treinamento de nossas redes profundas. Quando olharmos de perto, descobriremos que as diferentes camadas da nossa rede profunda est\u00e3o aprendendo em velocidades muito diferentes. Em particular, quando as camadas posteriores da rede est\u00e3o aprendendo bem, as camadas iniciais geralmente ficam presas durante o treinamento, aprendendo quase nada. Este empecilho n\u00e3o \u00e9 simplesmente devido \u00e0 m\u00e1 sorte. Em vez disso, descobriremos que existem raz\u00f5es fundamentais para a lentid\u00e3o do aprendizado, conectadas ao nosso uso de t\u00e9cnicas de aprendizado baseadas em gradientes.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c0 medida que nos aprofundamos no problema, aprenderemos que o fen\u00f4meno oposto tamb\u00e9m pode ocorrer: as primeiras camadas podem estar aprendendo bem, mas as camadas posteriores podem ficar presas. Na verdade, descobriremos que existe uma instabilidade intr\u00ednseca associada ao aprendizado por gradiente descendente em redes neurais profundas de muitas camadas. Essa instabilidade tende a resultar em camadas anteriores ou posteriores ficando presas durante o treinamento.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas, ao nos debru\u00e7armos sobre essas dificuldades, podemos come\u00e7ar a entender o que \u00e9 necess\u00e1rio para treinar redes profundas de maneira eficaz.\n  </span>\n  <span style=\"color: #000000;\">\n   E isso \u00e9 exatamente o que faremos nos pr\u00f3ximos cap\u00edtulos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora \u00e9 que come\u00e7a a divers\u00e3o. At\u00e9 l\u00e1.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n </div>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-884\" href=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-884\" href=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-884\" href=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-884\" href=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-884-5e0dd136d859d\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=884&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-884-5e0dd136d859d\" id=\"like-post-wrapper-140353593-884-5e0dd136d859d\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "34": "<h1 class=\"entry-title\" id=\"capitulo-34\">\n Cap\u00edtulo 34 \u2013 O Problema da Dissipa\u00e7\u00e3o do Gradiente\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ent\u00e3o,\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/por-que-as-redes-neurais-profundas-sao-dificeis-de-treinar/\" rel=\"noopener\" target=\"_blank\">\n     por que as redes neurais profundas s\u00e3o dif\u00edceis de treinar\n    </a>\n   </span>\n   ?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para responder a essa pergunta, primeiro revisitemos o caso de uma rede com apenas uma camada oculta. Como de costume, usaremos o problema de classifica\u00e7\u00e3o de d\u00edgitos MNIST o mesmo j\u00e1 estudado nos cap\u00edtulos anteriores e que voc\u00ea encontra no reposit\u00f3rio deste livro no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr\" rel=\"noopener\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A partir de um shell do Python, n\u00f3s carregamos os dados MNIST:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"mnist\" class=\"aligncenter wp-image-919 size-full\" data-attachment-id=\"919\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"mnist\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?fit=1024%2C101\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?fit=300%2C29\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?fit=1302%2C128\" data-orig-size=\"1302,128\" data-permalink=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/image1-4/\" data-recalc-dims=\"1\" height=\"115\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=1170%2C115\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?w=1302 1302w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=300%2C29 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=768%2C76 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=1024%2C101 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=200%2C20 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image1-2.png?resize=690%2C68 690w\" width=\"1170\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Montamos nossa rede:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"mnist\" class=\"aligncenter wp-image-920 size-full\" data-attachment-id=\"920\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"mnist\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?fit=672%2C128\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?fit=300%2C57\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?fit=672%2C128\" data-orig-size=\"672,128\" data-permalink=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/image2-4/\" data-recalc-dims=\"1\" height=\"128\" sizes=\"(max-width: 672px) 100vw, 672px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?resize=672%2C128\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?w=672 672w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?resize=300%2C57 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image2-2.png?resize=200%2C38 200w\" width=\"672\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esta rede possui 784 neur\u00f4nios na camada de entrada, correspondendo a 28 \u00d7 28 = 784 pixels na imagem de entrada. Utilizamos 30 neur\u00f4nios ocultos, assim como 10 neur\u00f4nios de sa\u00edda, correspondentes \u00e0s 10 classifica\u00e7\u00f5es poss\u00edveis para os d\u00edgitos MNIST (\u20180\u2019, \u20181\u2019, \u20182\u2019,\u2026, \u20189\u2019).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos tentar treinar nossa rede por 30 \u00e9pocas completas, usando mini-lotes de 10 exemplos de treinamento por vez, uma taxa de aprendizado \u03b7 = 0.1 e um par\u00e2metro de regulariza\u00e7\u00e3o \u03bb = 5.0. \u00c0 medida que treinarmos, monitoramos a precis\u00e3o da classifica\u00e7\u00e3o no conjunto de dados validation_data. Podemos executar o script test.py com todos os comandos. Via prompt de comando ou terminal, digitamos:\n   <strong>\n    python test.py\n   </strong>\n   (o treinamento pode levar muitos minutos dependendo da velocidade do computador).\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"treinamento\" class=\"aligncenter size-full wp-image-923\" data-attachment-id=\"923\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"treinamento\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?fit=982%2C228\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?fit=300%2C70\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?fit=982%2C228\" data-orig-size=\"982,228\" data-permalink=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/image3-3/\" data-recalc-dims=\"1\" height=\"228\" sizes=\"(max-width: 982px) 100vw, 982px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=982%2C228\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?w=982 982w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=300%2C70 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=768%2C178 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=200%2C46 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/image3-1.png?resize=690%2C160 690w\" width=\"982\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao final do treinamento, obtemos uma precis\u00e3o de classifica\u00e7\u00e3o de 96,48% (aproximadamente), compar\u00e1vel a nossos resultados anteriores com uma configura\u00e7\u00e3o semelhante.\n  </span>\n  <span style=\"color: #000000;\">\n   Agora, vamos adicionar outra camada oculta, tamb\u00e9m com 30 neur\u00f4nios, e tentar treinar com os mesmos hiperpar\u00e2metros. Usamos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    net = network2.Network([784, 30, 30, 10])\n   </span>\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isto d\u00e1 uma melhor precis\u00e3o de classifica\u00e7\u00e3o 96,90%. Isso \u00e9 encorajador: um pouco mais de profundidade est\u00e1 ajudando. Vamos adicionar outra camada oculta de 30 neur\u00f4nios.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    net = network2.Network([784, 30, 30, 30, 10])\n   </span>\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso n\u00e3o ajuda em nada. Na verdade, o resultado cai para 96,57%, pr\u00f3ximo \u00e0 nossa rede original. E suponha que inserimos mais uma camada oculta.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: center;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    net = network2.Network([784, 30, 30, 30, 30, 10])\n   </span>\n  </strong>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esse comportamento parece estranho. Intuitivamente, camadas ocultas extras devem tornar a rede capaz de aprender fun\u00e7\u00f5es de classifica\u00e7\u00e3o mais complexas e, assim, fazer uma melhor classifica\u00e7\u00e3o. Certamente, as coisas n\u00e3o devem piorar, j\u00e1 que as camadas extras podem, no pior dos casos, simplesmente n\u00e3o fazer nada. Mas n\u00e3o \u00e9 isso que est\u00e1 acontecendo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ent\u00e3o, o que est\u00e1 acontecendo? Vamos supor que as camadas ocultas extras realmente possam ajudar em princ\u00edpio e o problema \u00e9 que nosso algoritmo de aprendizado n\u00e3o est\u00e1 encontrando os pesos e vieses corretos. Gostar\u00edamos de descobrir o que est\u00e1 errado em nosso algoritmo de aprendizado e como fazer melhor.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para entender melhor o que est\u00e1 errado, vamos visualizar como a rede aprende. Abaixo, tra\u00e7amos parte de uma rede [784,30,30,10], ou seja, uma rede com duas camadas ocultas, cada uma contendo 30 neur\u00f4nios ocultos. Cada neur\u00f4nio no diagrama tem uma pequena barra nele, representando a rapidez com que o neur\u00f4nio est\u00e1 mudando \u00e0 medida que a rede aprende. Uma barra grande significa que o peso e o vi\u00e9s do neur\u00f4nio est\u00e3o mudando rapidamente, enquanto uma barra pequena significa que os pesos e o vi\u00e9s est\u00e3o mudando lentamente. Mais precisamente, as barras indicam o gradiente \u2202C / \u2202b para cada neur\u00f4nio, ou seja, a taxa de mudan\u00e7a do custo em rela\u00e7\u00e3o ao vi\u00e9s do neur\u00f4nio. Nos cap\u00edtulos anteriores, vimos que essa quantidade de gradiente controlava n\u00e3o apenas a rapidez com que o vi\u00e9s muda durante o aprendizado, mas tamb\u00e9m a rapidez com que os pesos inseridos no neur\u00f4nio tamb\u00e9m mudam. N\u00e3o se preocupe se voc\u00ea n\u00e3o se lembrar dos detalhes: a \u00fanica coisa a ter em mente \u00e9 simplesmente que essas barras mostram a rapidez com que os pesos e os vieses de cada neur\u00f4nio mudam conforme a rede aprende.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para manter o diagrama simples, mostrei apenas os seis principais neur\u00f4nios nas duas camadas ocultas. Eu omiti os neur\u00f4nios de entrada, pois eles n\u00e3o t\u00eam pesos nem vi\u00e9s para aprender. Eu tamb\u00e9m omiti os neur\u00f4nios de sa\u00edda, j\u00e1 que estamos fazendo compara\u00e7\u00f5es por camadas, e faz mais sentido comparar camadas com o mesmo n\u00famero de neur\u00f4nios. Os resultados foram plotados no in\u00edcio do treinamento, ou seja, imediatamente ap\u00f3s a inicializa\u00e7\u00e3o da rede. Aqui est\u00e3o eles:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"rede\" class=\"aligncenter size-full wp-image-924\" data-attachment-id=\"924\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?fit=470%2C620\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?fit=227%2C300\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?fit=470%2C620\" data-orig-size=\"470,620\" data-permalink=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/download-2/\" data-recalc-dims=\"1\" height=\"620\" sizes=\"(max-width: 470px) 100vw, 470px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?resize=470%2C620\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?w=470 470w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?resize=227%2C300 227w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/download.png?resize=200%2C264 200w\" width=\"470\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A rede foi inicializada aleatoriamente e, portanto, n\u00e3o \u00e9 surpreendente que haja muita varia\u00e7\u00e3o na rapidez com que os neur\u00f4nios aprendem. Ainda assim, uma coisa que vale ressaltar \u00e9 que as barras na segunda camada oculta s\u00e3o em sua maioria muito maiores que as barras na primeira camada oculta. Como resultado, os neur\u00f4nios da segunda camada oculta aprendem um pouco mais r\u00e1pido que os neur\u00f4nios da primeira camada oculta. Isso \u00e9 meramente uma coincid\u00eancia, ou os neur\u00f4nios da segunda camada oculta provavelmente aprender\u00e3o mais r\u00e1pido do que os neur\u00f4nios na primeira camada oculta em geral?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para determinar se esse \u00e9 o caso, \u00e9 \u00fatil ter uma maneira global de comparar a velocidade de aprendizado na primeira e segunda camadas ocultas. Para fazer isso, vamos indicar o gradiente como \u03b4lj = \u2202C / \u2202blj, ou seja, o gradiente para o neur\u00f4nio jth na camada lth.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos pensar no gradiente \u03b41 como um vetor cujas entradas determinam a rapidez com que a primeira camada oculta aprende, e \u03b42 como um vetor cujas entradas determinam a rapidez com que a segunda camada oculta aprende. Em seguida, usaremos os comprimentos desses vetores como medidas globais da velocidade na qual as camadas est\u00e3o aprendendo. Assim, por exemplo, o comprimento \u201c\u03b41\u201d mede a velocidade na qual a primeira camada oculta est\u00e1 aprendendo, enquanto o comprimento \u201c\u03b42\u201d mede a velocidade na qual a segunda camada oculta est\u00e1 aprendendo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com essas defini\u00e7\u00f5es, e na mesma configura\u00e7\u00e3o que foi plotada acima, encontramos \u03b4\u03b41 = 0.07\u2026 e \u03b4\u03b42 = 0.31\u2026. Isso confirma nossa suspeita anterior: os neur\u00f4nios na segunda camada oculta realmente est\u00e3o aprendendo muito mais r\u00e1pido que os neur\u00f4nios da primeira camada oculta.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O que acontece se adicionarmos mais camadas ocultas? Se tivermos tr\u00eas camadas ocultas, em uma rede [784,30,30,30,10], ent\u00e3o as respectivas velocidades de aprendizado ser\u00e3o 0,012, 0,060 e 0,283. Novamente, as camadas ocultas anteriores est\u00e3o aprendendo muito mais lentamente que as camadas ocultas posteriores. Suponha que adicionemos mais uma camada com 30 neur\u00f4nios ocultos. Nesse caso, as respectivas velocidades de aprendizado s\u00e3o 0,003, 0,017, 0,070 e 0,285. O padr\u00e3o \u00e9 v\u00e1lido: as camadas iniciais aprendem mais lentamente que as camadas posteriores.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Temos observado a velocidade de aprendizado no in\u00edcio do treinamento, ou seja, logo ap\u00f3s as redes serem inicializadas. Como a velocidade do aprendizado muda \u00e0 medida que treinamos nossas redes? Vamos voltar para ver a rede com apenas duas camadas ocultas. A velocidade de aprendizado muda da seguinte forma:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"training_speed_2_layers\" class=\"aligncenter size-full wp-image-926\" data-attachment-id=\"926\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"training_speed_2_layers\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?fit=800%2C600\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?fit=300%2C225\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?fit=800%2C600\" data-orig-size=\"800,600\" data-permalink=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/training_speed_2_layers/\" data-recalc-dims=\"1\" height=\"600\" sizes=\"(max-width: 800px) 100vw, 800px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=800%2C600\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?w=800 800w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=300%2C225 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=768%2C576 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=200%2C150 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_2_layers.png?resize=690%2C518 690w\" width=\"800\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para gerar esses resultados, usamos a descida do gradiente em lote com apenas 1.000 imagens de treinamento, treinadas em mais de 500 \u00e9pocas. Isso \u00e9 um pouco diferente do que normalmente treinamos nos cap\u00edtulos anteriores, mas acontece que o uso de gradiente estoc\u00e1stico em mini-lote d\u00e1 resultados muito mais ruidosos (embora muito similares, quando voc\u00ea mede o ru\u00eddo). Usar os par\u00e2metros que escolhemos \u00e9 uma maneira f\u00e1cil de suavizar os resultados, para que possamos ver o que est\u00e1 acontecendo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em qualquer caso, como voc\u00ea pode ver, as duas camadas come\u00e7am a aprender em velocidades muito diferentes (como j\u00e1 sabemos). A velocidade em ambas as camadas cai muito rapidamente, antes de se recuperar. Mas, apesar de tudo, a primeira camada oculta aprende muito mais lentamente do que a segunda camada oculta.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E quanto a redes mais complexas? Aqui est\u00e3o os resultados de uma experi\u00eancia semelhante, mas desta vez com tr\u00eas camadas ocultas (uma rede [784,30,30,30,10]):\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"training_speed_3_layers\" class=\"aligncenter size-full wp-image-927\" data-attachment-id=\"927\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"training_speed_3_layers\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?fit=800%2C600\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?fit=300%2C225\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?fit=800%2C600\" data-orig-size=\"800,600\" data-permalink=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/training_speed_3_layers/\" data-recalc-dims=\"1\" height=\"600\" sizes=\"(max-width: 800px) 100vw, 800px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=800%2C600\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=768%2C576 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=200%2C150 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_3_layers.png?resize=690%2C518 690w\" width=\"800\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mais uma vez, as primeiras camadas ocultas aprendem muito mais lentamente do que as camadas ocultas posteriores. Finalmente, vamos adicionar uma quarta camada oculta (uma rede [784,30,30,30,30,10]) e ver o que acontece quando treinamos:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"training_speed_4_layers\" class=\"aligncenter size-full wp-image-928\" data-attachment-id=\"928\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"training_speed_4_layers\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?fit=800%2C600\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?fit=300%2C225\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?fit=800%2C600\" data-orig-size=\"800,600\" data-permalink=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/training_speed_4_layers/\" data-recalc-dims=\"1\" height=\"600\" sizes=\"(max-width: 800px) 100vw, 800px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=800%2C600\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?w=800 800w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=300%2C225 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=768%2C576 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=200%2C150 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/12/training_speed_4_layers.png?resize=690%2C518 690w\" width=\"800\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mais uma vez, as primeiras camadas ocultas aprendem muito mais lentamente do que as camadas ocultas posteriores. Nesse caso, a primeira camada oculta est\u00e1 aprendendo aproximadamente 100 vezes mais lenta que a camada oculta final. Natural que estiv\u00e9ssemos tendo problemas para treinar essas redes antes!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Temos aqui uma observa\u00e7\u00e3o importante: em pelo menos algumas redes neurais profundas, o gradiente tende a diminuir \u00e0 medida que nos movemos para tr\u00e1s atrav\u00e9s das camadas ocultas. Isso significa que os neur\u00f4nios nas camadas anteriores aprendem muito mais lentamente que os neur\u00f4nios nas camadas posteriores. E, embora tenhamos visto isso em apenas uma \u00fanica rede, h\u00e1 raz\u00f5es fundamentais pelas quais isso acontece em muitas redes neurais. O fen\u00f4meno \u00e9 conhecido como\n   <strong>\n    O Problema da Dissipa\u00e7\u00e3o do Gradiente\n   </strong>\n   ou\n   <strong>\n    The Vanishing Gradient Problem\n   </strong>\n   . Esse \u00e9 um problema muito comum e ainda mais evidente em Redes Neurais Recorrentes, usadas em aplica\u00e7\u00f5es de Processamento de Linguagem Natural.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por que o problema de dissipa\u00e7\u00e3o do gradiente ocorre? Existem maneiras de evitar isso? E como devemos lidar com isso no treinamento de redes neurais profundas? Na verdade, aprenderemos rapidamente que n\u00e3o \u00e9 inevit\u00e1vel, embora a alternativa tamb\u00e9m n\u00e3o seja muito atraente: \u00e0s vezes, o gradiente fica muito maior nas camadas anteriores! Este problema \u00e9 chamado de explos\u00e3o do gradiente, e n\u00e3o \u00e9 uma not\u00edcia muito melhor do que o problema da dissipa\u00e7\u00e3o do gradiente. Geralmente, verifica-se que o gradiente em redes neurais profundas \u00e9 inst\u00e1vel, tendendo a explodir ou a desaparecer nas camadas anteriores. Essa instabilidade \u00e9 um problema fundamental para o aprendizado baseado em gradiente em redes neurais profundas. \u00c9 algo que precisamos entender e, se poss\u00edvel, tomar medidas para resolver.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Momentaneamente se afastando das redes neurais, imagine que estamos tentando minimizar numericamente uma fun\u00e7\u00e3o f(x) de uma \u00fanica vari\u00e1vel. N\u00e3o seria uma boa not\u00edcia se a derivada f\u2032(x) fosse pequena? Isso n\u00e3o significaria que j\u00e1 est\u00e1vamos perto de um extremo? De forma semelhante, o pequeno gradiente nas primeiras camadas de uma rede profunda pode significar que n\u00e3o precisamos fazer muito ajuste dos pesos e vieses?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Claro, isso n\u00e3o \u00e9 o caso. Lembre-se de que inicializamos aleatoriamente o peso e os vieses na rede. \u00c9 extremamente improv\u00e1vel que nossos pesos e vieses iniciais fa\u00e7am um bom trabalho em qualquer coisa que desejamos que nossa rede fa\u00e7a. Para ser concreto, considere a primeira camada de pesos em uma rede [784,30,30,30,10] para o problema MNIST. A inicializa\u00e7\u00e3o aleat\u00f3ria significa que a primeira camada elimina a maior parte das informa\u00e7\u00f5es sobre a imagem de entrada. Mesmo que as camadas posteriores tenham sido extensivamente treinadas, elas ainda achar\u00e3o extremamente dif\u00edcil identificar a imagem de entrada, simplesmente porque elas n\u00e3o possuem informa\u00e7\u00f5es suficientes. E assim, n\u00e3o \u00e9 poss\u00edvel que n\u00e3o seja preciso aprender muito na primeira camada. Se vamos treinar redes profundas, precisamos descobrir como resolver o problema da dissipa\u00e7\u00e3o do gradiente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Se eu fosse voc\u00ea, n\u00e3o perderia o pr\u00f3ximo cap\u00edtulo com uma explica\u00e7\u00e3o matem\u00e1tica para esse importante fen\u00f4meno no treinamento de redes neurais profundas (Deep Learning).\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n   (material usado com autoriza\u00e7\u00e3o do autor)\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n </div>\n <div class=\"sharedaddy sd-sharing-enabled\">\n </div>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-914\" href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-914\" href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-914\" href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-914\" href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-914-5e0dd138d62f0\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=914&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-914-5e0dd138d62f0\" id=\"like-post-wrapper-140353593-914-5e0dd138d62f0\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "35": "<h1 class=\"entry-title\" id=\"capitulo-35\">\n Cap\u00edtulo 35 \u2013 A Matem\u00e1tica do Problema de Dissipa\u00e7\u00e3o do Gradiente em Deep Learning\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos continuar a discuss\u00e3o iniciada no cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/\" rel=\"noopener\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   . Para entender porque o problema da dissipa\u00e7\u00e3o do gradiente ocorre, vamos considerar a rede neural profunda mais simples: uma com apenas um \u00fanico neur\u00f4nio em cada camada. Aqui est\u00e1 uma rede com tr\u00eas camadas ocultas:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"tikz37\" class=\"aligncenter size-full wp-image-956\" data-attachment-id=\"956\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tikz37\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?fit=545%2C47\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?fit=300%2C26\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?fit=545%2C47\" data-orig-size=\"545,47\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/tikz37/\" data-recalc-dims=\"1\" height=\"47\" sizes=\"(max-width: 545px) 100vw, 545px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?resize=545%2C47\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?w=545 545w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?resize=300%2C26 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz37.png?resize=200%2C17 200w\" width=\"545\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui, w1, w2,\u2026 s\u00e3o os pesos, b1, b2,\u2026 s\u00e3o os vieses e C \u00e9 alguma fun\u00e7\u00e3o de custo. Apenas para lembrar como isso funciona, a sa\u00edda aj do neur\u00f4nio j \u00e9 \u03c3(zj), onde \u03c3 \u00e9 a fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigm\u00f3ide usual, e zj = wjaj \u2212 1 + bj \u00e9 a entrada ponderada para o neur\u00f4nio. Eu desenhei o custo C no final para enfatizar que o custo \u00e9 uma fun\u00e7\u00e3o da sa\u00edda da rede, a4: se a sa\u00edda real da rede estiver pr\u00f3xima da sa\u00edda desejada, ent\u00e3o o custo ser\u00e1 baixo, enquanto se estiver longe, o custo ser\u00e1 alto.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos estudar o gradiente \u2202C / \u2202b1 associado ao primeiro neur\u00f4nio oculto. Definiremos uma express\u00e3o para \u2202C / \u2202b1 e, estudando essa express\u00e3o, entenderemos porque o problema da dissipa\u00e7\u00e3o do gradiente ocorre.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vou come\u00e7ar simplesmente mostrando a express\u00e3o para \u2202C / \u2202b1. Parece assustador, mas na verdade tem uma estrutura simples, que descreverei em breve. Aqui est\u00e1 a express\u00e3o (ignore a rede, por enquanto, e note que \u03c3 \u2032 \u00e9 apenas a derivada da fun\u00e7\u00e3o \u03c3):\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"tikz38\" class=\"aligncenter size-full wp-image-957\" data-attachment-id=\"957\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tikz38\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?fit=552%2C96\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?fit=300%2C52\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?fit=552%2C96\" data-orig-size=\"552,96\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/tikz38/\" data-recalc-dims=\"1\" height=\"96\" sizes=\"(max-width: 552px) 100vw, 552px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?resize=552%2C96\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?w=552 552w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?resize=300%2C52 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/tikz38.png?resize=200%2C35 200w\" width=\"552\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A estrutura na express\u00e3o \u00e9 a seguinte: existe um termo \u03c3 \u2032 (zj) no produto para cada neur\u00f4nio na rede; um peso wj para cada peso na rede; e um termo final \u2202C / \u2202a4, correspondente \u00e0 fun\u00e7\u00e3o de custo no final. Observe que coloquei cada termo na express\u00e3o acima da parte correspondente da rede. Ent\u00e3o a rede em si \u00e9 um mnem\u00f4nico para a express\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   H\u00e1 tamb\u00e9m uma explica\u00e7\u00e3o simples de porque a express\u00e3o acima \u00e9 verdadeira e, portanto, \u00e9 divertido (e talvez esclarecedor) dar uma olhada nessa explica\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Imagine que fazemos uma pequena mudan\u00e7a \u0394b1 no vi\u00e9s b1. Isso ir\u00e1 desencadear uma s\u00e9rie de mudan\u00e7as em cascata no resto da rede. Primeiro, causa uma mudan\u00e7a \u0394a1 na sa\u00edda do primeiro neur\u00f4nio oculto. Isso, por sua vez, causar\u00e1 uma mudan\u00e7a \u0394z2 na entrada ponderada para o segundo neur\u00f4nio oculto. Ent\u00e3o, uma mudan\u00e7a \u0394a2 na sa\u00edda do segundo neur\u00f4nio oculto. E assim por diante, at\u00e9 chegar a uma mudan\u00e7a de C no custo na sa\u00edda. N\u00f3s temos\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter size-full wp-image-958\" data-attachment-id=\"958\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?fit=288%2C168\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?fit=288%2C168\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?fit=288%2C168\" data-orig-size=\"288,168\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form1-5/\" data-recalc-dims=\"1\" height=\"168\" sizes=\"(max-width: 288px) 100vw, 288px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?resize=288%2C168\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?w=288 288w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form1.png?resize=200%2C117 200w\" width=\"288\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso sugere que podemos descobrir uma express\u00e3o para o gradiente \u2202C / \u2202b1, acompanhando cuidadosamente o efeito de cada etapa dessa cascata.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para fazer isso, vamos pensar em como \u0394b1 faz com que a sa\u00edda a1 do primeiro neur\u00f4nio oculto mude. N\u00f3s temos a1 = \u03c3 (z1) = \u03c3 (w1a0 + b1), ent\u00e3o:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-959\" data-attachment-id=\"959\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?fit=566%2C250\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?fit=300%2C133\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?fit=566%2C250\" data-orig-size=\"566,250\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form2-9/\" data-recalc-dims=\"1\" height=\"250\" sizes=\"(max-width: 566px) 100vw, 566px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?resize=566%2C250\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?w=566 566w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?resize=300%2C133 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form2.png?resize=200%2C88 200w\" width=\"566\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esse termo \u03c3 \u2032 (z1) deve parecer familiar: \u00e9 o primeiro termo em nossa express\u00e3o reivindicada para o gradiente \u2202C / \u2202b1. Intuitivamente, esse termo converte uma mudan\u00e7a \u0394b1 no vi\u00e9s em uma mudan\u00e7a \u0394a1 na ativa\u00e7\u00e3o de sa\u00edda. Essa mudan\u00e7a \u0394a1 por sua vez causa uma mudan\u00e7a na entrada ponderada z2 = w2a1 + b2 para o segundo neur\u00f4nio oculto:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form3\" class=\"aligncenter size-full wp-image-960\" data-attachment-id=\"960\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?fit=380%2C242\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?fit=300%2C191\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?fit=380%2C242\" data-orig-size=\"380,242\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form3-8/\" data-recalc-dims=\"1\" height=\"242\" sizes=\"(max-width: 380px) 100vw, 380px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?resize=380%2C242\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?w=380 380w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?resize=300%2C191 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form3.png?resize=200%2C127 200w\" width=\"380\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Combinando nossas express\u00f5es para \u0394z2 e \u0394a1, vemos como a mudan\u00e7a no vi\u00e9s b1 se propaga ao longo da rede para afetar z2:\n  </span>\n </p>\n <p>\n  <img alt=\"form\" class=\"aligncenter size-full wp-image-974\" data-attachment-id=\"974\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?fit=450%2C134\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?fit=300%2C89\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?fit=450%2C134\" data-orig-size=\"450,134\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form-7/\" data-recalc-dims=\"1\" height=\"134\" sizes=\"(max-width: 450px) 100vw, 450px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?resize=450%2C134\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?w=450 450w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?resize=300%2C89 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form.png?resize=200%2C60 200w\" width=\"450\"/>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Novamente, isso deve parecer familiar: agora temos os dois primeiros termos em nossa express\u00e3o reivindicada para o gradiente \u2202C / \u2202b1.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos continuar dessa maneira, rastreando a maneira como as altera\u00e7\u00f5es se propagam pelo resto da rede. Em cada neur\u00f4nio, pegamos um termo \u03c3 \u2032 (zj) e, em cada peso, escolhemos um termo wj. O resultado final \u00e9 uma express\u00e3o que relaciona a mudan\u00e7a final \u0394C no custo para a mudan\u00e7a inicial \u0394b1 no vi\u00e9s:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form5\" class=\"aligncenter size-full wp-image-961\" data-attachment-id=\"961\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?fit=814%2C160\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?fit=300%2C59\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?fit=814%2C160\" data-orig-size=\"814,160\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form5-4/\" data-recalc-dims=\"1\" height=\"160\" sizes=\"(max-width: 814px) 100vw, 814px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=814%2C160\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?w=814 814w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=300%2C59 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=768%2C151 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=200%2C39 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form5.png?resize=690%2C136 690w\" width=\"814\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Dividindo por \u0394b1, de fato, obtemos a express\u00e3o desejada para o gradiente:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form6\" class=\"aligncenter size-full wp-image-962\" data-attachment-id=\"962\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form6\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?fit=750%2C178\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?fit=300%2C71\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?fit=750%2C178\" data-orig-size=\"750,178\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form6-3/\" data-recalc-dims=\"1\" height=\"178\" sizes=\"(max-width: 750px) 100vw, 750px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=750%2C178\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?w=750 750w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=300%2C71 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=200%2C47 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form6.png?resize=690%2C164 690w\" width=\"750\"/>\n  </span>\n </p>\n <h2>\n </h2>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por que o problema da dissipa\u00e7\u00e3o do gradiente ocorre afinal?\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para entender porque o problema da dissipa\u00e7\u00e3o do gradiente ocorre, vamos escrever explicitamente a express\u00e3o inteira para o gradiente:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form7\" class=\"aligncenter size-full wp-image-963\" data-attachment-id=\"963\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form7\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?fit=938%2C168\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?fit=300%2C54\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?fit=938%2C168\" data-orig-size=\"938,168\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form7-2/\" data-recalc-dims=\"1\" height=\"168\" sizes=\"(max-width: 938px) 100vw, 938px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=938%2C168\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?w=938 938w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=300%2C54 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=768%2C138 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=200%2C36 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form7.png?resize=690%2C124 690w\" width=\"938\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com exce\u00e7\u00e3o do \u00faltimo termo, essa express\u00e3o \u00e9 um produto de termos da forma wj\u03c3 \u2032 (zj). Para entender como cada um desses termos se comporta, vamos ver um gr\u00e1fico da fun\u00e7\u00e3o \u03c3 \u2032:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form9\" class=\"aligncenter wp-image-965 size-full\" data-attachment-id=\"965\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form9\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?fit=1022%2C696\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?fit=300%2C204\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?fit=1022%2C696\" data-orig-size=\"1022,696\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form9/\" data-recalc-dims=\"1\" height=\"696\" sizes=\"(max-width: 1022px) 100vw, 1022px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=1022%2C696\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?w=1022 1022w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=300%2C204 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=768%2C523 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=200%2C136 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form9.png?resize=690%2C470 690w\" width=\"1022\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A derivada atinge um m\u00e1ximo em \u03c3 \u2032 (0) = 1/4. Agora, se usarmos nossa abordagem padr\u00e3o para inicializar os pesos na rede, escolheremos os pesos usando uma distribui\u00e7\u00e3o normal (Gaussiana) com m\u00e9dia 0 e desvio padr\u00e3o 1. Assim, os pesos geralmente satisfazem | wj | &lt; 1. Reunindo essas observa\u00e7\u00f5es, vemos que os termos wj\u03c3 \u2032 (zj) geralmente satisfazem | wj\u03c3 \u2032 (zj) | &lt; 1/4. E quando tomamos um produto de muitos desses termos, o produto tender\u00e1 a diminuir exponencialmente: quanto mais termos, menor ser\u00e1 o produto. Isso est\u00e1 come\u00e7ando a cheirar como uma poss\u00edvel explica\u00e7\u00e3o para o problema da dissipa\u00e7\u00e3o do gradiente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para tornar tudo isso um pouco mais expl\u00edcito, vamos comparar a express\u00e3o para \u2202C / \u2202b1 com uma express\u00e3o para o gradiente em rela\u00e7\u00e3o a um vi\u00e9s posterior, digamos \u2202C / \u2202b3. Naturalmente, n\u00e3o explicamos explicitamente uma express\u00e3o para \u2202C / \u2202b3, mas segue o mesmo padr\u00e3o descrito acima para \u2202C / \u2202b1. Aqui est\u00e1 a compara\u00e7\u00e3o das duas express\u00f5es:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form10\" class=\"aligncenter wp-image-966 size-large\" data-attachment-id=\"966\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form10\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?fit=1024%2C542\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?fit=300%2C159\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?fit=1180%2C624\" data-orig-size=\"1180,624\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/form10/\" data-recalc-dims=\"1\" height=\"542\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=1024%2C542\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=1024%2C542 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=300%2C159 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=768%2C406 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=200%2C106 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?resize=690%2C365 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/01/form10.png?w=1180 1180w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As duas express\u00f5es compartilham muitos termos. Mas o gradiente \u2202C / \u2202b1 inclui dois termos extras, cada um da forma wj\u03c3 \u2032 (zj). Como vimos, esses termos s\u00e3o tipicamente menores que 1/4 de magnitude. E assim o gradiente \u2202C / \u2202b1 normalmente ser\u00e1 um fator de 16 (ou mais) menor que \u2202C / \u2202b3. Esta \u00e9 a origem essencial do problema da dissipa\u00e7\u00e3o do gradiente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c9 claro que este \u00e9 um argumento informal, n\u00e3o uma prova rigorosa de que o problema da dissipa\u00e7\u00e3o do gradiente ocorrer\u00e1. Existem v\u00e1rias cl\u00e1usulas de escape poss\u00edveis. Em particular, podemos nos perguntar se os pesos wj poderiam crescer durante o treinamento. Se o fizerem, \u00e9 poss\u00edvel que os termos wj\u03c3 \u2032 (zj) no produto deixem de satisfazer | wj\u03c3 \u2032 (zj) | &lt; 1/4. De fato, se os termos se tornarem grandes o suficiente \u2013 maiores que 1 \u2013 ent\u00e3o n\u00e3o teremos mais um problema de dissipa\u00e7\u00e3o do gradiente. Em vez disso, o gradiente crescer\u00e1 exponencialmente \u00e0 medida que nos movemos para tr\u00e1s pelas camadas. Em vez de um problema de dissipa\u00e7\u00e3o do gradiente, teremos um problema de explos\u00e3o do gradiente. Mas isso \u00e9 assunto para o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para aprender todos os detalhes matem\u00e1ticos por tr\u00e1s desse processo, confira nosso curso \u00fanico e exclusivo no Brasil:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener\" target=\"_blank\">\n     Matem\u00e1tica Para Machine Learning\n    </a>\n   </span>\n   . At\u00e9 o pr\u00f3ximo cap\u00edtulo.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n   (material usado com autoriza\u00e7\u00e3o do autor)\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-955\" href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-955\" href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-955\" href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-955\" href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-955-5e0dd13b939f6\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=955&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-955-5e0dd13b939f6\" id=\"like-post-wrapper-140353593-955-5e0dd13b939f6\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "36": "<h1 class=\"entry-title\" id=\"capitulo-36\">\n Cap\u00edtulo 36 \u2013 Outros Problemas com o Gradiente em Redes Neurais Artificiais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/\" rel=\"noopener\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   descrevemos para voc\u00ea a Matem\u00e1tica que ajuda a explicar a causa do problema da dissipa\u00e7\u00e3o do gradiente. Mas a dissipa\u00e7\u00e3o n\u00e3o \u00e9 o \u00fanico problema que pode ocorrer. Neste cap\u00edtulo vamos descrever outros poss\u00edveis problemas com o gradiente em redes neurais artificiais.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Explos\u00e3o do Gradiente\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos ver um exemplo expl\u00edcito em que ocorre a explos\u00e3o dos gradientes. O exemplo \u00e9 bem simples e vamos alterar alguns par\u00e2metros na rede de maneira a garantir que tenhamos a explos\u00e3o do gradiente. Mesmo fazendo essa altera\u00e7\u00e3o para for\u00e7ar o problema, a explos\u00e3o do gradiente n\u00e3o \u00e9 apenas uma possibilidade hipot\u00e9tica e realmente pode acontecer.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   H\u00e1 duas etapas para obter a explos\u00e3o do gradiente. Primeiro, escolhemos todos os pesos na rede como grandes, digamos w1 = w2 = w3 = w4 = 100. Segundo, vamos escolher os vieses para que os termos \u03c3 \u2032 (zj) n\u00e3o sejam muito pequenos. Isso \u00e9 realmente muito f\u00e1cil de fazer: tudo o que precisamos \u00e9 escolher os vieses para garantir que a entrada ponderada para cada neur\u00f4nio seja zj = 0 (e ent\u00e3o \u03c3 \u2032 (zj) = 1/4). Ent\u00e3o, por exemplo, queremos z1 = w1a0 + b1 = 0. Podemos conseguir isso ajustando b1 = \u2212100 \u2217 a0. Podemos usar a mesma ideia para selecionar os outros vieses. Quando fazemos isso, vemos que todos os termos wj\u03c3 \u2032 (zj) s\u00e3o iguais a 100 \u2217 1/4 = 25. Com estas escolhas ocorre o problema da explos\u00e3o do gradiente.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Instabilidade do Gradiente\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O problema fundamental aqui n\u00e3o \u00e9 tanto o problema do gradiente que desaparece ou o problema do gradiente que explode. \u00c9 que o gradiente nas camadas iniciais \u00e9 o produto dos termos de todas as camadas posteriores. Quando h\u00e1 muitas camadas, essa \u00e9 uma situa\u00e7\u00e3o intrinsecamente inst\u00e1vel. A \u00fanica maneira que todas as camadas podem aprender perto da mesma velocidade \u00e9 se todos esses produtos de termos estiverem pr\u00f3ximos de se equilibrar. Sem algum mecanismo ou raz\u00e3o subjacente para que o equil\u00edbrio ocorra, \u00e9 altamente improv\u00e1vel que aconte\u00e7a simplesmente por acaso. Em suma, o problema real aqui \u00e9 que as redes neurais sofrem de um problema de instabilidade do gradiente. Como resultado, se usarmos t\u00e9cnicas de aprendizado baseadas em gradiente padr\u00e3o, camadas diferentes na rede tender\u00e3o a aprender em velocidades totalmente diferentes.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Problema Mais Comum \u00e9 Mesmo a Dissipa\u00e7\u00e3o do Gradiente\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vimos que o gradiente pode desaparecer ou explodir nas camadas iniciais de uma rede profunda. De fato, ao usar neur\u00f4nios sigm\u00f3ides, o gradiente geralmente desaparece. Para ver porque, considere novamente a express\u00e3o | w\u03c3 \u2032 (z) |. Para evitar o problema da dissipa\u00e7\u00e3o do gradiente, precisamos | w\u03c3 \u2032 (z) | \u22651. Voc\u00ea pode pensar que isso pode acontecer facilmente se w for muito grande. No entanto, \u00e9 mais dif\u00edcil do que parece. A raz\u00e3o \u00e9 que o termo \u03c3 \u2032 (z) tamb\u00e9m depende de w: \u03c3 \u2032 (z) = \u03c3 \u2032 (wa + b), onde a \u00e9 a ativa\u00e7\u00e3o da entrada. Ent\u00e3o, quando fazemos w grande, precisamos ter cuidado para que n\u00e3o tornemos simultaneamente \u03c3 \u2032 (wa + b) pequeno. Isso acaba sendo uma restri\u00e7\u00e3o consider\u00e1vel. A raz\u00e3o \u00e9 que quando fazemos w grande, tendemos a tornar o wa + b muito grande. Olhando para um gr\u00e1fico de \u03c3 \u2032 voc\u00ea pode ver que isso nos coloca fora das \u201casas\u201d da fun\u00e7\u00e3o \u03c3 \u2032, onde \u00e9 preciso valores muito pequenos. A \u00fanica maneira de evitar isso \u00e9 se a ativa\u00e7\u00e3o de entrada estiver dentro de um intervalo bastante estreito de valores. \u00c0s vezes isso vai acontecer, mas frequentemente, por\u00e9m, isso n\u00e3o acontece. E assim, no caso gen\u00e9rico, temos a dissipa\u00e7\u00e3o dos gradientes.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Gradientes Inst\u00e1veis em Redes Mais Complexas\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Temos estudado redes simples como exemplo, com apenas um neur\u00f4nio em cada camada oculta. E quanto a redes profundas mais complexas, com muitos neur\u00f4nios em cada camada oculta (tipicamente Deep Learning)?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    <img alt=\"network\" class=\"aligncenter size-full wp-image-986\" data-attachment-id=\"986\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"network\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?fit=560%2C279\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?fit=300%2C149\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?fit=560%2C279\" data-orig-size=\"560,279\" data-permalink=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/network-2/\" data-recalc-dims=\"1\" height=\"279\" sizes=\"(max-width: 560px) 100vw, 560px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?resize=560%2C279\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?w=560 560w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?resize=300%2C149 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/network.png?resize=200%2C100 200w\" width=\"560\"/>\n   </span>\n  </p>\n  <p>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    De fato, o mesmo comportamento ocorre em tais redes. Nos cap\u00edtulos anteriores onde estudamos retropropaga\u00e7\u00e3o, vimos que o gradiente na camada l de uma rede de camada L \u00e9 dado por:\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    <img alt=\"form\" class=\"aligncenter size-full wp-image-987\" data-attachment-id=\"987\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?fit=406%2C48\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?fit=300%2C35\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?fit=406%2C48\" data-orig-size=\"406,48\" data-permalink=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/form-8/\" data-recalc-dims=\"1\" height=\"48\" sizes=\"(max-width: 406px) 100vw, 406px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?resize=406%2C48\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?w=406 406w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?resize=300%2C35 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form-1.png?resize=200%2C24 200w\" width=\"406\"/>\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Aqui, \u03a3 \u2032 (zl) \u00e9 uma matriz diagonal cujas entradas s\u00e3o os valores \u03c3 \u2032 (z) para as entradas ponderadas para a camada l. As wl s\u00e3o as matrizes de peso para as diferentes camadas. E \u2207aC \u00e9 o vetor de derivadas parciais de C em rela\u00e7\u00e3o \u00e0s ativa\u00e7\u00f5es de sa\u00edda.\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Essa \u00e9 uma express\u00e3o muito mais complicada do que no caso de um \u00fanico neur\u00f4nio. Ainda assim, se voc\u00ea olhar de perto, a forma essencial \u00e9 muito semelhante, com muitos pares da forma:\n   </span>\n  </p>\n  <p>\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-991\" data-attachment-id=\"991\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?fit=91%2C31\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?fit=91%2C31\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?fit=91%2C31\" data-orig-size=\"91,31\" data-permalink=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/form2-10/\" data-recalc-dims=\"1\" height=\"31\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form2.png?resize=91%2C31\" width=\"91\"/>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Al\u00e9m disso, as matrizes \u03a3\u2032 (zj) possuem pequenas entradas na diagonal, nenhuma maior que 1/4. Desde que as matrizes de peso wj n\u00e3o sejam muito grandes, cada termo adicional:\n   </span>\n   <span style=\"color: #000000;\">\n    <img alt=\"form3\" class=\"aligncenter size-full wp-image-992\" data-attachment-id=\"992\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?fit=92%2C34\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?fit=92%2C34\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?fit=92%2C34\" data-orig-size=\"92,34\" data-permalink=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/form3-9/\" data-recalc-dims=\"1\" height=\"34\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/form3.png?resize=92%2C34\" width=\"92\"/>\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    tende a fazer o vetor gradiente menor, levando a uma dissipa\u00e7\u00e3o do gradiente. Mais genericamente, o grande n\u00famero de termos no produto tende a levar a um gradiente inst\u00e1vel, assim como no nosso exemplo anterior. Na pr\u00e1tica isso \u00e9 tipicamente encontrado em redes sigm\u00f3ides que os gradientes desaparecem exponencialmente de forma r\u00e1pida nas camadas anteriores. Como resultado, o aprendizado diminui nessas camadas. Essa desacelera\u00e7\u00e3o n\u00e3o \u00e9 apenas um acidente ou uma inconveni\u00eancia: \u00e9 uma consequ\u00eancia fundamental da abordagem que estamos adotando para o aprendizado da rede.\n   </span>\n  </p>\n  <h3 style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Outros Obst\u00e1culos Para a Aprendizagem Profunda\n   </span>\n  </h3>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Nestes \u00faltimos cap\u00edtulos, nos concentramos na dissipa\u00e7\u00e3o de gradientes \u2013 e, em geral, gradientes inst\u00e1veis \u2013 como um obst\u00e1culo \u00e0 aprendizagem profunda. De fato, gradientes inst\u00e1veis s\u00e3o apenas um obst\u00e1culo para o aprendizado profundo, embora seja um importante obst\u00e1culo fundamental. Muitas pesquisas em andamento t\u00eam como objetivo entender melhor os desafios que podem ocorrer quando se treinam redes profundas. Vamos mencionar brevemente alguns artigos, para dar a voc\u00ea o sabor de algumas das perguntas que as pessoas est\u00e3o fazendo.\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Como primeiro exemplo, em 2010\n    <span style=\"text-decoration: underline;\">\n     <a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\" rel=\"noopener\" target=\"_blank\">\n      Glorot e Bengio\n     </a>\n    </span>\n    encontraram evid\u00eancias sugerindo que o uso de fun\u00e7\u00f5es de ativa\u00e7\u00e3o sigm\u00f3ide pode causar problemas ao treinamento de redes profundas. Em particular, eles encontraram evid\u00eancias de que o uso de sigm\u00f3ides far\u00e1 com que as ativa\u00e7\u00f5es na camada oculta final saturem perto de 0 no in\u00edcio do treinamento, diminuindo substancialmente o aprendizado. Eles sugeriram algumas fun\u00e7\u00f5es de ativa\u00e7\u00e3o alternativas, que parecem n\u00e3o sofrer tanto com esse problema de satura\u00e7\u00e3o.\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Como um segundo exemplo, em 2013,\n    <span style=\"text-decoration: underline;\">\n     <a href=\"http://www.cs.toronto.edu/~hinton/absps/momentum.pdf\" rel=\"noopener\" target=\"_blank\">\n      Sutskever, Martens, Dahl e Hinton\n     </a>\n    </span>\n    estudaram o impacto na aprendizagem profunda tanto da inicializa\u00e7\u00e3o de peso aleat\u00f3rio quanto do cronograma de momentum na descida de gradiente estoc\u00e1stica baseada no momento. Em ambos os casos, fazer boas escolhas fez uma diferen\u00e7a substancial na capacidade de treinar redes profundas.\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Esses exemplos sugerem que \u201cO que dificulta o treinamento de redes profundas?\u201d \u00e9 uma quest\u00e3o complexa. Nestes \u00faltimos cap\u00edtulos, nos concentramos nas instabilidades associadas ao aprendizado baseado em gradiente em redes profundas. Os resultados dos dois \u00faltimos par\u00e1grafos sugerem que h\u00e1 tamb\u00e9m um papel desempenhado pela escolha da fun\u00e7\u00e3o de ativa\u00e7\u00e3o, a forma como os pesos s\u00e3o inicializados e at\u00e9 mesmo detalhes de como a aprendizagem por gradiente descendente \u00e9 implementada. E, claro, a escolha da arquitetura de rede e outros hiperpar\u00e2metros tamb\u00e9m \u00e9 importante. Assim, muitos fatores podem desempenhar um papel em dificultar a forma\u00e7\u00e3o de redes profundas, e a compreens\u00e3o de todos esses fatores ainda \u00e9 objeto de pesquisas em andamento. A boa not\u00edcia \u00e9 que, a partir do pr\u00f3ximo cap\u00edtulo, vamos mudar isso e desenvolver v\u00e1rias abordagens para o aprendizado profundo que, at\u00e9 certo ponto, conseguem superar ou direcionar todos esses desafios.\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    N\u00e3o \u00e9 incr\u00edvel o que estamos vivenciando neste exato momento da hist\u00f3ria humana? Tudo isso que estudamos at\u00e9 aqui forma a base de aplica\u00e7\u00f5es de Intelig\u00eancia Artificial que j\u00e1 s\u00e3o encontradas no mercado, em diversas aplica\u00e7\u00f5es e at\u00e9 mesmo em nossos smartphones. E ainda estamos apenas no come\u00e7o.\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    E voc\u00ea, quer ou n\u00e3o fazer parte desta incr\u00edvel revolu\u00e7\u00e3o trazida pela Intelig\u00eancia Artificial? Se a resposta for sim, o que est\u00e1 esperando?\n   </span>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Refer\u00eancias:\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n    (material usado com autoriza\u00e7\u00e3o do autor)\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" target=\"_blank\">\n     Machine Learning\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </p>\n  <p>\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </p>\n  <p>\n  </p>\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-984\" href=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-984\" href=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-984\" href=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-984\" href=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/outros-problemas-com-o-gradiente-em-redes-neurais-artificiais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-984-5e0dd13dd9917\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=984&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-984-5e0dd13dd9917\" id=\"like-post-wrapper-140353593-984-5e0dd13dd9917\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "37": "<h1 class=\"entry-title\" id=\"capitulo-37\">\n Cap\u00edtulo 37 \u2013 O Efeito do Batch Size no Treinamento de Redes Neurais Artificiais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A partir deste cap\u00edtulo voc\u00ea vai compreender em mais detalhes a arquitetura dos principais modelos de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-i\" rel=\"noopener\" target=\"_blank\">\n     Deep Learning\n    </a>\n   </span>\n   , com \u00eanfase nas escolhas dos hiperpar\u00e2metros e abordagens de treinamento. Vamos come\u00e7ar com O Efeito do Batch Size no Treinamento de Redes Neurais Artificiais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste experimento, vamos investigar o efeito do tamanho do lote (Batch Size) na din\u00e2mica de treinamento.\u00a0Tamanho do lote\u00a0(Batch Size) \u00e9 um termo usado em aprendizado de m\u00e1quina e refere-se ao n\u00famero de exemplos de treinamento usados em uma itera\u00e7\u00e3o. O Batch Size pode ser uma das tr\u00eas op\u00e7\u00f5es:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    <strong>\n     batch mode\n    </strong>\n    : onde o tamanho do lote \u00e9 igual ao conjunto de dados total, tornando os valores de itera\u00e7\u00e3o e \u00e9pocas equivalentes.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    <strong>\n     mini-batch mode\n    </strong>\n    : onde o tamanho do lote \u00e9 maior que um, mas menor que o tamanho total do conjunto de dados. Geralmente, um n\u00famero que pode ser dividido no tamanho total do conjunto de dados.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    <strong>\n     stochastic mode\n    </strong>\n    : onde o tamanho do lote \u00e9 igual a um. Portanto, o gradiente e os par\u00e2metros da rede neural s\u00e3o atualizados ap\u00f3s cada amostra.\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A m\u00e9trica em que nos concentraremos \u00e9 o\n   <em>\n    gap de generaliza\u00e7\u00e3o\n   </em>\n   , que \u00e9 definido como a diferen\u00e7a entre o valor do tempo de treinamento e o valor do tempo de teste. Vamos investigar o tamanho do lote no contexto da classifica\u00e7\u00e3o de imagens (o mesmo usado em diversos cap\u00edtulos anteriores). Especificamente, usaremos o conjunto de dados MNIST.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No nosso caso, a diferen\u00e7a de generaliza\u00e7\u00e3o \u00e9 simplesmente a diferen\u00e7a entre a precis\u00e3o da classifica\u00e7\u00e3o no tempo de teste e o tempo de treinamento. Estas experi\u00eancias foram destinadas a fornecer alguma intui\u00e7\u00e3o b\u00e1sica sobre os efeitos do tamanho do lote. \u00c9 bem conhecido na comunidade de aprendizado de m\u00e1quina que a dificuldade de fazer afirma\u00e7\u00f5es gerais sobre os efeitos de hiperpar\u00e2metros geralmente varia de conjunto de dados a conjunto de dados e modelo a modelo. Portanto, as conclus\u00f5es que fazemos aqui s\u00f3 podem servir como indica\u00e7\u00f5es em vez de declara\u00e7\u00f5es gerais sobre o tamanho do lote.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O tamanho do lote \u00e9 um dos hiperpar\u00e2metros mais importantes para sintonizar os modernos sistemas de aprendizagem profunda. Os\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" target=\"_blank\">\n     Cientistas de Dados\n    </a>\n   </span>\n   muitas vezes querem usar um tamanho de lote maior para treinar seu modelo, uma vez que permite acelera\u00e7\u00f5es computacionais do paralelismo das GPUs. No entanto, \u00e9 bem conhecido que um tamanho de lote muito grande levar\u00e1 a uma generaliza\u00e7\u00e3o deficiente (embora atualmente n\u00e3o se saiba exatamente porque isso acontece). Para as fun\u00e7\u00f5es convexas que estamos tentando otimizar, h\u00e1 uma disputa inerente entre os benef\u00edcios de tamanhos de lotes menores e maiores.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por um lado, usar um lote igual a todo o conjunto de dados garante a converg\u00eancia para o \u00f3timo global da fun\u00e7\u00e3o objetivo. No entanto, isso \u00e9 \u00e0 custa de uma converg\u00eancia emp\u00edrica mais lenta para esse \u00f3timo. Por outro lado, o uso de tamanhos menores de lotes mostrou empiricamente uma converg\u00eancia mais r\u00e1pida para solu\u00e7\u00f5es \u201cboas\u201d. Isso \u00e9 intuitivamente explicado pelo fato de que tamanhos de lote menores permitem que o modelo \u201cinicie o aprendizado antes de ver todos os dados\u201d. A desvantagem de usar um tamanho de lote menor \u00e9 que n\u00e3o h\u00e1 garantia que o modelo vai convergir para o \u00f3timo global. Ele ir\u00e1 saltar em torno do \u00f3timo global, dependendo da rela\u00e7\u00e3o entre o tamanho do lote e o tamanho do conjunto de dados. Portanto, sob nenhuma restri\u00e7\u00e3o computacional, muitas vezes \u00e9 aconselh\u00e1vel que se comece com um pequeno tamanho de lote, colhendo os benef\u00edcios de uma din\u00e2mica de treinamento mais r\u00e1pida, e aumente o tamanho do lote por meio de treinamento, aproveitando tamb\u00e9m os benef\u00edcios da converg\u00eancia garantida.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Observou-se empiricamente que tamanhos de lote menores n\u00e3o s\u00f3 t\u00eam uma din\u00e2mica de treinamento mais r\u00e1pida, mas tamb\u00e9m generaliza\u00e7\u00e3o para o conjunto de dados de teste versus tamanhos de lote maiores. Mas esta afirma\u00e7\u00e3o tem seus limites. Sabemos que um tamanho de lote de 1 geralmente funciona muito mal. \u00c9 geralmente aceito que existe um \u201cponto ideal\u201d para o tamanho do lote entre 1 e todo o conjunto de dados de treinamento que fornecer\u00e1 a melhor generaliza\u00e7\u00e3o. Esse \u201cponto ideal\u201d geralmente depende do conjunto de dados e do modelo em quest\u00e3o. A raz\u00e3o para uma melhor generaliza\u00e7\u00e3o \u00e9 vagamente atribu\u00edda \u00e0 exist\u00eancia de \u201cru\u00eddo\u201d no treinamento de pequeno tamanho de lote. Como os sistemas de redes neurais s\u00e3o extremamente propensos a ajustes excessivos (overfitting), a ideia \u00e9 que a visualiza\u00e7\u00e3o de v\u00e1rios tamanhos de lote pequenos, cada lote sendo uma representa\u00e7\u00e3o \u201cruidosa\u201d de todo o conjunto de dados, causar\u00e1 uma esp\u00e9cie de din\u00e2mica de \u201ctug-and-pull\u201d. Essa din\u00e2mica evita que a rede neural se ajuste excessivamente no conjunto de treinamento e, portanto, tenha um desempenho ruim no conjunto de testes.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Defini\u00e7\u00e3o do Problema\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O problema exato que ser\u00e1 investigado \u00e9 o da classifica\u00e7\u00e3o. Dada uma imagem X, o objetivo \u00e9 prever o r\u00f3tulo da imagem y. No caso do conjunto de dados MNIST, X s\u00e3o imagens em preto-e-branco dos d\u00edgitos 0 a 9 e y s\u00e3o as etiquetas de d\u00edgitos correspondentes \u201c0\u201d a \u201c9\u201d. Nosso modelo de escolha \u00e9 uma rede neural. Especificamente, usaremos um Perceptron Multicamada (MLP).\n  </span>\n  <span style=\"color: #000000;\">\n   Salvo disposi\u00e7\u00e3o em contr\u00e1rio, este \u00e9 o modelo padr\u00e3o foi usado no experimento:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    2 camadas ocultas totalmente conectadas (FC), 1024 unidades cada\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Fun\u00e7\u00e3o de perda: logaritmo negativo\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Otimizador: SGD (Stochastic Gradient Descent)\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Taxa de aprendizagem: 0,01\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    \u00c9pocas: 30\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em \u00faltima an\u00e1lise, a pergunta que queremos responder \u00e9 \u201cqual tamanho de lote devo usar ao treinar uma rede neural?\u201d\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Efeito do Tamanho do Lote\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A primeira coisa que devemos fazer para confirmar o problema que estamos tentando investigar \u00e9 mostrar a depend\u00eancia entre o intervalo de generaliza\u00e7\u00e3o e o tamanho do lote. Eu tenho me referido \u00e0 m\u00e9trica que estamos considerando como \u201cgap de generaliza\u00e7\u00e3o\u201d. Essa \u00e9 tipicamente a medida de autores sobre o tema usada em artigos e papers, mas para simplicidade em nosso estudo n\u00f3s apenas nos preocuparemos com a precis\u00e3o do teste sendo a mais alta poss\u00edvel. Como veremos, a precis\u00e3o de treinamento e teste depender\u00e1 do tamanho do lote, por isso \u00e9 mais significativo falar sobre a precis\u00e3o de teste em vez do gap de generaliza\u00e7\u00e3o. Mais especificamente, queremos que a precis\u00e3o do teste depois de um grande n\u00famero de \u00e9pocas de treinamento, seja alta. Quantas \u00e9pocas \u00e9 um \u201cgrande n\u00famero de \u00e9pocas\u201d? Idealmente, isso \u00e9 definido como o n\u00famero de \u00e9pocas de treinamento necess\u00e1rias, de modo que qualquer treinamento adicional forne\u00e7a pouco ou nenhum aumento na precis\u00e3o de teste. Na pr\u00e1tica, isso \u00e9 dif\u00edcil de determinar e teremos que adivinhar quantas \u00e9pocas s\u00e3o apropriadas para alcan\u00e7ar um comportamento ideal. Apresento as precis\u00f5es de teste do nosso modelo de rede neural treinado usando diferentes tamanhos de lote abaixo (para aprender a fazer tudo isso na pr\u00e1tica, clique\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   ).\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"grafico\" class=\"aligncenter size-full wp-image-1005\" data-attachment-id=\"1005\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"grafico\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?fit=646%2C232\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?fit=300%2C108\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?fit=646%2C232\" data-orig-size=\"646,232\" data-permalink=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/grafico/\" data-recalc-dims=\"1\" height=\"232\" sizes=\"(max-width: 646px) 100vw, 646px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?resize=646%2C232\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?w=646 646w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?resize=300%2C108 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico.png?resize=200%2C72 200w\" width=\"646\"/>\n </p>\n <p>\n  <img alt=\"grafico2\" class=\"aligncenter size-full wp-image-1006\" data-attachment-id=\"1006\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"grafico2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?fit=646%2C235\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?fit=300%2C109\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?fit=646%2C235\" data-orig-size=\"646,235\" data-permalink=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/grafico2/\" data-recalc-dims=\"1\" height=\"235\" sizes=\"(max-width: 646px) 100vw, 646px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?resize=646%2C235\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?w=646 646w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?resize=300%2C109 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/grafico2.png?resize=200%2C73 200w\" width=\"646\"/>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Curvas em laranja: tamanho do lote 64\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em verde: tamanho do lote 256\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em lil\u00e1s: tamanho do lote 1024\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Descoberta: tamanhos maiores de lotes levam a uma precis\u00e3o menor nos dados de teste.\n   </span>\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O eixo x mostra o n\u00famero de \u00e9pocas de treinamento. O eixo y \u00e9 rotulado para cada plotagem. MNIST \u00e9 obviamente um conjunto de dados f\u00e1cil de treinar; podemos alcan\u00e7ar 100% de precis\u00e3o em treino e 98% em teste com apenas nosso modelo MLP base no tamanho de lote 64. Al\u00e9m disso, vemos uma clara tend\u00eancia entre o tamanho do lote e a precis\u00e3o do teste (e treinamento!). A nossa primeira conclus\u00e3o \u00e9 a seguinte: maiores tamanhos de lotes levam a uma menor precis\u00e3o nos dados de teste. Esses padr\u00f5es parecem existir em seu extremo para o conjunto de dados MNIST. Eu tentei tamanho de lote igual a 2 e alcan\u00e7ou uma precis\u00e3o de teste ainda melhor de 99% (versus 98% para tamanho de lote 64)! Como aviso pr\u00e9vio, n\u00e3o espere que tamanhos de lote muito baixos, como 2, funcionem bem em conjuntos de dados mais complexos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas neste experimento n\u00e3o consideramos altera\u00e7\u00f5es na taxa de aprendizagem. Poder\u00edamos aumentar a acur\u00e1cia em teste com tamanhos de lote maiores, ajustando a taxa de aprendizagem (learning rate)? N\u00e3o perca o pr\u00f3ximo cap\u00edtulo para descobrir! At\u00e9 l\u00e1.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"color: #0000ff;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener\" style=\"color: #0000ff;\" target=\"_blank\">\n    <span style=\"text-decoration: underline;\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Don\u2019t Decay the Learning Rate, Increase the Batch Size\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Effect of batch size on training dynamics\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n    (material usado com autoriza\u00e7\u00e3o do autor)\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #0000ff; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener\" style=\"color: #0000ff; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-995\" href=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-995\" href=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-995\" href=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-995\" href=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-995-5e0dd14016664\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=995&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-995-5e0dd14016664\" id=\"like-post-wrapper-140353593-995-5e0dd14016664\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "38": "<h1 class=\"entry-title\" id=\"capitulo-38\">\n Cap\u00edtulo 38 \u2013 O Efeito da Taxa de Aprendizagem no Treinamento de Redes Neurais Artificiais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos retomar a discuss\u00e3o do cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   e tentar melhorar a precis\u00e3o do modelo nos dados de teste a partir de um tamanho de lote maior, aumentando a taxa de aprendizado (learning rate). Vamos estudar O Efeito da Taxa de Aprendizagem no Treinamento de Redes Neurais Artificiais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Algumas pesquisas na literatura sobre otimiza\u00e7\u00e3o em Machine Learning mostraram que aumentar a taxa de aprendizado pode compensar tamanhos maiores de lotes. Com isso em mente, aumentamos a taxa de aprendizado do nosso modelo para ver se podemos recuperar a precis\u00e3o nos dados de teste (que havia sido reduzida quando aumentando o tamanho do lote). Os gr\u00e1ficos abaixo mostram as conclus\u00f5es:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"graf1\" class=\"aligncenter size-full wp-image-1022\" data-attachment-id=\"1022\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graf1\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?fit=645%2C237\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?fit=300%2C110\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?fit=645%2C237\" data-orig-size=\"645,237\" data-permalink=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf1/\" data-recalc-dims=\"1\" height=\"237\" sizes=\"(max-width: 645px) 100vw, 645px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?resize=645%2C237\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?w=645 645w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?resize=300%2C110 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf1.png?resize=200%2C73 200w\" width=\"645\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"graf2\" class=\"aligncenter size-full wp-image-1023\" data-attachment-id=\"1023\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graf2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?fit=647%2C244\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?fit=300%2C113\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?fit=647%2C244\" data-orig-size=\"647,244\" data-permalink=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf2/\" data-recalc-dims=\"1\" height=\"244\" sizes=\"(max-width: 647px) 100vw, 647px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?resize=647%2C244\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?w=647 647w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?resize=300%2C113 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf2.png?resize=200%2C75 200w\" width=\"647\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Curvas em laranja: tamanho do lote 64, taxa de aprendizagem 0.01 (refer\u00eancia)\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em lil\u00e1s: tamanho do lote 1024, taxa de aprendizado de 0,01 (refer\u00eancia)\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em azul: tamanho do lote 1024, taxa de aprendizagem 0,1\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As curvas em laranja e lil\u00e1s s\u00e3o para refer\u00eancia e s\u00e3o as mesmas do conjunto de gr\u00e1ficos do cap\u00edtulo anterior. Como a curva lil\u00e1s, a curva azul treina com um tamanho de lote grande de 1024. No entanto, a curva azul tem uma taxa de aprendizado aumentada em 10 vezes.\n   <strong>\n    Curiosamente, podemos recuperar a precis\u00e3o nos dados de teste a partir de um tamanho de lote maior, aumentando a taxa de aprendizado\n   </strong>\n   . Usando um tamanho de lote de 64 (laranja) atinge uma precis\u00e3o de teste de 98%, enquanto o uso de um tamanho de lote de 1024 atinge apenas cerca de 96%. Mas aumentando a taxa de aprendizado, usando um tamanho de lote de 1024 tamb\u00e9m alcan\u00e7a uma precis\u00e3o de teste de 98%. Assim como com nossa conclus\u00e3o anterior, tenha cautela ao analisar esses resultados. Sabe-se que o simples aumento da taxa de aprendizado n\u00e3o compensa totalmente grandes tamanhos de lotes em conjuntos de dados mais complexos do que o MNIST.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E qual o efeito ao reduzir o tamanho do lote durante o treinamento do modelo?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A pr\u00f3xima pergunta interessante a ser feita \u00e9 se o treinamento com lotes grandes \u201cinicia voc\u00ea em um caminho ruim do qual voc\u00ea n\u00e3o pode se recuperar\u201d. Ou seja, se come\u00e7armos a treinar com tamanho de lote 1024, ent\u00e3o mudamos para tamanho de lote 64, podemos ainda alcan\u00e7ar a maior precis\u00e3o em teste de 98%?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Investigamos tr\u00eas casos: treinar usando um tamanho de lote pequeno para uma \u00fanica \u00e9poca, mudar para um tamanho de lote grande, treinar usando um tamanho de lote pequeno para muitas \u00e9pocas e mudar para um tamanho maior de lote e treinar usando um tamanho grande de lote e ent\u00e3o usar uma taxa de aprendizado mais alta com o mesmo tamanho de lote. Resultados abaixo:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"graf3\" class=\"aligncenter size-full wp-image-1024\" data-attachment-id=\"1024\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graf3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?fit=656%2C239\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?fit=300%2C109\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?fit=656%2C239\" data-orig-size=\"656,239\" data-permalink=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf3/\" data-recalc-dims=\"1\" height=\"239\" sizes=\"(max-width: 656px) 100vw, 656px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?resize=656%2C239\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?w=656 656w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?resize=300%2C109 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf3.png?resize=200%2C73 200w\" width=\"656\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"graf4\" class=\"aligncenter size-full wp-image-1025\" data-attachment-id=\"1025\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graf4\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?fit=653%2C248\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?fit=300%2C114\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?fit=653%2C248\" data-orig-size=\"653,248\" data-permalink=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/graf4/\" data-recalc-dims=\"1\" height=\"248\" sizes=\"(max-width: 653px) 100vw, 653px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?resize=653%2C248\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?w=653 653w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?resize=300%2C114 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/02/graf4.png?resize=200%2C76 200w\" width=\"653\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Curvas em laranja: treinar em tamanho de lote 64 por 30 \u00e9pocas (refer\u00eancia)\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em amarelo neon: treinar em tamanho de lote 1024 por 60 \u00e9pocas (refer\u00eancia)\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em verde: treinar em tamanho de lote 1024 para 1 \u00e9poca e depois mudar para tamanho de lote 64 por 30 \u00e9pocas (total de 31 \u00e9pocas)\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em amarelo escuro: treinar em tamanho de lote 1024 por 30 \u00e9pocas e depois mudar para tamanho de lote 64 por 30 \u00e9pocas (total de 60 \u00e9pocas)\n  </span>\n  <br/>\n  <span style=\"color: #000000;\">\n   Curvas em lil\u00e1s: treinamento em tamanho de lote 1024 e aumento da taxa de aprendizado em 10x na \u00e9poca 31 (total de 60 \u00e9pocas)\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como antes, as curvas em laranja s\u00e3o para um tamanho de lote pequeno. As curvas em amarelo neon servem como um controle para garantir que n\u00e3o estamos melhorando a precis\u00e3o do teste porque estamos simplesmente treinando mais. Se voc\u00ea prestar muita aten\u00e7\u00e3o ao eixo x, as \u00e9pocas s\u00e3o enumeradas de 0 a 30. Isso ocorre porque somente as \u00faltimas 30 \u00e9pocas de treinamento s\u00e3o mostradas. Para experimentos com mais de 30 \u00e9pocas de treinamento no total, as primeiras x \u2013 30 \u00e9pocas foram omitidas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c9 dif\u00edcil ver as outras 3 linhas porque elas est\u00e3o sobrepostas, mas n\u00e3o importa, porque nos tr\u00eas casos recuperamos a precis\u00e3o em teste de 98%! Em conclus\u00e3o, come\u00e7ar com um tamanho grande de lote n\u00e3o \u201cpega o modelo preso\u201d em alguma vizinhan\u00e7a de \u00f3timos locais ideais. O modelo pode alternar para um tamanho de lote menor ou uma taxa de aprendizado mais alta a qualquer momento para obter uma precis\u00e3o de teste melhor.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para compreender o comportamento matem\u00e1tico por tr\u00e1s de todo esse processo, precisamos trazer o gradiente para esta discuss\u00e3o. \u00c9 o que faremos no pr\u00f3ximo cap\u00edtulo. At\u00e9 l\u00e1.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Effect of batch size on training dynamics\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1021\" href=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1021\" href=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1021\" href=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1021\" href=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-efeito-da-taxa-de-aprendizagem-no-treinamento-de-redes-neurais-artificiais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1021-5e0dd14a71fc1\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1021&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1021-5e0dd14a71fc1\" id=\"like-post-wrapper-140353593-1021-5e0dd14a71fc1\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "39": "<h1 class=\"entry-title\" id=\"capitulo-39\">\n Cap\u00edtulo 39 \u2013 Rela\u00e7\u00e3o Entre o Tamanho do Lote e o C\u00e1lculo do Gradiente\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos continuar com a discuss\u00e3o dos dois\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     cap\u00edtulos anteriores\n    </a>\n   </span>\n   e investigar a Rela\u00e7\u00e3o Entre o Tamanho do Lote e o C\u00e1lculo do Gradiente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como explicar porque o treinamento com lotes maiores leva a uma precis\u00e3o menor nos testes? Uma hip\u00f3tese pode ser que as amostras de treinamento no mesmo lote interfiram (competem) com o gradiente um do outro. Uma amostra deseja mover os pesos do modelo em uma dire\u00e7\u00e3o, enquanto outra amostra deseja mover os pesos na dire\u00e7\u00e3o oposta. Portanto, seus gradientes tendem a ser cancelados e voc\u00ea obt\u00e9m um pequeno gradiente geral. Talvez, se as amostras forem divididas em dois lotes, a concorr\u00eancia ser\u00e1 reduzida, pois o modelo poder\u00e1 encontrar pesos que satisfar\u00e3o as duas amostras, se forem feitas em sequ\u00eancia. Em outras palavras, a otimiza\u00e7\u00e3o sequencial de amostras \u00e9 mais f\u00e1cil do que a otimiza\u00e7\u00e3o simult\u00e2nea em espa\u00e7os de par\u00e2metros complexos e de alta dimens\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A hip\u00f3tese \u00e9 representada graficamente abaixo. A seta lil\u00e1s mostra um \u00fanico degrau de gradiente descendente usando um tamanho de lote de 2. As setas azul e vermelha mostram duas etapas sucessivas de descida do gradiente usando um tamanho de lote 1. A seta preta \u00e9 a soma vetorial das setas azul e vermelha e representa o progresso geral que o modelo faz em duas etapas de tamanho de lote 1. Ambos os experimentos come\u00e7am com os mesmos pesos no espa\u00e7o de pesos. Embora n\u00e3o seja explicitamente mostrado na imagem, a hip\u00f3tese \u00e9 que a linha lil\u00e1s \u00e9 muito mais curta que a linha preta, devido \u00e0 concorr\u00eancia dos gradientes. Em outras palavras, o gradiente de uma \u00fanica etapa de tamanho de lote grande \u00e9 menor que a soma de gradientes de v\u00e1rias etapas de tamanho de lote pequeno.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"grad\" class=\"aligncenter size-full wp-image-1035\" data-attachment-id=\"1035\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"grad\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?fit=200%2C188\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?fit=200%2C188\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?fit=200%2C188\" data-orig-size=\"200,188\" data-permalink=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/grad/\" data-recalc-dims=\"1\" height=\"188\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/grad.png?resize=200%2C188\" width=\"200\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O experimento envolve a replica\u00e7\u00e3o da imagem mostrada acima. N\u00f3s treinamos o modelo para um determinado estado. Em seguida, o grupo de controle (seta lil\u00e1s) \u00e9 calculado encontrando o gradiente de etapa \u00fanica com tamanho de lote 1024. O grupo experimental (seta preta) \u00e9 calculado fazendo v\u00e1rias etapas de gradiente e encontrando a soma vetorial desses gradientes usando um tamanho de lote menor. O produto do n\u00famero de etapas e do tamanho do lote \u00e9 constante em 1024. Isso representa modelos diferentes que veem um n\u00famero fixo de amostras.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por exemplo, para um tamanho de lote de 64, fazemos 1024/64 = 16 etapas, somando os 16 gradientes para encontrar o gradiente geral. Para tamanho de lote 1024, fazemos 1024/1024 = 1 passo. Observe que, para os tamanhos de lote menores, amostras diferentes s\u00e3o desenhadas para cada lote. A ideia \u00e9 comparar os gradientes do modelo para diferentes tamanhos de lotes ap\u00f3s os modelos terem visto o mesmo n\u00famero de amostras. Como \u00faltima advert\u00eancia, para simplificar, apenas medimos o gradiente da \u00faltima camada do nosso modelo MLP (Multilayer Perceptron), que possui 1024 \u22c5 10 = 10240 pesos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Investigamos os seguintes tamanhos de lote: 1, 2, 3, 4, 5, 6, 7, 8, 16, 32, 64, 128, 256, 512, 1024.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Um teste significou:\n  </span>\n </p>\n <ol>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Carregamento / redefini\u00e7\u00e3o dos pesos do modelo para um ponto treinado fixo (usamos os pesos do modelo ap\u00f3s o treinamento para 2/30 epochs em tamanho de lote 1024).\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Amostragem aleat\u00f3ria de 1024 amostras de dados do conjunto de treinamento.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Treinamento do modelo atrav\u00e9s de todas as 1024 amostras de dados uma vez, com diferentes tamanhos de lote.\n   </span>\n  </li>\n </ol>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para cada tamanho de lote, repetimos o experimento mil vezes. N\u00e3o coletamos mais dados porque armazenar os tensores de gradiente \u00e9 realmente muito caro (mantivemos os tensores de cada tentativa para computar estat\u00edsticas de ordem mais alta mais tarde). O tamanho total do arquivo com tensores de gradiente foi de 600 MB.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para cada um dos 1000 ensaios, computamos a norma euclidiana do tensor de gradiente somado (seta preta na nossa imagem). Ent\u00e3o, calculamos a m\u00e9dia e o desvio padr\u00e3o dessas normas ao longo dos 1000 testes. Isso \u00e9 feito para cada tamanho de lote.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Eu queria investigar dois regimes de peso diferentes: no in\u00edcio do treinamento, quando os pesos n\u00e3o convergiam e muito aprendizado ocorria e, mais tarde, durante o treinamento, quando os pesos quase convergiam e o aprendizado m\u00ednimo estava ocorrendo. Para o regime inicial, eu treinei o modelo MLP para 2 \u00e9pocas com tamanho de lote 1024 e para o regime posterior, eu treinei o modelo por 30 \u00e9pocas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"graph\" class=\"aligncenter size-full wp-image-1036\" data-attachment-id=\"1036\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"graph\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?fit=578%2C434\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?fit=300%2C225\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?fit=578%2C434\" data-orig-size=\"578,434\" data-permalink=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/graph-2/\" data-recalc-dims=\"1\" height=\"434\" sizes=\"(max-width: 578px) 100vw, 578px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?resize=578%2C434\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?w=578 578w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/graph.png?resize=200%2C150 200w\" width=\"578\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Descoberta: tamanhos de lotes maiores produzem etapas de gradiente maiores do que tamanhos de lotes menores para o mesmo n\u00famero de amostras vistas.\n   </span>\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O eixo x mostra o tamanho do lote. O eixo y mostra a norma euclidiana m\u00e9dia de tensores de gradiente em 1000 tentativas. As barras de erro indicam a varia\u00e7\u00e3o da norma euclidiana em 1000 tentativas. Os pontos azuis \u00e9 o experimento realizado no regime inicial, onde o modelo foi treinado por 2 \u00e9pocas. Os pontos verdes \u00e9 o regime posterior em que o modelo foi treinado por 30 \u00e9pocas. Como esperado, o gradiente \u00e9 maior no in\u00edcio do treinamento (os pontos azuis s\u00e3o maiores que os pontos verdes).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao contr\u00e1rio da nossa hip\u00f3tese, a norma gradiente m\u00e9dia aumenta com o tamanho do lote! Esper\u00e1vamos que os gradientes fossem menores para um tamanho de lote maior devido \u00e0 competi\u00e7\u00e3o entre as amostras de dados. Em vez disso, o que encontramos \u00e9 que tamanhos maiores de lotes fazem etapas de gradiente maiores do que tamanhos de lote menores para o mesmo n\u00famero de amostras vistas. Observe que a norma euclidiana pode ser interpretada como a dist\u00e2ncia euclidiana entre o novo conjunto de pesos e o conjunto inicial de pesos. Portanto, o treinamento com lotes grandes tende a se afastar dos pesos iniciais depois de ver um n\u00famero fixo de amostras do que o treinamento com tamanhos de lote menores. A rela\u00e7\u00e3o entre o tamanho do lote e a norma de gradiente \u00e9 \u221ax. Em outras palavras, a rela\u00e7\u00e3o entre o tamanho do lote e a norma do gradiente quadrado \u00e9 linear.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Al\u00e9m disso, a varia\u00e7\u00e3o \u00e9 muito menor para tamanhos menores de lotes. No entanto, o que podemos nos interessar \u00e9 a magnitude da vari\u00e2ncia em rela\u00e7\u00e3o \u00e0 magnitude da m\u00e9dia. Portanto, para fazer uma compara\u00e7\u00e3o mais perspicaz, dimensiono a m\u00e9dia e o desvio padr\u00e3o de cada tamanho de lote para a m\u00e9dia do tamanho do lote 1024. Em outras palavras,\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form\" class=\"aligncenter size-full wp-image-1037\" data-attachment-id=\"1037\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?fit=468%2C328\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?fit=300%2C210\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?fit=468%2C328\" data-orig-size=\"468,328\" data-permalink=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/form-9/\" data-recalc-dims=\"1\" height=\"328\" sizes=\"(max-width: 468px) 100vw, 468px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?resize=468%2C328\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?w=468 468w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?resize=300%2C210 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form.png?resize=200%2C140 200w\" width=\"468\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   onde as barras representam valores normalizados e i indica um determinado tamanho de lote.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Justificamos o dimensionamento da m\u00e9dia e do desvio padr\u00e3o da norma de gradiente porque isso \u00e9 equivalente a aumentar a taxa de aprendizado para o experimento com tamanhos de lote menores. Essencialmente, queremos saber \u201cpela mesma dist\u00e2ncia afastada dos pesos iniciais, qual \u00e9 a varia\u00e7\u00e3o nas normas de gradiente para diferentes tamanhos de lote\u201d? Tenha em mente que estamos medindo a varia\u00e7\u00e3o nas normas de gradiente e n\u00e3o a varia\u00e7\u00e3o nos gradientes em si, que \u00e9 uma m\u00e9trica muito mais precisa.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-1038\" data-attachment-id=\"1038\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?fit=583%2C420\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?fit=300%2C216\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?fit=583%2C420\" data-orig-size=\"583,420\" data-permalink=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/form2-11/\" data-recalc-dims=\"1\" height=\"420\" sizes=\"(max-width: 583px) 100vw, 583px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?resize=583%2C420\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?w=583 583w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?resize=300%2C216 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/form2.png?resize=200%2C144 200w\" width=\"583\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Descoberta: Para a mesma dist\u00e2ncia m\u00e9dia da norma euclidiana dos pesos iniciais do modelo, tamanhos de lote maiores t\u00eam maior vari\u00e2ncia na dist\u00e2ncia.\n   </span>\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   N\u00f3s vemos um resultado muito surpreendente acima. Para a mesma dist\u00e2ncia m\u00e9dia da norma euclidiana dos pesos iniciais do modelo, tamanhos de lote maiores t\u00eam maior vari\u00e2ncia na dist\u00e2ncia. Isso \u00e9 bastante! Em suma, dados dois modelos treinados com tamanhos de lotes diferentes, em qualquer gradiente em particular, se a taxa de aprendizado for ajustada de modo que ambos os modelos se movam em m\u00e9dia na mesma dist\u00e2ncia, o modelo com o tamanho maior do lote variar\u00e1 mais em rela\u00e7\u00e3o a sua movimenta\u00e7\u00e3o. Isso \u00e9 um pouco contra-intuitivo, pois \u00e9 bem conhecido que tamanhos menores de lotes s\u00e3o \u201cruidosos\u201d e, portanto, voc\u00ea pode esperar que a varia\u00e7\u00e3o da norma de gradiente seja maior. Observe que, para cada teste, estamos usando 1024 amostras diferentes, em vez de usar as mesmas 1024 amostras em todos os testes. Observe tamb\u00e9m que a varia\u00e7\u00e3o entre os testes pode ser causada por duas coisas: as diferentes amostras que s\u00e3o extra\u00eddas do conjunto de dados entre os diferentes testes e a semente aleat\u00f3ria para cada teste (que n\u00e3o foi controlada, mas deve realmente ser). Seguindo em frente, vou supor que a varia\u00e7\u00e3o \u00e9 causada pelo primeiro fator: amostras diferentes. Este \u00e9 um resultado interessante e poder\u00edamos seguir esta investiga\u00e7\u00e3o por muitos cap\u00edtulos. Mas por hora, voc\u00ea j\u00e1 recebeu informa\u00e7\u00e3o suficiente para compreender a Rela\u00e7\u00e3o Entre o Tamanho do Lote e o C\u00e1lculo do Gradiente. Esse tipo de experimento \u00e9 muito importante em trabalhos de pesquisa e desenvolvimento de novos modelos!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos assim concluir mais uma etapa deste livro e come\u00e7ar nossa \u00faltima e mais emocionante jornada de aprendizagem, estudando as principais arquiteturas de Deep Learning. Eu n\u00e3o perderia se fosse voc\u00ea! At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000;\" target=\"_blank\">\n    <span style=\"text-decoration: underline;\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Effect of batch size on training dynamics\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1034\" href=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1034\" href=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1034\" href=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1034\" href=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/relacao-entre-o-tamanho-do-lote-e-o-calculo-do-gradiente/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1034-5e0dd14c9510a\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1034&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1034-5e0dd14c9510a\" id=\"like-post-wrapper-140353593-1034-5e0dd14c9510a\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "40": "<h1 class=\"entry-title\" id=\"capitulo-40\">\n Cap\u00edtulo 40 \u2013 Introdu\u00e7\u00e3o as Redes Neurais Convolucionais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nos primeiros cap\u00edtulos deste livro ensinamos nossas redes neurais a fazer um bom trabalho reconhecendo imagens de d\u00edgitos manuscritos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"digits\" class=\"aligncenter wp-image-1063 size-medium\" data-attachment-id=\"1063\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"digits\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?fit=623%2C128\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?fit=300%2C62\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?fit=623%2C128\" data-orig-size=\"623,128\" data-permalink=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/digits-3/\" data-recalc-dims=\"1\" height=\"62\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?resize=300%2C62\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?resize=300%2C62 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?resize=200%2C41 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/digits.png?w=623 623w\" width=\"300\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Fizemos isso usando redes nas quais camadas adjacentes s\u00e3o totalmente conectadas umas \u00e0s outras. Ou seja, todos os neur\u00f4nios da rede est\u00e3o conectados a todos os neur\u00f4nios em camadas adjacentes:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede\" class=\"aligncenter size-full wp-image-1064\" data-attachment-id=\"1064\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?fit=560%2C279\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?fit=300%2C149\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?fit=560%2C279\" data-orig-size=\"560,279\" data-permalink=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/rede-10/\" data-recalc-dims=\"1\" height=\"279\" sizes=\"(max-width: 560px) 100vw, 560px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?resize=560%2C279\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?w=560 560w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?resize=300%2C149 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede.png?resize=200%2C100 200w\" width=\"560\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em particular, para cada pixel na imagem de entrada, codificamos a intensidade do pixel como o valor de um neur\u00f4nio correspondente na camada de entrada. Para as imagens de 28 \u00d7 28 pixels que estamos usando, isso significa que nossa rede tem 784 (= 28 \u00d7 28) neur\u00f4nios de entrada. Em seguida, treinamos os pesos e vieses da rede para que a sa\u00edda da rede identificasse corretamente a imagem de entrada: \u20180\u2019, \u20181\u2019, \u20182\u2019, \u2026, \u20188\u2019 ou \u20189\u2019.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nossas redes anteriores funcionam muito bem: obtivemos uma precis\u00e3o de classifica\u00e7\u00e3o melhor que 98%, usando dados de treinamento e teste do conjunto de dados de d\u00edgitos manuscritos MNIST. Mas, ap\u00f3s reflex\u00e3o, \u00e9 estranho usar redes com camadas totalmente conectadas para classificar imagens. A raz\u00e3o \u00e9 que tal arquitetura de rede n\u00e3o leva em conta a estrutura espacial das imagens. Por exemplo, ela trata os pixels de entrada que est\u00e3o distantes e pr\u00f3ximos exatamente no mesmo n\u00edvel. Tais conceitos de estrutura espacial devem ser inferidos dos dados de treinamento. Mas e se, em vez de come\u00e7armos com uma arquitetura de rede que \u00e9 rasa, utiliz\u00e1ssemos uma arquitetura que tenta tirar proveito da estrutura espacial? \u00c9 onde entram as redes neurais convolucionais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essas redes usam uma arquitetura especial que \u00e9 particularmente bem adaptada para classificar imagens. O uso dessa arquitetura torna as redes convolucionais r\u00e1pidas de treinar. Isso, por sua vez, nos ajuda a treinar redes profundas de muitas camadas, que s\u00e3o muito boas na classifica\u00e7\u00e3o de imagens. Hoje, redes neurais convolucionais ou alguma variante pr\u00f3xima s\u00e3o usadas na maioria das redes neurais para reconhecimento de imagem.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As origens das redes neurais convolucionais remontam aos anos 70. Mas o artigo seminal que estabeleceu o tema moderno das redes convolucionais foi um artigo de 1998, \u201c\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-based learning applied to document recognition\n    </a>\n   </span>\n   \u201c, de Yann LeCun, L\u00e9on Bottou, Yoshua Bengio e Patrick Haffner. Desde ent\u00e3o, LeCun fez uma observa\u00e7\u00e3o interessante sobre a terminologia para redes convolucionais: \u201cA inspira\u00e7\u00e3o neural [biol\u00f3gica] em modelos como redes convolucionais \u00e9 muito t\u00eanue. \u00c9 por isso que eu os chamo de \u2018redes convolucionais\u2019 e n\u00e3o \u2018redes neurais convolucionais\u2019, e por isso os n\u00f3s eu chamo de \u2018unidades\u2019 e n\u00e3o \u2018neur\u00f4nios\u2019 \u201c. Apesar desta observa\u00e7\u00e3o, as redes convolucionais usam muitas das mesmas ideias que as redes neurais que estudamos at\u00e9 agora: ideias como retropropaga\u00e7\u00e3o, gradiente descendente, regulariza\u00e7\u00e3o, fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares e assim por diante. E assim, vamos seguir a pr\u00e1tica comum e consider\u00e1-las um tipo de rede neural. Usarei os termos \u201crede neural convolucional\u201d e \u201crede convolucional\u201d alternadamente. Tamb\u00e9m usarei os termos \u201cneur\u00f4nio [artificial]\u201d e \u201cunidade\u201d alternadamente.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Defini\u00e7\u00e3o\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma Rede Neural Convolucional (ConvNet / Convolutional Neural Network / CNN) \u00e9 um algoritmo de Aprendizado Profundo que pode captar uma imagem de entrada, atribuir import\u00e2ncia (pesos e vieses que podem ser aprendidos) a v\u00e1rios aspectos / objetos da imagem e ser capaz de diferenciar um do outro. O pr\u00e9-processamento exigido em uma ConvNet \u00e9 muito menor em compara\u00e7\u00e3o com outros algoritmos de classifica\u00e7\u00e3o. Enquanto nos m\u00e9todos primitivos os filtros s\u00e3o feitos \u00e0 m\u00e3o, com treinamento suficiente, as ConvNets t\u00eam a capacidade de aprender esses filtros / caracter\u00edsticas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A arquitetura de uma ConvNet \u00e9 an\u00e1loga \u00e0quela do padr\u00e3o de conectividade de neur\u00f4nios no c\u00e9rebro humano e foi inspirada na organiza\u00e7\u00e3o do Visual Cortex. Os neur\u00f4nios individuais respondem a est\u00edmulos apenas em uma regi\u00e3o restrita do campo visual conhecida como Campo Receptivo. Uma cole\u00e7\u00e3o desses campos se sobrep\u00f5e para cobrir toda a \u00e1rea visual. Veremos isso em detalhes mais a frente!\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por que usar ConvNets e n\u00e3o rede feed-forward?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma imagem n\u00e3o \u00e9 nada al\u00e9m de uma matriz de valores de pixels, certo? Ent\u00e3o, por que n\u00e3o apenas achatar a imagem (por exemplo, converter uma matriz 3\u00d73 em um vetor 9\u00d71. Se a image \u00e9 uma matriz, nenhum problema em converter em uma vetor) e aliment\u00e1-lo para um Perceptron Multi-Layer para fins de classifica\u00e7\u00e3o? Na verdade n\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em casos de imagens bin\u00e1rias extremamente b\u00e1sicas, o m\u00e9todo pode mostrar uma pontua\u00e7\u00e3o de precis\u00e3o m\u00e9dia durante a previs\u00e3o de classes, mas teria pouca ou nenhuma precis\u00e3o quando se trata de imagens complexas com depend\u00eancias de pixel por toda parte.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma ConvNet \u00e9 capaz de capturar com sucesso as depend\u00eancias espaciais e temporais em uma imagem atrav\u00e9s da aplica\u00e7\u00e3o de filtros relevantes. A arquitetura executa um melhor ajuste ao conjunto de dados da imagem devido \u00e0 redu\u00e7\u00e3o no n\u00famero de par\u00e2metros envolvidos e \u00e0 capacidade de reutiliza\u00e7\u00e3o dos pesos. Em outras palavras, a rede pode ser treinada para entender melhor a sofistica\u00e7\u00e3o da imagem.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede2\" class=\"aligncenter size-full wp-image-1065\" data-attachment-id=\"1065\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?fit=550%2C400\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?fit=300%2C218\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?fit=550%2C400\" data-orig-size=\"550,400\" data-permalink=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/rede2-7/\" data-recalc-dims=\"1\" height=\"400\" sizes=\"(max-width: 550px) 100vw, 550px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?resize=550%2C400\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?w=550 550w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?resize=300%2C218 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/03/rede2.png?resize=200%2C145 200w\" width=\"550\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na figura acima, temos uma imagem RGB (Red \u2013 Green \u2013 Blue) que foi separada por seus tr\u00eas planos coloridos \u2013 Vermelho, Verde e Azul. Existem v\u00e1rios desses espa\u00e7os de cores nos quais existem imagens \u2013 Escala de cinza, RGB, HSV, CMYK etc.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea pode imaginar como a computa\u00e7\u00e3o ficaria intensiva assim que as imagens atingissem dimens\u00f5es, digamos, 8K (7680 \u00d7 4320). A fun\u00e7\u00e3o da ConvNet \u00e9 reduzir as imagens para uma forma mais f\u00e1cil de processar, sem perder recursos que s\u00e3o cr\u00edticos para obter uma boa previs\u00e3o. Isso \u00e9 importante quando queremos projetar uma arquitetura que n\u00e3o seja apenas boa em recursos de aprendizado, mas que tamb\u00e9m seja escal\u00e1vel para conjuntos de dados massivos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa \u00e9 uma das arquiteturas de Deep Learning mais incr\u00edveis e com mais aplica\u00e7\u00f5es pr\u00e1ticas e dedicaremos alguns cap\u00edtulos a este arquitetura.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As redes neurais convolucionais usam tr\u00eas ideias b\u00e1sicas: campos receptivos locais, pesos compartilhados e pooling. Vamos dar uma olhada em cada uma dessas ideias? Ent\u00e3o n\u00e3o perca os pr\u00f3ximos cap\u00edtulos!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Obs: caso esteja muito ansioso, voc\u00ea j\u00e1 pode come\u00e7ar a estudar essa arquitetura agora mesmo. Em nosso curso gratuito\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=microsoft-power-bi-para-data-science\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Microsoft Power BI Para Data Science\n    </a>\n   </span>\n   , o b\u00f4nus do curso no \u00faltimo cap\u00edtulo \u00e9 um exemplo completo passo a passo em linguagem Python mostrando essa arquitetura. Se quiser ir mais a fundo e estudar a arquitetura em detalhes, ent\u00e3o confira os cursos\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-i\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Deep Learning I\n    </a>\n   </span>\n   e\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=viso-computacional-e-reconhecimento-de-imagem\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Vis\u00e3o Computacional\n    </a>\n   </span>\n   , onde a arquitetura \u00e9 explicada nos m\u00ednimos detalhes e usada em diversas aplica\u00e7\u00f5es pr\u00e1ticas.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Don\u2019t Decay the Learning Rate, Increase the Batch Size\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     A Comprehensive Guide to Convolutional Neural Networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1062\" href=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1062\" href=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1062\" href=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1062\" href=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1062-5e0dd157391df\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1062&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1062-5e0dd157391df\" id=\"like-post-wrapper-140353593-1062-5e0dd157391df\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "41": "<h1 class=\"entry-title\" id=\"capitulo-41\">\n Cap\u00edtulo 41 \u2013 Campos Receptivos Locais em Redes Neurais Convolucionais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos estudar em detalhes a partir de agora as Redes Neurais Convolucionais, uma das principais arquiteturas de Deep Learning, amplamente usada em Vis\u00e3o Computacional. E come\u00e7aremos compreendendo o que s\u00e3o os Campos Receptivos Locais. Mas antes, afinal, o que \u00e9 Vis\u00e3o Computacional, amplamente usada em aplica\u00e7\u00f5es de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Intelig\u00eancia Artificial\n    </a>\n   </span>\n   ?\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Que \u00e9 Vis\u00e3o Computacional?\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Imagine a seguinte situa\u00e7\u00e3o: voc\u00ea est\u00e1 em uma sala com mais duas pessoas. Uma delas arremessa uma bola e voc\u00ea a pega com as m\u00e3os. Nada poderia ser mais simples, certo?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Errado. Na verdade, este \u00e9 um dos processos mais complexos que j\u00e1 tentamos compreender, ou seja, como o c\u00e9rebro processa a vis\u00e3o de modo que sabemos exatamente o que \u00e9 uma bola e quando ela est\u00e1 vindo em nossa dire\u00e7\u00e3o? E ensinar uma m\u00e1quina que seja capaz de ver da mesma forma que n\u00f3s seres humanos \u00e9 uma tarefa realmente dif\u00edcil, n\u00e3o s\u00f3 porque \u00e9 dif\u00edcil fazer computadores executarem um c\u00e1lculo matem\u00e1tico que reproduza a vis\u00e3o humana, mas porque n\u00e3o estamos inteiramente certos de como o processo da vis\u00e3o realmente funciona.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Primeiro vamos descrever de forma sucinta e aproximada como ocorre o processo de vis\u00e3o no caso do arremesso da bola: a imagem da esfera passa atrav\u00e9s de seu olho e chega a sua retina, que faz alguma an\u00e1lise elementar e envia o resultado ao c\u00e9rebro, onde o c\u00f3rtex visual analisa mais profundamente a imagem. Em seguida, ele envia para o resto do c\u00f3rtex, que compara a tudo o que j\u00e1 sabe, classifica os objetos e dimens\u00f5es e, finalmente, decide sobre algo a fazer: levantar a m\u00e3o e pegar a bola (tendo previsto o seu caminho). Isso ocorre em uma pequena fra\u00e7\u00e3o de segundo, com quase nenhum esfor\u00e7o consciente e quase nunca falha. Assim, recriar a vis\u00e3o humana n\u00e3o \u00e9 apenas um problema dif\u00edcil, \u00e9 um conjunto deles, cada um dos quais depende do outro.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=viso-computacional-e-reconhecimento-de-imagem\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Vis\u00e3o Computacional\n    </a>\n   </span>\n   \u00e9 o processo de modelagem e replica\u00e7\u00e3o da vis\u00e3o humana usando software e hardware. A Vis\u00e3o Computacional \u00e9 uma disciplina que estuda como reconstruir, interromper e compreender uma cena 3d a partir de suas imagens 2d em termos das propriedades da estrutura presente na cena.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vis\u00e3o Computacional e reconhecimento de imagem s\u00e3o termos frequentemente usados como sin\u00f4nimos, mas o primeiro abrange mais do que apenas analisar imagens. Isso porque, mesmo para os seres humanos, \u201cver\u201d tamb\u00e9m envolve a percep\u00e7\u00e3o em muitas outras frentes, juntamente com uma s\u00e9rie de an\u00e1lises. Cada ser humano usa cerca de dois ter\u00e7os do seu c\u00e9rebro para o processamento visual, por isso n\u00e3o \u00e9 nenhuma surpresa que os computadores precisariam usar mais do que apenas o reconhecimento de imagem para obter sua vis\u00e3o de forma correta.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O reconhecimento de imagens em si \u2013 a an\u00e1lise de pixel e padr\u00e3o de imagens \u2013 \u00e9 uma parte integrante do processo de Vis\u00e3o Computacional que envolve tudo, desde reconhecimento de objetos e caracteres at\u00e9 an\u00e1lise de texto e sentimento. O reconhecimento de imagem de hoje, ainda na maior parte, apenas identifica objetos b\u00e1sicos como \u201cuma banana ou uma bicicleta em uma imagem.\u201d Mesmo crian\u00e7as podem fazer isso, mas o potencial da Vis\u00e3o Computacional \u00e9 sobre-humano: ser capaz de ver claramente no escuro, atrav\u00e9s de paredes, em longas dist\u00e2ncias e processar todos esses dados rapidamente e em volume maci\u00e7o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   J\u00e1 a Vis\u00e3o Computacional em seu sentido mais pleno est\u00e1 sendo usada na vida cotidiana e nos neg\u00f3cios para conduzir todos os tipos de tarefas, incluindo identificar doen\u00e7as m\u00e9dicas em raios-x, identificar produtos e onde compr\u00e1-los, an\u00fancios dentro de imagens editoriais, entre outros. A Vis\u00e3o Computacional pode ser usada para digitalizar plataformas de m\u00eddia social a fim de encontrar imagens relevantes que n\u00e3o podem ser descobertas por meio de pesquisas tradicionais. A tecnologia \u00e9 complexa e, assim como todas as tarefas acima mencionadas, requer mais do que apenas reconhecimento de imagem, mas tamb\u00e9m an\u00e1lise sem\u00e2ntica de grandes conjuntos de dados. E a Vis\u00e3o Computacional \u00e9 a principal t\u00e9cnica por tr\u00e1s dos ve\u00edculos aut\u00f4nomos, que devem mudar completamente o mundo como o conhecemos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ningu\u00e9m nunca disse que isso seria f\u00e1cil. Exceto, talvez, Marvin Minsky, pioneiro da Intelig\u00eancia Artificial, que, em 1966, instru\u00eda um estudante de p\u00f3s-gradua\u00e7\u00e3o a \u201cconectar uma c\u00e2mera a um computador e fazer com que descrevesse o que v\u00ea\u201d. Mas a verdade \u00e9 que 50 anos depois, ainda estamos trabalhando nisso, por\u00e9m agora h\u00e1 uma diferen\u00e7a (ou duas talvez): temos Big Data e Processamento Paralelo em GPU\u2019s. E acredite. Isso est\u00e1 realmente fazendo a diferen\u00e7a.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As Redes Neurais Convolucionais formam uma das arquiteturas de Deep Learning mais amplamente usada em tarefas de Vis\u00e3o Computacional e reconhecimento de imagens e a partir de agora vamos compreender porque.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Campos Receptivos Locais\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nas camadas totalmente conectadas mostradas no cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/introducao-as-redes-neurais-convolucionais/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   , as entradas foram representadas como uma linha vertical de neur\u00f4nios. Ou seja, convertemos uma imagem 28 x 28 (que \u00e9 uma matriz) em um vetor de apenas uma dimens\u00e3o (falaremos sobre isso mais adiante). Mas para compreender o que \u00e9 um campo receptivo local, vamos considerar a imagem de seu formato padr\u00e3o de 28x 28 (tamanho das imagens no dataset MNIST). Em uma imagem, cada pixel \u00e9 um valor num\u00e9rico que representa a intensidade de cor de acordo com a escala de cor utilizada, como RGB (Red \u2013 Green \u2013 Blue), por exemplo, ou apenas intensidade em escala de cinza para imagens em preto e branco.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede1\" class=\"aligncenter size-full wp-image-1082\" data-attachment-id=\"1082\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?fit=236%2C258\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?fit=236%2C258\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?fit=236%2C258\" data-orig-size=\"236,258\" data-permalink=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede1-2/\" data-recalc-dims=\"1\" height=\"258\" sizes=\"(max-width: 236px) 100vw, 236px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?resize=236%2C258\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?w=236 236w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede1.png?resize=200%2C219 200w\" width=\"236\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como de costume, vamos conectar os pixels de entrada a uma camada de neur\u00f4nios ocultos. Mas n\u00e3o vamos conectar todos os pixels de entrada a cada neur\u00f4nio oculto. Em vez disso, apenas fazemos conex\u00f5es em regi\u00f5es pequenas e localizadas da imagem de entrada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para ser mais preciso, cada neur\u00f4nio na primeira camada oculta ser\u00e1 conectado a uma pequena regi\u00e3o dos neur\u00f4nios de entrada, digamos, por exemplo, uma regi\u00e3o de 5 \u00d7 5, correspondendo a 25 pixels de entrada. Assim, para um neur\u00f4nio oculto em particular, podemos ter conex\u00f5es que se parecem com isso:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede2\" class=\"aligncenter size-full wp-image-1083\" data-attachment-id=\"1083\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?fit=353%2C258\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?fit=300%2C219\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?fit=353%2C258\" data-orig-size=\"353,258\" data-permalink=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede2-8/\" data-recalc-dims=\"1\" height=\"258\" sizes=\"(max-width: 353px) 100vw, 353px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?resize=353%2C258\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?w=353 353w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?resize=300%2C219 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede2.png?resize=200%2C146 200w\" width=\"353\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa regi\u00e3o na imagem de entrada \u00e9 chamada de\n   <strong>\n    campo receptivo local\n   </strong>\n   para o neur\u00f4nio oculto. \u00c9 uma pequena janela nos pixels de entrada. Cada conex\u00e3o aprende um peso e o neur\u00f4nio oculto tamb\u00e9m aprende um vi\u00e9s (bias) geral. Voc\u00ea pode pensar nesse neur\u00f4nio oculto particular como aprendendo a analisar seu campo receptivo local espec\u00edfico.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em seguida, deslizamos o campo receptivo local por toda a imagem de entrada. Para cada campo receptivo local, existe um neur\u00f4nio oculto diferente na primeira camada oculta. Para ilustrar isso concretamente, vamos come\u00e7ar com um campo receptivo local no canto superior esquerdo:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede3\" class=\"aligncenter size-full wp-image-1084\" data-attachment-id=\"1084\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?fit=502%2C258\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?fit=300%2C154\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?fit=502%2C258\" data-orig-size=\"502,258\" data-permalink=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede3-5/\" data-recalc-dims=\"1\" height=\"258\" sizes=\"(max-width: 502px) 100vw, 502px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?resize=502%2C258\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?w=502 502w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?resize=300%2C154 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede3.png?resize=200%2C103 200w\" width=\"502\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ent\u00e3o, deslizamos o campo receptivo local por um pixel para a direita (ou seja, por um neur\u00f4nio), para conectar a um segundo neur\u00f4nio oculto:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rede4\" class=\"aligncenter size-full wp-image-1085\" data-attachment-id=\"1085\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rede4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?fit=502%2C258\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?fit=300%2C154\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?fit=502%2C258\" data-orig-size=\"502,258\" data-permalink=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/rede4-3/\" data-recalc-dims=\"1\" height=\"258\" sizes=\"(max-width: 502px) 100vw, 502px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?resize=502%2C258\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?w=502 502w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?resize=300%2C154 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/rede4.png?resize=200%2C103 200w\" width=\"502\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E assim por diante, construindo a primeira camada oculta. Observe que, se tivermos uma imagem de entrada 28 \u00d7 28 e campos receptivos locais 5 \u00d7 5, haver\u00e1 24 \u00d7 24 neur\u00f4nios na camada oculta. Isso ocorre porque s\u00f3 podemos mover o campo receptivo local 23 neur\u00f4nios para o lado (ou 23 neur\u00f4nios para baixo), antes de colidir com o lado direito (ou inferior) da imagem de entrada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mostramos o campo receptivo local sendo movido por um pixel por vez. Na verdade, \u00e0s vezes, um comprimento de passada diferente \u00e9 usado. Por exemplo, podemos mover o campo receptivo local 2 pixels para a direita (ou para baixo), caso em que dir\u00edamos que um comprimento de passada de 2 \u00e9 usado. Esse \u00e9 um dos hyperpar\u00e2metros de uma rede neural convolucional, chamado stride length. No exemplo acima usado um stride length de 1, mas vale a pena saber que as pessoas \u00e0s vezes experimentam comprimentos de passada diferentes.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como foi feito nos cap\u00edtulos anteriores, se estivermos interessados em testar comprimentos de passada diferentes, podemos usar os dados de valida\u00e7\u00e3o para escolher o comprimento da passada que oferece o melhor desempenho. A mesma abordagem tamb\u00e9m pode ser usada para escolher o tamanho do campo receptivo local \u2013 n\u00e3o h\u00e1, \u00e9 claro, nada de especial sobre o uso de um campo receptivo local 5 \u00d7 5. Em geral, campos receptivos locais maiores tendem a ser \u00fateis quando as imagens de entrada s\u00e3o significativamente maiores que as imagens MNIST de 28 \u00d7 28 pixels.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Eu j\u00e1 disse que cada neur\u00f4nio oculto tem um vi\u00e9s e pesos 5 \u00d7 5 conectados ao seu campo receptivo local. O que eu ainda n\u00e3o mencionei \u00e9 que vamos usar os mesmos pesos e vieses para cada um dos 24 \u00d7 24 neur\u00f4nios ocultos. Quer saber como faremos isso matematicamente? Ent\u00e3o n\u00e3o perca o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  Refer\u00eancias:\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     O Que \u00e9 Vis\u00e3o Computacional?\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" target=\"_blank\">\n    A Comprehensive Guide to Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1081\" href=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1081\" href=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1081\" href=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1081\" href=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1081-5e0dd159ee015\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1081&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1081-5e0dd159ee015\" id=\"like-post-wrapper-140353593-1081-5e0dd159ee015\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "42": "<h1 class=\"entry-title\" id=\"capitulo-42\">\n Cap\u00edtulo 42 \u2013 Compartilhamento de Pesos em Redes Neurais Convolucionais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos continuar estudando Deep Learning e investigar como funciona o Compartilhamento de Pesos em Redes Neurais Convolucionais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   J\u00e1 dissemos que cada neur\u00f4nio tem um vi\u00e9s e pesos 5 \u00d7 5 conectados ao seu\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/campos-receptivos-locais-em-redes-neurais-convolucionais/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     campo receptivo local\n    </a>\n   </span>\n   . O que eu n\u00e3o mencionamos \u00e9 que vamos usar os mesmos pesos e vieses para cada um dos 24 \u00d7 24 neur\u00f4nios ocultos. Em outras palavras, para o neur\u00f4nio oculto, a sa\u00edda \u00e9:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form\" class=\"aligncenter wp-image-1099 size-medium\" data-attachment-id=\"1099\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?fit=492%2C174\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?fit=300%2C106\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?fit=492%2C174\" data-orig-size=\"492,174\" data-permalink=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/form-10/\" data-recalc-dims=\"1\" height=\"106\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?resize=300%2C106\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?resize=300%2C106 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?resize=200%2C71 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/form.png?w=492 492w\" width=\"300\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso significa que todos os neur\u00f4nios da primeira camada oculta detectam exatamente o mesmo recurso, apenas em locais diferentes na imagem de entrada. Para entender porque isso faz sentido, suponha que os pesos e os vieses sejam tais que o neur\u00f4nio oculto possa escolher, digamos, uma borda vertical em um campo receptivo local espec\u00edfico. Essa habilidade tamb\u00e9m \u00e9 \u00fatil em outros lugares da imagem. Por isso, \u00e9 \u00fatil aplicar o mesmo detector de recursos em toda a imagem. Para colocar esse conceito em termos um pouco mais abstratos, as redes convolucionais s\u00e3o bem adaptadas \u00e0 invari\u00e2ncia da transala\u00e7\u00e3o das imagens: girar uma foto de um gato 90 graus, ainda faz dela a imagem de um gato, embora os pixels agora estejam organizados de forma diferente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por esse motivo, \u00e0s vezes chamamos o mapa da camada de entrada para a camada oculta de um mapa de recursos. Chamamos os pesos que definem o mapa de recursos de pesos compartilhados. E n\u00f3s chamamos o vi\u00e9s usado no mapa de recursos desta maneira de vi\u00e9s compartilhado. Os pesos e vieses compartilhados costumam definir um kernel ou filtro. Na literatura, as pessoas \u00e0s vezes usam esses termos de maneiras ligeiramente diferentes e mais a frente veremos alguns exemplos concretos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A estrutura de rede que descrevemos at\u00e9 agora pode detectar apenas um \u00fanico tipo de recurso localizado. Para fazer reconhecimento de imagem, precisaremos de mais de um mapa de recursos. E assim, uma camada convolucional completa consiste em v\u00e1rios mapas de recursos, diferentes:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"\" class=\"aligncenter wp-image-1100 size-full\" data-attachment-id=\"1100\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"image\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?fit=537%2C256\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?fit=300%2C143\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?fit=537%2C256\" data-orig-size=\"537,256\" data-permalink=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/image/\" data-recalc-dims=\"1\" height=\"256\" sizes=\"(max-width: 537px) 100vw, 537px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?resize=537%2C256\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?w=537 537w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?resize=300%2C143 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/image.png?resize=200%2C95 200w\" width=\"537\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No exemplo mostrado acima, existem 3 mapas de recursos. Cada mapa de recursos \u00e9 definido por um conjunto de pesos compartilhados de 5 \u00d7 5 e um \u00fanico vi\u00e9s compartilhado. O resultado \u00e9 que a rede pode detectar tr\u00eas tipos diferentes de recursos, sendo cada recurso detect\u00e1vel em toda a imagem.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No exemplo temos apenas 3 mapas de recursos, para manter o diagrama acima simples. No entanto, na pr\u00e1tica, as redes convolucionais podem usar mais (e talvez muito mais) mapas de recursos. Uma das primeiras redes convolucionais, a\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/lenet/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     LeNet-5\n    </a>\n   </span>\n   , usou 6 mapas de recursos, cada um associado a um campo receptivo local 5 \u00d7 5, para reconhecer d\u00edgitos MNIST. Portanto, o exemplo ilustrado acima est\u00e1 bem pr\u00f3ximo do LeNet-5. Nos exemplos que desenvolveremos mais adiante neste livro, usaremos camadas convolucionais com 20 e 40 mapas de recursos. Vamos dar uma olhada r\u00e1pida em alguns dos recursos que s\u00e3o aprendidos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"features\" class=\"aligncenter wp-image-1101 size-medium\" data-attachment-id=\"1101\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"features\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?fit=800%2C600\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?fit=300%2C225\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?fit=800%2C600\" data-orig-size=\"800,600\" data-permalink=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/features/\" data-recalc-dims=\"1\" height=\"225\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=300%2C225\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=300%2C225 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=768%2C576 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=200%2C150 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?resize=690%2C518 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/features.png?w=800 800w\" width=\"300\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As 20 imagens correspondem a 20 diferentes mapas de recursos (ou filtros ou kernels). Cada mapa \u00e9 representado como uma imagem 5 \u00d7 5, correspondendo aos pesos 5 \u00d7 5 no campo receptivo local. Blocos mais brancos significam um peso menor (normalmente, mais negativo), portanto, o mapa de recursos responde menos aos pixels de entrada correspondentes. Blocos mais escuros significam um peso maior, portanto, o mapa de recursos responde mais aos pixels de entrada correspondentes. Muito grosso modo, as imagens acima mostram o tipo de caracter\u00edsticas que a camada convolucional responde.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ent\u00e3o, o que podemos concluir desses mapas de recursos? Est\u00e1 claro que h\u00e1 estrutura espacial aqui al\u00e9m do que esperamos ao acaso: muitos dos recursos t\u00eam claras sub-regi\u00f5es de luz e escurid\u00e3o. Isso mostra que nossa rede realmente est\u00e1 aprendendo coisas relacionadas \u00e0 estrutura espacial. No entanto, al\u00e9m disso, \u00e9 dif\u00edcil ver o que esses detectores de recursos est\u00e3o aprendendo. De fato, agora h\u00e1 muito trabalho para entender melhor os recursos aprendidos pelas redes convolucionais. Se voc\u00ea estiver interessado em acompanhar esse trabalho, sugiro come\u00e7ar com o artigo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://arxiv.org/abs/1311.2901\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Visualizando e Compreendendo Redes Convolucionais\n    </a>\n   </span>\n   de Matthew Zeiler e Rob Fergus (2013) ou ent\u00e3o\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=viso-computacional-e-reconhecimento-de-imagem\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Vis\u00e3o Computacional e Reconhecimento de Imagens\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma grande vantagem do compartilhamento de pesos e vieses \u00e9 que ele reduz bastante o n\u00famero de par\u00e2metros envolvidos em uma rede convolucional. Para cada mapa de recursos, precisamos de 5 \u00d7 5 = 25 pesos compartilhados, al\u00e9m de um \u00fanico vi\u00e9s compartilhado. Portanto, cada mapa de recursos requer 26 par\u00e2metros. Se temos 20 mapas de recursos, um total de 20 \u00d7 26 = 520 par\u00e2metros define a camada convolucional. Em compara\u00e7\u00e3o, suponhamos que tiv\u00e9ssemos uma primeira camada totalmente conectada, com 784 = 28 \u00d7 28 neur\u00f4nios de entrada e relativamente modestos 30 neur\u00f4nios ocultos, como usamos em muitos dos exemplos anteriores no livro. Isso \u00e9 um total de 784 \u00d7 30 pesos, al\u00e9m de um extra de 30 vieses, para um total de 23.550 par\u00e2metros. Em outras palavras, a camada totalmente conectada teria mais de 40 vezes mais par\u00e2metros que a camada convolucional.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c9 claro que n\u00e3o podemos fazer uma compara\u00e7\u00e3o direta entre o n\u00famero de par\u00e2metros, j\u00e1 que os dois modelos s\u00e3o diferentes em termos essenciais. Mas, intuitivamente, parece prov\u00e1vel que o uso de invari\u00e2ncia de tradu\u00e7\u00e3o pela camada convolucional reduza o n\u00famero de par\u00e2metros necess\u00e1rios para obter o mesmo desempenho que o modelo totalmente conectado. Isso, por sua vez, resultar\u00e1 em um treinamento mais r\u00e1pido para o modelo convolucional e, em \u00faltima an\u00e1lise, nos ajudar\u00e1 a construir redes profundas usando camadas convolucionais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A prop\u00f3sito, o nome convolucional vem do fato de que a opera\u00e7\u00e3o na equa\u00e7\u00e3o mostrada no in\u00edcio deste cap\u00edtulo \u00e9 \u00e0s vezes conhecida como uma convolu\u00e7\u00e3o. Um pouco mais precisamente, as pessoas \u00e0s vezes escrevem essa equa\u00e7\u00e3o como a1 = \u03c3 (b + w \u2217 a0), onde a1 denota o conjunto de ativa\u00e7\u00f5es de sa\u00edda de um mapa de recursos, a0 \u00e9 o conjunto de ativa\u00e7\u00f5es de entrada e \u2217 \u00e9 chamado de opera\u00e7\u00e3o de convolu\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No pr\u00f3ximo cap\u00edtulo estudaremos as camas de Pooling, outro \u201csegredo\u201d por tr\u00e1s das Redes Neurais Convolucionais e ent\u00e3o estaremos prontos para colocar tudo isso junto.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    O Que \u00e9 Vis\u00e3o Computacional?\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" target=\"_blank\">\n    A Comprehensive Guide to Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1098\" href=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1098\" href=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1098\" href=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1098\" href=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/compartilhamento-de-pesos-em-redes-neurais-convolucionais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1098-5e0dd15c5bd3a\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1098&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1098-5e0dd15c5bd3a\" id=\"like-post-wrapper-140353593-1098-5e0dd15c5bd3a\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "43": "<h1 class=\"entry-title\" id=\"capitulo-43\">\n Cap\u00edtulo 43 \u2013 Camadas de Pooling em Redes Neurais Convolucionais\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Al\u00e9m das camadas convolucionais que acabamos de descrever nos cap\u00edtulos anteriores, as redes neurais convolucionais tamb\u00e9m cont\u00eam camadas de agrupamento (ou Pooling). Camadas de Pooling s\u00e3o geralmente usadas imediatamente ap\u00f3s camadas convolucionais e o que fazem \u00e9 simplificar as informa\u00e7\u00f5es na sa\u00edda da camada convolucional. Vejamos o que s\u00e3o e como funcionam as Camadas de Pooling em Redes Neurais Convolucionais e na sequ\u00eancia vamos colocar todas as camadas juntas para compreender como funciona todo o processo nesta importante arquitetura de Deep Learning.\n  </span>\n </p>\n <h2>\n  <span style=\"color: #000000;\">\n   A Camada de Pooling\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma camada de pooling recebe cada sa\u00edda do mapa de caracter\u00edsticas da camada convolucional e prepara um mapa de caracter\u00edsticas condensadas. Por exemplo, cada unidade na camada de pooling pode resumir uma regi\u00e3o de (digamos) 2 \u00d7 2 neur\u00f4nios na camada anterior. Como um exemplo concreto, um procedimento comum para o pooling \u00e9 conhecido como pool m\u00e1ximo (ou Max-Pooling). No Max-Pooling, uma unidade de pooling simplesmente gera a ativa\u00e7\u00e3o m\u00e1xima na regi\u00e3o de entrada 2 \u00d7 2, conforme ilustrado no diagrama a seguir:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"pooling\" class=\"aligncenter size-full wp-image-1114\" data-attachment-id=\"1114\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"pooling\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?fit=441%2C236\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?fit=300%2C161\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?fit=441%2C236\" data-orig-size=\"441,236\" data-permalink=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/pooling/\" data-recalc-dims=\"1\" height=\"236\" sizes=\"(max-width: 441px) 100vw, 441px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?resize=441%2C236\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?w=441 441w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?resize=300%2C161 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling.png?resize=200%2C107 200w\" width=\"441\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Note que, como temos 24 \u00d7 24 neur\u00f4nios emitidos da camada convolucional, ap\u00f3s o agrupamento, temos 12 \u00d7 12 neur\u00f4nios.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como mencionado acima, a camada convolucional geralmente envolve mais do que um \u00fanico mapa de caracter\u00edsticas. Aplicamos o Max-Pooling para cada mapa de recursos separadamente. Portanto, se houvesse tr\u00eas mapas de recursos, as camadas combinadas, convolutional e Max-Pooling, se pareceriam com:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"pooling2\" class=\"aligncenter size-full wp-image-1115\" data-attachment-id=\"1115\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"pooling2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?fit=521%2C199\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?fit=300%2C115\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?fit=521%2C199\" data-orig-size=\"521,199\" data-permalink=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/pooling2/\" data-recalc-dims=\"1\" height=\"199\" sizes=\"(max-width: 521px) 100vw, 521px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?resize=521%2C199\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?w=521 521w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?resize=300%2C115 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling2.png?resize=200%2C76 200w\" width=\"521\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos pensar em Max-Pooling como uma forma de a rede perguntar se um determinado recurso \u00e9 encontrado em qualquer lugar de uma regi\u00e3o da imagem. Em seguida, elimina a informa\u00e7\u00e3o posicional exata. A intui\u00e7\u00e3o \u00e9 que, uma vez que um recurso tenha sido encontrado, sua localiza\u00e7\u00e3o exata n\u00e3o \u00e9 t\u00e3o importante quanto sua localiza\u00e7\u00e3o aproximada em rela\u00e7\u00e3o a outros recursos. Um grande benef\u00edcio \u00e9 que h\u00e1 muito menos recursos agrupados e, portanto, isso ajuda a reduzir o n\u00famero de par\u00e2metros necess\u00e1rios nas camadas posteriores. Genial, n\u00e3o?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Max-Pooling n\u00e3o \u00e9 a \u00fanica t\u00e9cnica usada para o pooling. Outra abordagem comum \u00e9 conhecida como Pooling L2. Aqui, em vez de tomar a ativa\u00e7\u00e3o m\u00e1xima de uma regi\u00e3o 2 \u00d7 2 de neur\u00f4nios, tomamos a raiz quadrada da soma dos quadrados das ativa\u00e7\u00f5es na regi\u00e3o 2 \u00d7 2. Embora os detalhes sejam diferentes, a intui\u00e7\u00e3o \u00e9 semelhante ao agrupamento m\u00e1ximo: o Pooling L2 \u00e9 uma maneira de condensar informa\u00e7\u00f5es da camada convolucional. Na pr\u00e1tica, ambas as t\u00e9cnicas t\u00eam sido amplamente utilizadas. E \u00e0s vezes as pessoas usam outros tipos de opera\u00e7\u00e3o de Pooling. Se voc\u00ea estiver realmente tentando otimizar o desempenho, poder\u00e1 usar dados de valida\u00e7\u00e3o para comparar v\u00e1rias abordagens diferentes ao Pooling e escolher a abordagem que funciona melhor.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Juntando Tudo\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos agora juntar todas essas ideias para formar uma rede neural convolucional completa. \u00c9 semelhante \u00e0 arquitetura que est\u00e1vamos estudando nos cap\u00edtulos anteriores, mas tem a adi\u00e7\u00e3o de uma camada de 10 neur\u00f4nios de sa\u00edda, correspondentes aos 10 valores poss\u00edveis para d\u00edgitos MNIST (\u20180\u2019, \u20181\u2019, \u20182\u2019, etc):\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"pooling3\" class=\"aligncenter size-full wp-image-1116\" data-attachment-id=\"1116\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"pooling3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?fit=582%2C224\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?fit=300%2C115\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?fit=582%2C224\" data-orig-size=\"582,224\" data-permalink=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/pooling3/\" data-recalc-dims=\"1\" height=\"224\" sizes=\"(max-width: 582px) 100vw, 582px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?resize=582%2C224\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?w=582 582w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?resize=300%2C115 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/04/pooling3.png?resize=200%2C77 200w\" width=\"582\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A rede come\u00e7a com 28 \u00d7 28 neur\u00f4nios de entrada (cada image de cada d\u00edgito do dataset MNIST tem 28 x 28 pixels), que s\u00e3o usados \u200b\u200bpara codificar as intensidades de pixel para uma imagem no dataset MNIST. Este \u00e9 ent\u00e3o seguido por uma camada convolucional usando um campo receptivo local de 5 x 5 e tr\u00eas mapas de caracter\u00edsticas. O resultado \u00e9 uma camada de 3 \u00d7 24 \u00d7 24 neur\u00f4nios ocultos. A pr\u00f3xima etapa \u00e9 uma camada de Max-Pooling, aplicada a regi\u00f5es 2 \u00d7 2, em cada um dos tr\u00eas mapas de recursos. O resultado \u00e9 uma camada de 3 \u00d7 12 \u00d7 12 neur\u00f4nios ocultos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A camada final de conex\u00f5es na rede \u00e9 uma camada totalmente conectada. Ou seja, essa camada conecta todos os neur\u00f4nios da camada de max-pooling a cada um dos 10 neur\u00f4nios de sa\u00edda. Essa arquitetura totalmente conectada \u00e9 a mesma que usamos nos cap\u00edtulos anteriores. Note, no entanto, que no diagrama acima, usei uma \u00fanica seta, por simplicidade, em vez de mostrar todas as conex\u00f5es. Claro, voc\u00ea pode facilmente imaginar as conex\u00f5es.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ou seja, temos uma rede composta de muitas unidades simples, cujos comportamentos s\u00e3o determinados por seus pesos e vieses. E o objetivo geral ainda \u00e9 o mesmo: usar dados de treinamento para treinar os pesos e vieses da rede para que a rede fa\u00e7a um bom trabalho classificando os d\u00edgitos de entrada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em particular, assim como no in\u00edcio do livro, n\u00f3s vamos treinar nossa rede usando descida estoc\u00e1stica do gradiente e retropropaga\u00e7\u00e3o. Isso ocorre principalmente da mesma maneira que nos cap\u00edtulos anteriores. No entanto, precisamos fazer algumas modifica\u00e7\u00f5es no procedimento de retropropaga\u00e7\u00e3o. A raz\u00e3o \u00e9 que nossa deriva\u00e7\u00e3o anterior da retropropaga\u00e7\u00e3o foi para redes com camadas totalmente conectadas. Felizmente, \u00e9 simples modificar a deriva\u00e7\u00e3o para camadas convolucional e max-pooling.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Quer ver tudo isso funcionando em linguagem Python? Ent\u00e3o n\u00e3o perca o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    O Que \u00e9 Vis\u00e3o Computacional?\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" target=\"_blank\">\n    A Comprehensive Guide to Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1113\" href=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1113\" href=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1113\" href=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1113\" href=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/camadas-de-pooling-em-redes-neurais-convolucionais/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1113-5e0dd1afb2614\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1113&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1113-5e0dd1afb2614\" id=\"like-post-wrapper-140353593-1113-5e0dd1afb2614\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "44": "<h1 class=\"entry-title\" id=\"capitulo-44\">\n Cap\u00edtulo 44 \u2013 Reconhecimento de Imagens com Redes Neurais Convolucionais em Python \u2013 Parte 1\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nossa tarefa \u00e9 simples: vamos fornecer a um modelo de Deep Learning uma imagem e o modelo ter\u00e1 que classificar se a imagem \u00e9 de um cachorro ou gato! Parece f\u00e1cil, n\u00e3o? Na verdade, n\u00e3o! Para que isso funcione precisamos construir e treinar um modelo de Deep Learning, o que envolve conhecimentos de Matem\u00e1tica, Estat\u00edstica, Programa\u00e7\u00e3o, Vis\u00e3o Computacional, Pr\u00e9-Processamento de imagens, entre outras \u00e1reas. Mas n\u00f3s vamos fazer isso juntos. Nos pr\u00f3ximos cap\u00edtulos vamos construir e treinar um modelo de Deep Learning para Reconhecimento de Imagens com Redes Neurais Convolucionais em Python. Ser\u00e1 uma excelente oportunidade de praticar tudo que estudamos no livro at\u00e9 aqui e preparar voc\u00ea para os cap\u00edtulos mais avan\u00e7ados deste livro, quando estudaremos outras arquiteturas de Deep Learning. Vamos come\u00e7ar?\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Plano de Trabalho\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para come\u00e7ar, precisamos definir claramente o problema a ser resolvido e como vamos resolv\u00ea-lo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Problema\n   </strong>\n   : Dada uma imagem, \u00e9 um cachorro ou um gato?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A primeira coisa que precisamos \u00e9 de muitas imagens de cachorros e gatos, para poder treinar um algoritmo de Deep Learning. Usaremos, portanto, uma abordagem de aprendizagem supervisionada, onde apresentaremos ao algoritmo diversas imagens, devidamente marcadas como sendo imagens de c\u00e3es e gatos e ent\u00e3o treinaremos o algoritmo. Ao final do treinamento, teremos um modelo que poder\u00e1 receber novas imagens (desta vez n\u00e3o marcadas previamente) e ent\u00e3o o modelo dever\u00e1 ser capaz de classificar como sendo imagem de c\u00e3o ou gato.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para essa tarefa, usaremos uma arquitetura de Rede Neural Convolucional, a mesma que estudamos nos cap\u00edtulos anteriores. Essa arquitetura usa m\u00e9todos de convolu\u00e7\u00e3o para poder prever caracter\u00edsticas espec\u00edficas de uma imagem de acordo com o que aprende em um conjunto de treinamento. Por exemplo, podemos dizer que \u00e9 poss\u00edvel perceber a diferen\u00e7a ao procurar bigodes em um gato ou focinho comprido em um cachorro. Mas uma Rede Neural Convolucional procura muitos outros recursos baseados no que temos em um conjunto de treinamento.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Solu\u00e7\u00e3o: Usar uma Rede Neural Convolucional para aprender recursos de imagens e assim prever se uma imagem cont\u00e9m um cachorro ou um gato.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Defini\u00e7\u00e3o dos Dados\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Data Science, Deep Learning, Machine Learning, Intelig\u00eancia Artificial. Nada disso faz sentido sem dados, muitos dados (por isso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=big-data-fundamentos\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Big Data\n    </a>\n   </span>\n   \u00e9 cada vez mais importante nos dias de hoje). E para esta tarefa, teremos os seguintes dados:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Conjunto de dados de treino: Teremos 12.500 imagens de c\u00e3es e 12.500 imagens de gatos para o conjunto de treinamento.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Conjunto de dados de valida\u00e7\u00e3o: Teremos 12.500 imagens de c\u00e3es e gatos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Conjunto de dados de teste: Teremos 1.000 imagens de c\u00e3es e gatos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa \u00e1 uma quest\u00e3o onde os iniciantes tem muitas d\u00favidas. Por que precisamos de dados de treino, valida\u00e7\u00e3o e teste? Usamos os dados de treino para treinar o algoritmo e ent\u00e3o criar o modelo preditivo. Usamos os dados de valida\u00e7\u00e3o, para avaliar o modelo durante o treinamento. Usamos os dados de teste para validar a performance do modelo j\u00e1 treinado, ou seja, apresentamos ao modelo dados que ele n\u00e3o viu durante o treinamento, a fim de garantir que ele \u00e9 capaz de fazer previs\u00f5es.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Estrutura de Trabalho\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos realizar as seguintes atividades na constru\u00e7\u00e3o do modelo de Reconhecimento de Imagens com Redes Neurais Convolucionais em Python:\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   1- Visualiza\u00e7\u00e3o de Dados\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Come\u00e7aremos nosso trabalho com tarefas de visualiza\u00e7\u00e3o de dados e an\u00e1lise explorat\u00f3ria. Precisamos compreender os dados antes de qualquer outra coisa, como as dimens\u00f5es das imagens, escala de cores, detalhes de sombras e se as imagens possuem mais detalhes n\u00e3o relacionados diretamente a um cachorro ou gato, como nesta imagem abaixo! Afinal, o modelo ter\u00e1 que aprender que um cachorro vestido de Hello Kitty ainda \u00e9 um cachorro!\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"dog\" class=\"aligncenter size-full wp-image-1177\" data-attachment-id=\"1177\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"dog\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?fit=289%2C480\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?fit=181%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?fit=289%2C480\" data-orig-size=\"289,480\" data-permalink=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/dog/\" data-recalc-dims=\"1\" height=\"480\" sizes=\"(max-width: 289px) 100vw, 289px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?resize=289%2C480\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?w=289 289w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?resize=181%2C300 181w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/dog.jpeg?resize=200%2C332 200w\" width=\"289\"/>\n  </span>\n </p>\n <p>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   2- Construindo a Rede Neural Convolucional\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nosso pr\u00f3ximo passo ser\u00e1 construir a arquitetura de rede. Definiremos quantas camadas ser\u00e3o usadas, camadas de convolu\u00e7\u00e3o e pooling, fun\u00e7\u00f5es de ativa\u00e7\u00e3o, m\u00e9trica de avalia\u00e7\u00e3o, descida do gradiente com backpropagation e outros detalhes. Discutiremos porque estamos fazendo nossas escolhas.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   3- Treinamento\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com a arquitetura definida, podemos ent\u00e3o treinar a nossa rede, o que consiste em apresentar os dados ao algoritmo para que o aprendizado ocorra. Definiremos os hiperpar\u00e2metros e por quanto tempo treinaremos a rede. Vamos aproveitar e discutir sobre t\u00e9cnicas para evitar o overfitting (quando o modelo se ajusta demais aos dados de treino).\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   4- Fazer Previs\u00f5es\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por fim, usaremos o modelo treinado para fazer as classifica\u00e7\u00f5es. Entregaremos ao modelo novas imagens e ele ter\u00e1 que classificar se a imagem \u00e9 de um cachorro ou gato! Na pr\u00e1tica, o modelo nunca faz a classifica\u00e7\u00e3o com 100% de certeza. O que ele faz \u00e9 uma previs\u00e3o (para ser ainda mais preciso, uma infer\u00eancia) e nosso trabalho \u00e9 garantir que essa previs\u00e3o tenha o mais alto n\u00edvel de acur\u00e1cia poss\u00edvel.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ferramentas\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para este trabalho usaremos, claro, linguagem Python com Keras e TensorFlow. Vamos explicar a voc\u00ea quais s\u00e3o as op\u00e7\u00f5es de frameworks e cada linha de c\u00f3digo ser\u00e1 explicada em detalhes. Vamos considerar que voc\u00ea j\u00e1 conhece linguagem Python e sabe executar um Jupyter Notebook. Se n\u00e3o sabe, ent\u00e3o acesse pelo menos o Cap\u00edtulo 1 do nosso curso gratuito\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Python Fundamentos Para An\u00e1lise de Dados\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Est\u00e1 pronto? Ent\u00e3o nos encontramos no pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <a href=\"http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/\" rel=\"noopener noreferrer\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    O Que \u00e9 Vis\u00e3o Computacional?\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" target=\"_blank\">\n    A Comprehensive Guide to Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1174\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1174\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1174\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1174\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-1/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1174-5e0dd1b1c6918\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1174&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1174-5e0dd1b1c6918\" id=\"like-post-wrapper-140353593-1174-5e0dd1b1c6918\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "45": "<h1 class=\"entry-title\" id=\"capitulo-45\">\n Cap\u00edtulo 45 \u2013 Reconhecimento de Imagens com Redes Neurais Convolucionais em Python \u2013 Parte 2\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos iniciar nosso trabalho de Reconhecimento de Imagens com Redes Neurais Convolucionais em Python cuidando da nossa mat\u00e9ria-prima: dados. Precisamos fazer o download das imagens e organiz\u00e1-las para ent\u00e3o iniciar o trabalho.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Usaremos como fonte de dados, o famoso dataset\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Dogs and Cats\n    </a>\n   </span>\n   oferecido pelo Kaggle, o portal sobre Competi\u00e7\u00f5es de Data Science, onde inclusive a Data Science Academy promove entre os alunos matriculados nas Forma\u00e7\u00f5es as Competi\u00e7\u00f5es DSA de Machine Learning. O Kaggle oferece diversos datasets p\u00fablicos que podem ser usados para voc\u00ea desenvolver seus projetos e incluir no seu portf\u00f3lio, uma excelente forma de demonstrar suas habilidades em Data Science e Machine Learning. Mostramos como construir um portf\u00f3lio de projetos de Data Science aqui:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=preparao-para-carreira-de-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Prepara\u00e7\u00e3o Para Carreira de Cientista de Dados\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea pode fazer o download das imagens neste endere\u00e7o:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Dogs vs. Cats\n    </a>\n   </span>\n   . Mas n\u00f3s j\u00e1 fizemos o download e disponibilizamos para voc\u00ea junto com o Jupyter Notebook no reposit\u00f3rio deste livro no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   . Como este dataset \u00e9 bastante famoso, alternativamente, voc\u00ea pode fazer o download oferecido pela Microsoft Research neste endere\u00e7o\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=54765\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Kaggle Cats and Dogs Dataset\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Feito o download das imagens (voc\u00ea vai precisar de aproximadamente 1 GB de espa\u00e7o em disco para as imagens), precisamos organizar os arquivos em uma estrutura de diret\u00f3rios da seguinte forma:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"diretorios\" class=\"aligncenter size-full wp-image-1198\" data-attachment-id=\"1198\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"diretorios\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?fit=570%2C222\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?fit=300%2C117\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?fit=570%2C222\" data-orig-size=\"570,222\" data-permalink=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/diretorios/\" data-recalc-dims=\"1\" height=\"222\" sizes=\"(max-width: 570px) 100vw, 570px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?resize=570%2C222\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?w=570 570w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?resize=300%2C117 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/diretorios.png?resize=200%2C78 200w\" width=\"570\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Crie um diret\u00f3rio (por exemplo Cap45, mas pode ser o nome que voc\u00ea quiser). Dentro dele crie mais 3 pastas: dataset_treino, dataset_validation e dataset_teste. N\u00e3o use espa\u00e7os no nome e muito menos acentos nas palavras, pois isso causa diversos problemas em programa\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Dentro da pasta dataset_treino, crie mais duas pastas, cats (que vai receber as 12.500 imagens de gatos) e dogs (que vai receber as 12.500 imagens de cachorros).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nas pastas dataset_validation e dataset_teste n\u00e3o \u00e9 necess\u00e1rio criar sub pastas e dentro delas colocaremos as 12.500 imagens de valida\u00e7\u00e3o e 1.000 imagens de teste, respectivamente. As imagens de valida\u00e7\u00e3o ser\u00e3o usadas para avaliar o modelo durante o treinamento e as imagens de teste ser\u00e3o usadas para avaliar o modelo depois do treinamento.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos agora visualizar algumas imagens usando o Jupyter Notebook, que voc\u00ea encontra no reposit\u00f3rio deste livro no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr/DeepLearningBook\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   . Caso n\u00e3o tenha familiaridade com o Jupyter Notebook, acesse o Cap\u00edtulo 1 do curso gratuito\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Python Fundamentos Para An\u00e1lise de Dados\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Se voc\u00ea criou a estrutura de diret\u00f3rios de forma correta, ent\u00e3o as seguintes c\u00e9lulas mostrar\u00e3o algumas das imagens:\n  </span>\n </p>\n <p>\n  <img alt=\"image1\" class=\"aligncenter wp-image-1203 size-large\" data-attachment-id=\"1203\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"image1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?fit=1024%2C851\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?fit=300%2C249\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?fit=1056%2C878\" data-orig-size=\"1056,878\" data-permalink=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/image1-5/\" data-recalc-dims=\"1\" height=\"851\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=1024%2C851\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=1024%2C851 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=300%2C249 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=768%2C639 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=200%2C166 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?resize=690%2C574 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image1.png?w=1056 1056w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"image2\" class=\"aligncenter wp-image-1204 size-large\" data-attachment-id=\"1204\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"image2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?fit=951%2C1024\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?fit=279%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?fit=1012%2C1090\" data-orig-size=\"1012,1090\" data-permalink=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/image2-5/\" data-recalc-dims=\"1\" height=\"1024\" sizes=\"(max-width: 951px) 100vw, 951px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=951%2C1024\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=951%2C1024 951w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=279%2C300 279w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=768%2C827 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=200%2C215 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?resize=690%2C743 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/05/image2.png?w=1012 1012w\" width=\"951\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Se as imagens foram mostradas de forma correta, ent\u00e3o os dados est\u00e3o prontos para serem explorados. \u00c9 o que faremos no pr\u00f3ximo cap\u00edtulo! At\u00e9 l\u00e1.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    O Que \u00e9 Vis\u00e3o Computacional?\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" target=\"_blank\">\n    A Comprehensive Guide to Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1193\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1193\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1193\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1193\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-2/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1193-5e0dd1b449ab0\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1193&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1193-5e0dd1b449ab0\" id=\"like-post-wrapper-140353593-1193-5e0dd1b449ab0\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "46": "<h1 class=\"entry-title\" id=\"capitulo-46\">\n Cap\u00edtulo 46 \u2013 Reconhecimento de Imagens com Redes Neurais Convolucionais em Python \u2013 Parte 3\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma das principais d\u00favidas de quem est\u00e1 iniciando em Machine Learning e se depara com as Redes Neurais Convolucionais, \u00e9 sobre como ocorre o aprendizado dos par\u00e2metros (aquilo que o algoritmo realmente aprende durante o treinamento). O que exatamente est\u00e1 sendo feito quando apresentamos uma imagem a um algoritmo de Rede Neural Convolucional?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"dogs_cats\" class=\"aligncenter wp-image-1224 size-full\" data-attachment-id=\"1224\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"dogs_cats\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?fit=1024%2C576\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?fit=300%2C169\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?fit=1600%2C900\" data-orig-size=\"1600,900\" data-permalink=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/dogs_cats/\" data-recalc-dims=\"1\" height=\"658\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/dogs_cats.gif?resize=1170%2C658\" width=\"1170\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Sabemos que em cada camada de convolu\u00e7\u00e3o, a rede tenta entender os padr\u00f5es b\u00e1sicos. Por exemplo: Na primeira camada de convolu\u00e7\u00e3o, a rede tenta aprender padr\u00f5es e bordas das imagens. Na segunda camada, ela tenta entender a forma / cor e outras coisas. Uma camada final chamada camada de recurso / camada totalmente conectada tenta classificar a imagem.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Antes de prepararmos o script com nossa rede convolucional, vamos compreender como definimos a arquitetura da rede e em quais camadas os par\u00e2metros s\u00e3o aprendidos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Camada de Entrada (Input Layer)\n   </strong>\n   : O que a camada de entrada faz \u00e9 ler a imagem. Portanto, n\u00e3o h\u00e1 par\u00e2metros a serem aprendidos aqui.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Camada Convolucional (Convolutional Layer)\n   </strong>\n   : considere uma camada convolucional que usa os mapas de recursos \u201cl\u201d como entrada e tem os mapas de recursos \u201ck\u201d como sa\u00edda. O tamanho do filtro \u00e9 \u201cn * m\u201d.\n  </span>\n </p>\n <p>\n  <img alt=\"conv_layer\" class=\"aligncenter wp-image-1227 size-medium\" data-attachment-id=\"1227\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"conv_layer\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?fit=400%2C352\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?fit=300%2C264\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?fit=400%2C352\" data-orig-size=\"400,352\" data-permalink=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/conv_layer/\" data-recalc-dims=\"1\" height=\"264\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?resize=300%2C264\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?resize=300%2C264 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?resize=200%2C176 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/06/conv_layer.png?w=400 400w\" width=\"300\"/>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui, a entrada tem l = 32 mapas de recursos como entradas, k = 64 mapas de recursos como sa\u00eddas e o tamanho do filtro \u00e9 n = 3 e m = 3. \u00c9 importante entender que n\u00e3o temos apenas um filtro 3 * 3, mas, na verdade, temos um filtro 3 * 3 * 32, j\u00e1 que nossa entrada tem 32 dimens\u00f5es. E como uma sa\u00edda da primeira camada convolucional, aprendemos 64 diferentes filtros 3 * 3 * 32 cujo peso total \u00e9 \u201cn * m * k * l\u201d. Depois, h\u00e1 um termo chamado bias para cada mapa de recursos. Portanto, o n\u00famero total de par\u00e2metros \u00e9 \u201c(n * m * l + 1) * k\u201d.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Camada de Pooling (Pooling Layer)\n   </strong>\n   : N\u00e3o h\u00e1 par\u00e2metros a aprender na camada de pooling. Essa camada \u00e9 usada apenas para reduzir o tamanho da dimens\u00e3o da imagem.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Obs: Podemos ter v\u00e1rias combina\u00e7\u00f5es de camada convolucional / camada de pooling em nossa arquitetura, sendo esta decis\u00e3o do\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Cientista de Dados\n    </a>\n   </span>\n   ou\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Engenheiro de Intelig\u00eancia Artificial\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Camada Totalmente Conectada (Fully Connected Layer)\n   </strong>\n   : Nesta camada, todas as unidades de entrada possuem um peso separ\u00e1vel para cada unidade de sa\u00edda. Para entradas \u201cn\u201d e sa\u00eddas \u201cm\u201d, o n\u00famero de pesos \u00e9 \u201cn * m\u201d. Al\u00e9m disso, essa camada possui o bias para cada n\u00f3 de sa\u00edda, portanto, os par\u00e2metros aprendidos s\u00e3o \u201c(n + 1) * m\u201d.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Camada de Sa\u00edda (Output Layer)\n   </strong>\n   : Esta camada \u00e9 totalmente conectada, portanto, os par\u00e2metros aprendidos tamb\u00e9m s\u00e3o \u201c(n + 1) m\u201d, quando \u201cn\u201d \u00e9 o n\u00famero de entradas e \u201cm\u201d \u00e9 o n\u00famero de sa\u00eddas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A principal dificuldade ao definir a arquitetura de um CNN \u00e9 a primeira camada totalmente conectada. N\u00e3o sabemos a dimensionalidade da camada totalmente conectada. Para calcular isso, temos que come\u00e7ar com o tamanho da imagem de entrada e calcular o tamanho de cada camada convolucional.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No caso simples, o tamanho da camada totalmente conectada \u00e9 calculado como \u201cinput_size \u2013 (filter_size \u2013 1)\u201d. Por exemplo, se o tamanho da imagem de entrada for (50,50) e o filtro for (3,3), ent\u00e3o (50- (3-1)) = 48. Mas o tamanho da imagem de entrada de uma rede convolucional n\u00e3o deve ser menor que a entrada, ent\u00e3o precisamos definir o padding.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para calcular o padding usamos: input_size + 2 * padding_size \u2013 (filter_size-1). Para o caso acima, (50 + (2 * 1) \u2013 (3\u20131) = 52 \u2013 2 = 50) que fornece o mesmo tamanho de entrada. Perfeito, pois assim n\u00e3o perdemos nenhum detalhe da imagem.\n  </span>\n  <span style=\"color: #000000;\">\n   Se quisermos explicitamente diminuir a imagem durante a convolu\u00e7\u00e3o, podemos definir um passo (stride).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Finalmente, para calcular o n\u00famero de par\u00e2metros que a rede vai aprender usamos: (n * m * k + 1) * f. Veremos tudo isso em c\u00f3digo Python no pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Caso queira mais detalhes sobre como definir a arquitetura da rede, consulte o famoso e excelente paper da AlexNet:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n     ImageNet Classification with Deep Convolutional Neural Networks\n    </a>\n    .\n   </span>\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias\n  </span>\n  :\n </p>\n <p>\n  <a href=\"http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/\" rel=\"noopener noreferrer\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    O Que \u00e9 Vis\u00e3o Computacional?\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    ImageNet Classification with Deep Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u2019t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca\" rel=\"noopener noreferrer\" target=\"_blank\">\n    How to calculate the number of parameters in the CNN?\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" target=\"_blank\">\n    A Comprehensive Guide to Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1221\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1221\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1221\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1221\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-3/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1221-5e0dd1bb4902c\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1221&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1221-5e0dd1bb4902c\" id=\"like-post-wrapper-140353593-1221-5e0dd1bb4902c\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "47": "<h1 class=\"entry-title\" id=\"capitulo-47\">\n Cap\u00c3\u00adtulo 47 \u2013 Reconhecimento de Imagens com Redes Neurais Convolucionais em Python \u00e2\u0080\u0093 Parte 4\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vejamos como implementar nosso modelo de Rede Neural Convolucional que vai aprender a diferen\u00c3\u00a7a nas imagens de c\u00c3\u00a3es e gatos e quando apresentarmos novas imagens ao modelo, ele ser\u00c3\u00a1 capaz de prever se a imagem \u00c3\u00a9 de um c\u00c3\u00a3o ou gato de forma autom\u00c3\u00a1tica.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Abaixo voc\u00c3\u00aa encontra todos os detalhes de constru\u00c3\u00a7\u00c3\u00a3o da rede, treinamento do modelo e avalia\u00c3\u00a7\u00c3\u00a3o (use a barra de rolagem para visualizar todo o c\u00c3\u00b3digo). O Jupyter Notebook completo est\u00c3\u00a1 dispon\u00c3\u00advel no reposit\u00c3\u00b3rio deste livro no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://github.com/dsacademybr\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Github\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <style type=\"text/css\">\n  .errordiv { padding:10px; margin:10px; border: 1px solid #555555;color: #000000;background-color: #f8f8f8; width:500px; }#advanced_iframe {visibility:visible;opacity:1;}#ai-layer-div-advanced_iframe p {height:100%;margin:0;padding:0}\n </style>\n <script type=\"text/javascript\">\n  var ai_iframe_width_advanced_iframe = 0;  var ai_iframe_height_advanced_iframe = 0;var aiIsIe8=false;var aiOnloadScrollTop=\"true\";\r\nif (typeof aiReadyCallbacks === 'undefined') {\r\n    var aiReadyCallbacks = [];  \r\n} else if (!(aiReadyCallbacks instanceof Array)) {\r\n    var aiReadyCallbacks = [];\r\n}    function aiShowIframeId(id_iframe) { jQuery(\"#\"+id_iframe).css(\"visibility\", \"visible\");    }    function aiResizeIframeHeight(height) { aiResizeIframeHeight(height,advanced_iframe); }    function aiResizeIframeHeightId(height,width,id) {aiResizeIframeHeightById(id,height);}\n </script>\n <iframe allowtransparency=\"true\" frameborder=\"0\" height=\"900\" id=\"advanced_iframe\" name=\"advanced_iframe\" src=\"http://deeplearningbook.com.br/wp-content/uploads/2019/06/Notebook\u00e2\u0080\u0093V2.html\" style=\";width:100%;height:900px;\" width=\"100%\">\n </iframe>\n <script type=\"text/javascript\">\n  var ifrm_advanced_iframe = document.getElementById(\"advanced_iframe\");var hiddenTabsDoneadvanced_iframe = false;\r\nfunction resizeCallbackadvanced_iframe() {}function aiChangeUrl(loc) {}\n </script>\n <script type=\"text/javascript\">\n </script>\n <hr/>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com isso conclu\u00c3\u00admos esta introdu\u00c3\u00a7\u00c3\u00a3o a uma das mais famosas arquiteturas de Deep Learning, as Redes neurais Convolucionais. A partir do pr\u00c3\u00b3ximo cap\u00c3\u00adtulo estudaremos outras arquiteturas. At\u00c3\u00a9 l\u00c3\u00a1.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Lembrando que a DSA oferece um programa completo para quem deseja aprender Deep Learning e Intelig\u00c3\u00aancia Artificial na pr\u00c3\u00a1tica e de forma profissional, a\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Forma\u00c3\u00a7\u00c3\u00a3o Intelig\u00c3\u00aancia Artificial\n    </a>\n   </span>\n   . Fa\u00c3\u00a7a como milhares de alunos no Brasil e no mundo e comece sua capacita\u00c3\u00a7\u00c3\u00a3o agora mesmo. O curso \u00c3\u00a9 100% online e 100% em portugu\u00c3\u00aas. Seja um profissional em alta demanda!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00c3\u00aancias:\n  </span>\n </p>\n <p>\n  <a href=\"http://datascienceacademy.com.br/blog/o-que-e-visao-computacional/\" rel=\"noopener noreferrer\" target=\"_blank\">\n   <span style=\"text-decoration: underline;\">\n    O Que \u00c3\u00a9 Vis\u00c3\u00a3o Computacional?\n   </span>\n  </a>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00c3\u00a7\u00c3\u00a3o Intelig\u00c3\u00aancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00c3\u00a7\u00c3\u00a3o An\u00c3\u00a1lise Estat\u00c3\u00adstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Forma\u00c3\u00a7\u00c3\u00a3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    ImageNet Classification with Deep Convolutional Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1711.00489.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Don\u00e2\u0080\u0099t Decay the Learning Rate, Increase the Batch Size\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca\" rel=\"noopener noreferrer\" target=\"_blank\">\n    How to calculate the number of parameters in the CNN?\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\" rel=\"noopener noreferrer\" target=\"_blank\">\n    A Comprehensive Guide to Convolutional Neural Networks\u00e2\u0080\u008a\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n </div>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1248\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1248\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1248\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1248\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/reconhecimento-de-imagens-com-redes-neurais-convolucionais-em-python-parte-4/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1248-5e0b861ce6ec0\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1248&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1248-5e0b861ce6ec0\" id=\"like-post-wrapper-140353593-1248-5e0b861ce6ec0\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "48": "<h1 class=\"entry-title\" id=\"capitulo-48\">\n Cap\u00edtulo 48 \u2013 Redes Neurais Recorrentes\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A partir deste cap\u00edtulo estudaremos diversas outras arquiteturas de Deep Learning, que est\u00e3o sendo usadas em aplica\u00e7\u00f5es de Intelig\u00eancia Artificial de \u00faltima gera\u00e7\u00e3o. Todas essas arquiteturas s\u00e3o estudadas e utilizadas na constru\u00e7\u00e3o de sistema de IA na\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n   . Continue conosco nessa incr\u00edvel jornada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os humanos n\u00e3o come\u00e7am a pensar do zero a cada segundo. Ao ler este texto, voc\u00ea entende cada palavra com base em sua compreens\u00e3o das palavras anteriores. Voc\u00ea n\u00e3o joga tudo fora e come\u00e7a a pensar de novo. Seus pensamentos t\u00eam persist\u00eancia.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As redes neurais artificiais tradicionais n\u00e3o podem fazer isso, o que traz algumas limita\u00e7\u00f5es para esses tipos de modelos. Por exemplo, imagine que voc\u00ea queira classificar o tipo de evento que est\u00e1 acontecendo em todos os pontos de um filme. N\u00e3o est\u00e1 claro como uma rede neural tradicional poderia usar seu racioc\u00ednio sobre eventos anteriores no filme para informar os posteriores.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Redes Neurais Recorrentes resolvem esse problema. S\u00e3o redes com loops, permitindo que as informa\u00e7\u00f5es persistam.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As redes recorrentes s\u00e3o um tipo de rede neural artificial projetada para reconhecer padr\u00f5es em sequ\u00eancias de dados, como texto, genomas, caligrafia, palavra falada ou dados de s\u00e9ries num\u00e9ricas que emanam de sensores, bolsas de valores e ag\u00eancias governamentais. Esses algoritmos consideram tempo e sequ\u00eancia, eles t\u00eam uma dimens\u00e3o temporal.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A pesquisa mostra que eles s\u00e3o um dos tipos mais poderosos e \u00fateis de rede neural, juntamente com o mecanismo de aten\u00e7\u00e3o e as redes de mem\u00f3ria. As RNNs s\u00e3o aplic\u00e1veis at\u00e9 mesmo a imagens, que podem ser decompostas em uma s\u00e9rie de amostras e tratadas como uma sequ\u00eancia.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como as redes recorrentes possuem um certo tipo de mem\u00f3ria, e a mem\u00f3ria tamb\u00e9m faz parte da condi\u00e7\u00e3o humana, faremos analogias com a mem\u00f3ria do c\u00e9rebro, para uma melhor compreens\u00e3o. Continue a leitura.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Revis\u00e3o de Redes Feedforward\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para entender as redes recorrentes, primeiro vamos revisar o b\u00e1sico das redes feedforward, que estudamos em cap\u00edtulos anteriores aqui mesmo no Deep Learning Book.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ambas as redes recebem o nome da forma como canalizam informa\u00e7\u00f5es atrav\u00e9s de uma s\u00e9rie de opera\u00e7\u00f5es matem\u00e1ticas realizadas nos n\u00f3s da rede. As redes feedforward alimentam informa\u00e7\u00f5es diretamente (nunca tocando em um determinado n\u00f3 duas vezes), enquanto as redes recorrentes percorrem atrav\u00e9s de um loop.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No caso de redes feedforward, exemplos de entrada s\u00e3o alimentados na rede e transformados em uma sa\u00edda; com a aprendizagem supervisionada, a sa\u00edda seria por exemplo um r\u00f3tulo, um nome aplicado \u00e0 entrada. Ou seja, elas mapeiam dados brutos para categorias, reconhecendo padr\u00f5es que podem sinalizar, por exemplo, que uma imagem de entrada deve ser rotulada como \u201cgato\u201d ou \u201ccachorro\u201d. A imagem abaixo mostra um exemplo de rede feedforward.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"Rede Feed Forward\" class=\"aligncenter size-full wp-image-1271\" data-attachment-id=\"1271\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Rede Feed Forward\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?fit=783%2C851\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?fit=276%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?fit=783%2C851\" data-orig-size=\"783,851\" data-permalink=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/feedf/\" data-recalc-dims=\"1\" height=\"851\" sizes=\"(max-width: 783px) 100vw, 783px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=783%2C851\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?w=783 783w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=276%2C300 276w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=768%2C835 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=200%2C217 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/feedf.png?resize=690%2C750 690w\" width=\"783\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma rede feedforward pode ent\u00e3o ser treinada em imagens rotuladas, por exemplo, at\u00e9 minimizar o erro ao classificar suas categorias. Com o conjunto treinado de par\u00e2metros (ou pesos, conhecidos coletivamente como um modelo), a rede procura categorizar os dados que nunca viu. Uma rede feedforward treinada pode ser exposta a qualquer cole\u00e7\u00e3o aleat\u00f3ria de fotografias, e a primeira fotografia a que est\u00e1 exposta n\u00e3o alterar\u00e1 necessariamente como classifica a segunda. Ver a fotografia de um gato n\u00e3o levar\u00e1 a rede a perceber um cachorro em seguida.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ou seja, uma rede feedforward n\u00e3o tem no\u00e7\u00e3o de ordem no tempo, e a \u00fanica entrada que considera \u00e9 o exemplo atual a que foi exposta. As redes feedforward s\u00e3o amn\u00e9sicas em rela\u00e7\u00e3o ao seu passado recente; elas lembram nostalgicamente apenas os momentos formativos do treinamento.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Redes Neurais Recorrentes\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As redes recorrentes, por outro lado, tomam como entrada n\u00e3o apenas o exemplo de entrada atual que veem, mas tamb\u00e9m o que perceberam anteriormente no tempo. Aqui est\u00e1 um diagrama com a representa\u00e7\u00e3o de uma rede neural recorrente e uma rede feedforward.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"Nets\" class=\"aligncenter size-full wp-image-1272\" data-attachment-id=\"1272\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"Nets\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?fit=1024%2C566\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?fit=300%2C166\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?fit=2600%2C1437\" data-orig-size=\"2600,1437\" data-permalink=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/rnn-2/\" data-recalc-dims=\"1\" height=\"647\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=1170%2C647\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?w=2600 2600w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=300%2C166 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=768%2C424 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=1024%2C566 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=200%2C111 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?resize=690%2C381 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.png?w=2340 2340w\" width=\"1170\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A decis\u00e3o de uma rede recorrente alcan\u00e7ada na etapa de tempo t-1 afeta a decis\u00e3o que alcan\u00e7ar\u00e1 um momento mais tarde na etapa de tempo t. Assim, as redes recorrentes t\u00eam duas fontes de entrada, o presente e o passado recente, que se combinam para determinar como respondem a novos dados, da mesma forma que fazemos na vida. Nas redes neurais recorrentes isso \u00e9 feito, claro, com a ajuda da nossa querida Matem\u00e1tica. E por isso o curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Matem\u00e1tica Para Machine Learning\n    </a>\n   </span>\n   \u00e9 um dos cursos de maior sucesso na Data Science Academy.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As redes recorrentes s\u00e3o diferenciadas das redes feedforward pelo loop de feedback conectado \u00e0s suas decis\u00f5es anteriores, ingerindo suas pr\u00f3prias sa\u00eddas momento ap\u00f3s momento como entrada. Costuma-se dizer que as redes recorrentes t\u00eam mem\u00f3ria. A adi\u00e7\u00e3o de mem\u00f3ria \u00e0s redes neurais tem uma finalidade: h\u00e1 informa\u00e7\u00f5es na pr\u00f3pria sequ\u00eancia e as redes recorrentes a utilizam para executar tarefas que as redes de feedforward n\u00e3o conseguem.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa informa\u00e7\u00e3o sequencial \u00e9 preservada no estado oculto da rede recorrente, que consegue passar por muitas etapas de tempo \u00e0 medida que ela avan\u00e7a em cascata para afetar o processamento de cada novo exemplo. Essas correla\u00e7\u00f5es entre eventos s\u00e3o separadas por muitos momentos, e essas correla\u00e7\u00f5es s\u00e3o chamadas de \u201c\n   <strong>\n    depend\u00eancias de longo prazo\n   </strong>\n   \u201d, porque um evento no tempo depende e \u00e9 uma fun\u00e7\u00e3o de um ou mais eventos que vieram antes. Uma maneira objetiva de pensar sobre as RNNs \u00e9 a seguinte: elas s\u00e3o uma forma de compartilhar pesos ao longo do tempo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim como a mem\u00f3ria humana circula invisivelmente dentro de um corpo, afetando nosso comportamento sem revelar sua forma completa, a informa\u00e7\u00e3o circula nos estados ocultos de redes recorrentes. A l\u00edngua portuguesa est\u00e1 cheia de palavras que descrevem os ciclos de feedback da mem\u00f3ria. Quando dizemos que uma pessoa \u00e9 assombrada por seus atos, por exemplo, estamos simplesmente falando sobre as consequ\u00eancias que as produ\u00e7\u00f5es passadas causam no tempo presente. Os franceses chamam isso de \u201cLe pass\u00e9 qui ne passe pas\u201d ou \u201cO passado que n\u00e3o passa\u201d.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Descreveremos o processo de levar a mem\u00f3ria adiante matematicamente da seguinte forma:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"\" class=\"aligncenter size-full wp-image-1273\" data-attachment-id=\"1273\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"recurrent_equation\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?fit=484%2C66\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?fit=300%2C41\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?fit=484%2C66\" data-orig-size=\"484,66\" data-permalink=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/recurrent_equation/\" data-recalc-dims=\"1\" height=\"66\" sizes=\"(max-width: 484px) 100vw, 484px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?resize=484%2C66\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?w=484 484w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?resize=300%2C41 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/recurrent_equation.png?resize=200%2C27 200w\" width=\"484\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O estado oculto na etapa de tempo t \u00e9 h_t. \u00c9 uma fun\u00e7\u00e3o da entrada na mesma etapa de tempo x_t, modificada por uma matriz de peso W (como a que usamos para redes feedforward) adicionada ao estado oculto do passo de tempo anterior h_t-1 multiplicado por seu pr\u00f3prio estado oculto \u2013 para a matriz de estado oculto U, tamb\u00e9m conhecida como matriz de transi\u00e7\u00e3o e semelhante a uma cadeia de Markov. As matrizes de peso s\u00e3o filtros que determinam quanta import\u00e2ncia deve ser dada tanto \u00e0 entrada atual quanto ao estado oculto do passado. O erro que eles geram retornar\u00e1 por meio de retropropaga\u00e7\u00e3o (backpropagation) e ser\u00e1 usado para ajustar seus pesos at\u00e9 que o erro n\u00e3o diminua mais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A soma da entrada de peso e do estado oculto \u00e9 comprimida pela fun\u00e7\u00e3o \u03c6 \u2013 ou uma fun\u00e7\u00e3o sigmoide log\u00edstica ou tanh, dependendo \u2013 que \u00e9 uma ferramenta padr\u00e3o para condensar valores muito grandes ou muito pequenos em um espa\u00e7o log\u00edstico, bem como tornar os gradientes vi\u00e1veis para retropropaga\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como esse loop de feedback ocorre a cada etapa da s\u00e9rie, cada estado oculto cont\u00e9m tra\u00e7os n\u00e3o apenas do estado oculto anterior, mas tamb\u00e9m de todos aqueles que precederam h_t-1 pelo tempo que a mem\u00f3ria pode persistir.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Dada uma s\u00e9rie de letras, uma rede recorrente usar\u00e1 o primeiro caractere para ajudar a determinar sua percep\u00e7\u00e3o do segundo caractere, de tal forma que um q inicial possa lev\u00e1-lo a inferir que a pr\u00f3xima letra ser\u00e1 u.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas para que as redes neurais recorrentes possam aprender de forma efetiva, precisamos de uma vers\u00e3o levemente modificada do Backpropagation, o BPTT (Backpropagation Through Time). Mas isso \u00e9 assunto para o pr\u00f3ximo cap\u00edtulo! Estudaremos ainda duas importantes varia\u00e7\u00f5es das RNNs, as Long Short-Term Memory Units (LSTMs) e as Gated Recurrent Units (GRUs).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Enquanto isso, caso queira come\u00e7ar a se divertir com as RNNs, acompanhe esse tutorial oficial do TensorFlow:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.tensorflow.org/tutorials/sequences/recurrent\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Recurrent Neural Networks\n    </a>\n   </span>\n   . Para aulas pr\u00e1ticas em portugu\u00eas com linguagem Python, acesse\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent Neural Networks Cheatsheet\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://skymind.ai/wiki/lstm\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     A Beginner\u2019s Guide to LSTMs and Recurrent Neural Networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1269\" href=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1269\" href=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1269\" href=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1269\" href=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1269-5e0dd1be127f0\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1269&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1269-5e0dd1be127f0\" id=\"like-post-wrapper-140353593-1269-5e0dd1be127f0\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "49": "<h1 class=\"entry-title\" id=\"capitulo-49\">\n Cap\u00edtulo 49 \u2013 A Matem\u00e1tica do Backpropagation Through Time (BPTT)\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Estudamos no cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/redes-neurais-recorrentes/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   as Redes Neurais Recorrentes. Mas para que elas funcionem, o algoritmo de treinamento precisa de um pequeno ajuste, uma vez que esse tipo de rede possui o que podemos chamar de \u201cmem\u00f3ria\u201d durante seu treinamento. E o que faz isso acontecer \u00e9 A Matem\u00e1tica do Backpropagation Through Time (BPTT), assunto deste cap\u00edtulo do Deep Learning Book.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Um Pouco Mais Sobre as RNNs\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A ideia por tr\u00e1s dos RNNs \u00e9 fazer uso de informa\u00e7\u00f5es sequenciais. Em uma rede neural tradicional, assumimos que todas as entradas (e sa\u00eddas) s\u00e3o independentes umas das outras. Mas para muitas tarefas isso \u00e9 uma ideia muito ruim. Se voc\u00ea quiser prever a pr\u00f3xima palavra em uma frase, \u00e9 melhor saber quais palavras vieram antes dela. As RNNs s\u00e3o chamadas de recorrentes porque executam a mesma tarefa para todos os elementos de uma sequ\u00eancia, com a sa\u00edda sendo dependente dos c\u00e1lculos anteriores. Outra maneira de pensar sobre RNNs \u00e9 que elas t\u00eam uma \u201cmem\u00f3ria\u201d que captura informa\u00e7\u00f5es sobre o que foi calculado at\u00e9 agora. Em teoria, RNNs podem fazer uso de informa\u00e7\u00f5es em sequ\u00eancias arbitrariamente longas, mas, na pr\u00e1tica, limitam-se a olhar para tr\u00e1s apenas alguns passos (mais sobre isso adiante). Aqui est\u00e1 o que uma RNN t\u00edpica parece:\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"rnn\" class=\"aligncenter size-full wp-image-1301\" data-attachment-id=\"1301\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rnn\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?fit=795%2C319\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?fit=300%2C120\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?fit=795%2C319\" data-orig-size=\"795,319\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/rnn-3/\" data-recalc-dims=\"1\" height=\"319\" sizes=\"(max-width: 795px) 100vw, 795px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=795%2C319\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?w=795 795w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=300%2C120 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=768%2C308 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=200%2C80 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn.jpg?resize=690%2C277 690w\" width=\"795\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O diagrama acima mostra uma RNN sendo \u201cdesenrolada\u201d ou \u201cdesdobrada\u201d (termo unfolded em ingl\u00eas) em uma rede completa. Ao desenrolar, simplesmente queremos dizer que escrevemos a rede para a sequ\u00eancia completa. Por exemplo, se a sequ\u00eancia que nos interessa \u00e9 uma senten\u00e7a de 5 palavras, a rede seria desdobrada em uma rede neural de 5 camadas, uma camada para cada palavra. As f\u00f3rmulas que governam o c\u00e1lculo que acontece em uma RNN s\u00e3o as seguintes:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   1. xt \u00e9 a entrada no passo de tempo t. Por exemplo, x1 poderia ser um vetor one-hot correspondente \u00e0 segunda palavra de uma senten\u00e7a.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   2. st \u00e9 o estado oculto no passo de tempo t. \u00c9 a \u201cmem\u00f3ria\u201d da rede. O termo st \u00e9 calculado com base no estado oculto anterior e a entrada na etapa atual atrav\u00e9s da f\u00f3rmula: st = f(Uxt + Wst-1). A fun\u00e7\u00e3o geralmente \u00e9 uma n\u00e3o-linearidade, como tanh ou ReLU. J\u00e1 s -1, que \u00e9 necess\u00e1rio para calcular o primeiro estado oculto, \u00e9 tipicamente inicializado com zero.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   3. ot \u00e9 a sa\u00edda na etapa t. Por exemplo, se quis\u00e9ssemos prever a pr\u00f3xima palavra em uma frase, seria um vetor de probabilidades em todo o nosso vocabul\u00e1rio. ot = softmax(Vst).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   H\u00e1 algumas coisas a serem observadas aqui:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea pode pensar no estado oculto st como a mem\u00f3ria da rede. st captura informa\u00e7\u00f5es sobre o que aconteceu em todas as etapas de tempo anteriores. A sa\u00edda na etapa ot \u00e9 calculada exclusivamente com base na mem\u00f3ria no tempo t. \u00c9 um pouco mais complicado na pr\u00e1tica, porque normalmente n\u00e3o \u00e9 poss\u00edvel capturar informa\u00e7\u00f5es de muitas etapas de tempo anteriores.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao contr\u00e1rio de uma rede neural profunda tradicional, que usa par\u00e2metros diferentes em cada camada, uma RNN compartilha os mesmos par\u00e2metros (U, V, W acima) em todas as etapas. Isso reflete o fato de que estamos executando a mesma tarefa em cada etapa, apenas com entradas diferentes. Isso reduz muito o n\u00famero total de par\u00e2metros que precisamos aprender.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O diagrama acima tem sa\u00eddas em cada etapa de tempo, mas dependendo da tarefa, isso pode n\u00e3o ser necess\u00e1rio. Por exemplo, ao prever o sentimento de uma frase, podemos nos preocupar apenas com a sa\u00edda final, n\u00e3o com o sentimento ap\u00f3s cada palavra. Da mesma forma, podemos n\u00e3o precisar de entradas em cada etapa de tempo. A principal caracter\u00edstica de uma RNN \u00e9 seu estado oculto, que captura algumas informa\u00e7\u00f5es sobre uma sequ\u00eancia.\n  </span>\n </p>\n <h3>\n  <span style=\"color: #000000;\">\n   E o Backpropagation?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Lembre-se, o objetivo das redes neurais recorrentes \u00e9 classificar com precis\u00e3o uma entrada sequencial (por exemplo, dada uma frase, prever o sentimento ou mesmo a pr\u00f3xima palavra). Contamos com a retropropaga\u00e7\u00e3o (backpropagation) do erro e o gradiente descendente para faz\u00ea-lo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A retropropaga\u00e7\u00e3o em redes feedforward retrocede do erro final atrav\u00e9s das sa\u00eddas, pesos e entradas de cada camada oculta, atribuindo a esses pesos a responsabilidade por uma parte do erro calculando suas derivadas parciais \u2013 \u2202E / \u2202w, ou a rela\u00e7\u00e3o entre suas taxas de mudan\u00e7a. Essas deriva\u00e7\u00f5es s\u00e3o ent\u00e3o usadas por nossa regra de aprendizado, gradiente descendente, para ajustar os pesos para cima ou para baixo, qualquer que seja a dire\u00e7\u00e3o que diminua o erro. J\u00e1 estudamos isso nos cap\u00edtulos anteriores aqui do Deep Learning Book.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As redes recorrentes dependem de uma extens\u00e3o da retropropaga\u00e7\u00e3o, chamada Backpropagation Through Time, ou BPTT. O tempo, neste caso, \u00e9 simplesmente expresso por uma s\u00e9rie ordenada e bem definida de c\u00e1lculos, ligando um passo de tempo ao seguinte, o que significa que toda a retropropaga\u00e7\u00e3o precisa funcionar.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Redes neurais, sejam elas recorrentes ou n\u00e3o, s\u00e3o simplesmente fun\u00e7\u00f5es compostas aninhadas como f(g(h(x))). A adi\u00e7\u00e3o de um elemento de tempo apenas estende a s\u00e9rie de fun\u00e7\u00f5es para as quais calculamos derivadas com a regra da cadeia (chain rule). Matem\u00e1tica pura!\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A Matem\u00e1tica do Backpropagation Through Time (BPTT)\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por falar em Matem\u00e1tica, vamos compreender o BPTT atrav\u00e9s de f\u00f3rmulas e alguns gr\u00e1ficos. Para as devidas refer\u00eancias, sempre consulte as notas ao final do cap\u00edtulo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Considere que estamos criando uma rede neural recorrente que seja capaz de prever a pr\u00f3xima palavra em um texto, o que pode ser \u00fatil em aplica\u00e7\u00f5es de IA para criar peti\u00e7\u00f5es e assim ajudar advogados a automatizar o trabalho. Algo que j\u00e1 \u00e9 feito pelo\n   <a href=\"https://rossintelligence.com/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"text-decoration: underline;\">\n     ROSS\n    </span>\n   </a>\n   , o Rob\u00f4 Advogado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos come\u00e7ar com a equa\u00e7\u00e3o b\u00e1sica de uma Rede Neural Recorrente:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter size-full wp-image-1291\" data-attachment-id=\"1291\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?fit=198%2C79\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?fit=198%2C79\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?fit=198%2C79\" data-orig-size=\"198,79\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form1-6/\" data-recalc-dims=\"1\" height=\"79\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1.png?resize=198%2C79\" width=\"198\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Tamb\u00e9m definimos nossa perda, ou erro, como a perda de entropia cruzada, dada por:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-1292\" data-attachment-id=\"1292\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?fit=208%2C134\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?fit=208%2C134\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?fit=208%2C134\" data-orig-size=\"208,134\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form2-12/\" data-recalc-dims=\"1\" height=\"134\" sizes=\"(max-width: 208px) 100vw, 208px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?resize=208%2C134\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?w=208 208w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2.png?resize=200%2C129 200w\" width=\"208\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui, yt \u00e9 a palavra correta no momento do passo t, e y^t \u00e9 nossa previs\u00e3o. Normalmente, tratamos a sequ\u00eancia completa (senten\u00e7a) como um exemplo de treinamento, portanto, o erro total \u00e9 apenas a soma dos erros em cada etapa de tempo (palavra).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rnn-bptt1\" class=\"aligncenter size-full wp-image-1293\" data-attachment-id=\"1293\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rnn-bptt1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?fit=953%2C550\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?fit=300%2C173\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?fit=953%2C550\" data-orig-size=\"953,550\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/rnn-bptt1/\" data-recalc-dims=\"1\" height=\"550\" sizes=\"(max-width: 953px) 100vw, 953px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=953%2C550\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?w=953 953w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=300%2C173 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=768%2C443 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=200%2C115 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt1.png?resize=690%2C398 690w\" width=\"953\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Lembre-se de que nosso objetivo \u00e9 calcular os gradientes do erro em rela\u00e7\u00e3o aos nossos par\u00e2metros U, V e W e, em seguida, aprender bons par\u00e2metros usando o Gradiente Descendente Estoc\u00e1stico. Assim como resumimos os erros, tamb\u00e9m somamos os gradientes em cada etapa de tempo para um exemplo de treinamento:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form3\" class=\"aligncenter size-full wp-image-1294\" data-attachment-id=\"1294\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?fit=101%2C35\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?fit=101%2C35\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?fit=101%2C35\" data-orig-size=\"101,35\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form3-10/\" data-recalc-dims=\"1\" height=\"35\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3.png?resize=101%2C35\" width=\"101\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para calcular esses gradientes, usamos a regra de diferencia\u00e7\u00e3o da cadeia. Esse \u00e9 o algoritmo de retropropaga\u00e7\u00e3o quando aplicado para tr\u00e1s a partir do erro. Usaremos o E_3 como exemplo, apenas para trabalhar com n\u00fameros concretos.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form4\" class=\"aligncenter size-full wp-image-1295\" data-attachment-id=\"1295\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?fit=148%2C106\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?fit=148%2C106\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?fit=148%2C106\" data-orig-size=\"148,106\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form4-6/\" data-recalc-dims=\"1\" height=\"106\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4.png?resize=148%2C106\" width=\"148\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Acima, z3 = Vs3 e a \u00faltima linha \u00e9 o produto externo de dois vetores. N\u00e3o se preocupe se voc\u00ea n\u00e3o seguir os passos acima, n\u00f3s pulamos\n  </span>\n  <span style=\"color: #000000;\">\n   v\u00e1rios passos e voc\u00ea pode tentar calcular essas derivadas voc\u00ea mesmo (bom exerc\u00edcio!). O ponto que estou tentando transmitir \u00e9 que\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"latex\" class=\"aligncenter size-full wp-image-1305\" data-attachment-id=\"1305\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"latex\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?fit=22%2C21\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?fit=22%2C21\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?fit=22%2C21\" data-orig-size=\"22,21\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/latex/\" data-recalc-dims=\"1\" height=\"21\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex.png?resize=22%2C21\" width=\"22\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   depende apenas dos valores no momento atual, y\u02c63, y3, s3. Se voc\u00ea tem estes valores, calculando o gradiente para V \u00e9 uma multiplica\u00e7\u00e3o de matriz simples.\n  </span>\n  <span style=\"color: #000000;\">\n   Mas a hist\u00f3ria \u00e9 diferente para W (e para U):\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"latex (1)\" class=\"aligncenter size-full wp-image-1306\" data-attachment-id=\"1306\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"latex (1)\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?fit=22%2C21\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?fit=22%2C21\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?fit=22%2C21\" data-orig-size=\"22,21\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/latex-1/\" data-recalc-dims=\"1\" height=\"21\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/latex-1.png?resize=22%2C21\" width=\"22\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para entender porque, escrevemos a regra da cadeia, como acima:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form5\" class=\"aligncenter size-full wp-image-1296\" data-attachment-id=\"1296\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?fit=140%2C39\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?fit=140%2C39\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?fit=140%2C39\" data-orig-size=\"140,39\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form5-5/\" data-recalc-dims=\"1\" height=\"39\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form5.png?resize=140%2C39\" width=\"140\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora, note que s3 = tanh(Uxt + Ws2) depende de s2, que depende de W e s1, e assim por diante. Ent\u00e3o, se pegarmos a derivada em rela\u00e7\u00e3o a W, n\u00e3o podemos simplesmente tratar s2 como uma constante! Precisamos aplicar a regra da cadeia novamente e o que realmente temos \u00e9 o seguinte:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form6\" class=\"aligncenter size-full wp-image-1297\" data-attachment-id=\"1297\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form6\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?fit=195%2C49\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?fit=195%2C49\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?fit=195%2C49\" data-orig-size=\"195,49\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form6-4/\" data-recalc-dims=\"1\" height=\"49\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form6.png?resize=195%2C49\" width=\"195\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Somamos as contribui\u00e7\u00f5es de cada passo de tempo para o gradiente. Em outras palavras, como W \u00e9 usado em todas as etapas at\u00e9 a sa\u00edda que nos interessa, precisamos retroceder gradientes na rede de t = 3 at\u00e9 t = 0:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rnn-bptt-with-gradients\" class=\"aligncenter size-full wp-image-1298\" data-attachment-id=\"1298\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rnn-bptt-with-gradients\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?fit=953%2C550\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?fit=300%2C173\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?fit=953%2C550\" data-orig-size=\"953,550\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/rnn-bptt-with-gradients/\" data-recalc-dims=\"1\" height=\"550\" sizes=\"(max-width: 953px) 100vw, 953px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=953%2C550\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?w=953 953w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=300%2C173 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=768%2C443 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=200%2C115 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/rnn-bptt-with-gradients.png?resize=690%2C398 690w\" width=\"953\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Observe que isso \u00e9 exatamente o mesmo que o algoritmo de retropropaga\u00e7\u00e3o padr\u00e3o que usamos nas Redes Neurais Profundas da Feedforward. A principal diferen\u00e7a \u00e9 que resumimos os gradientes para W em cada etapa de tempo. Em um rede neural tradicional, n\u00e3o compartilhamos par\u00e2metros entre camadas, portanto, n\u00e3o precisamos somar nada. Mas, na minha opini\u00e3o, o BPTT \u00e9 apenas um nome sofisticado para retropropaga\u00e7\u00e3o padr\u00e3o em uma RNN \u201cdesenrolada\u201d. Assim como com Backpropagation, voc\u00ea pode definir um vetor delta que voc\u00ea repassa, por exemplo:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form7\" class=\"aligncenter size-full wp-image-1299\" data-attachment-id=\"1299\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form7\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?fit=183%2C30\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?fit=183%2C30\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?fit=183%2C30\" data-orig-size=\"183,30\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/form7-3/\" data-recalc-dims=\"1\" height=\"30\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form7.png?resize=183%2C30\" width=\"183\"/>\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Aqui um exemplo de como seria a implementa\u00e7\u00e3o do BPTT em Python:\n  </span>\n </p>\n <p>\n  <a href=\"https://github.com/dsacademybr\" rel=\"noopener noreferrer\" target=\"_blank\">\n   <img alt=\"bptt\" class=\"aligncenter wp-image-1308 size-large\" data-attachment-id=\"1308\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"bptt\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?fit=1024%2C791\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?fit=300%2C232\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?fit=1248%2C964\" data-orig-size=\"1248,964\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/bptt-2/\" data-recalc-dims=\"1\" height=\"791\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=1024%2C791\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=1024%2C791 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=300%2C232 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=768%2C593 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=200%2C154 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?resize=690%2C533 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/bptt-1.png?w=1248 1248w\" width=\"1024\"/>\n  </a>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso tamb\u00e9m deve lhe dar uma ideia do motivo pelo qual as RNNs s\u00e3o dif\u00edceis de treinar: as sequ\u00eancias (frases) podem ser bastante longas, talvez 20 palavras ou mais, e, portanto, voc\u00ea precisa retroceder atrav\u00e9s de v\u00e1rias camadas. Na pr\u00e1tica, muitas pessoas truncam a retropropaga\u00e7\u00e3o em poucos passos. Mas isso \u00e9 assunto para o pr\u00f3ximo cap\u00edtulo! At\u00e9 l\u00e1.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O BPTT \u00e9 estudado em detalhes no curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   , na Data Science Academy e usado para construir aplica\u00e7\u00f5es de tradu\u00e7\u00e3o de idiomas e gera\u00e7\u00e3o autom\u00e1tica de legendas em v\u00eddeos. O conceito tamb\u00e9m \u00e9 aplicado no curso de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Processamento de Linguagem Natural\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Cheatsheet\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://skymind.ai/wiki/lstm\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    A Beginner\u2019s Guide to LSTMs and Recurrent Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Recurrent Neural Networks Tutorial, Part 1 \u2013 Introduction to RNNs\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Tutorial, Part 3 \u2013 Backpropagation Through Time and Vanishing Gradients\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1482129989&amp;sr=8-1-fkmr0&amp;keywords=Machine+Learning+%28McGraw-Hill+International+Editions+Computer+Science+Series%29\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1289\" href=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1289\" href=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1289\" href=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1289\" href=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1289-5e0dd1c52d1d7\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1289&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1289-5e0dd1c52d1d7\" id=\"like-post-wrapper-140353593-1289-5e0dd1c52d1d7\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "50": "<h1 class=\"entry-title\" id=\"capitulo-50\">\n Cap\u00edtulo 50 \u2013 A Matem\u00e1tica da Dissipa\u00e7\u00e3o do Gradiente e Aplica\u00e7\u00f5es das RNNs\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No\n   <a href=\"http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"text-decoration: underline;\">\n     Cap\u00edtulo 34\n    </span>\n   </a>\n   n\u00f3s discutimos sobre o problema da dissipa\u00e7\u00e3o do gradiente e a dificuldade em treinar as redes neurais artificiais. Com as RNNs esse problema \u00e9 ainda mais acentuado e por isso vamos agora estudar A Matem\u00e1tica da Dissipa\u00e7\u00e3o do Gradiente e Aplica\u00e7\u00f5es das RNNs e compreender matematicamente porque o problema acontece.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mencionamos anteriormente que as RNNs t\u00eam dificuldades em aprender depend\u00eancias de longo alcance \u2013 intera\u00e7\u00f5es entre palavras que est\u00e3o separadas por v\u00e1rios passos, por exemplo. Isso \u00e9 problem\u00e1tico porque o significado de uma frase em portugu\u00eas \u00e9 geralmente determinado por palavras que n\u00e3o s\u00e3o muito pr\u00f3ximas: \u201cO homem que usava uma peruca entrou no bar\u201d. A frase \u00e9 realmente sobre um homem entrando em um bar, n\u00e3o sobre a peruca. Mas \u00e9 improv\u00e1vel que uma RNN simples seja capaz de capturar essas informa\u00e7\u00f5es. Para entender porque, vamos dar uma olhada mais de perto no gradiente que calculamos no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/a-matematica-do-backpropagation-through-time-bptt/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     cap\u00edtulo anterior\n    </a>\n   </span>\n   :\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter size-full wp-image-1321\" data-attachment-id=\"1321\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?fit=387%2C96\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?fit=300%2C74\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?fit=387%2C96\" data-orig-size=\"387,96\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form1-7/\" data-recalc-dims=\"1\" height=\"96\" sizes=\"(max-width: 387px) 100vw, 387px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?resize=387%2C96\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?w=387 387w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?resize=300%2C74 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form1-1.png?resize=200%2C50 200w\" width=\"387\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Observe que:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-1322\" data-attachment-id=\"1322\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?fit=43%2C55\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?fit=43%2C55\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?fit=43%2C55\" data-orig-size=\"43,55\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form2-13/\" data-recalc-dims=\"1\" height=\"55\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form2-1.png?resize=43%2C55\" width=\"43\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00e9 uma regra de cadeia em si! Por exemplo:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form3\" class=\"aligncenter size-full wp-image-1323\" data-attachment-id=\"1323\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?fit=191%2C55\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?fit=191%2C55\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?fit=191%2C55\" data-orig-size=\"191,55\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form3-11/\" data-recalc-dims=\"1\" height=\"55\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form3-1.png?resize=191%2C55\" width=\"191\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Observe tamb\u00e9m que, como estamos tomando a derivada de uma fun\u00e7\u00e3o vetorial em rela\u00e7\u00e3o a um vetor, o resultado \u00e9 uma matriz (chamada de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\" rel=\"noopener noreferrer\" target=\"_blank\">\n     matriz jacobiana\n    </a>\n   </span>\n   ) cujos elementos s\u00e3o todos derivadas\n   <em>\n    pointwise\n   </em>\n   . Podemos reescrever o gradiente acima da seguinte forma:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form4\" class=\"aligncenter size-full wp-image-1324\" data-attachment-id=\"1324\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?fit=557%2C102\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?fit=300%2C55\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?fit=557%2C102\" data-orig-size=\"557,102\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/form4-7/\" data-recalc-dims=\"1\" height=\"102\" sizes=\"(max-width: 557px) 100vw, 557px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?resize=557%2C102\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?w=557 557w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?resize=300%2C55 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/form4-1.png?resize=200%2C37 200w\" width=\"557\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Acontece (esse\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n     paper\n    </a>\n   </span>\n   explica isso em detalhes) que a norma (na \u00e1lgebra linear, an\u00e1lise funcional e \u00e1reas relacionadas da matem\u00e1tica, uma norma \u00e9 uma fun\u00e7\u00e3o que atribui um comprimento ou tamanho estritamente positivo a cada vetor em um espa\u00e7o vetorial \u2013 exceto para o vetor zero, ao qual \u00e9 atribu\u00eddo um comprimento de zero. Mais detalhes\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   .), que voc\u00ea pode pensar como um valor absoluto, da matriz jacobiana acima tem um limite superior de 1. Isso porque a nossa fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh (ou sigm\u00f3ide) mapeia todos os valores em um intervalo entre -1 e 1, e a derivada \u00e9 limitada por 1 (1/4 no caso de sigmoide) tamb\u00e9m:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"tanh\" class=\"aligncenter size-full wp-image-1325\" data-attachment-id=\"1325\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"tanh\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?fit=640%2C480\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?fit=300%2C225\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?fit=640%2C480\" data-orig-size=\"640,480\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/tanh/\" data-recalc-dims=\"1\" height=\"480\" sizes=\"(max-width: 640px) 100vw, 640px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?resize=640%2C480\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?w=640 640w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?resize=300%2C225 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/07/tanh.png?resize=200%2C150 200w\" width=\"640\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Voc\u00ea pode ver que as fun\u00e7\u00f5es tanh e sigmoid t\u00eam derivadas de 0 em ambas as extremidades, onde se aproximam de uma linha plana. Quando isso acontece, dizemos que os neur\u00f4nios correspondentes est\u00e3o saturados. Eles t\u00eam um gradiente nulo e conduzem outros gradientes nas camadas anteriores para 0. Assim, com valores pequenos nas multiplica\u00e7\u00f5es de matriz e m\u00faltiplas matrizes (t-k em particular) os valores de gradiente est\u00e3o diminuindo exponencialmente r\u00e1pido, desaparecendo completamente ap\u00f3s alguns passos de tempo. Contribui\u00e7\u00f5es gradientes de etapas \u201clong\u00ednquas\u201d se tornam zero e o estado nessas etapas n\u00e3o contribui para o que a rede est\u00e1 aprendendo: a rede acaba n\u00e3o aprendendo depend\u00eancias de longo alcance. Dissipa\u00e7\u00f5es do gradiente n\u00e3o s\u00e3o exclusivos das RNNs e tamb\u00e9m acontecem em Redes Neurais Profundas Feedforward. Mas as RNNs tendem a ser muito profundas (t\u00e3o profundas quanto a dura\u00e7\u00e3o da senten\u00e7a, em um problema de Processamento de Linguagem Natural por exemplo), o que torna o problema muito mais comum.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   \u00c9 f\u00e1cil imaginar que, dependendo de nossas fun\u00e7\u00f5es de ativa\u00e7\u00e3o e par\u00e2metros de rede, poder\u00edamos obter explos\u00e3o em vez de dissipa\u00e7\u00e3o de gradientes, se os valores da matriz Jacobiana forem grandes. Na verdade, isso \u00e9 chamado de problema de explos\u00e3o do gradiente. A raz\u00e3o pela qual as dissipa\u00e7\u00f5es do gradiente receberam mais aten\u00e7\u00e3o do que as explos\u00f5es \u00e9 dupla. Por um lado, explos\u00e3o de gradientes s\u00e3o \u00f3bvias. Seus gradientes se tornar\u00e3o NaN (n\u00e3o um n\u00famero) e seu programa falhar\u00e1. Em segundo lugar, recortar os gradientes em um limiar pr\u00e9-definido \u00e9 uma solu\u00e7\u00e3o muito simples e eficaz para evitar a explos\u00e3o dos gradientes. As dissipa\u00e7\u00f5es dos gradientes s\u00e3o mais problem\u00e1ticas porque n\u00e3o s\u00e3o \u00f3bvias quando ocorrem ou \u00e9 mais complicado lidar com elas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Felizmente, existem algumas maneiras de combater o problema da dissipa\u00e7\u00e3o do gradiente. A inicializa\u00e7\u00e3o adequada da matriz W pode reduzir o efeito do problema. Ou seja, aplicamos regulariza\u00e7\u00e3o. Uma solu\u00e7\u00e3o mais interessante \u00e9 usar as fun\u00e7\u00f5es de ativa\u00e7\u00e3o ReLU em vez de tanh ou sigm\u00f3ide. A derivada ReLU \u00e9 uma constante de 0 ou 1, por isso n\u00e3o \u00e9 t\u00e3o prov\u00e1vel que sofra de dissipa\u00e7\u00e3o do gradiente. Uma solu\u00e7\u00e3o ainda mais popular \u00e9 usar as arquiteturas Long Short-Term Memory (LSTM) ou Gated Recurrent Unit (GRU). As LSTMs foram propostas pela primeira vez em 1997 e s\u00e3o os modelos talvez mais amplamente usados em Processamento de Linguagem Natural atualmente. As GRUs, propostas pela primeira vez em 2014, s\u00e3o vers\u00f5es simplificadas das LSTMs. Ambas as arquiteturas RNN foram explicitamente projetadas para lidar com dissipa\u00e7\u00e3o do gradiente e aprender eficientemente depend\u00eancias de longo alcance. Vamos cobrir as duas arquiteturas nos pr\u00f3ximos cap\u00edtulos.\n  </span>\n </p>\n <p>\n </p>\n <h2 style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Mas o que podemos fazer com as RNNs?\n   </span>\n  </strong>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As RNNs mostraram grande sucesso em muitas tarefas de Processamento de Linguagem Natural. Neste ponto, devo mencionar que os tipos de RNN mais usados s\u00e3o as LSTMs, que s\u00e3o muito melhores na captura de depend\u00eancias de longo prazo do que as RNNs em sua arquitetura padr\u00e3o. Mas n\u00e3o se preocupe, as LSTMs s\u00e3o essencialmente a mesma coisa que as RNNs, mas apenas t\u00eam uma maneira diferente de computar o estado oculto. Cobriremos as LSTMs com mais detalhes no pr\u00f3ximo cap\u00edtulo. Aqui est\u00e3o alguns exemplos de aplica\u00e7\u00f5es de RNNs em Processamento de Linguagem Natural (o que n\u00e3o \u00e9 uma lista definitiva).\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Modelagem de Linguagem e Gera\u00e7\u00e3o de Texto\n   </span>\n  </strong>\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Dada uma sequ\u00eancia de palavras, queremos prever a probabilidade de cada palavra dada \u00e0s palavras anteriores. Os Modelos de Linguagem nos permitem medir a probabilidade de uma senten\u00e7a, que \u00e9 uma entrada importante para a Tradu\u00e7\u00e3o Autom\u00e1tica (j\u00e1 que as senten\u00e7as de alta probabilidade est\u00e3o normalmente corretas). Um efeito colateral de poder prever a pr\u00f3xima palavra \u00e9 que obtemos um modelo generativo, que nos permite gerar um novo texto por amostragem a partir das probabilidades de sa\u00edda. E dependendo de quais s\u00e3o nossos dados de treinamento, podemos gerar todos os tipos de coisas. Em Modelagem de Linguagem, nossa entrada \u00e9 tipicamente uma sequ\u00eancia de palavras (codificadas como vetores \u00fanicos), e nossa sa\u00edda \u00e9 a sequ\u00eancia de palavras previstas. Ao treinar a rede, definimos ot = x{t + 1}, pois queremos que a sa\u00edda na etapa t seja a pr\u00f3xima palavra real.\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Machine Translation\n   </span>\n  </strong>\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A tradu\u00e7\u00e3o autom\u00e1tica \u00e9 semelhante \u00e0 modelagem de linguagem, pois nossa entrada \u00e9 uma sequ\u00eancia de palavras em nosso idioma de origem (por exemplo portugu\u00eas). Queremos produzir uma sequ\u00eancia de palavras em nosso idioma de destino (por exemplo, ingl\u00eas). A principal diferen\u00e7a \u00e9 que nossa sa\u00edda s\u00f3 \u00e9 iniciada depois de termos visto a entrada completa, porque a primeira palavra de nossas senten\u00e7as traduzidas pode exigir informa\u00e7\u00f5es capturadas da sequ\u00eancia de entrada completa.\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Reconhecimento de Fala\n   </span>\n  </strong>\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Dada uma sequ\u00eancia de entrada de sinais ac\u00fasticos de uma onda sonora, podemos prever uma sequ\u00eancia de segmentos fon\u00e9ticos juntamente com suas probabilidades.\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Gerar Descri\u00e7\u00f5es de Imagens\n   </span>\n  </strong>\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Juntamente com as redes neurais convolucionais, as RNNs foram usados como parte de um modelo para gerar descri\u00e7\u00f5es de imagens n\u00e3o rotuladas. \u00c9 incr\u00edvel como isso parece funcionar. O modelo combinado alinha as palavras geradas com os recursos encontrados nas imagens.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Todos esses exemplos de aplica\u00e7\u00f5es das RNNs s\u00e3o mostrados na pr\u00e1tica nos cursos\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   e\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Processamento de Linguagem Natural\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000;\" target=\"_blank\">\n    <span style=\"text-decoration: underline;\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.aclweb.org/anthology/P14-1140\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     A Recursive Recurrent Neural Network for Statistical Machine Translation\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Sequence to Sequence Learning with Neural Networks\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent Neural Networks Cheatsheet\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     On the difficulty of training recurrent neural networks\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://skymind.ai/wiki/lstm\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     A Beginner\u2019s Guide to LSTMs and Recurrent Neural Networks\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent Neural Networks Tutorial, Part 1 \u2013 Introduction to RNNs\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent Neural Networks Tutorial, Part 3 \u2013 Backpropagation Through Time and Vanishing Gradients\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Recurrent neural network based language model\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\" style=\"text-align: justify;\">\n </div>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1320\" href=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1320\" href=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1320\" href=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1320\" href=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1320-5e0dd1c76a067\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1320&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1320-5e0dd1c76a067\" id=\"like-post-wrapper-140353593-1320-5e0dd1c76a067\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "51": "<h1 class=\"entry-title\" id=\"capitulo-51\">\n Cap\u00edtulo 51 \u2013 Arquitetura de Redes Neurais Long Short Term Memory (LSTM)\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Estudamos as redes neurais recorrentes e suas limita\u00e7\u00f5es nos cap\u00edtulos anteriores. Para superar alguns dos problemas das RNNs, podemos usar algumas de suas varia\u00e7\u00f5es. Uma delas \u00e9 chamada LSTM ou Long Short Term Memory, um tipo de rede neural recorrente, que \u00e9 usada em diversos cen\u00e1rios de Processamento de Linguagem Natural. Neste cap\u00edtulo estudaremos a Arquitetura de Redes Neurais Long Short Term Memory.\n  </span>\n </p>\n <h2>\n  <span style=\"color: #000000;\">\n   Precisamos de Mem\u00f3ria\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os humanos n\u00e3o come\u00e7am a pensar do zero a cada segundo. Ao ler este cap\u00edtulo, voc\u00ea entende cada palavra com base em sua compreens\u00e3o das palavras anteriores. Voc\u00ea n\u00e3o joga tudo fora e come\u00e7a a pensar de novo a cada palavra que voc\u00ea l\u00ea. Seus pensamentos t\u00eam persist\u00eancia.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As redes neurais tradicionais n\u00e3o podem fazer isso, o que dificulta sua aplica\u00e7\u00e3o para resolver diversos problemas. Por exemplo, imagine que voc\u00ea queira classificar o tipo de evento que est\u00e1 acontecendo em todos os pontos de um filme. N\u00e3o est\u00e1 claro como uma rede neural tradicional poderia usar o aprendizado sobre eventos anteriores no filme para informar os posteriores.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Redes neurais recorrentes resolvem esse problema. S\u00e3o redes com loops, permitindo que as informa\u00e7\u00f5es persistam.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esses la\u00e7os fazem com que as redes neurais recorrentes pare\u00e7am misteriosas. No entanto, se voc\u00ea pensar um pouco mais, perceber\u00e1 que n\u00e3o s\u00e3o t\u00e3o diferentes de uma rede neural\n   <em>\n    normal\n   </em>\n   . Uma rede neural recorrente pode ser imaginada como m\u00faltiplas c\u00f3pias da mesma rede, cada uma passando uma mensagem a um sucessor. Considere o que acontece se desenrolarmos o loop:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"RNN-unrolled\" class=\"aligncenter wp-image-1351 size-large\" data-attachment-id=\"1351\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"RNN-unrolled\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?fit=1024%2C269\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?fit=300%2C79\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?fit=2706%2C711\" data-orig-size=\"2706,711\" data-permalink=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/rnn-unrolled/\" data-recalc-dims=\"1\" height=\"269\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=1024%2C269\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=1024%2C269 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=300%2C79 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=768%2C202 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=200%2C53 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?resize=690%2C181 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/RNN-unrolled.png?w=2340 2340w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa natureza de cadeia revela que redes neurais recorrentes est\u00e3o intimamente relacionadas a sequ\u00eancias e listas, uma arquitetura natural da rede neural a ser usada para esses dados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nos \u00faltimos anos, tem havido um incr\u00edvel sucesso ao aplicar as RNNs a uma variedade de problemas: reconhecimento de fala, modelagem de idiomas, tradu\u00e7\u00e3o, legendas de imagens\u2026 A lista continua. Deixarei a discuss\u00e3o sobre os incr\u00edveis feitos que podemos alcan\u00e7ar com as RNNs com o excelente post de Andrej Karpathy,\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     The Unreasonable Effectiveness of Recurrent Neural Networks\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Entretanto, boa parte do sucesso das RNNs se deve a uma de suas varia\u00e7\u00f5es, as LSTMs, um tipo muito especial de rede neural recorrente que funciona, para muitas tarefas, muito melhor do que a vers\u00e3o padr\u00e3o. Quase todos os resultados empolgantes baseados em redes neurais recorrentes s\u00e3o alcan\u00e7ados com LSTMs. Vamos ent\u00e3o compreender o que torna as LSTMs t\u00e3o especiais.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Arquitetura da LSTM\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A LSTM \u00e9 uma arquitetura de rede neural recorrente (RNN) que \u201c\n   <em>\n    lembra\u201d\n   </em>\n   valores em intervalos arbitr\u00e1rios. A LSTM \u00e9 bem adequada para classificar, processar e prever s\u00e9ries temporais com intervalos de tempo de dura\u00e7\u00e3o desconhecida. A insensibilidade relativa ao comprimento do gap d\u00e1 uma vantagem \u00e0 LSTM em rela\u00e7\u00e3o a RNNs tradicionais (tamb\u00e9m chamadas \u201cvanilla\u201d), Modelos Ocultos de Markov (MOM) e outros m\u00e9todos de aprendizado de sequ\u00eancias.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A estrutura de uma RNN \u00e9 muito semelhante ao Modelo Oculto de Markov. No entanto, a principal diferen\u00e7a \u00e9 como os par\u00e2metros s\u00e3o calculados e constru\u00eddos. Uma das vantagens da LSTM \u00e9 a insensibilidade ao comprimento do gap. RNN e MOM dependem do estado oculto antes da emiss\u00e3o / sequ\u00eancia. Se quisermos prever a sequ\u00eancia ap\u00f3s 1.000 intervalos em vez de 10, o modelo esqueceu o ponto de partida at\u00e9 ent\u00e3o. Mas um modelo LSTM \u00e9 capaz de \u201clembrar\u201d por conta de sua estrutura de c\u00e9lulas, o diferencial da arquitetura LSTM.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   A LSTM possui uma estrutura em cadeia que cont\u00e9m quatro redes neurais e diferentes blocos de mem\u00f3ria chamados c\u00e9lulas.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"lstmcell\" class=\"aligncenter size-full wp-image-1361\" data-attachment-id=\"1361\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"lstmcell\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?fit=542%2C357\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?fit=300%2C198\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?fit=542%2C357\" data-orig-size=\"542,357\" data-permalink=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/lstmcell/\" data-recalc-dims=\"1\" height=\"357\" sizes=\"(max-width: 542px) 100vw, 542px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?resize=542%2C357\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?w=542 542w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?resize=300%2C198 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/08/lstmcell.png?resize=200%2C132 200w\" width=\"542\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A informa\u00e7\u00e3o \u00e9 retida pelas c\u00e9lulas e as manipula\u00e7\u00f5es de mem\u00f3ria s\u00e3o feitas pelos port\u00f5es (gates). Existem tr\u00eas port\u00f5es:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Forget Gate\n   </strong>\n   : As informa\u00e7\u00f5es que n\u00e3o s\u00e3o mais \u00fateis no estado da c\u00e9lula s\u00e3o removidas com o forget gate. Duas entradas: x_t (entrada no momento espec\u00edfico) e h_t-1 (sa\u00edda de c\u00e9lula anterior) s\u00e3o alimentadas ao gate e multiplicadas por matrizes de peso, seguidas pela adi\u00e7\u00e3o do bias. O resultante \u00e9 passado por uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o que fornece uma sa\u00edda bin\u00e1ria. Se para um determinado estado de c\u00e9lula a sa\u00edda for 0, a informa\u00e7\u00e3o \u00e9 esquecida e para a sa\u00edda 1, a informa\u00e7\u00e3o \u00e9 retida para uso futuro.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Input Gate\n   </strong>\n   : A adi\u00e7\u00e3o de informa\u00e7\u00f5es \u00fateis ao estado da c\u00e9lula \u00e9 feita pelo input gate. Primeiro, a informa\u00e7\u00e3o \u00e9 regulada usando a fun\u00e7\u00e3o sigmoide que filtra os valores a serem lembrados de forma similar ao forget gate usando as entradas h_t-1 e x_t. Ent\u00e3o, um vetor \u00e9 criado usando a fun\u00e7\u00e3o tanh que d\u00e1 sa\u00edda de -1 a +1, que cont\u00e9m todos os valores poss\u00edveis de h_t-1 e x_t. Os valores do vetor e os valores regulados s\u00e3o multiplicados para obter as informa\u00e7\u00f5es \u00fateis\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Output Gate\n   </strong>\n   : A tarefa de extrair informa\u00e7\u00f5es \u00fateis do estado da c\u00e9lula atual para ser apresentadas como uma sa\u00edda \u00e9 feita pelo output gate. Primeiro, um vetor \u00e9 gerado aplicando a fun\u00e7\u00e3o tanh na c\u00e9lula. Ent\u00e3o, a informa\u00e7\u00e3o \u00e9 regulada usando a fun\u00e7\u00e3o sigm\u00f3ide que filtra os valores a serem lembrados usando as entradas h_t-1 e x_t. Os valores do vetor e os valores regulados s\u00e3o multiplicados para serem enviados como uma sa\u00edda e entrada para a pr\u00f3xima c\u00e9lula.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A c\u00e9lula RNN recebe duas entradas, a sa\u00edda do \u00faltimo estado oculto e a observa\u00e7\u00e3o no tempo = t. Al\u00e9m do estado oculto, n\u00e3o h\u00e1 informa\u00e7\u00f5es sobre o passado para se lembrar. A mem\u00f3ria de longo prazo \u00e9 geralmente chamada de estado da c\u00e9lula. As setas em loop indicam a natureza recursiva da c\u00e9lula. Isso permite que as informa\u00e7\u00f5es dos intervalos anteriores sejam armazenadas na c\u00e9lula LSTM. O estado da c\u00e9lula \u00e9 modificado pelo forget gate colocado abaixo do estado da c\u00e9lula e tamb\u00e9m ajustado pela porta de modula\u00e7\u00e3o de entrada. Da equa\u00e7\u00e3o, o estado da c\u00e9lula anterior esquece, multiplica-se com a porta do esquecimento e adiciona novas informa\u00e7\u00f5es atrav\u00e9s da sa\u00edda das portas de entrada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Algumas das famosas aplica\u00e7\u00f5es das LSTMs incluem:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Modelagem de Linguagem\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Tradu\u00e7\u00e3o de Idiomas\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Legendas em Imagens\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Gera\u00e7\u00e3o de Texto\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Chatbots\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Todas as aplica\u00e7\u00f5es acima mencionadas s\u00e3o mostrados na pr\u00e1tica nos cursos\n  </span>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Deep Learning II\n   </a>\n  </span>\n  e\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz\" rel=\"noopener noreferrer\" target=\"_blank\">\n    Processamento de Linguagem Natural\n   </a>\n  </span>\n  .\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Continuamos no pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.aclweb.org/anthology/P14-1140\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    A Recursive Recurrent Neural Network for Statistical Machine Translation\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Sequence to Sequence Learning with Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Cheatsheet\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    On the difficulty of training recurrent neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://skymind.ai/wiki/lstm\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    A Beginner\u2019s Guide to LSTMs and Recurrent Neural Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Long Short-Term Memory (LSTM): Concept\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Tutorial, Part 1 \u2013 Introduction to RNNs\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Tutorial, Part 3 \u2013 Backpropagation Through Time and Vanishing Gradients\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent neural network based language model\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1350\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1350\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1350\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1350\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1350-5e0dd1ca8d736\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1350&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1350-5e0dd1ca8d736\" id=\"like-post-wrapper-140353593-1350-5e0dd1ca8d736\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "52": "<h1 class=\"entry-title\" id=\"capitulo-52\">\n Cap\u00edtulo 52 \u2013 Arquitetura de Redes Neurais Gated Recurrent Unit (GRU)\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste cap\u00edtulo estudaremos um tipo realmente fascinante de rede neural. Introduzido por\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1406.1078v3.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Cho, et al.\n    </a>\n   </span>\n   em 2014, a GRU (Gated Recurrent Unit) visa resolver o problema da dissipa\u00e7\u00e3o do gradiente que \u00e9 comum em uma rede neural recorrente padr\u00e3o. A GRU tamb\u00e9m pode ser considerada uma varia\u00e7\u00e3o da\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     LSTM\n    </a>\n   </span>\n   porque ambas s\u00e3o projetadas de maneira semelhante e, em alguns casos, produzem resultados igualmente excelentes. Para acompanhar este cap\u00edtulo voc\u00ea precisa ter conclu\u00eddo os cap\u00edtulos anteriores. A GRU \u00e9 estudada na pr\u00e1tica\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Problema, Mem\u00f3ria de Curto Prazo\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Redes neurais recorrentes sofrem de mem\u00f3ria de curto prazo. Se uma sequ\u00eancia for longa o suficiente, elas ter\u00e3o dificuldade em transportar informa\u00e7\u00f5es das etapas anteriores para as posteriores. Portanto, se voc\u00ea estiver tentando processar um par\u00e1grafo de texto para fazer previs\u00f5es, as RNNs poder\u00e3o deixar de fora informa\u00e7\u00f5es importantes desde o in\u00edcio.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Durante a etapa de backpropagation, as redes neurais recorrentes sofrem com o problema da dissipa\u00e7\u00e3o do gradiente. Gradientes s\u00e3o valores usados para atualizar os pesos das redes neurais. O problema da dissipa\u00e7\u00e3o do gradiente \u00e9 quando o gradiente diminui \u00e0 medida que se propaga novamente ao longo do tempo. Se um valor de gradiente se torna extremamente pequeno, n\u00e3o contribui muito com o aprendizado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim, nas redes neurais recorrentes, as camadas que recebem uma pequena atualiza\u00e7\u00e3o gradiente param de aprender. Portanto, como essas camadas n\u00e3o aprendem, as RNNs podem esquecer o que foi visto em sequ\u00eancias mais longas, tendo assim uma mem\u00f3ria de curto prazo.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   LSTM e GRU Como Solu\u00e7\u00e3o\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   LSTM e GRU foram criadas como a solu\u00e7\u00e3o para a mem\u00f3ria de curto prazo. Elas t\u00eam mecanismos internos chamados port\u00f5es que podem regular o fluxo de informa\u00e7\u00f5es.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rnns\" class=\"aligncenter wp-image-1386 size-large\" data-attachment-id=\"1386\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rnns\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?fit=1024%2C651\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?fit=300%2C191\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?fit=1516%2C964\" data-orig-size=\"1516,964\" data-permalink=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/rnns/\" data-recalc-dims=\"1\" height=\"651\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=1024%2C651\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=1024%2C651 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=300%2C191 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=768%2C488 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=200%2C127 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?resize=690%2C439 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/rnns.png?w=1516 1516w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esses port\u00f5es podem aprender quais dados em uma sequ\u00eancia s\u00e3o importantes para manter ou jogar fora. Ao fazer isso, eles podem transmitir informa\u00e7\u00f5es relevantes ao longo de uma longa cadeia de sequ\u00eancias para fazer previs\u00f5es. Quase todos os resultados de \u00faltima gera\u00e7\u00e3o baseados em redes neurais recorrentes s\u00e3o alcan\u00e7ados com essas duas redes. LSTM e GRU podem ser usadas em reconhecimento de voz, s\u00edntese de fala e gera\u00e7\u00e3o de texto. Voc\u00ea pode at\u00e9 us\u00e1-las para gerar legendas em v\u00eddeos. Essas s\u00e3o aplica\u00e7\u00f5es de ponta em\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Intelig\u00eancia Artificial.\n    </a>\n   </span>\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como as GRUs Funcionam?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A GRU \u00e9 a nova gera\u00e7\u00e3o de redes neurais recorrentes e \u00e9 bastante semelhante a uma LSTM. As GRUs se livraram do estado da c\u00e9lula e usaram o estado oculto para transferir informa\u00e7\u00f5es. Essa arquitetura possui apenas dois port\u00f5es, um port\u00e3o de redefini\u00e7\u00e3o (reset gate) e um port\u00e3o de atualiza\u00e7\u00e3o (update date). As GRUs s\u00e3o uma vers\u00e3o melhorada da rede neural recorrente padr\u00e3o. Mas o que as torna t\u00e3o especiais e eficazes?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"gru4\" class=\"aligncenter wp-image-1387 size-large\" data-attachment-id=\"1387\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gru4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?fit=1024%2C835\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?fit=300%2C245\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?fit=1042%2C850\" data-orig-size=\"1042,850\" data-permalink=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gru4/\" data-recalc-dims=\"1\" height=\"835\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=1024%2C835\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=1024%2C835 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=300%2C245 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=768%2C626 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=200%2C163 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?resize=690%2C563 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru4.png?w=1042 1042w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para resolver o problema da dissipa\u00e7\u00e3o do gradiente de uma RNN padr\u00e3o, a GRU usa dois port\u00f5es, reset e update gate. Basicamente, eles s\u00e3o dois vetores que decidem quais informa\u00e7\u00f5es devem ser passadas para a sa\u00edda. O que h\u00e1 de especial neles \u00e9 que eles podem ser treinados para manter informa\u00e7\u00f5es de muito tempo atr\u00e1s, sem dissip\u00e1-las com o tempo ou remover informa\u00e7\u00f5es irrelevantes para a previs\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A estrutura da GRU permite capturar adaptativamente depend\u00eancias de grandes sequ\u00eancias de dados sem descartar informa\u00e7\u00f5es de partes anteriores da sequ\u00eancia. Isso \u00e9 alcan\u00e7ado atrav\u00e9s de suas unidades de port\u00f5es, semelhantes \u00e0s das LSTMs. Esses port\u00f5es s\u00e3o respons\u00e1veis por regular as informa\u00e7\u00f5es a serem mantidas ou descartadas a cada etapa do tempo.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    <img alt=\"gru5\" class=\"aligncenter size-full wp-image-1393\" data-attachment-id=\"1393\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gru5\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?fit=912%2C477\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?fit=300%2C157\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?fit=912%2C477\" data-orig-size=\"912,477\" data-permalink=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gru5/\" data-recalc-dims=\"1\" height=\"477\" sizes=\"(max-width: 912px) 100vw, 912px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=912%2C477\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?w=912 912w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=300%2C157 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=768%2C402 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=200%2C105 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru5.jpg?resize=690%2C361 690w\" width=\"912\"/>\n   </span>\n  </p>\n  <p>\n  </p>\n  <p style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    A capacidade da GRU de manter depend\u00eancias ou mem\u00f3ria de longo prazo decorre dos c\u00e1lculos na c\u00e9lula da GRU para produzir o estado oculto. Enquanto as LSTMs t\u00eam dois estados diferentes passados entre as c\u00e9lulas \u2013 o estado da c\u00e9lula e o estado oculto, que carregam a mem\u00f3ria de longo e curto prazo, respectivamente \u2013 as GRUs t\u00eam apenas um estado oculto transferido entre as etapas do tempo. Esse estado oculto \u00e9 capaz de manter as depend\u00eancias de longo e curto prazo ao mesmo tempo, devido aos mecanismos de restri\u00e7\u00e3o e c\u00e1lculos pelos quais o estado oculto e os dados de entrada passam.\n   </span>\n  </p>\n  <p>\n  </p>\n  <p style=\"text-align: justify;\">\n   <p style=\"text-align: justify;\">\n    <span style=\"color: #000000;\">\n     <img alt=\"gruxlstm\" class=\"aligncenter size-full wp-image-1394\" data-attachment-id=\"1394\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gruxlstm\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1024%2C411\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=300%2C120\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1031%2C414\" data-orig-size=\"1031,414\" data-permalink=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gruxlstm/\" data-recalc-dims=\"1\" height=\"414\" sizes=\"(max-width: 1031px) 100vw, 1031px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1031%2C414\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?w=1031 1031w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=300%2C120 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=768%2C308 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1024%2C411 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=200%2C80 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=690%2C277 690w\" width=\"1031\"/>\n    </span>\n   </p>\n   <p>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"color: #000000;\">\n     Assim como os port\u00f5es das LSTMs, os port\u00f5es na GRU s\u00e3o treinados para filtrar seletivamente qualquer informa\u00e7\u00e3o irrelevante, mantendo o que \u00e9 \u00fatil. Esses port\u00f5es s\u00e3o essencialmente vetores contendo valores entre 0 e 1 que ser\u00e3o multiplicados com os dados de entrada e / ou estado oculto. Um valor 0 nos vetores indica que os dados correspondentes no estado de entrada ou oculto n\u00e3o s\u00e3o importantes e, portanto, retornar\u00e3o como zero. Por outro lado, um valor 1 no vetor significa que os dados correspondentes s\u00e3o importantes e ser\u00e3o usados.\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"color: #000000;\">\n     No pr\u00f3ximo cap\u00edtulo veremos os detalhes matem\u00e1ticos por tr\u00e1s da GRU, uma das arquiteturas mais interessantes de Deep Learning e que tem obtido resultados formid\u00e1veis especialmente em Processamento de Linguagem Natural.\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"color: #000000;\">\n     Aqui voc\u00ea encontra uma anima\u00e7\u00e3o que ajuda a compreender o funcionamento das arquiteturas de Deep Learning do tipo recorrente:\n     <span style=\"text-decoration: underline;\">\n      <a href=\"https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45\" rel=\"noopener noreferrer\" target=\"_blank\">\n       Animated RNN, LSTM and GRU.\n      </a>\n     </span>\n    </span>\n   </p>\n   <p>\n    <span style=\"color: #000000;\">\n     At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n    </span>\n   </p>\n   <p>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"color: #000000;\">\n     Refer\u00eancias:\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Forma\u00e7\u00e3o Cientista de Dados\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n      </a>\n     </span>\n    </span>\n   </p>\n   <p>\n    <span style=\"text-decoration: underline;\">\n     <a href=\"https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\" rel=\"noopener noreferrer\" target=\"_blank\">\n      <span style=\"color: #000000; text-decoration: underline;\">\n       Understanding GRU Networks\n      </span>\n     </a>\n    </span>\n   </p>\n   <p>\n    <span style=\"text-decoration: underline;\">\n     <a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\" rel=\"noopener noreferrer\" target=\"_blank\">\n      <span style=\"color: #000000; text-decoration: underline;\">\n       Illustrated Guide to LSTM\u2019s and GRU\u2019s: A step by step explanation\n      </span>\n     </a>\n    </span>\n   </p>\n   <p>\n    <span style=\"text-decoration: underline;\">\n     <a href=\"https://blog.floydhub.com/gru-with-pytorch/\" rel=\"noopener noreferrer\" target=\"_blank\">\n      <span style=\"color: #000000; text-decoration: underline;\">\n       Gated Recurrent Unit (GRU)\n      </span>\n     </a>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://www.aclweb.org/anthology/P14-1140\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       A Recursive Recurrent Neural Network for Statistical Machine Translation\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Sequence to Sequence Learning with Neural Networks\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Recurrent Neural Networks Cheatsheet\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       On the difficulty of training recurrent neural networks\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://skymind.ai/wiki/lstm\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       A Beginner\u2019s Guide to LSTMs and Recurrent Neural Networks\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Long Short-Term Memory (LSTM): Concept\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Recurrent Neural Networks Tutorial, Part 1 \u2013 Introduction to RNNs\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Recurrent Neural Networks Tutorial, Part 3 \u2013 Backpropagation Through Time and Vanishing Gradients\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Practical Recommendations for Gradient-Based Training of Deep Architectures\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Gradient-Based Learning Applied to Document Recognition\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Neural Networks &amp; The Backpropagation Algorithm, Explained\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Neural Networks and Deep Learning\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Recurrent neural network based language model\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Gradient Descent For Machine Learning\n      </a>\n     </span>\n    </span>\n   </p>\n   <p style=\"text-align: justify;\">\n    <span style=\"text-decoration: underline;\">\n     <span style=\"color: #000000; text-decoration: underline;\">\n      <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n       Pattern Recognition and Machine Learning\n      </a>\n     </span>\n    </span>\n   </p>\n   <div class=\"sharedaddy sd-sharing-enabled\">\n    <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n     <h3 class=\"sd-title\">\n      Compartilhe isso:\n     </h3>\n     <div class=\"sd-content\">\n      <ul>\n       <li class=\"share-twitter\">\n        <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1381\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n         <span>\n         </span>\n         <span class=\"sharing-screen-reader-text\">\n          Clique para compartilhar no Twitter(abre em nova janela)\n         </span>\n        </a>\n       </li>\n       <li class=\"share-facebook\">\n        <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1381\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n         <span>\n         </span>\n         <span class=\"sharing-screen-reader-text\">\n          Clique para compartilhar no Facebook(abre em nova janela)\n         </span>\n        </a>\n       </li>\n       <li class=\"share-linkedin\">\n        <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1381\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n         <span>\n         </span>\n         <span class=\"sharing-screen-reader-text\">\n          Clique para compartilhar no LinkedIn(abre em nova janela)\n         </span>\n        </a>\n       </li>\n       <li class=\"share-pinterest\">\n        <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1381\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n         <span>\n         </span>\n         <span class=\"sharing-screen-reader-text\">\n          Clique para compartilhar no Pinterest(abre em nova janela)\n         </span>\n        </a>\n       </li>\n       <li class=\"share-tumblr\">\n        <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n         <span>\n         </span>\n         <span class=\"sharing-screen-reader-text\">\n          Clique para compartilhar no Tumblr(abre em nova janela)\n         </span>\n        </a>\n       </li>\n       <li class=\"share-jetpack-whatsapp\">\n        <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n         <span>\n         </span>\n         <span class=\"sharing-screen-reader-text\">\n          Clique para compartilhar no WhatsApp(abre em nova janela)\n         </span>\n        </a>\n       </li>\n       <li class=\"share-end\">\n       </li>\n      </ul>\n     </div>\n    </div>\n   </div>\n   <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1381-5e0dd1ccafe28\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1381&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1381-5e0dd1ccafe28\" id=\"like-post-wrapper-140353593-1381-5e0dd1ccafe28\">\n    <h3 class=\"sd-title\">\n     Curtir isso:\n    </h3>\n    <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n     <span class=\"button\">\n      <span>\n       Curtir\n      </span>\n     </span>\n     <span class=\"loading\">\n      Carregando...\n     </span>\n    </div>\n    <span class=\"sd-text-color\">\n    </span>\n    <a class=\"sd-link-color\">\n    </a>\n   </div>\n   <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n    <h3 class=\"jp-relatedposts-headline\">\n     <em>\n      Relacionado\n     </em>\n    </h3>\n   </div>\n  </p>\n </p>\n</div>\n", "53": "<h1 class=\"entry-title\" id=\"capitulo-53\">\n Cap\u00edtulo 53 \u2013 Matem\u00e1tica na GRU, Dissipa\u00e7\u00e3o e Clipping do Gradiente\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A capacidade da rede GRU de manter depend\u00eancias ou mem\u00f3ria de longo prazo decorre dos c\u00e1lculos na c\u00e9lula na GRU para produzir o estado oculto. As LSTMs t\u00eam dois estados diferentes passados entre as c\u00e9lulas \u2013 o estado da c\u00e9lula e o estado oculto, que carregam a mem\u00f3ria de longo e curto prazo, respectivamente \u2013 as GRUs t\u00eam apenas um estado oculto transferido entre as etapas do tempo. Esse estado oculto \u00e9 capaz de manter as depend\u00eancias de longo e curto prazo ao mesmo tempo, devido aos mecanismos de restri\u00e7\u00e3o (port\u00f5es) e c\u00e1lculos pelos quais o estado oculto e os dados de entrada passam.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"gruxlstm\" class=\"aligncenter size-full wp-image-1394\" data-attachment-id=\"1394\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gruxlstm\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1024%2C411\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=300%2C120\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?fit=1031%2C414\" data-orig-size=\"1031,414\" data-permalink=\"http://deeplearningbook.com.br/arquitetura-de-redes-neurais-gated-recurrent-unit-gru/gruxlstm/\" data-recalc-dims=\"1\" height=\"414\" sizes=\"(max-width: 1031px) 100vw, 1031px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1031%2C414\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?w=1031 1031w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=300%2C120 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=768%2C308 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=1024%2C411 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=200%2C80 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gruxlstm.jpg?resize=690%2C277 690w\" width=\"1031\"/>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A c\u00e9lula GRU cont\u00e9m apenas dois port\u00f5es: o port\u00e3o de atualiza\u00e7\u00e3o e o port\u00e3o de redefini\u00e7\u00e3o. Assim como os port\u00f5es das LSTMs, os port\u00f5es na GRU s\u00e3o treinados para filtrar seletivamente qualquer informa\u00e7\u00e3o irrelevante, mantendo o que \u00e9 \u00fatil. Esses port\u00f5es s\u00e3o essencialmente vetores contendo valores entre 0 e 1 que ser\u00e3o multiplicados com os dados de entrada e/ou estado oculto. Um valor 0 nos vetores indica que os dados correspondentes no estado de entrada ou oculto n\u00e3o s\u00e3o importantes e, portanto, retornar\u00e3o como zero. Por outro lado, um valor 1 no vetor significa que os dados correspondentes s\u00e3o importantes e ser\u00e3o usados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Usaremos os termos gate e vetor de forma intercambi\u00e1vel para o restante deste cap\u00edtulo, pois eles se referem \u00e0 mesma coisa.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A estrutura de uma unidade GRU \u00e9 mostrada abaixo.\n  </span>\n </p>\n <p>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"gru1\" class=\"aligncenter size-full wp-image-1408\" data-attachment-id=\"1408\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gru1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?fit=912%2C477\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?fit=300%2C157\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?fit=912%2C477\" data-orig-size=\"912,477\" data-permalink=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/gru1/\" data-recalc-dims=\"1\" height=\"477\" sizes=\"(max-width: 912px) 100vw, 912px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=912%2C477\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?w=912 912w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=300%2C157 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=768%2C402 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=200%2C105 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gru1.jpg?resize=690%2C361 690w\" width=\"912\"/>\n  </span>\n </p>\n <p>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E\n  </span>\n  <span style=\"color: #000000;\">\n   mbora a estrutura possa parecer bastante complicada devido ao grande n\u00famero de conex\u00f5es, o mecanismo por tr\u00e1s dela pode ser dividido em tr\u00eas etapas principais.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Reset Gate\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na primeira etapa, criamos o port\u00e3o de redefini\u00e7\u00e3o (reset gate). Essa porta \u00e9 derivada e calculada usando o estado oculto da etapa anterior e os dados de entrada na etapa atual.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Matematicamente, isso \u00e9 conseguido multiplicando o estado oculto anterior e a entrada atual com seus respectivos pesos e somando-os antes de passar a soma atrav\u00e9s de uma fun\u00e7\u00e3o sigm\u00f3ide. A fun\u00e7\u00e3o sigmoide transformar\u00e1 os valores entre 0 e 1, permitindo que o port\u00e3o filtre entre as informa\u00e7\u00f5es menos importantes e mais importantes nas etapas subsequentes. A f\u00f3rmula matem\u00e1tica \u00e9 representada abaixo:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter size-full wp-image-1409\" data-attachment-id=\"1409\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?fit=978%2C126\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?fit=300%2C39\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?fit=978%2C126\" data-orig-size=\"978,126\" data-permalink=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form1-8/\" data-recalc-dims=\"1\" height=\"126\" sizes=\"(max-width: 978px) 100vw, 978px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=978%2C126\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?w=978 978w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=300%2C39 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=768%2C99 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=200%2C26 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form1.png?resize=690%2C89 690w\" width=\"978\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Quando toda a rede \u00e9 treinada atrav\u00e9s de backpropagation, os pesos na equa\u00e7\u00e3o ser\u00e3o atualizados de forma que o vetor aprenda a reter apenas os recursos \u00fateis.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O estado oculto anterior ser\u00e1 primeiro multiplicado por um peso trein\u00e1vel e passar\u00e1 por uma multiplica\u00e7\u00e3o por elementos (produto Hadamard) com o vetor de redefini\u00e7\u00e3o. Esta opera\u00e7\u00e3o decidir\u00e1 quais informa\u00e7\u00f5es ser\u00e3o mantidas nas etapas anteriores, juntamente com as novas entradas. Ao mesmo tempo, a entrada atual tamb\u00e9m ser\u00e1 multiplicada por um peso trein\u00e1vel antes de ser somada com o produto do vetor de redefini\u00e7\u00e3o e do estado oculto anterior acima. Por fim, uma fun\u00e7\u00e3o tanh de ativa\u00e7\u00e3o n\u00e3o linear ser\u00e1 aplicada ao resultado final para obter r na equa\u00e7\u00e3o abaixo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-1410\" data-attachment-id=\"1410\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?fit=962%2C146\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?fit=300%2C46\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?fit=962%2C146\" data-orig-size=\"962,146\" data-permalink=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form2-14/\" data-recalc-dims=\"1\" height=\"146\" sizes=\"(max-width: 962px) 100vw, 962px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=962%2C146\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?w=962 962w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=300%2C46 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=768%2C117 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=200%2C30 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form2.png?resize=690%2C105 690w\" width=\"962\"/>\n  </span>\n </p>\n <h2>\n </h2>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Update Gate\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em seguida, temos o port\u00e3o de atualiza\u00e7\u00e3o (update gate). Assim como o reset gate, o update gate \u00e9 calculado usando o estado oculto anterior e os dados de entrada atuais.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os vetores Update e Reset gate s\u00e3o criados usando a mesma f\u00f3rmula, mas os pesos multiplicados pela entrada e pelo estado oculto s\u00e3o exclusivos para cada port\u00e3o, o que significa que os vetores finais para cada port\u00e3o s\u00e3o diferentes. Isso permite que os port\u00f5es sirvam a seus prop\u00f3sitos espec\u00edficos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form3\" class=\"aligncenter size-full wp-image-1411\" data-attachment-id=\"1411\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?fit=1024%2C144\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?fit=300%2C42\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?fit=1052%2C148\" data-orig-size=\"1052,148\" data-permalink=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form3-12/\" data-recalc-dims=\"1\" height=\"148\" sizes=\"(max-width: 1052px) 100vw, 1052px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=1052%2C148\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?w=1052 1052w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=300%2C42 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=768%2C108 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=1024%2C144 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=200%2C28 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form3.png?resize=690%2C97 690w\" width=\"1052\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O vetor Update ser\u00e1 submetido a multiplica\u00e7\u00e3o por elementos com o estado oculto anterior para obter u em nossa equa\u00e7\u00e3o abaixo, que ser\u00e1 usada para calcular nossa sa\u00edda final posteriormente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"\" class=\"aligncenter size-full wp-image-1412\" data-attachment-id=\"1412\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?fit=500%2C166\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?fit=300%2C100\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?fit=500%2C166\" data-orig-size=\"500,166\" data-permalink=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form4-8/\" data-recalc-dims=\"1\" height=\"166\" sizes=\"(max-width: 500px) 100vw, 500px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?resize=500%2C166\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?w=500 500w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?resize=300%2C100 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form4.png?resize=200%2C66 200w\" width=\"500\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O vetor Update tamb\u00e9m ser\u00e1 usado em outra opera\u00e7\u00e3o posteriormente ao obter nossa sa\u00edda final. O objetivo do update gate aqui \u00e9 ajudar o modelo a determinar quanto das informa\u00e7\u00f5es passadas armazenadas no estado oculto anterior precisam ser retidas para o futuro.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Combinando as Sa\u00eddas\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na \u00faltima etapa, reutilizaremos o portal Update e obteremos o estado oculto atualizado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Desta vez, pegaremos a vers\u00e3o inversa em elementos do mesmo vetor Update (1 \u2013 Update gate) e faremos uma multiplica\u00e7\u00e3o em elementos com a nossa sa\u00edda do reset gate, r. O objetivo desta opera\u00e7\u00e3o \u00e9 o gate Update determinar qual parte das novas informa\u00e7\u00f5es deve ser armazenada no estado oculto.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por fim, o resultado das opera\u00e7\u00f5es acima ser\u00e1 resumido com a nossa sa\u00edda do port\u00e3o Update na etapa anterior, u. Isso nos dar\u00e1 nosso novo e atualizado estado oculto.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form5\" class=\"aligncenter size-full wp-image-1413\" data-attachment-id=\"1413\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?fit=652%2C114\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?fit=300%2C52\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?fit=652%2C114\" data-orig-size=\"652,114\" data-permalink=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/form5-6/\" data-recalc-dims=\"1\" height=\"114\" sizes=\"(max-width: 652px) 100vw, 652px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?resize=652%2C114\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?w=652 652w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?resize=300%2C52 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/form5.png?resize=200%2C35 200w\" width=\"652\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos usar esse novo estado oculto como nossa sa\u00edda para esse intervalo de tempo, passando-o por uma camada de ativa\u00e7\u00e3o linear.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Solu\u00e7\u00e3o do Problema de Dissipa\u00e7\u00e3o/Explos\u00e3o do Gradiente\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vimos os port\u00f5es em a\u00e7\u00e3o. Sabemos como eles transformam nossos dados. Agora, vamos revisar seu papel geral no gerenciamento da mem\u00f3ria da rede e falar sobre como eles resolvem o problema de dissipa\u00e7\u00e3o/explos\u00e3o do gradiente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como vimos nos mecanismos acima, o Reset Gate \u00e9 respons\u00e1vel por decidir quais partes do estado oculto anterior devem ser combinadas com a entrada atual para propor um novo estado oculto.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E o Update Gate \u00e9 respons\u00e1vel por determinar quanto do estado oculto anterior deve ser retido e qual parte do novo estado oculto proposto (derivado do Reset Gate) deve ser adicionado ao estado oculto final. Quando o Update Gate \u00e9 multiplicado pela primeira vez com o estado oculto anterior, a rede escolhe quais partes do estado oculto anterior ele manter\u00e1 em sua mem\u00f3ria enquanto descarta o restante. Posteriormente, ele corrige as partes ausentes das informa\u00e7\u00f5es quando usa o inverso do gate Update para filtrar o novo estado oculto proposto a partir do Reset Gate.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso permite que a rede retenha depend\u00eancias de longo prazo. O Update Gate pode optar por manter a maioria das mem\u00f3rias anteriores no estado oculto se os valores do vetor Update estiverem pr\u00f3ximos de 1 sem recalcular ou alterar todo o estado oculto.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O problema de dissipa\u00e7\u00e3o/explos\u00e3o do gradiente ocorre durante a propaga\u00e7\u00e3o de retorno (backpropagation) ao treinar a RNN, especialmente se a RNN estiver processando longas sequ\u00eancias ou tiver v\u00e1rias camadas. O erro do gradiente calculado durante o treinamento \u00e9 usado para atualizar o peso da rede na dire\u00e7\u00e3o certa e na magnitude certa. No entanto, esse gradiente \u00e9 calculado com a regra da cadeia (\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/a-matematica-do-problema-de-dissipacao-do-gradiente-em-deep-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     chain rule\n    </a>\n   </span>\n   ), come\u00e7ando no final da rede. Portanto, durante o backpropagation, os gradientes sofrer\u00e3o continuamente multiplica\u00e7\u00f5es de matrizes e encolher\u00e3o ou explodir\u00e3o exponencialmente por sequ\u00eancias longas. Ter um gradiente muito pequeno significa que o modelo n\u00e3o atualiza seus pesos de maneira eficaz, enquanto gradientes extremamente grandes fazem com que o modelo seja inst\u00e1vel.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os port\u00f5es nas LSTM e GRUs ajudam a resolver esse problema devido ao componente aditivo dos port\u00f5es de atualiza\u00e7\u00e3o. Enquanto as RNNs tradicionais sempre substituem todo o conte\u00fado do estado oculto a cada etapa, as LSTMs e GRUs mant\u00eam a maior parte do estado oculto existente enquanto adicionam novo conte\u00fado sobre ele. Isso permite que os erros dos gradientes sejam propagados de volta sem desaparecer ou explodir muito rapidamente devido \u00e0s opera\u00e7\u00f5es de adi\u00e7\u00e3o.\n  </span>\n </p>\n <h2>\n  <span style=\"color: #000000;\">\n   Clipping (Recorte) do Gradiente\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Embora LSTMs e GRUs sejam as corre\u00e7\u00f5es mais usadas para o problema acima, outra solu\u00e7\u00e3o para o problema de explos\u00e3o de gradientes \u00e9 o clipping do gradiente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O clipping do gradiente \u00e9 mais comum em redes neurais recorrentes. Quando os gradientes est\u00e3o sendo propagados no tempo, eles podem desaparecer porque s\u00e3o continuamente multiplicados por n\u00fameros menores que um. Isso \u00e9 chamado de problema de dissipa\u00e7\u00e3o do gradiente, podendo ser resolvido por LSTMs e GRUs e, se voc\u00ea estiver usando uma rede profunda feed-forward, isso \u00e9 resolvido por conex\u00f5es residuais. Por outro lado, voc\u00ea tamb\u00e9m pode ter\u00a0 explos\u00e3o dos gradientes. \u00c9 quando eles se tornam exponencialmente grandes por serem multiplicados por n\u00fameros maiores que 1. O clipping do gradiente\n   <em>\n    cortar\u00e1\n   </em>\n   os gradientes entre dois n\u00fameros para impedir que eles fiquem muito grandes.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O clipping define um valor limite definido nos gradientes, o que significa que, mesmo se um gradiente aumentar al\u00e9m do valor predefinido durante o treinamento, seu valor ainda ser\u00e1 limitado ao limite definido. Dessa forma, a dire\u00e7\u00e3o do gradiente permanece inalterada e apenas a magnitude do gradiente \u00e9 alterada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O clipping do gradiente e os demais temas estudados neste cap\u00edtulo s\u00e3o abordados em detalhes e na pr\u00e1tica no curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   e ent\u00e3o aplicados em problemas do mundo real no curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Processamento de Linguagem Natural\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Understanding GRU Networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Illustrated Guide to LSTM\u2019s and GRU\u2019s: A step by step explanation\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://blog.floydhub.com/gru-with-pytorch/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gated Recurrent Unit (GRU)\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.aclweb.org/anthology/P14-1140\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    A Recursive Recurrent Neural Network for Statistical Machine Translation\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Sequence to Sequence Learning with Neural Networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Cheatsheet\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    On the difficulty of training recurrent neural networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://skymind.ai/wiki/lstm\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    A Beginner\u2019s Guide to LSTMs and Recurrent Neural Networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Long Short-Term Memory (LSTM): Concept\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Tutorial, Part 1 \u2013 Introduction to RNNs\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent Neural Networks Tutorial, Part 3 \u2013 Backpropagation Through Time and Vanishing Gradients\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent neural network based language model\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1407\" href=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1407\" href=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1407\" href=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1407\" href=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/matematica-na-gru-dissipacao-e-clipping-do-gradiente/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1407-5e0dd1cfeeef0\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1407&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1407-5e0dd1cfeeef0\" id=\"like-post-wrapper-140353593-1407-5e0dd1cfeeef0\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "54": "<h1 class=\"entry-title\" id=\"capitulo-54\">\n Cap\u00edtulo 54 \u2013 Introdu\u00e7\u00e3o \u00e0s Redes Advers\u00e1rias Generativas (GANs \u2013 Generative Adversarial Networks)\n</h1>\n<div class=\"entry-content\">\n <blockquote>\n  <p style=\"text-align: center;\">\n   <span style=\"color: #000000;\">\n    <strong>\n     Voc\u00ea pode n\u00e3o achar que os programadores s\u00e3o artistas, mas a programa\u00e7\u00e3o \u00e9 uma profiss\u00e3o extremamente criativa. \u00c9 criatividade baseada em l\u00f3gica. \u2013 John Romero\n    </strong>\n   </span>\n  </p>\n </blockquote>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Redes Advers\u00e1rias Generativas (GANs) s\u00e3o arquiteturas de redes neurais profundas compostas por duas redes colocadas uma contra a outra (da\u00ed o nome \u201cadvers\u00e1rias\u201d). Esta \u00e9 uma das arquiteturas mais recentes e mais fascinantes em Deep Learning e que estudaremos a partir de agora!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As GANs foram introduzidos em um\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://arxiv.org/abs/1406.2661\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     artigo\n    </a>\n   </span>\n   de Ian Goodfellow e outros pesquisadores da Universidade de Montreal, incluindo Yoshua Bengio, em 2014. Referindo-se \u00e0s GANs, o diretor de pesquisa de IA do Facebook, Yann LeCun, chamou o treinamento advers\u00e1rio de \u201ca ideia mais interessante nos \u00faltimos 10 anos em Machine Learning\u201d.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O potencial das GANs \u00e9 enorme porque elas podem aprender a imitar qualquer distribui\u00e7\u00e3o de dados. Ou seja, as GANs podem ser ensinadas a criar mundos estranhamente semelhantes aos nossos em qualquer dom\u00ednio: imagens, m\u00fasica, fala, prosa. Elas s\u00e3o artistas rob\u00f3ticos, em certo sentido, e sua produ\u00e7\u00e3o \u00e9 impressionante \u2013 at\u00e9 comovente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em uma virada surreal, a\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://twitter.com/ChristiesInc\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Christie\u2019s\n    </a>\n   </span>\n   (famosa rede inglesa de leil\u00f5es) vendeu um retrato de US $ 432.000 gerado por uma GAN, com base no c\u00f3digo-fonte aberto escrito por Robbie Barrat, de Stanford. Como a maioria dos artistas de verdade, ele n\u00e3o viu nada do dinheiro, que foi para a empresa francesa Obvious.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"GANs\" class=\"aligncenter size-full wp-image-1428\" data-attachment-id=\"1428\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"GANs\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?fit=1024%2C502\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?fit=300%2C147\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?fit=2358%2C1156\" data-orig-size=\"2358,1156\" data-permalink=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gans-2/\" data-recalc-dims=\"1\" height=\"574\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=1170%2C574\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?w=2358 2358w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=300%2C147 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=768%2C377 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=1024%2C502 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=200%2C98 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gans.png?resize=690%2C338 690w\" width=\"1170\"/>\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Dois dos nus gerados por GANs. Imagens de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.theverge.com/2018/10/23/18013190/ai-art-portrait-auction-christies-belamy-obvious-robbie-barrat-gans\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Robbie Barrat\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em 2019, o DeepMind mostrou que os Autoencoders Variacionais (VAEs) poderiam superar as GANs na gera\u00e7\u00e3o de faces. Estudaremos os Autoencoders mais a frente aqui no livro. GANs e VAEs s\u00e3o estudadas na pr\u00e1tica no curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep Learning II.\n    </a>\n   </span>\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Algoritmos Generativos vs. Discriminativos\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Antes de entrar nos detalhes, vamos dar uma r\u00e1pida vis\u00e3o geral do que s\u00e3o feitas as GANs. As Redes Advers\u00e1rias Generativas pertencem ao conjunto de modelos generativos. Isso significa que eles s\u00e3o capazes de produzir / gerar (veremos como) novo conte\u00fado. Naturalmente, essa capacidade de gerar novo conte\u00fado faz com que as GANs pare\u00e7am um pouco \u201cm\u00e1gicas\u201d, pelo menos \u00e0 primeira vista. Nos cap\u00edtulos seguintes superaremos a aparente m\u00e1gica das GANs para mergulhar na matem\u00e1tica e modelagem por tr\u00e1s desses modelos. N\u00e3o apenas discutiremos as no\u00e7\u00f5es fundamentais de que as Redes Advers\u00e1rias Generativas se baseiam, mas estudaremos os conceitos principais dessa arquitetura passo a passo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para entender as GANs, voc\u00ea deve saber como os algoritmos generativos funcionam e, para isso, contrast\u00e1-los com algoritmos discriminativos \u00e9 instrutivo. Algoritmos discriminativos tentam classificar os dados de entrada; isto \u00e9, dados os recursos de uma inst\u00e2ncia de dados, eles prev\u00eaem um r\u00f3tulo ou categoria \u00e0 qual esses dados pertencem. Praticamente tudo que estudamos no\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/deep-learning-a-tempestade-perfeita/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     livro\n    </a>\n   </span>\n   at\u00e9 aqui se refere a algoritmos discriminativos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por exemplo, dadas todas as palavras em um email (a inst\u00e2ncia de dados), um algoritmo discriminativo poderia prever se a mensagem \u00e9 spam ou n\u00e3o \u00e9 spam. O spam \u00e9 um dos r\u00f3tulos e o conjunto de palavras coletadas no email s\u00e3o os recursos que constituem os dados de entrada. Quando esse problema \u00e9 expresso matematicamente, o r\u00f3tulo \u00e9 chamado y e os recursos s\u00e3o chamados x. A formula\u00e7\u00e3o p (y | x) \u00e9 usada para significar \u201ca probabilidade de y dado x\u201d, que neste caso seria traduzido para \u201ca probabilidade de um email ser spam, dadas as palavras que ele cont\u00e9m\u201d.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Algoritmos discriminativos mapeiam recursos para r\u00f3tulos e est\u00e3o preocupados apenas com essa correla\u00e7\u00e3o. Uma maneira de pensar sobre algoritmos generativos \u00e9 que eles fazem o oposto. Em vez de prever um r\u00f3tulo com determinados recursos, eles tentam prever os recursos com um determinado r\u00f3tulo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A pergunta que um algoritmo generativo tenta responder \u00e9: Supondo que este email seja spam, qual a probabilidade desses recursos? Enquanto os modelos discriminativos se preocupam com a rela\u00e7\u00e3o entre y e x, os modelos generativos se preocupam com \u201ccomo voc\u00ea obt\u00e9m x\u201d. Eles permitem capturar p (x | y), a probabilidade de x dado y ou a probabilidade de recursos com um r\u00f3tulo ou categoria. Dito isto, algoritmos generativos tamb\u00e9m podem ser usados \u200b\u200bcomo classificadores. Acontece que eles podem fazer mais do que categorizar dados de entrada. O que mais eles podem fazer? Ainda estamos descobrindo, pois a evolu\u00e7\u00e3o est\u00e1 acontecendo agora nesse momento com pesquisas em todo mundo e\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Cientistas de Dados\n    </a>\n   </span>\n   e\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Engenheiros de IA\n    </a>\n   </span>\n   trabalhando em diferentes aplica\u00e7\u00f5es.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Outra maneira de pensar sobre isso \u00e9 distinguir discriminativo de generativo assim:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Modelos discriminativos aprendem a fronteira entre classes.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Modelos generativos modelam a distribui\u00e7\u00e3o de classes individuais.\n   </span>\n  </li>\n </ul>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como as GANs Funcionam\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vejamos uma vis\u00e3o geral das GANs e nos pr\u00f3ximos cap\u00edtulos entraremos nos detalhes matem\u00e1ticos e estat\u00edsticos da arquitetura.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma rede neural, chamada de gerador, gera novas inst\u00e2ncias de dados, enquanto a outra, o discriminador, avalia sua autenticidade; ou seja, o discriminador decide se cada inst\u00e2ncia de dados que ele analisa pertence ou n\u00e3o ao conjunto de dados de treinamento real (a imagem abaixo demonstra isso).\n  </span>\n </p>\n <p>\n  <img alt=\"gan_schema\" class=\"aligncenter size-full wp-image-1429\" data-attachment-id=\"1429\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gan_schema\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?fit=1024%2C375\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?fit=300%2C110\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?fit=1406%2C515\" data-orig-size=\"1406,515\" data-permalink=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gan_schema/\" data-recalc-dims=\"1\" height=\"429\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=1170%2C429\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?w=1406 1406w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=300%2C110 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=768%2C281 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=1024%2C375 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=200%2C73 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan_schema.png?resize=690%2C253 690w\" width=\"1170\"/>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Digamos que estamos tentando fazer algo mais banal do que imitar a Mona Lisa. Geraremos n\u00fameros escritos \u00e0 m\u00e3o, como os encontrados no conjunto de dados MNIST, retirado do mundo real. O objetivo do discriminador, quando mostrada uma inst\u00e2ncia do verdadeiro conjunto de dados MNIST, \u00e9 reconhecer aqueles que s\u00e3o aut\u00eanticos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Enquanto isso, o gerador est\u00e1 criando novas imagens sint\u00e9ticas que s\u00e3o transmitidas ao discriminador. O gerador gera as imagens fake na esperan\u00e7a de que elas tamb\u00e9m sejam consideradas aut\u00eanticas, mesmo sendo falsas. O objetivo do gerador \u00e9 gerar d\u00edgitos manuscritos cada vez melhores. O objetivo do discriminador \u00e9 identificar imagens falsas do gerador. Ou seja, s\u00e3o duas redes advers\u00e1rias, uma discriminativa (padr\u00e3o que j\u00e1 estudamos at\u00e9 aqui no livro) e uma generativa que, em termos gerais, faz o oposto das redes discriminativas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui est\u00e3o as etapas de uma GAN:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    O gerador considera n\u00fameros aleat\u00f3rios e retorna uma imagem.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Essa imagem gerada \u00e9 inserida no discriminador ao lado de um fluxo de imagens tiradas do conjunto de dados real e verdadeiro.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    O discriminador obt\u00e9m imagens reais e falsas e retorna probabilidades, um n\u00famero entre 0 e 1, com 1 representando uma previs\u00e3o de imagem aut\u00eantica e 0 representando previs\u00e3o de imagens falsas (geradas pela rede generativa).\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ent\u00e3o voc\u00ea tem um loop de feedback duplo assim:\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"GANs\" class=\"aligncenter size-full wp-image-1430\" data-attachment-id=\"1430\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"GANs\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?fit=1024%2C447\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?fit=300%2C131\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?fit=1213%2C529\" data-orig-size=\"1213,529\" data-permalink=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gans-1/\" data-recalc-dims=\"1\" height=\"510\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=1170%2C510\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?w=1213 1213w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=300%2C131 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=768%2C335 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=1024%2C447 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=200%2C87 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/GANs-1.png?resize=690%2C301 690w\" width=\"1170\"/>\n </p>\n <p>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para o MNIST, a rede discriminadora \u00e9 uma rede convolucional padr\u00e3o que pode categorizar as imagens alimentadas, um classificador binomial que rotula as imagens como reais ou falsas. O gerador \u00e9 uma rede convolucional inversa, em certo sentido: enquanto um classificador convolucional padr\u00e3o recebe uma imagem e reduz a amostragem para produzir uma probabilidade, o gerador pega um vetor de ru\u00eddo aleat\u00f3rio e faz o upsample para uma imagem. O primeiro joga fora os dados por meio de t\u00e9cnicas de downsampling, como o maxpool, e o segundo gera novos dados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ambas as redes est\u00e3o tentando otimizar uma fun\u00e7\u00e3o objetivo (fun\u00e7\u00e3o de perda) diferente e oposta. \u00c0 medida que o discriminador muda seu comportamento, o gerador tamb\u00e9m muda e vice-versa. Suas perdas empurram um contra o outro. Uma ideia simples e genial! Durante o treinamento, a rede generativa vai aprendendo a criar uma imagem fake que fica cada vez mais pr\u00f3xima de uma imagem real. Alguns pesquisadores tem feito o mesmo com a voz, gerando falas que s\u00e3o falsas, mas se parecem muito com falas verdadeiras. Esse \u00e9 o perigo das Deep Fakes, quando a tecnologia \u00e9 usada para o mal, infelizmente. Aqui tem um exemplo de Deep Fake gerada com GAN para fala do ex-presidente dos EUA, Barack Obama:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.youtube.com/watch?v=AmUC4m6w1wo\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Fake Obama created using AI video tool \u2013 BBC News\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"gan\" class=\"aligncenter size-full wp-image-1432\" data-attachment-id=\"1432\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"gan\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?fit=800%2C399\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?fit=300%2C150\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?fit=800%2C399\" data-orig-size=\"800,399\" data-permalink=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/gan/\" data-recalc-dims=\"1\" height=\"399\" sizes=\"(max-width: 800px) 100vw, 800px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=800%2C399\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?w=800 800w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=300%2C150 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=768%2C383 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=200%2C100 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/09/gan.png?resize=690%2C344 690w\" width=\"800\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Quer aprender mais sobre as GANs? Ent\u00e3o acompanhe os pr\u00f3ximos cap\u00edtulos!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://skymind.ai/wiki/generative-adversarial-network-gan\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     A Beginner\u2019s Guide to Generative Adversarial Networks (GANs)\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     A Leap into the Future: Generative Adversarial Networks\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Understanding Generative Adversarial Networks (GANs)\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     How A.I. Is Creating Building Blocks to Reshape Music and Art\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent neural network based language model\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1426\" href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1426\" href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1426\" href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1426\" href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1426-5e0dd1d2016c6\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1426&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1426-5e0dd1d2016c6\" id=\"like-post-wrapper-140353593-1426-5e0dd1d2016c6\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "55": "<h1 class=\"entry-title\" id=\"capitulo-55\">\n Cap\u00edtulo 55 \u2013 Gera\u00e7\u00e3o de Vari\u00e1veis Aleat\u00f3rias \u2013 Uma das Bases dos Modelos Generativos em GANs (Generative Adversarial Networks)\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste cap\u00edtulo discutiremos o processo de gera\u00e7\u00e3o de vari\u00e1veis aleat\u00f3rias a partir de uma determinada distribui\u00e7\u00e3o, tema importante para compreender o funcionamento dos modelos generativos das GANs e nos pr\u00f3ximos cap\u00edtulos veremos os detalhes de funcionamento dos modelos generativos e os detalhes matem\u00e1ticos das GANs. Estamos considerando que voc\u00ea leu o cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vari\u00e1veis \u200b\u200baleat\u00f3rias uniformes podem ser geradas pseudo-aleatoriamente\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos come\u00e7ar discutindo o processo de gera\u00e7\u00e3o de vari\u00e1veis \u200b\u200baleat\u00f3rias. Veremos alguns m\u00e9todos existentes e, mais especificamente, o m\u00e9todo de transforma\u00e7\u00e3o inversa que permite gerar vari\u00e1veis \u200b\u200baleat\u00f3rias complexas a partir de vari\u00e1veis \u200b\u200baleat\u00f3rias uniformes simples. Embora tudo isso possa parecer um pouco distante do nosso assunto, GANs, veremos no pr\u00f3ximo cap\u00edtulo o v\u00ednculo profundo que existe com os modelos generativos. Se o conceito de Estat\u00edstica for algo que voc\u00ea esteja vendo pela primeira vez, a DSA oferece cursos completos de Estat\u00edstica e Matem\u00e1tica na\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n    .\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma vari\u00e1vel aleat\u00f3ria \u00e9 uma vari\u00e1vel quantitativa, cujo resultado (valor) depende de fatores aleat\u00f3rios. Um exemplo de uma vari\u00e1vel aleat\u00f3ria \u00e9 o resultado do lan\u00e7amento de um dado que pode resultar em qualquer n\u00famero entre 1 e 6. Embora possamos conhecer os seus poss\u00edveis resultados, o resultado em si depende de fatores de sorte (\u00e1lea). Uma vari\u00e1vel aleat\u00f3ria pode ser uma medi\u00e7\u00e3o de um par\u00e2metro que pode gerar valores diferentes. O conceito de vari\u00e1vel aleat\u00f3ria \u00e9 essencial em Estat\u00edstica e em outros m\u00e9todos quantitativos para a representa\u00e7\u00e3o de fen\u00f4menos incertos. Este conceito \u00e9 estudado em detalhes no curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=analise-estatistica-para-data-science-i\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     An\u00e1lise Estat\u00edstica Para Data Science I com R e SAS\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   As vari\u00e1veis aleat\u00f3rias podem ser classificadas em vari\u00e1veis aleat\u00f3rias discretas, cont\u00ednuas e mistas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os computadores s\u00e3o fundamentalmente determin\u00edsticos. Portanto, \u00e9 teoricamente imposs\u00edvel gerar n\u00fameros realmente aleat\u00f3rios (a pergunta \u201co que realmente \u00e9 aleatoriedade?\u201d \u00e9 algo dif\u00edcil de responder). No entanto, \u00e9 poss\u00edvel definir algoritmos que geram sequ\u00eancias de n\u00fameros cujas propriedades est\u00e3o muito pr\u00f3ximas das propriedades das sequ\u00eancias te\u00f3ricas de n\u00fameros aleat\u00f3rios. Em particular, um computador \u00e9 capaz, usando um gerador de n\u00fameros pseudo-aleat\u00f3rios, de gerar uma sequ\u00eancia de n\u00fameros que segue aproximadamente uma distribui\u00e7\u00e3o aleat\u00f3ria uniforme entre 0 e 1. O caso uniforme \u00e9 muito simples, no qual vari\u00e1veis \u200b\u200baleat\u00f3rias mais complexas podem ser constru\u00eddas.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vari\u00e1veis \u200b\u200baleat\u00f3rias expressas como resultado de uma opera\u00e7\u00e3o ou processo\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Existem diferentes t\u00e9cnicas que visam gerar vari\u00e1veis \u200b\u200baleat\u00f3rias mais complexas. Entre elas, podemos encontrar, por exemplo, m\u00e9todo de transforma\u00e7\u00e3o inversa, amostragem por rejei\u00e7\u00e3o, algoritmo Metropolis-Hastings entre outros. Todos esses m\u00e9todos se baseiam em diferentes truques matem\u00e1ticos que consistem principalmente em representar a vari\u00e1vel aleat\u00f3ria que queremos gerar como resultado de uma opera\u00e7\u00e3o (sobre vari\u00e1veis \u200b\u200baleat\u00f3rias mais simples) ou de um processo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://en.wikipedia.org/wiki/Rejection_sampling\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     amostragem por rejei\u00e7\u00e3o\n    </a>\n   </span>\n   expressa a vari\u00e1vel aleat\u00f3ria como resultado de um processo que consiste em amostrar n\u00e3o da distribui\u00e7\u00e3o complexa, mas de uma distribui\u00e7\u00e3o simples bem conhecida e para aceitar ou rejeitar o valor amostrado, dependendo de alguma condi\u00e7\u00e3o. Repetindo esse processo at\u00e9 que o valor amostrado seja aceito, podemos mostrar que, com a condi\u00e7\u00e3o correta de aceita\u00e7\u00e3o, o valor que ser\u00e1 efetivamente amostrado seguir\u00e1 a distribui\u00e7\u00e3o correta.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No algoritmo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Metropolis-Hastings\n    </a>\n   </span>\n   , a ideia \u00e9 encontrar uma Cadeia de Markov (MC \u2013 Markov Chain) de modo que a distribui\u00e7\u00e3o estacion\u00e1ria dessa MC corresponda \u00e0 distribui\u00e7\u00e3o da qual gostar\u00edamos de amostrar nossa vari\u00e1vel aleat\u00f3ria. Uma vez encontrada essa MC, podemos simular uma trajet\u00f3ria longa o suficiente sobre ela para considerar que atingimos um estado estacion\u00e1rio e, em seguida, o \u00faltimo valor que obtemos dessa maneira pode ser considerado como extra\u00eddo da distribui\u00e7\u00e3o de interesse.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   N\u00e3o iremos mais adiante nos detalhes da amostragem por rejei\u00e7\u00e3o e do Metropolis-Hastings, porque esses m\u00e9todos n\u00e3o s\u00e3o os que nos levar\u00e3o \u00e0 no\u00e7\u00e3o por tr\u00e1s das GANs. No entanto, vamos nos concentrar um pouco mais no m\u00e9todo de transforma\u00e7\u00e3o inversa.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O m\u00e9todo de transforma\u00e7\u00e3o inversa\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A ideia do m\u00e9todo de transforma\u00e7\u00e3o inversa \u00e9 simplesmente representar nossa \u201ccomplexa\u201d vari\u00e1vel aleat\u00f3ria como resultado de uma fun\u00e7\u00e3o aplicada a um vari\u00e1vel aleat\u00f3ria uniforme, que n\u00f3s sabemos como gerar.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Consideramos abaixo um exemplo unidimensional. Seja X uma vari\u00e1vel aleat\u00f3ria complexa da qual queremos amostrar e U seja uma vari\u00e1vel aleat\u00f3ria uniforme sobre [0,1] que sabemos como amostrar. Lembramos que uma vari\u00e1vel aleat\u00f3ria \u00e9 totalmente definida por sua Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Cumulativa (CDF). O CDF de uma vari\u00e1vel aleat\u00f3ria \u00e9 uma fun\u00e7\u00e3o do dom\u00ednio de defini\u00e7\u00e3o da vari\u00e1vel aleat\u00f3ria at\u00e9 o intervalo [0,1] e definido, em uma dimens\u00e3o, de modo que:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter size-full wp-image-1453\" data-attachment-id=\"1453\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?fit=634%2C42\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?fit=300%2C20\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?fit=634%2C42\" data-orig-size=\"634,42\" data-permalink=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form1-9/\" data-recalc-dims=\"1\" height=\"42\" sizes=\"(max-width: 634px) 100vw, 634px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?resize=634%2C42\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?w=634 634w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?resize=300%2C20 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1.png?resize=200%2C13 200w\" width=\"634\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No caso particular de nossa vari\u00e1vel aleat\u00f3ria uniforme U, temos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-1454\" data-attachment-id=\"1454\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?fit=746%2C42\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?fit=300%2C17\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?fit=746%2C42\" data-orig-size=\"746,42\" data-permalink=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form2-15/\" data-recalc-dims=\"1\" height=\"42\" sizes=\"(max-width: 746px) 100vw, 746px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=746%2C42\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?w=746 746w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=300%2C17 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=200%2C11 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.png?resize=690%2C39 690w\" width=\"746\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por uma quest\u00e3o de simplicidade, vamos supor aqui que a fun\u00e7\u00e3o CDF_X \u00e9 invers\u00edvel e seu inverso \u00e9 indicado por:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form3\" class=\"aligncenter size-full wp-image-1456\" data-attachment-id=\"1456\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?fit=134%2C50\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?fit=134%2C50\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?fit=134%2C50\" data-orig-size=\"134,50\" data-permalink=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form3-13/\" data-recalc-dims=\"1\" height=\"50\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3.png?resize=134%2C50\" width=\"134\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   (o m\u00e9todo pode ser facilmente estendido ao caso n\u00e3o invers\u00edvel usando o inverso generalizado da fun\u00e7\u00e3o, mas n\u00e3o \u00e9 realmente o ponto principal em que queremos focar aqui). Ent\u00e3o, se definirmos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form4\" class=\"aligncenter size-full wp-image-1457\" data-attachment-id=\"1457\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?fit=282%2C50\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?fit=282%2C50\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?fit=282%2C50\" data-orig-size=\"282,50\" data-permalink=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form4-9/\" data-recalc-dims=\"1\" height=\"50\" sizes=\"(max-width: 282px) 100vw, 282px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?resize=282%2C50\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?w=282 282w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?resize=200%2C35 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.png?resize=280%2C50 280w\" width=\"282\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   temos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form5\" class=\"aligncenter size-full wp-image-1458\" data-attachment-id=\"1458\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?fit=1024%2C38\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?fit=300%2C11\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?fit=1358%2C50\" data-orig-size=\"1358,50\" data-permalink=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/form5-7/\" data-recalc-dims=\"1\" height=\"43\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=1170%2C43\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?w=1358 1358w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=300%2C11 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=768%2C28 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=1024%2C38 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=200%2C7 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5.png?resize=690%2C25 690w\" width=\"1170\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como podemos ver, Y e X t\u00eam o mesmo CDF e depois definem a mesma vari\u00e1vel aleat\u00f3ria. Assim, definindo Y como acima (em fun\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria uniforme), conseguimos definir uma vari\u00e1vel aleat\u00f3ria com a distribui\u00e7\u00e3o alvo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para resumir, o m\u00e9todo de transforma\u00e7\u00e3o inversa \u00e9 uma maneira de gerar uma vari\u00e1vel aleat\u00f3ria que segue uma determinada distribui\u00e7\u00e3o, fazendo uma vari\u00e1vel aleat\u00f3ria uniforme passar por uma \u201cfun\u00e7\u00e3o de transforma\u00e7\u00e3o\u201d bem projetada (CDF inverso). Essa no\u00e7\u00e3o de \u201cm\u00e9todo de transforma\u00e7\u00e3o inversa\u201d pode, de fato, ser estendida \u00e0 no\u00e7\u00e3o de \u201cm\u00e9todo de transforma\u00e7\u00e3o\u201d que consiste, de maneira mais geral, em gerar vari\u00e1veis \u200b\u200baleat\u00f3rias em fun\u00e7\u00e3o de algumas vari\u00e1veis \u200b\u200baleat\u00f3rias mais simples (n\u00e3o necessariamente uniformes e, em seguida, a fun\u00e7\u00e3o de transforma\u00e7\u00e3o \u00e9 n\u00e3o mais o CDF inverso). Conceitualmente, o objetivo da \u201cfun\u00e7\u00e3o de transforma\u00e7\u00e3o\u201d \u00e9 deformar / remodelar a distribui\u00e7\u00e3o de probabilidade inicial: a fun\u00e7\u00e3o de transforma\u00e7\u00e3o come\u00e7a de onde a distribui\u00e7\u00e3o inicial \u00e9 muito alta em compara\u00e7\u00e3o com a distribui\u00e7\u00e3o de destino e a coloca onde \u00e9 muito baixa. Foi exatamente isso que pensou o criador do modelo GAN e muitos consideram o conceito como uma esp\u00e9cie de \u201chack\u201d na teoria estat\u00edstica, o que gerou o modelo GAN.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Observe a ilustra\u00e7\u00e3o do m\u00e9todo de transforma\u00e7\u00e3o inversa abaixo. Em azul: a distribui\u00e7\u00e3o uniforme em [0,1]. Em laranja: a distribui\u00e7\u00e3o gaussiana (normal) padr\u00e3o. Em cinza: o mapeamento da distribui\u00e7\u00e3o uniforme para a gaussiana (CDF inverso).\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   <img alt=\"dist\" class=\"aligncenter wp-image-1464 size-large\" data-attachment-id=\"1464\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"dist\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?fit=1024%2C367\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?fit=300%2C108\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?fit=4000%2C1434\" data-orig-size=\"4000,1434\" data-permalink=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/dist/\" data-recalc-dims=\"1\" height=\"367\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=1024%2C367\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=1024%2C367 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=300%2C108 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=768%2C275 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=200%2C72 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?resize=690%2C247 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?w=2340 2340w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dist.jpeg?w=3510 3510w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Compreendeu o conceito? Isso \u00e9 o que est\u00e1 por tr\u00e1s dos modelos generativos nas GANs, que veremos no pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://skymind.ai/wiki/generative-adversarial-network-gan\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    A Beginner\u2019s Guide to Generative Adversarial Networks (GANs)\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    A Leap into the Future: Generative Adversarial Networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Understanding Generative Adversarial Networks (GANs)\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    How A.I. Is Creating Building Blocks to Reshape Music and Art\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks and Deep Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent neural network based language model\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1452\" href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1452\" href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1452\" href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1452\" href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1452-5e0dd1d401fdf\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1452&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1452-5e0dd1d401fdf\" id=\"like-post-wrapper-140353593-1452-5e0dd1d401fdf\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "56": "<h1 class=\"entry-title\" id=\"capitulo-56\">\n Cap\u00edtulo 56 \u2013 Modelos Generativos \u2013 O Diferencial das GANs (Generative Adversarial Networks)\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste cap\u00edtulo discutiremos o funcionamento dos modelos generativos, o principal diferencial nas GANs (Generative Adversarial Networks). Estamos considerando que voc\u00ea leu o cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Suponha que estamos interessados \u200b\u200bem gerar imagens quadradas em preto e branco de c\u00e3es com um tamanho de n por n pixels. Podemos remodelar cada dado como um vetor dimensional N = n x n (empilhando colunas umas sobre as outras), de modo que uma imagem de cachorro possa ser representada por um vetor (quem sabe conseguimos criar um modelo capaz de diferenciar um Muffin de um Chihuahua).\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"dogs\" class=\"aligncenter size-full wp-image-1483\" data-attachment-id=\"1483\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"dogs\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?fit=800%2C350\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?fit=300%2C131\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?fit=800%2C350\" data-orig-size=\"800,350\" data-permalink=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/dogs/\" data-recalc-dims=\"1\" height=\"350\" sizes=\"(max-width: 800px) 100vw, 800px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=800%2C350\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?w=800 800w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=300%2C131 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=768%2C336 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=200%2C88 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/dogs.jpeg?resize=690%2C302 690w\" width=\"800\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No entanto, isso n\u00e3o significa que todos os vetores representem um c\u00e3o que foi moldado de volta a um quadrado! Portanto, podemos dizer que os vetores dimensionais N que efetivamente geram algo que se parece com um cachorro s\u00e3o distribu\u00eddos de acordo com uma distribui\u00e7\u00e3o de probabilidade muito espec\u00edfica em todo o espa\u00e7o vetorial dimensional N (alguns pontos desse espa\u00e7o provavelmente representam c\u00e3es, enquanto \u00e9 improv\u00e1vel para alguns outros). No mesmo esp\u00edrito, existe, nesse espa\u00e7o vetorial dimensional N, distribui\u00e7\u00f5es de probabilidade para imagens de gatos, p\u00e1ssaros e assim por diante.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ent\u00e3o, o problema de gerar uma nova imagem do c\u00e3o \u00e9 equivalente ao problema de gerar um novo vetor ap\u00f3s a \u201cdistribui\u00e7\u00e3o de probabilidade do c\u00e3o\u201d no espa\u00e7o vetorial dimensional N. De fato, estamos enfrentando um problema de gerar uma vari\u00e1vel aleat\u00f3ria com rela\u00e7\u00e3o a uma distribui\u00e7\u00e3o de probabilidade espec\u00edfica (lembra do que discutimos no cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   ?).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste ponto, podemos mencionar duas coisas importantes. Primeiro, a \u201cdistribui\u00e7\u00e3o de probabilidade canina\u201d que mencionamos \u00e9 uma distribui\u00e7\u00e3o muito complexa em um espa\u00e7o muito grande. Segundo, mesmo se pudermos assumir a exist\u00eancia de tal distribui\u00e7\u00e3o subjacente (na verdade existem imagens que se parecem com cachorro e outras que n\u00e3o), obviamente n\u00e3o sabemos como expressar explicitamente essa distribui\u00e7\u00e3o. Os dois pontos anteriores dificultam bastante o processo de gera\u00e7\u00e3o de vari\u00e1veis \u200b\u200baleat\u00f3rias a partir dessa distribui\u00e7\u00e3o. Vamos tentar resolver esses dois problemas a seguir.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nosso primeiro problema ao tentar gerar nossa nova imagem de cachorro \u00e9 que a \u201cdistribui\u00e7\u00e3o de probabilidade do cachorro\u201d no espa\u00e7o vetorial dimensional N \u00e9 muito complexa e n\u00e3o sabemos como gerar diretamente vari\u00e1veis \u200b\u200baleat\u00f3rias complexas. No entanto, como sabemos muito bem como gerar N vari\u00e1veis \u200b\u200baleat\u00f3rias uniformes n\u00e3o correlacionadas, poder\u00edamos fazer uso do m\u00e9todo de transforma\u00e7\u00e3o. Para fazer isso, precisamos expressar nossa vari\u00e1vel aleat\u00f3ria N dimensional como resultado de uma fun\u00e7\u00e3o muito complexa aplicada a uma vari\u00e1vel aleat\u00f3ria N dimensional simples!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui, podemos enfatizar o fato de que encontrar a fun\u00e7\u00e3o de transforma\u00e7\u00e3o n\u00e3o \u00e9 t\u00e3o simples quanto tomar a inversa de forma fechada da Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Cumulativa (que obviamente n\u00e3o sabemos) como fizemos ao descrever o m\u00e9todo de transforma\u00e7\u00e3o inversa no cap\u00edtulo\n   <a href=\"http://deeplearningbook.com.br/geracao-de-variaveis-aleatorias-uma-das-bases-dos-modelos-generativos-em-gans-generative-adversarial-networks/\" rel=\"noopener noreferrer\" target=\"_blank\">\n    anterior\n   </a>\n   . A fun\u00e7\u00e3o de transforma\u00e7\u00e3o n\u00e3o pode ser expressa explicitamente e, ent\u00e3o, precisamos aprender com os dados (essa \u00e9 uma das raz\u00f5es pelas quais treinamos um modelo de Machine Learning e esse conceito \u00e9 estudado em detalhes\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-engineer\" rel=\"noopener noreferrer\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   )\n  </span>\n  <span style=\"color: #000000;\">\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em seguida, a ideia \u00e9 modelar a fun\u00e7\u00e3o de transforma\u00e7\u00e3o por uma rede neural que tome como entrada uma vari\u00e1vel aleat\u00f3ria uniforme N dimensional simples e que retorne como sa\u00edda outra vari\u00e1vel aleat\u00f3ria N dimensional que deve seguir, ap\u00f3s o treinamento, a \u201cdistribui\u00e7\u00e3o de probabilidade canina\u201d correta . Depois que a arquitetura da rede foi projetada, ainda precisamos trein\u00e1-la. Nas pr\u00f3ximas duas se\u00e7\u00f5es, discutiremos duas maneiras de treinar essas redes generativas, incluindo a ideia de treinamento antag\u00f4nico por tr\u00e1s das GANs!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"model1\" class=\"aligncenter wp-image-1487 size-large\" data-attachment-id=\"1487\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"model1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?fit=1024%2C488\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?fit=300%2C143\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?fit=2486%2C1185\" data-orig-size=\"2486,1185\" data-permalink=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/model1/\" data-recalc-dims=\"1\" height=\"488\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=1024%2C488\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=1024%2C488 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=300%2C143 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=768%2C366 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=200%2C95 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?resize=690%2C329 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/model1.png?w=2340 2340w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Redes de Correspond\u00eancia Generativa (Generative Matching Networks)\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Obs: a denomina\u00e7\u00e3o de \u201credes de correspond\u00eancia generativa\u201d n\u00e3o \u00e9 padr\u00e3o. No entanto, podemos encontrar na literatura, por exemplo, \u201cRedes de correspond\u00eancia de momentos generativos\u201d ou tamb\u00e9m \u201cRedes de correspond\u00eancia de recursos generativos\u201d. S\u00f3 queremos aqui usar uma denomina\u00e7\u00e3o um pouco mais geral para o que descrevemos abaixo.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Treinando Modelos Generativos\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 agora, mostramos que nosso problema de gerar uma nova imagem de cachorro pode ser reformulado em um problema de gerar um vetor aleat\u00f3rio no espa\u00e7o vetorial dimensional N que segue a \u201cdistribui\u00e7\u00e3o de probabilidade canina\u201d e sugerimos o uso de um m\u00e9todo de transforma\u00e7\u00e3o , com uma rede neural para modelar a fun\u00e7\u00e3o de transforma\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora, ainda precisamos treinar (otimizar) a rede para expressar a fun\u00e7\u00e3o de transforma\u00e7\u00e3o correta. Para isso, podemos sugerir dois m\u00e9todos diferentes de treinamento: um direto e um indireto. O m\u00e9todo de treinamento direto consiste em comparar as distribui\u00e7\u00f5es de probabilidade verdadeira e gerada e\n   <em>\n    retropropagar\n   </em>\n   a diferen\u00e7a (o erro) atrav\u00e9s da rede. Essa \u00e9 a ideia que governa as redes de correspond\u00eancia generativa (GMNs \u2013 Generative Matching Networks)). Para o m\u00e9todo de treinamento indireto, n\u00e3o comparamos diretamente as distribui\u00e7\u00f5es verdadeiras e geradas. Em vez disso, treinamos a rede generativa, fazendo com que essas duas distribui\u00e7\u00f5es passem por um ajuste escolhido de forma que o processo de otimiza\u00e7\u00e3o da rede generativa em rela\u00e7\u00e3o \u00e0 tarefa ajustada imponha que a distribui\u00e7\u00e3o gerada esteja pr\u00f3xima da verdadeira distribui\u00e7\u00e3o. Essa \u00faltima ideia \u00e9 a que est\u00e1 por tr\u00e1s das Redes Advers\u00e1rias Generativas (GANs) que apresentaremos na pr\u00f3xima se\u00e7\u00e3o. Mas, por enquanto, vamos come\u00e7ar com o m\u00e9todo direto e as GMNs.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Comparando duas distribui\u00e7\u00f5es de probabilidade com base em amostras\n   </span>\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como mencionado, a ideia das GMNs \u00e9 treinar a rede generativa comparando diretamente a distribui\u00e7\u00e3o gerada com a verdadeira. No entanto, n\u00e3o sabemos como expressar explicitamente a verdadeira \u201cdistribui\u00e7\u00e3o de probabilidade de c\u00e3es\u201d e tamb\u00e9m podemos dizer que a distribui\u00e7\u00e3o gerada \u00e9 complexa demais para ser expressa explicitamente. Portanto, compara\u00e7\u00f5es baseadas em express\u00f5es expl\u00edcitas n\u00e3o s\u00e3o poss\u00edveis. Logo, se tivermos uma maneira de comparar distribui\u00e7\u00f5es de probabilidade com base em amostras, podemos us\u00e1-la para treinar a rede. De fato, temos uma amostra de dados verdadeiros e podemos, a cada itera\u00e7\u00e3o do processo de treinamento, produzir uma amostra de dados gerados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Embora, em teoria, qualquer dist\u00e2ncia (ou medida de similaridade) capaz de comparar efetivamente duas distribui\u00e7\u00f5es baseadas em amostras possa ser usada, podemos mencionar, em particular, a abordagem da Discrep\u00e2ncia M\u00e9dia M\u00e1xima (MMD). O MMD define uma dist\u00e2ncia entre duas distribui\u00e7\u00f5es de probabilidade que podem ser calculadas (estimadas) com base em amostras dessas distribui\u00e7\u00f5es. Embora n\u00e3o esteja totalmente fora do escopo deste cap\u00edtulo, decidimos n\u00e3o gastar muito mais tempo descrevendo o MMD. No entanto, caso voc\u00ea queira mais detalhes sobre isso, recomendamos esses 3 papers abaixo:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Learning features to compare distributions\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     A Kernel Method for the Two-Sample-Problem\n    </span>\n   </a>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     A Kernel Two-Sample Test\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    Retropropaga\u00e7\u00e3o do erro de correspond\u00eancia de distribui\u00e7\u00e3o\n   </span>\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim, uma vez que definimos uma maneira de comparar duas distribui\u00e7\u00f5es com base em amostras, podemos definir o processo de treinamento da rede generativa em GMNs. Dada uma vari\u00e1vel aleat\u00f3ria com distribui\u00e7\u00e3o de probabilidade uniforme como entrada, queremos que a distribui\u00e7\u00e3o de probabilidade da sa\u00edda gerada seja a \u201cdistribui\u00e7\u00e3o de probabilidade do c\u00e3o\u201d. A ideia das GMNs \u00e9 otimizar a rede repetindo as seguintes etapas:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Gerar algumas entradas uniformes.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Fazer essas entradas passarem pela rede e coletar as sa\u00eddas geradas.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Comparar a verdadeira \u201cdistribui\u00e7\u00e3o de probabilidade de c\u00e3es\u201d e a gerada com base nas amostras dispon\u00edveis (por exemplo, calculando a dist\u00e2ncia MMD entre a amostra de imagens reais de c\u00e3es e a amostra de imagens geradas).\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Usar retropropaga\u00e7\u00e3o para fazer uma etapa de descida de gradiente para diminuir a dist\u00e2ncia (por exemplo, MMD) entre distribui\u00e7\u00f5es verdadeiras e geradas.\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Conforme descrito acima, ao seguir estas etapas, aplicamos uma descida do gradiente na rede com uma fun\u00e7\u00e3o de perda que \u00e9 a dist\u00e2ncia entre as distribui\u00e7\u00f5es verdadeiras e as distribui\u00e7\u00f5es geradas na itera\u00e7\u00e3o atual.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O m\u00e9todo de treinamento \u201cindireto\u201d\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A abordagem \u201cdireta\u201d apresentada acima compara diretamente a distribui\u00e7\u00e3o gerada com a verdadeira ao treinar a rede generativa. A brilhante ideia que governa as GANs consiste em substituir essa compara\u00e7\u00e3o direta por uma indireta que assume a forma de uma tarefa ajustada sobre essas duas distribui\u00e7\u00f5es.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O treinamento da rede generativa \u00e9 ent\u00e3o realizado com rela\u00e7\u00e3o a essa tarefa, de modo que for\u00e7a a distribui\u00e7\u00e3o gerada a se aproximar cada vez mais da distribui\u00e7\u00e3o verdadeira.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A tarefa ajustada das GANs \u00e9 uma tarefa de discrimina\u00e7\u00e3o entre amostras verdadeiras e geradas. Ou poder\u00edamos dizer uma tarefa de \u201cn\u00e3o discrimina\u00e7\u00e3o\u201d, pois queremos que a discrimina\u00e7\u00e3o falhe o m\u00e1ximo poss\u00edvel. Portanto, em uma arquitetura GAN, temos um discriminador, que coleta amostras de dados verdadeiros e gerados e tenta classific\u00e1-los da melhor maneira poss\u00edvel, e um gerador treinado para enganar o discriminador o m\u00e1ximo poss\u00edvel. Vamos ver em um exemplo simples porque as abordagens diretas e indiretas que mencionamos devem, em teoria, levar ao mesmo gerador ideal.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    O caso ideal: gerador e discriminador perfeitos\n   </span>\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para entender melhor por que treinar um gerador para enganar um discriminador levar\u00e1 ao mesmo resultado que treinar diretamente o gerador para corresponder \u00e0 distribui\u00e7\u00e3o de destino, vamos dar um exemplo unidimensional simples. Esquecemos, por enquanto, como o gerador e o discriminador s\u00e3o representados e os consideramos como no\u00e7\u00f5es abstratas (que ser\u00e3o especificadas na pr\u00f3xima subse\u00e7\u00e3o). Al\u00e9m disso, ambos s\u00e3o supostos \u201cperfeitos\u201d (com capacidades infinitas) no sentido de que n\u00e3o s\u00e3o limitados por nenhum tipo de modelo (parametrizado).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Suponha que tenhamos uma distribui\u00e7\u00e3o verdadeira, por exemplo, um gaussiano unidimensional e que desejemos um gerador que fa\u00e7a amostras dessa distribui\u00e7\u00e3o de probabilidade. O que chamamos de m\u00e9todo de treinamento \u201cdireto\u201d consistiria em ajustar iterativamente o gerador (itera\u00e7\u00f5es de descida de gradiente) para corrigir a diferen\u00e7a / erro medido entre distribui\u00e7\u00f5es verdadeiras e geradas. Por fim, supondo que o processo de otimiza\u00e7\u00e3o seja perfeito, devemos terminar com a distribui\u00e7\u00e3o gerada que corresponda exatamente \u00e0 verdadeira distribui\u00e7\u00e3o.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"norm\" class=\"aligncenter wp-image-1480 size-large\" data-attachment-id=\"1480\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"norm\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?fit=1024%2C985\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?fit=300%2C288\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?fit=3684%2C3542\" data-orig-size=\"3684,3542\" data-permalink=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/norm/\" data-recalc-dims=\"1\" height=\"985\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=1024%2C985\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=1024%2C985 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=300%2C288 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=768%2C738 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=200%2C192 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?resize=690%2C663 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?w=2340 2340w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm.jpeg?w=3510 3510w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para a abordagem \u201cindireta\u201d, devemos considerar tamb\u00e9m um discriminador. Presumimos por enquanto que esse discriminador \u00e9 um tipo de or\u00e1culo que sabe exatamente quais s\u00e3o as distribui\u00e7\u00f5es verdadeira e gerada e que \u00e9 capaz, com base nessas informa\u00e7\u00f5es, de prever uma classe (\u201cverdadeira\u201d ou \u201cgerada\u201d) para qualquer ponto. Se as duas distribui\u00e7\u00f5es estiverem distantes, o discriminador ser\u00e1 capaz de classificar facilmente e com um alto n\u00edvel de confian\u00e7a a maioria dos pontos que apresentamos. Se queremos enganar o discriminador, precisamos aproximar a distribui\u00e7\u00e3o gerada da verdadeira. O discriminador ter\u00e1 mais dificuldade em prever a classe quando as duas distribui\u00e7\u00f5es ser\u00e3o iguais em todos os pontos: nesse caso, para cada ponto, haver\u00e1 chances iguais de ser \u201cverdadeiro\u201d ou \u201cgerado\u201d e, em seguida, o discriminador poder\u00e1 \u201d fazer melhor do que ser verdadeiro em um caso em dois em m\u00e9dia.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"norm2\" class=\"aligncenter wp-image-1481 size-large\" data-attachment-id=\"1481\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"norm2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?fit=1024%2C957\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?fit=300%2C280\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?fit=3791%2C3542\" data-orig-size=\"3791,3542\" data-permalink=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/norm2/\" data-recalc-dims=\"1\" height=\"957\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=1024%2C957\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=1024%2C957 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=300%2C280 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=768%2C718 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=200%2C187 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?resize=690%2C645 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?w=2340 2340w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/norm2.jpeg?w=3510 3510w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nesse ponto, parece leg\u00edtimo se perguntar se esse m\u00e9todo indireto \u00e9 realmente uma boa ideia. De fato, parece ser mais complicado (temos que otimizar o gerador com base em uma tarefa ajustada, em vez de diretamente nas distribui\u00e7\u00f5es) e requer um discriminador que consideramos aqui como um determinado or\u00e1culo, mas que, na realidade, n\u00e3o \u00e9 conhecido. Nem perfeito. Para o primeiro ponto, a dificuldade de comparar diretamente duas distribui\u00e7\u00f5es de probabilidade com base em amostras contrabalan\u00e7a a aparente maior complexidade do m\u00e9todo indireto. Para o segundo ponto, \u00e9 \u00f3bvio que o discriminador n\u00e3o \u00e9 conhecido. No entanto, pode ser aprendido!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <strong>\n   <span style=\"color: #000000;\">\n    A aproxima\u00e7\u00e3o: redes neurais advers\u00e1rias\n   </span>\n  </strong>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos agora descrever a forma espec\u00edfica que assume o gerador e o discriminador na arquitetura das GANs. O gerador \u00e9 uma rede neural que modela uma fun\u00e7\u00e3o de transforma\u00e7\u00e3o. Ele assume como entrada uma vari\u00e1vel aleat\u00f3ria simples e deve retornar, uma vez treinada, uma vari\u00e1vel aleat\u00f3ria que segue a distribui\u00e7\u00e3o de destino. Como \u00e9 muito complicado e desconhecido, decidimos modelar o discriminador com outra rede neural. Essa rede neural modela uma fun\u00e7\u00e3o discriminativa. Ele toma como entrada um ponto (no nosso exemplo de cachorro, um vetor dimensional N) e retorna como sa\u00edda a probabilidade desse ponto ser \u201cverdadeiro\u201d.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Observe que o fato de impormos agora um modelo parametrizado para expressar tanto o gerador quanto o discriminador (em vez das vers\u00f5es idealizadas na subse\u00e7\u00e3o anterior) n\u00e3o tem, na pr\u00e1tica, um grande impacto no argumento / intui\u00e7\u00e3o te\u00f3rica acima: apenas trabalhamos em alguns espa\u00e7os parametrizados em vez de espa\u00e7os completos ideais e, portanto, os pontos ideais que devemos alcan\u00e7ar no caso ideal podem ser vistos como \u201carredondados\u201d pela capacidade de precis\u00e3o dos modelos parametrizados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Uma vez definidas, as duas redes podem ser treinadas em conjunto (ao mesmo tempo) com objetivos opostos:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    O objetivo do gerador \u00e9 enganar o discriminador; portanto, a rede neural generativa \u00e9 treinada para maximizar o erro de classifica\u00e7\u00e3o final (entre dados verdadeiros e gerados).\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    O objetivo do discriminador \u00e9 detectar dados falsos gerados, para que a rede neural discriminativa seja treinada para minimizar o erro de classifica\u00e7\u00e3o final.\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Portanto, a cada itera\u00e7\u00e3o do processo de treinamento, os pesos da rede generativa s\u00e3o atualizados para aumentar o erro de classifica\u00e7\u00e3o (subida do gradiente de erro sobre os par\u00e2metros do gerador), enquanto os pesos da rede discriminativa s\u00e3o atualizados para diminuir esse erro (descida do gradiente de erro sobre os par\u00e2metros do discriminador).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Esses objetivos opostos e a no\u00e7\u00e3o impl\u00edcita de treinamento antag\u00f4nico das duas redes explicam o nome de \u201credes advers\u00e1rias\u201d: ambas as redes tentam se derrotar e, ao faz\u00ea-lo, est\u00e3o cada vez melhor. A competi\u00e7\u00e3o entre elas faz com que essas duas redes \u201cprogridam\u201d com rela\u00e7\u00e3o a seus respectivos objetivos. Do ponto de vista da teoria dos jogos, podemos pensar nessa configura\u00e7\u00e3o como um jogo minimax para dois jogadores, em que o estado de equil\u00edbrio corresponde \u00e0 situa\u00e7\u00e3o em que o gerador produz dados a partir da distribui\u00e7\u00e3o exata e onde o discriminador prediz \u201cverdadeiro\u201d ou \u201cgerado\u201d com probabilidade 1/2 para qualquer ponto que receber.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Os modelos GAN est\u00e3o entre os mais avan\u00e7ados em Deep Learning e a ideia por tr\u00e1s da sua concep\u00e7\u00e3o \u00e9 simples e brilhante.\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Agora sim, estamos prontos para compreender a Matem\u00e1tica que faz tudo isso acontecer!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Est\u00e1 gostando do Deep Learning Book? Ent\u00e3o ajude a continuarmos esse trabalho e compartilhe o Deep Learning Book entre seus amigos. Quanto mais dividimos o conhecimento, mas ele se multiplica.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 o pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://skymind.ai/wiki/generative-adversarial-network-gan\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     A Beginner\u2019s Guide to Generative Adversarial Networks (GANs)\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     A Leap into the Future: Generative Adversarial Networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Understanding Generative Adversarial Networks (GANs)\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     How A.I. Is Creating Building Blocks to Reshape Music and Art\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent neural network based language model\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1479\" href=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1479\" href=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1479\" href=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1479\" href=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1479-5e0dd1d5ea4ca\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1479&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1479-5e0dd1d5ea4ca\" id=\"like-post-wrapper-140353593-1479-5e0dd1d5ea4ca\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "57": "<h1 class=\"entry-title\" id=\"capitulo-57\">\n Cap\u00edtulo 57 \u2013 Os Detalhes Matem\u00e1ticos das GANs (Generative Adversarial Networks)\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste cap\u00edtulo vamos concluir nosso estudo das GANs com os detalhes matem\u00e1ticos, antes de avan\u00e7ar para outra arquitetura de Deep Learning que estudaremos na sequ\u00eancia. Estamos considerando que voc\u00ea leu os cap\u00edtulos anteriores sobre GANs.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A modelagem de redes neurais requer essencialmente definir duas coisas: uma arquitetura e uma fun\u00e7\u00e3o de perda. J\u00e1 descrevemos a\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/modelos-generativos-o-diferencial-das-gans-generative-adversarial-networks/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     arquitetura de redes advers\u00e1rias generativas\n    </a>\n   </span>\n   . Consiste em duas redes:\n  </span>\n </p>\n <ol>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Uma rede generativa G que recebe uma entrada aleat\u00f3ria z com densidade p_z e retorna uma sa\u00edda x_g = G (z) que deve seguir (ap\u00f3s o treinamento) a distribui\u00e7\u00e3o de probabilidade alvo.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Uma rede discriminativa D que recebe uma entrada x que pode ser uma entrada \u201cverdadeira\u201d (x_t, cuja densidade \u00e9 denotada p_t) ou uma entrada \u201cgerada\u201d (x_g, cuja densidade p_g \u00e9 a densidade induzida pela densidade p_z atrav\u00e9s de G) e que retorna a probabilidade D (x) de x para ser um dado \u201cverdadeiro\u201d.\n   </span>\n  </li>\n </ol>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos agora examinar mais de perto a fun\u00e7\u00e3o de perda \u201cte\u00f3rica\u201d das GANs. Se enviarmos ao discriminador dados \u201cverdadeiros\u201d e \u201cgerados\u201d nas mesmas propor\u00e7\u00f5es, o erro absoluto esperado do discriminador poder\u00e1 ser expresso como:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter wp-image-1513 size-full\" data-attachment-id=\"1513\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?fit=854%2C178\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?fit=300%2C63\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?fit=854%2C178\" data-orig-size=\"854,178\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form1-10/\" data-recalc-dims=\"1\" height=\"178\" sizes=\"(max-width: 854px) 100vw, 854px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=854%2C178\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?w=854 854w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=300%2C63 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=768%2C160 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=200%2C42 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form1-1.png?resize=690%2C144 690w\" width=\"854\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O objetivo do gerador \u00e9 enganar o discriminador cujo objetivo \u00e9 ser capaz de distinguir entre dados verdadeiros e dados gerados. Portanto, ao treinar o gerador, queremos maximizar esse erro enquanto tentamos minimiz\u00e1-lo para o discriminador. Isso nos d\u00e1 a f\u00f3rmula abaixo:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-1514\" data-attachment-id=\"1514\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?fit=412%2C154\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?fit=300%2C112\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?fit=412%2C154\" data-orig-size=\"412,154\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form2-16/\" data-recalc-dims=\"1\" height=\"154\" sizes=\"(max-width: 412px) 100vw, 412px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?resize=412%2C154\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?w=412 412w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?resize=300%2C112 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form2.jpg?resize=200%2C75 200w\" width=\"412\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para qualquer gerador G (juntamente com a densidade de probabilidade induzida p_g), o melhor discriminador poss\u00edvel \u00e9 aquele que minimiza a integral abaixo:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form3\" class=\"aligncenter wp-image-1515 size-large\" data-attachment-id=\"1515\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?fit=1024%2C79\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?fit=300%2C23\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?fit=1188%2C92\" data-orig-size=\"1188,92\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form3-14/\" data-recalc-dims=\"1\" height=\"79\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=1024%2C79\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=1024%2C79 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=300%2C23 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=768%2C59 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=200%2C15 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?resize=690%2C53 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form3-1.png?w=1188 1188w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para minimizar (em rela\u00e7\u00e3o a D) essa integral, podemos minimizar a fun\u00e7\u00e3o dentro da integral para cada valor de x. Em seguida, definimos o melhor discriminador poss\u00edvel para um determinado gerador:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form4\" class=\"aligncenter size-full wp-image-1516\" data-attachment-id=\"1516\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?fit=272%2C108\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?fit=272%2C108\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?fit=272%2C108\" data-orig-size=\"272,108\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form4-10/\" data-recalc-dims=\"1\" height=\"108\" sizes=\"(max-width: 272px) 100vw, 272px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?resize=272%2C108\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?w=272 272w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form4.jpg?resize=200%2C79 200w\" width=\"272\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   (de fato, um dos melhores porque x valores tais que p_t (x) = p_g (x) podem ser manipulados de outra maneira, mas isso n\u00e3o importa para o que segue). Em seguida, pesquisamos G que maximiza:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form5\" class=\"aligncenter wp-image-1517 size-large\" data-attachment-id=\"1517\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?fit=1024%2C87\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?fit=300%2C25\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?fit=1084%2C92\" data-orig-size=\"1084,92\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form5-8/\" data-recalc-dims=\"1\" height=\"87\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=1024%2C87\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=1024%2C87 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=300%2C25 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=768%2C65 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=200%2C17 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?resize=690%2C59 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form5-1.png?w=1084 1084w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Parece complexo? \u00c9 menos do que parece e explicamos sobre os fundamentos matem\u00e1ticos por tr\u00e1s dessas f\u00f3rmulas no curso\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Matem\u00e1tica Para Machine Learning\n    </a>\n   </span>\n   . Novamente, para maximizar (em rela\u00e7\u00e3o a G) essa integral, podemos maximizar a fun\u00e7\u00e3o dentro da integral para cada valor de x. Como a densidade p_t \u00e9 independente do gerador G, n\u00e3o podemos fazer melhor do que definir G de modo que:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form7\" class=\"aligncenter size-full wp-image-1518\" data-attachment-id=\"1518\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form7\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?fit=326%2C112\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?fit=300%2C103\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?fit=326%2C112\" data-orig-size=\"326,112\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form7-4/\" data-recalc-dims=\"1\" height=\"112\" sizes=\"(max-width: 326px) 100vw, 326px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?resize=326%2C112\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?w=326 326w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?resize=300%2C103 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form7.jpg?resize=200%2C69 200w\" width=\"326\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Obviamente, como p_g \u00e9 uma densidade de probabilidade que deve se integrar a 1, necessariamente temos o melhor G:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form8\" class=\"aligncenter size-full wp-image-1519\" data-attachment-id=\"1519\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form8\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?fit=280%2C74\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?fit=280%2C74\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?fit=280%2C74\" data-orig-size=\"280,74\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form8-4/\" data-recalc-dims=\"1\" height=\"74\" sizes=\"(max-width: 280px) 100vw, 280px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?resize=280%2C74\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?w=280 280w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form8.jpg?resize=200%2C53 200w\" width=\"280\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim, mostramos que, em um caso ideal com gerador e discriminador de capacidade ilimitada, o ponto ideal do cen\u00e1rio advers\u00e1rio \u00e9 tal que o gerador produz a mesma densidade que a densidade real e o discriminador n\u00e3o pode fazer melhor do que ser verdadeiro em um caso a cada dois, exatamente como a intui\u00e7\u00e3o nos disse. Por fim, observe tamb\u00e9m que G maximiza:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form6\" class=\"aligncenter wp-image-1520 size-large\" data-attachment-id=\"1520\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form6\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?fit=1024%2C90\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?fit=300%2C26\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?fit=1112%2C98\" data-orig-size=\"1112,98\" data-permalink=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/form6-5/\" data-recalc-dims=\"1\" height=\"90\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=1024%2C90\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=1024%2C90 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=300%2C26 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=768%2C68 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=200%2C18 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?resize=690%2C61 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/10/form6.png?w=1112 1112w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na f\u00f3rmula acima vemos que G deseja maximizar a probabilidade esperada de o discriminador estar errado.\n  </span>\n </p>\n <p>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Conclus\u00e3o Sobre as GANs\n  </span>\n </h3>\n <p>\n  <span style=\"color: #000000;\">\n   As GANs s\u00e3o bem recentes e possuem uma ideia inovadora sobre como treinar uma arquitetura de rede neural. Muitos estudos e aplica\u00e7\u00f5es vem sendo feitos em todo mundo e j\u00e1 existem at\u00e9 algumas aplica\u00e7\u00f5es comerciais usando essa arquitetura de Deep Learning. Caso queira aprender a construir GANs na pr\u00e1tica, acesse\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n     aqui\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Pontos mais importantes sobre as GANs:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Computadores podem basicamente gerar vari\u00e1veis \u200b\u200bpseudo-aleat\u00f3rias simples (por exemplo, eles podem gerar vari\u00e1veis \u200b\u200bque seguem muito de perto uma distribui\u00e7\u00e3o uniforme).\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Existem maneiras diferentes de gerar vari\u00e1veis \u200b\u200baleat\u00f3rias mais complexas, incluindo a no\u00e7\u00e3o de \u201cm\u00e9todo de transforma\u00e7\u00e3o\u201d que consiste em expressar uma vari\u00e1vel aleat\u00f3ria em fun\u00e7\u00e3o de algumas vari\u00e1veis \u200b\u200baleat\u00f3rias mais simples.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    No aprendizado de m\u00e1quina, os modelos generativos tentam gerar dados de uma determinada distribui\u00e7\u00e3o de probabilidade (complexa).\n   </span>\n   <span style=\"color: #000000;\">\n    Modelos generativos de aprendizado profundo s\u00e3o modelados como redes neurais (fun\u00e7\u00f5es muito complexas) que recebem como entrada uma vari\u00e1vel aleat\u00f3ria simples e retornam uma vari\u00e1vel aleat\u00f3ria que segue a distribui\u00e7\u00e3o direcionada (como \u201cm\u00e9todo de transforma\u00e7\u00e3o\u201d).\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Essas redes generativas podem ser treinadas \u201cdiretamente\u201d (comparando a distribui\u00e7\u00e3o dos dados gerados com a verdadeira distribui\u00e7\u00e3o): esta \u00e9 a ideia das redes correspondentes generativas\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Essas redes generativas tamb\u00e9m podem ser treinadas \u201cindiretamente\u201d (tentando enganar outra rede treinada ao mesmo tempo para distinguir dados \u201cgerados\u201d de dados \u201cverdadeiros\u201d): essa \u00e9 a ideia das redes adversas generativas.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    <span style=\"color: #000000;\">\n     Mesmo que o \u201chype\u201d que rodeia os GANs seja talvez um pouco exagerado, podemos dizer que a ideia de treinamento antag\u00f4nico sugerida por Ian Goodfellow e seus co-autores \u00e9 realmente \u00f3tima. Essa maneira de distorcer a fun\u00e7\u00e3o de perda, passando de uma compara\u00e7\u00e3o direta para uma indireta, \u00e9 realmente algo que pode ser muito inspirador para futuros trabalhos na \u00e1rea de aprendizado profundo.\n    </span>\n   </span>\n   <p style=\"text-align: justify;\">\n   </p>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://skymind.ai/wiki/generative-adversarial-network-gan\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     A Beginner\u2019s Guide to Generative Adversarial Networks (GANs)\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://medium.com/datadriveninvestor/a-leap-into-the-future-generative-adversarial-networks-96a780ed8ee6\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     A Leap into the Future: Generative Adversarial Networks\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Understanding Generative Adversarial Networks (GANs)\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     How A.I. Is Creating Building Blocks to Reshape Music and Art\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1705.08741.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks and Deep Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent neural network based language model\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1512\" href=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1512\" href=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1512\" href=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1512\" href=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/os-detalhes-matematicos-das-gans-generative-adversarial-networks/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1512-5e0dd1d80a886\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1512&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1512-5e0dd1d80a886\" id=\"like-post-wrapper-140353593-1512-5e0dd1d80a886\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "58": "<h1 class=\"entry-title\" id=\"capitulo-58\">\n Cap\u00edtulo 58 \u2013 Introdu\u00e7\u00e3o aos Autoencoders\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 aqui neste livro descrevemos arquiteturas de redes neurais profundas (Deep Learning) usadas em aprendizagem supervisionada, na qual rotulamos exemplos de treinamento, fornecendo ao algoritmo dados de entrada (X) e de sa\u00edda (Y). Agora, suponha que tenhamos apenas um conjunto de exemplos de treinamento n\u00e3o rotulados {x1, x2, x3,\u2026}, onde x (i) \u2208\u211cn. Uma rede neural Autoencoder \u00e9 um algoritmo de aprendizado n\u00e3o supervisionado que aplica backpropagation, definindo os valores de destino como iguais \u00e0s entradas. Ou seja, usa y (i) = x (i). Estudaremos essa arquitetura de Deep Learning a partir de agora.\n  </span>\n </p>\n <h3>\n  <span style=\"color: #000000;\">\n   O que s\u00e3o Autoencoders?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os Autoencoders s\u00e3o uma t\u00e9cnica de aprendizado n\u00e3o supervisionado, na qual usamos as redes neurais para a tarefa de aprendizado de representa\u00e7\u00e3o. Especificamente, projetaremos uma arquitetura de rede neural de modo a impor um\n   <em>\n    gargalo\n   </em>\n   na rede que for\u00e7a uma representa\u00e7\u00e3o de conhecimento compactada da entrada original. Se os recursos de entrada fossem independentes um do outro, essa compress\u00e3o e reconstru\u00e7\u00e3o subsequente seriam uma tarefa muito dif\u00edcil. No entanto, se houver algum tipo de estrutura nos dados (ou seja, correla\u00e7\u00f5es entre os recursos de entrada), essa estrutura poder\u00e1 ser aprendida e consequentemente aproveitada ao for\u00e7ar a entrada atrav\u00e9s do\n   <em>\n    gargalo\n   </em>\n   da rede.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Autoencoders (AE) s\u00e3o redes neurais que visam copiar suas entradas para suas sa\u00eddas. Eles trabalham compactando a entrada em uma representa\u00e7\u00e3o de espa\u00e7o latente e, em seguida, reconstruindo a sa\u00edda dessa representa\u00e7\u00e3o. Esse tipo de rede \u00e9 composto de duas partes:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Codificador (Encoder)\n   </strong>\n   : \u00e9 a parte da rede que compacta a entrada em uma representa\u00e7\u00e3o de espa\u00e7o latente (codificando a entrada). Pode ser representado por uma fun\u00e7\u00e3o de codifica\u00e7\u00e3o h = f (x).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Decodificador (Decoder)\n   </strong>\n   : Esta parte tem como objetivo reconstruir a entrada da representa\u00e7\u00e3o do espa\u00e7o latente. Pode ser representado por uma fun\u00e7\u00e3o de decodifica\u00e7\u00e3o r = g (h).\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"mushroom_encoder\" class=\"aligncenter wp-image-1545 size-full\" data-attachment-id=\"1545\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"mushroom_encoder\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?fit=878%2C341\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?fit=300%2C117\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?fit=878%2C341\" data-orig-size=\"878,341\" data-permalink=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/mushroom_encoder/\" data-recalc-dims=\"1\" height=\"341\" sizes=\"(max-width: 878px) 100vw, 878px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=878%2C341\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?w=878 878w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=300%2C117 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=768%2C298 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=200%2C78 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/mushroom_encoder.png?resize=690%2C268 690w\" width=\"878\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Autoencoder tenta aprender uma fun\u00e7\u00e3o h W, b (x) \u2248 x. Em outras palavras, ele est\u00e1 tentando aprender uma aproxima\u00e7\u00e3o com a fun\u00e7\u00e3o de identidade, de modo a gerar x\u2019 semelhante a x. A fun\u00e7\u00e3o de identidade parece uma fun\u00e7\u00e3o particularmente trivial para tentar aprender; mas colocando restri\u00e7\u00f5es na rede, como limitando o n\u00famero de unidades ocultas, podemos descobrir uma estrutura interessante sobre os dados. Como um exemplo concreto, suponha que as entradas x sejam os valores de intensidade de pixel de uma imagem 10 \u00d7 10 (100 pixels), portanto n = 100 e haja s = 50 unidades ocultas na camada L2. Observe que tamb\u00e9m temos y\u2208\u211c100. Como existem apenas 50 unidades ocultas, a rede \u00e9 for\u00e7ada a aprender uma representa\u00e7\u00e3o \u201ccompactada\u201d da entrada. Ou seja, dado apenas o vetor de ativa\u00e7\u00f5es de unidades ocultas, ele deve tentar \u201dreconstruir\u201d a entrada de 100 pixels x. Se a entrada fosse completamente aleat\u00f3ria \u2013 digamos, cada xi prov\u00e9m de um ID Gaussiano independente dos outros recursos, essa tarefa de compacta\u00e7\u00e3o seria muito dif\u00edcil. Mas se houver estrutura nos dados, por exemplo, se alguns dos recursos de entrada estiverem correlacionados, esse algoritmo poder\u00e1 descobrir algumas dessas correla\u00e7\u00f5es. De fato, esse Autoencoder simples geralmente acaba aprendendo uma representa\u00e7\u00e3o de baixa dimens\u00e3o muito semelhante aos PCAs (\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Principal Component Analysis\n    </a>\n   </span>\n   ).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nosso argumento acima se baseava no n\u00famero de unidades ocultas s sendo pequenas. Mas mesmo quando o n\u00famero de unidades ocultas \u00e9 grande (talvez at\u00e9 maior que o n\u00famero de pixels de entrada), ainda podemos descobrir uma estrutura interessante, impondo outras restri\u00e7\u00f5es \u00e0 rede. Em particular, se impusermos uma restri\u00e7\u00e3o de \u201cesparsidade\u201d nas unidades ocultas, o Autoencoder ainda descobrir\u00e1 uma estrutura interessante nos dados, mesmo que o n\u00famero de unidades ocultas seja grande.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <img alt=\"autoencoders\" class=\"aligncenter wp-image-1537 size-large\" data-attachment-id=\"1537\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"autoencoders\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?fit=1024%2C252\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?fit=300%2C74\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?fit=1248%2C307\" data-orig-size=\"1248,307\" data-permalink=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/autoencoders/\" data-recalc-dims=\"1\" height=\"252\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=1024%2C252\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=1024%2C252 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=300%2C74 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=768%2C189 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=200%2C49 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?resize=690%2C170 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/autoencoders.png?w=1248 1248w\" width=\"1024\"/>\n </p>\n <p>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Por que copiar a entrada para a sa\u00edda?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Se o \u00fanico objetivo dos Autoencoders fosse copiar a entrada para a sa\u00edda, eles seriam in\u00fateis. De fato, esperamos que, treinando o Autoencoder para copiar a entrada para a sa\u00edda, a representa\u00e7\u00e3o latente h tenha propriedades \u00fateis.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Isso pode ser conseguido criando restri\u00e7\u00f5es na tarefa de c\u00f3pia. Uma maneira de obter recursos \u00fateis do Autoencoder \u00e9 restringir h a ter dimens\u00f5es menores que x; nesse caso, o Autoencoder \u00e9 chamado de incompleto. Ao treinar uma representa\u00e7\u00e3o incompleta, for\u00e7amos o Autoencoder a aprender os recursos mais importantes dos dados de treinamento. Se for dada muita capacidade ao Autoencoder, ele poder\u00e1 aprender a executar a tarefa de c\u00f3pia sem extrair nenhuma informa\u00e7\u00e3o \u00fatil sobre a distribui\u00e7\u00e3o dos dados. Isso tamb\u00e9m pode ocorrer se a dimens\u00e3o da representa\u00e7\u00e3o latente for a mesma que a entrada e, no caso de excesso de conclus\u00e3o, em que a dimens\u00e3o da representa\u00e7\u00e3o latente for maior que a entrada. Nesses casos, mesmo um codificador linear e um decodificador linear podem aprender a copiar a entrada na sa\u00edda sem aprender nada \u00fatil sobre a distribui\u00e7\u00e3o de dados. Idealmente, algu\u00e9m poderia treinar qualquer arquitetura de Autoencoder com sucesso, escolhendo a dimens\u00e3o do c\u00f3digo e a capacidade do codificador e decodificador com base na complexidade da distribui\u00e7\u00e3o a ser modelada.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para que s\u00e3o usados \u200b\u200bos Autoencoders?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Atualmente, o denoising de dados (remo\u00e7\u00e3o de ru\u00eddos) e a redu\u00e7\u00e3o de dimensionalidade para visualiza\u00e7\u00e3o de dados s\u00e3o considerados duas principais aplica\u00e7\u00f5es pr\u00e1ticas interessantes de Autoencoders. Com restri\u00e7\u00f5es de dimensionalidade e esparsidade apropriadas, os Autoencoders podem aprender proje\u00e7\u00f5es de dados mais interessantes que o PCA ou outras t\u00e9cnicas b\u00e1sicas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os Autoencoders aprendem automaticamente a partir de exemplos de dados. Isso significa que \u00e9 f\u00e1cil treinar inst\u00e2ncias especializadas do algoritmo que ter\u00e3o bom desempenho em um tipo espec\u00edfico de entrada e que n\u00e3o requer nenhuma nova engenharia de recursos, apenas os dados de treinamento apropriados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Outra aplica\u00e7\u00e3o dos Autoencoders \u00e9 como tarefa preliminar ao reconhecimento de imagens com CNNs (Redes Neurais Convolucionais). Observando a imagem acima voc\u00ea percebe que a sa\u00edda do Autoencoder \u00e9 a imagem do n\u00famero 4 muito mais suave. Ou seja, aplicamos os Autoencoders para remover ru\u00eddo dos dados (denoising) e depois usamos a sa\u00edda dos Autoencoders para treinar um modelo CNN.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os Autoencoders s\u00e3o treinados para preservar o m\u00e1ximo de informa\u00e7\u00f5es poss\u00edvel quando uma entrada \u00e9 passada pelo codificador e depois pelo decodificador, mas tamb\u00e9m s\u00e3o treinados para fazer com que a nova representa\u00e7\u00e3o tenha v\u00e1rias propriedades agrad\u00e1veis. Diferentes tipos de Autoencoders visam atingir diferentes tipos de propriedades.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Autoencoders s\u00e3o estudados na pr\u00e1tica em\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Deep Learning II\n    </a>\n    .\n   </span>\n   Nos pr\u00f3ximos cap\u00edtulos estudaremos os detalhes matem\u00e1ticos e estat\u00edsticos dessa fascinante arquitetura de Deep Learning.\n  </span>\n </p>\n <p>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Autoencoders \u2013 Unsupervised Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep inside: Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.jeremyjordan.me/autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Introduction to Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent neural network based language model\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1536\" href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1536\" href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1536\" href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1536\" href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1536-5e0dd1da07767\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1536&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1536-5e0dd1da07767\" id=\"like-post-wrapper-140353593-1536-5e0dd1da07767\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "59": "<h1 class=\"entry-title\" id=\"capitulo-59\">\n Cap\u00edtulo 59 \u2013 Principais Tipos de Redes Neurais Artificiais Autoencoders\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste cap\u00edtulo vamos estudar os tipos principais de Autoencoders (estamos considerando que voc\u00ea leu o cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   ):\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os Autoencoders codificam os valores de entrada x usando uma fun\u00e7\u00e3o f. Em seguida, decodificam os valores codificados f (x) usando uma fun\u00e7\u00e3o g para criar valores de sa\u00edda id\u00eanticos aos valores de entrada. O objetivo do Autoencoder \u00e9 minimizar o erro de reconstru\u00e7\u00e3o entre a entrada e a sa\u00edda. Isso ajuda os Autoencoders a aprender os recursos importantes presentes nos dados. Quando uma representa\u00e7\u00e3o permite uma boa reconstru\u00e7\u00e3o de sua entrada, ela ret\u00e9m grande parte das informa\u00e7\u00f5es presentes na entrada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E existem diferentes tipos de Autoencoders. Confira:\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   1. Autoencoder Padr\u00e3o\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na sua forma mais simples, o Autoencoder \u00e9 uma rede neural artificial de tr\u00eas camadas, isto \u00e9, uma rede neural com uma camada de entrada, uma oculta e uma camada de sa\u00edda. A entrada e a sa\u00edda s\u00e3o as mesmas e aprendemos a reconstruir a entrada, por exemplo, usando o otimizador adam e a fun\u00e7\u00e3o de perda de erro quadr\u00e1tico m\u00e9dio.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   2. Autoencoder Multicamada\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Se uma camada oculta n\u00e3o for suficiente, obviamente podemos estender o Autoencoder para mais camadas ocultas. Nossa implementa\u00e7\u00e3o poderia usar 3 camadas ocultas em vez de apenas uma. Qualquer uma das camadas ocultas pode ser escolhida como representa\u00e7\u00e3o de recurso, mas o ideal \u00e9 tornar a rede sim\u00e9trica e usar a camada mais intermedi\u00e1ria.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   3. Autoencoder Convolucional\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Tamb\u00e9m podemos nos perguntar: os Autoencoders podem ser usados com convolu\u00e7\u00f5es em vez de camadas totalmente conectadas?\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A resposta \u00e9 sim e o princ\u00edpio \u00e9 o mesmo, mas usando imagens (vetores 3D) em vez de vetores 1D\n   <em>\n    achatados (flattened)\n   </em>\n   . A imagem de entrada \u00e9 reduzida para fornecer uma representa\u00e7\u00e3o latente de dimens\u00f5es menores e for\u00e7ar o Autoencoder a aprender uma vers\u00e3o compactada das imagens.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   4. Autoencoder Regularizado\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Existem outras maneiras pelas quais podemos restringir a reconstru\u00e7\u00e3o de um Autoencoder, al\u00e9m de impor uma camada oculta de menor dimens\u00e3o que a entrada. Em vez de limitar a capacidade do modelo mantendo o codificador e o decodificador rasos e o tamanho do c\u00f3digo pequeno, os Autoencoders regularizados usam uma fun\u00e7\u00e3o de perda que incentiva o modelo a ter outras propriedades al\u00e9m da capacidade de copiar sua entrada para sua sa\u00edda. Na pr\u00e1tica, geralmente encontramos dois tipos de Autoencoder Regularizado: o Autoencoder Esparso e o Autoencoder Denoising.\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   4.1. Autoencoder Esparso\n  </span>\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os Autoencoders Esparsos geralmente s\u00e3o usados \u200b\u200bpara aprender recursos para outra tarefa, como classifica\u00e7\u00e3o. Um Autoencoder que foi regularizado para ser esparso deve responder a recursos estat\u00edsticos exclusivos do conjunto de dados em que foi treinado, em vez de simplesmente atuar como uma fun\u00e7\u00e3o de identidade. Dessa forma, o treinamento para executar a tarefa de c\u00f3pia com uma penalidade de escassez pode produzir um modelo que aprendeu recursos \u00fateis como subproduto.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Outra maneira de restringir a reconstru\u00e7\u00e3o do Autoencoder \u00e9 impor uma restri\u00e7\u00e3o \u00e0 sua perda. Poder\u00edamos, por exemplo, adicionar um termo de reguraliza\u00e7\u00e3o na fun\u00e7\u00e3o de perda. Isso far\u00e1 com que nosso Autoencoder aprenda representa\u00e7\u00e3o esparsa de dados.\n  </span>\n  <span style=\"color: #000000;\">\n   Em nossa camada oculta, podemos adicionar um regularizador de atividades L1, que aplicar\u00e1 uma penalidade na fun\u00e7\u00e3o de perda durante a fase de otimiza\u00e7\u00e3o. Como resultado, a representa\u00e7\u00e3o ser\u00e1 mais esparsa em compara\u00e7\u00e3o com o Autoencoder Padr\u00e3o. Abaixo uma representa\u00e7\u00e3o do Autoencoder Esparso:\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n </h4>\n <h4 style=\"text-align: justify;\">\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"esparso\" class=\"aligncenter size-full wp-image-1567\" data-attachment-id=\"1567\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"esparso\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?fit=495%2C693\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?fit=214%2C300\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?fit=495%2C693\" data-orig-size=\"495,693\" data-permalink=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/esparso/\" data-recalc-dims=\"1\" height=\"693\" sizes=\"(max-width: 495px) 100vw, 495px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?resize=495%2C693\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?w=495 495w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?resize=214%2C300 214w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/esparso.png?resize=200%2C280 200w\" width=\"495\"/>\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n </h4>\n <h4 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   4.2. Autoencoder Denoising\n  </span>\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em vez de adicionar uma penalidade \u00e0 fun\u00e7\u00e3o de perda, podemos obter um Autoencoder que aprende algo \u00fatil alterando o termo do erro de reconstru\u00e7\u00e3o da fun\u00e7\u00e3o de perda. Isso pode ser feito adicionando algum ru\u00eddo \u00e0 imagem de entrada e fazendo o Autoencoder aprender a remov\u00ea-la. Dessa maneira, o Autoencoder extrair\u00e1 os recursos mais importantes e aprender\u00e1 uma representa\u00e7\u00e3o robusta dos dados.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Denoising refere-se \u00e0 adi\u00e7\u00e3o intencional de ru\u00eddo \u00e0 entrada bruta antes de fornec\u00ea-la \u00e0 rede. Pode-se obter denoising usando o mapeamento estoc\u00e1stico. Abaixo uma representa\u00e7\u00e3o do Autoencoder Denoising:\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n </h4>\n <h4 style=\"text-align: justify;\">\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"denoising\" class=\"aligncenter size-full wp-image-1568\" data-attachment-id=\"1568\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"denoising\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?fit=414%2C516\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?fit=241%2C300\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?fit=414%2C516\" data-orig-size=\"414,516\" data-permalink=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/denoising/\" data-recalc-dims=\"1\" height=\"516\" sizes=\"(max-width: 414px) 100vw, 414px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?resize=414%2C516\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?w=414 414w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?resize=241%2C300 241w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/denoising.png?resize=200%2C249 200w\" width=\"414\"/>\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n </h3>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   5. Contractive Autoencoders(CAE)\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O objetivo do Autoencoder Contrativo (CAE) \u00e9 ter uma representa\u00e7\u00e3o aprendida robusta, menos sens\u00edvel a pequenas varia\u00e7\u00f5es nos dados. A robustez da representa\u00e7\u00e3o para os dados \u00e9 feita aplicando um termo de penalidade \u00e0 fun\u00e7\u00e3o de perda. O termo da penalidade \u00e9 a norma Frobenius da matriz jacobiana. A norma de Frobenius da matriz jacobiana para a camada oculta \u00e9 calculada em rela\u00e7\u00e3o \u00e0 entrada. A norma de Frobenius da matriz jacobiana \u00e9 a soma do quadrado de todos os elementos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Autoencoder Contrativo \u00e9 outra t\u00e9cnica de regulariza\u00e7\u00e3o, como os Autoencoders Esparsos e os Autoencoders Denoising.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O CAE supera os resultados obtidos pela regulariza\u00e7\u00e3o do Autoencoder usando decaimento de peso ou denoising. O CAE \u00e9 uma escolha melhor do que o Autoencoder Denoising para aprender a extra\u00e7\u00e3o de recursos \u00fateis. O termo de penalidade gera mapeamento que contrai fortemente os dados e, portanto, o nome Autoencoder Contrativo.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   6. Deep Autoencoders\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Deep Autoencoders consistem em duas redes de cren\u00e7as profundas id\u00eanticas (Deep Belief Networks). Uma rede para codifica\u00e7\u00e3o e outra para decodifica\u00e7\u00e3o. Os Autoencoders tipicamente profundos t\u00eam de 4 a 5 camadas para codifica\u00e7\u00e3o e as pr\u00f3ximas 4 a 5 camadas para decodifica\u00e7\u00e3o. Usamos camada n\u00e3o supervisionada por camada, pr\u00e9-treinamento.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A M\u00e1quina Boltzmann Restrita (RBM) \u00e9 o alicerce b\u00e1sico das Deep Belief Networks. Na figura abaixo, tiramos uma imagem com 784 pixels. Treinamos usando uma pilha de 4 RBMs, desenrolamos e ajustamos com Backpropagation. A camada de codifica\u00e7\u00e3o final \u00e9 compacta e r\u00e1pida!\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n </h4>\n <h4 style=\"text-align: justify;\">\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"deep\" class=\"aligncenter size-full wp-image-1571\" data-attachment-id=\"1571\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"deep\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?fit=711%2C367\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?fit=300%2C155\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?fit=711%2C367\" data-orig-size=\"711,367\" data-permalink=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/deep/\" data-recalc-dims=\"1\" height=\"367\" sizes=\"(max-width: 711px) 100vw, 711px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=711%2C367\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?w=711 711w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=300%2C155 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=200%2C103 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/11/deep.png?resize=690%2C356 690w\" width=\"711\"/>\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n </h4>\n <h4 style=\"text-align: justify;\">\n </h4>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   7. Variational Autoencoders (VAEs)\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nos \u00faltimos anos, modelos generativos baseados em aprendizado profundo ganharam cada vez mais interesse devido a (e implicando) algumas melhorias surpreendentes no campo. Contando com uma enorme quantidade de dados, arquiteturas de rede bem projetadas e t\u00e9cnicas de treinamento inteligentes, os modelos geradores profundos demonstraram uma capacidade incr\u00edvel de produzir pe\u00e7as de conte\u00fado altamente realistas de v\u00e1rios tipos, como imagens, textos e sons. Entre esses modelos geradores profundos, duas fam\u00edlias principais se destacam e merecem uma aten\u00e7\u00e3o especial: Redes Advers\u00e1rias Generativas (GANs) e Autoencoders Variacionais (VAEs). As GANs j\u00e1 estudamos nos cap\u00edtulos anteriores.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   E a VAE \u00e9 t\u00e3o especial que merece um cap\u00edtulo inteiro. N\u00e3o perca o pr\u00f3ximo cap\u00edtulo.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000;\" target=\"_blank\">\n    <span style=\"text-decoration: underline;\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Autoencoders \u2013 Unsupervised Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep inside: Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://medium.com/datadriveninvestor/deep-learning-different-types-of-autoencoders-41d4fa5f7570\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep Learning \u2014 Different Types of Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.icml-2011.org/papers/455_icmlpaper.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Contractive Auto-Encoders \u2013 Explicit Invariance During Feature Extraction\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.jeremyjordan.me/autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Introduction to Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent neural network based language model\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1565\" href=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1565\" href=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1565\" href=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1565\" href=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/principais-tipos-de-redes-neurais-artificiais-autoencoders/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1565-5e0dd224c0f8b\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1565&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1565-5e0dd224c0f8b\" id=\"like-post-wrapper-140353593-1565-5e0dd224c0f8b\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "60": "<h1 class=\"entry-title\" id=\"capitulo-60\">\n Cap\u00edtulo 60 \u2013 Variational Autoencoders (VAEs) \u2013 Defini\u00e7\u00e3o, Redu\u00e7\u00e3o de Dimensionalidade, Espa\u00e7o Latente e Regulariza\u00e7\u00e3o\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nos \u00faltimos anos, modelos generativos baseados em aprendizado profundo ganharam cada vez mais interesse devido a (e implicando) algumas melhorias surpreendentes em Intelig\u00eancia Artificial. Contando com uma enorme quantidade de dados, arquiteturas de rede bem projetadas e t\u00e9cnicas de treinamento inteligentes, os modelos generativos profundos demonstraram uma capacidade incr\u00edvel de produzir pe\u00e7as de conte\u00fado altamente realistas de v\u00e1rios tipos, como imagens, textos e sons. Entre esses modelos, duas fam\u00edlias principais se destacam e merecem uma aten\u00e7\u00e3o especial: Redes Advers\u00e1rias Generativas (GANs) e Autoencoders Variacionais (VAEs). O primeiro j\u00e1 estudamos e agora estudaremos o segundo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em um\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/introducao-as-redes-adversarias-generativas-gans-generative-adversarial-networks/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     cap\u00edtulo anterior\n    </a>\n   </span>\n   , discutimos em profundidade as Redes Advers\u00e1rias Generativas (GANs) e mostramos, em particular, como o treinamento antag\u00f4nico pode opor duas redes, um gerador e um discriminador, para pressionar ambas a melhorar itera\u00e7\u00e3o ap\u00f3s itera\u00e7\u00e3o. Apresentamos agora, neste cap\u00edtulo, outro tipo de modelo generativo profundo: Autoencoders Variacionais (VAEs \u2013 Variational Autoencoders).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em resumo, um VAE \u00e9 um Autoencoder cuja distribui\u00e7\u00e3o de codifica\u00e7\u00f5es \u00e9 regularizada durante o treinamento, a fim de garantir que seu espa\u00e7o latente tenha boas propriedades, o que nos permite gerar novos dados. Al\u00e9m disso, o termo \u201cvariacional\u201d vem da estreita rela\u00e7\u00e3o que existe entre a regulariza\u00e7\u00e3o e o m\u00e9todo de infer\u00eancia variacional em Estat\u00edstica. Essa arquitetura de Deep Learning \u00e9 estudada na pr\u00e1tica em\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas o conceito por tr\u00e1s dos VAEs tamb\u00e9m pode levantar muitas quest\u00f5es. Qual \u00e9 o espa\u00e7o latente e por que regulariz\u00e1-lo? Como gerar novos dados a partir dos VAEs? Qual \u00e9 a liga\u00e7\u00e3o entre VAEs e infer\u00eancia variacional? Para descrever os VAEs da melhor maneira poss\u00edvel, tentaremos responder a todas essas perguntas (e muitas outras!) e fornecer a voc\u00ea o m\u00e1ximo de informa\u00e7\u00e3o e conhecimento poss\u00edvel (variando de intui\u00e7\u00f5es b\u00e1sicas a detalhes matem\u00e1ticos mais avan\u00e7ados). Assim, o objetivo deste cap\u00edtulo n\u00e3o \u00e9 apenas discutir as no\u00e7\u00f5es fundamentais dos Variational Autoencoders (VAEs), mas tamb\u00e9m construir passo a passo e come\u00e7ar desde o in\u00edcio o racioc\u00ednio que leva a essas no\u00e7\u00f5es (como sempre fazemos em nossos cursos na\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/todos-os-cursos-dsa\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Data Science Academy\n    </a>\n   </span>\n   ). E vamos come\u00e7ar com o conceito de redu\u00e7\u00e3o de dimensionalidade.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Que \u00e9 Redu\u00e7\u00e3o de Dimensionalidade?\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No aprendizado de m\u00e1quina, a redu\u00e7\u00e3o de dimensionalidade \u00e9 o processo de redu\u00e7\u00e3o do n\u00famero de recursos (atributos) que descrevem alguns dados. Essa redu\u00e7\u00e3o \u00e9 feita por sele\u00e7\u00e3o (apenas alguns recursos existentes s\u00e3o conservados) ou por extra\u00e7\u00e3o (um n\u00famero reduzido de novos recursos \u00e9 criado com base nos recursos antigos) e pode ser \u00fatil em muitas situa\u00e7\u00f5es que exigem dados de baixa dimensionalidade (visualiza\u00e7\u00e3o de dados, armazenamento, computa\u00e7\u00e3o pesada, etc\u2026). Embora existam muitos m\u00e9todos diferentes de redu\u00e7\u00e3o de dimensionalidade, podemos definir uma estrutura global que seja compat\u00edvel com a maioria desses m\u00e9todos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Primeiro, vamos codificar o processo que produz a representa\u00e7\u00e3o de \u201cnovos recursos\u201d a partir da representa\u00e7\u00e3o de \u201crecursos antigos\u201d (por sele\u00e7\u00e3o ou por extra\u00e7\u00e3o) e decodificar o processo inverso. A redu\u00e7\u00e3o de dimensionalidade pode ent\u00e3o ser interpretada como compacta\u00e7\u00e3o de dados, onde o codificador compacta os dados (do espa\u00e7o inicial para o espa\u00e7o codificado, tamb\u00e9m chamado de espa\u00e7o latente) enquanto o decodificador os descompacta. Obviamente, dependendo da distribui\u00e7\u00e3o inicial dos dados, da dimens\u00e3o do espa\u00e7o latente e da defini\u00e7\u00e3o do codificador, essa compacta\u00e7\u00e3o pode ser perdida, o que significa que uma parte da informa\u00e7\u00e3o \u00e9 perdida durante o processo de codifica\u00e7\u00e3o e n\u00e3o pode ser recuperada durante a decodifica\u00e7\u00e3o.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"reduc\" class=\"aligncenter size-large wp-image-1606\" data-attachment-id=\"1606\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"reduc\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?fit=1024%2C560\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?fit=300%2C164\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?fit=1586%2C868\" data-orig-size=\"1586,868\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/reduc/\" data-recalc-dims=\"1\" height=\"560\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=1024%2C560\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=1024%2C560 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=300%2C164 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=768%2C420 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=1536%2C841 1536w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=200%2C109 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?resize=690%2C378 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/reduc.jpg?w=1586 1586w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O principal objetivo de um m\u00e9todo de redu\u00e7\u00e3o de dimensionalidade \u00e9 encontrar o melhor par codificador / decodificador entre uma determinada fam\u00edlia. Em outras palavras, para um determinado conjunto de codificadores e decodificadores poss\u00edveis, estamos procurando o par que mant\u00e9m o m\u00e1ximo de informa\u00e7\u00f5es ao codificar e, portanto, tem o m\u00ednimo de erro de reconstru\u00e7\u00e3o ao decodificar. Se denotarmos respectivamente E e D as fam\u00edlias de codificadores e decodificadores que estamos considerando, o problema da redu\u00e7\u00e3o de dimensionalidade pode ser escrito:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter wp-image-1608 size-medium\" data-attachment-id=\"1608\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?fit=592%2C158\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?fit=300%2C80\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?fit=592%2C158\" data-orig-size=\"592,158\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/form1-11/\" data-recalc-dims=\"1\" height=\"80\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?resize=300%2C80\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?resize=300%2C80 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?resize=200%2C53 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1.jpg?w=592 592w\" width=\"300\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Onde:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter wp-image-1609 size-full\" data-attachment-id=\"1609\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?fit=270%2C108\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?fit=270%2C108\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?fit=270%2C108\" data-orig-size=\"270,108\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/form2-17/\" data-recalc-dims=\"1\" height=\"108\" sizes=\"(max-width: 270px) 100vw, 270px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?resize=270%2C108\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?w=270 270w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2.jpg?resize=200%2C80 200w\" width=\"270\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   define a medida do erro de reconstru\u00e7\u00e3o entre os dados de entrada x e os dados codificados e decodificados d (e (x)).\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   An\u00e1lise de Componentes Principais (PCA) e Autoencoders\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Um dos primeiros m\u00e9todos que v\u00eam \u00e0 mente quando se fala em redu\u00e7\u00e3o de dimensionalidade \u00e9 a an\u00e1lise de componentes principais (PCA). Para mostrar como ele se encaixa na estrutura que acabamos de descrever e criar o link para os Autoencoders, vamos dar uma vis\u00e3o geral de como o PCA funciona, deixando a maioria dos detalhes de lado (caso queira estudar o PCA em detalhes e na pr\u00e1tica, h\u00e1 um cap\u00edtulo inteiro dedicado a esta t\u00e9cnica em\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Matem\u00e1tica Para Machine Learning\n    </a>\n   </span>\n   ) .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A ideia do PCA \u00e9 construir novos recursos independentes, que s\u00e3o combina\u00e7\u00f5es lineares dos novos recursos antigos e, de modo que as proje\u00e7\u00f5es dos dados no subespa\u00e7o definido por esses novos recursos sejam o mais pr\u00f3ximo poss\u00edvel dos dados iniciais (em termos de dist\u00e2ncia euclidiana). Em outras palavras, o PCA est\u00e1 procurando o melhor subespa\u00e7o linear do espa\u00e7o inicial (descrito por uma base ortogonal de novos recursos), de modo que o erro de aproximar os dados por suas proje\u00e7\u00f5es nesse subespa\u00e7o seja o menor poss\u00edvel.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Traduzido em nossa estrutura global, procuramos um codificador na fam\u00edlia E das matrizes n_e por n_d (transforma\u00e7\u00e3o linear) cujas linhas s\u00e3o ortonormais (independ\u00eancia de recursos) e pelo decodificador associado entre a fam\u00edlia D de matrizes n_d por n_e. Pode-se mostrar que os autovetores unit\u00e1rios correspondentes aos n_e maiores autovalores (em norma) da matriz de caracter\u00edsticas de covari\u00e2ncia s\u00e3o ortogonais (ou podem ser escolhidos assim) e definem o melhor subespa\u00e7o da dimens\u00e3o n_e para projetar dados com erro m\u00ednimo de aproxima\u00e7\u00e3o. Assim, esses n_e autovetores podem ser escolhidos como nossos novos recursos e, portanto, o problema de redu\u00e7\u00e3o de dimens\u00e3o pode ser expresso como um problema de autovalor / autovetor. Al\u00e9m disso, tamb\u00e9m pode ser mostrado que, nesse caso, a matriz decodificadora \u00e9 a transposta da matriz codificadora (se esses conceitos de Matem\u00e1tica parecem estranhos a voc\u00ea recomendamos\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=matematica-para-machine-learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Matem\u00e1tica Para Machine Learning\n    </a>\n    )\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Os Autoencoders s\u00e3o em ess\u00eancia, redes neurais para redu\u00e7\u00e3o de dimensionalidade. A ideia geral dos Autoencoders \u00e9 bastante simples e consiste em definir um codificador e um decodificador como redes neurais e aprender o melhor esquema de codifica\u00e7\u00e3o-decodifica\u00e7\u00e3o usando um processo de otimiza\u00e7\u00e3o iterativo. Assim, a cada itera\u00e7\u00e3o, alimentamos a arquitetura do autoencoder (o codificador seguido pelo decodificador) com alguns dados, comparamos a sa\u00edda decodificada com os dados iniciais e retropropagamos o erro na arquitetura para atualizar os pesos das redes.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim, intuitivamente, a arquitetura geral do autoencoder (codificador + decodificador) cria um gargalo de dados que garante que apenas a parte estruturada principal da informa\u00e7\u00e3o possa passar e ser reconstru\u00edda. Observando nossa estrutura geral, a fam\u00edlia E dos codificadores considerados \u00e9 definida pela arquitetura da rede do codificador, a fam\u00edlia D dos decodificadores considerados \u00e9 definida pela arquitetura da rede do decodificador e a busca do codificador e decodificador que minimiza o erro de reconstru\u00e7\u00e3o \u00e9 feita por descida do gradiente sobre os par\u00e2metros dessas redes.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"autoencoder\" class=\"aligncenter size-large wp-image-1611\" data-attachment-id=\"1611\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"autoencoder\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?fit=1024%2C560\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?fit=300%2C164\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?fit=1262%2C690\" data-orig-size=\"1262,690\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/autoencoder/\" data-recalc-dims=\"1\" height=\"560\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=1024%2C560\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=1024%2C560 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=300%2C164 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=768%2C420 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=200%2C109 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?resize=690%2C377 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/autoencoder.png?w=1262 1262w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos primeiro supor que nossas arquiteturas de codificador e decodificador tenham apenas uma camada sem n\u00e3o linearidade (autoencoder linear). Esse codificador e decodificador s\u00e3o transforma\u00e7\u00f5es lineares simples que podem ser expressas como matrizes. Em tal situa\u00e7\u00e3o, podemos ver um v\u00ednculo claro com o PCA no sentido de que, assim como o PCA, estamos procurando o melhor subespa\u00e7o linear para projetar dados com o m\u00ednimo de perda de informa\u00e7\u00f5es poss\u00edvel ao faz\u00ea-lo. As matrizes de codifica\u00e7\u00e3o e decodifica\u00e7\u00e3o obtidas com o PCA definem naturalmente uma das solu\u00e7\u00f5es que gostar\u00edamos de alcan\u00e7ar por descida do gradiente, mas devemos destacar que essa n\u00e3o \u00e9 a \u00fanica. De fato, v\u00e1rias bases podem ser escolhidas para descrever o mesmo subespa\u00e7o ideal e, portanto, v\u00e1rios pares de codificador / decodificador podem fornecer o erro de reconstru\u00e7\u00e3o ideal. Al\u00e9m disso, para autoencoders lineares e, ao contr\u00e1rio do PCA, os novos recursos n\u00e3o precisam ser independentes (sem restri\u00e7\u00f5es de ortogonalidade nas redes neurais).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Agora, vamos supor que o codificador e o decodificador sejam profundos e n\u00e3o lineares. Nesse caso, quanto mais complexa a arquitetura, mais o autoencoder pode prosseguir para uma alta redu\u00e7\u00e3o de dimensionalidade, mantendo baixa a perda de reconstru\u00e7\u00e3o. Intuitivamente, se nosso codificador e nosso decodificador tiver graus de liberdade suficientes, podemos reduzir qualquer dimensionalidade inicial para 1. De fato, um codificador com \u201cpoder infinito\u201d poderia teoricamente pegar nossos N pontos de dados iniciais e codific\u00e1-los como 1, 2, 3, \u2026 at\u00e9 N (ou mais geralmente, como N inteiro no eixo real) e o decodificador associado pode fazer a transforma\u00e7\u00e3o reversa, sem perda durante o processo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui, por\u00e9m, devemos ter duas coisas em mente. Primeiro, uma importante redu\u00e7\u00e3o de dimensionalidade sem perda de reconstru\u00e7\u00e3o costuma ter um pre\u00e7o: a falta de estruturas interpret\u00e1veis \u200b\u200be explor\u00e1veis \u200b\u200bno espa\u00e7o latente (falta de regularidade). Segundo, na maioria das vezes, o objetivo final da redu\u00e7\u00e3o da dimensionalidade n\u00e3o \u00e9 apenas reduzir o n\u00famero de dimens\u00f5es dos dados, mas reduzir esse n\u00famero de dimens\u00f5es, mantendo a maior parte das informa\u00e7\u00f5es da estrutura de dados nas representa\u00e7\u00f5es reduzidas. Por essas duas raz\u00f5es, a dimens\u00e3o do espa\u00e7o latente e a \u201cprofundidade\u201d dos autoencoders (que definem o grau e a qualidade da compress\u00e3o) devem ser cuidadosamente controladas e ajustadas, dependendo do objetivo final da redu\u00e7\u00e3o da dimensionalidade.\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Variational Autoencoders (VAEs)\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/introducao-aos-autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     cap\u00edtulo 58\n    </a>\n   </span>\n   introduzimos o conceito de Autoencoders, que s\u00e3o arquiteturas de codificador-decodificador que podem ser treinadas por descida do gradiente. Vamos agora fazer o link com o problema de gera\u00e7\u00e3o de conte\u00fado, ver as limita\u00e7\u00f5es dos Autoencoders em sua forma atual para esse problema e introduzir os Autoencoders Variacionais.\n  </span>\n </p>\n <h4 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Limita\u00e7\u00f5es de Autoencoders para Gera\u00e7\u00e3o de Conte\u00fado\n  </span>\n </h4>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nesse ponto, uma pergunta natural que vem \u00e0 mente \u00e9 \u201cqual \u00e9 o link entre os Autoencoders e a gera\u00e7\u00e3o de conte\u00fado?\u201d. De fato, uma vez que o Autoencoder foi treinado, temos um codificador e um decodificador, mas ainda n\u00e3o h\u00e1 uma maneira real de produzir qualquer novo conte\u00fado. \u00c0 primeira vista, poder\u00edamos ficar tentados a pensar que, se o espa\u00e7o latente for regular o suficiente (bem \u201corganizado\u201d pelo codificador durante o processo de treinamento), poder\u00edamos pegar um ponto aleatoriamente nesse espa\u00e7o latente e decodific\u00e1-lo para obter um novo conte\u00fado. O decodificador agiria mais ou menos como o gerador de uma Rede Advers\u00e1ria Generativa .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"vae\" class=\"aligncenter wp-image-1597 size-large\" data-attachment-id=\"1597\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"vae\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?fit=1024%2C533\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?fit=300%2C156\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?fit=1389%2C723\" data-orig-size=\"1389,723\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae/\" data-recalc-dims=\"1\" height=\"533\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=1024%2C533\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=1024%2C533 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=300%2C156 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=768%2C400 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=200%2C104 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?resize=690%2C359 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae.png?w=1389 1389w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No entanto, a regularidade do espa\u00e7o latente para Autoencoders \u00e9 um ponto dif\u00edcil que depende da distribui\u00e7\u00e3o dos dados no espa\u00e7o inicial, da dimens\u00e3o do espa\u00e7o latente e da arquitetura do codificador. Portanto, \u00e9 bastante dif\u00edcil (se n\u00e3o imposs\u00edvel) garantir, a priori, que o codificador organize o espa\u00e7o latente de maneira inteligente, compat\u00edvel com o processo generativo que acabamos de descrever.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para ilustrar esse ponto, vamos considerar o exemplo no qual descrevemos um codificador e um decodificador suficientemente poderosos para colocar N dados de treinamento inicial no eixo real (cada ponto de dados sendo codificado como um valor real) e decodific\u00e1-los sem nenhum perda de reconstru\u00e7\u00e3o. Nesse caso, o alto grau de liberdade do Autoencoder que possibilita a codifica\u00e7\u00e3o e decodifica\u00e7\u00e3o sem perda de informa\u00e7\u00f5es (apesar da baixa dimensionalidade do espa\u00e7o latente) leva a uma super adapta\u00e7\u00e3o severa, o que implica que alguns pontos do espa\u00e7o latente fornecer\u00e3o conte\u00fado sem sentido uma vez decodificado. Se esse exemplo unidimensional tiver sido voluntariamente escolhido para ser extremo, podemos notar que o problema da regularidade espacial latente dos Autoencoders \u00e9 muito mais geral do que isso e merece uma aten\u00e7\u00e3o especial.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"vae2\" class=\"aligncenter wp-image-1598 size-large\" data-attachment-id=\"1598\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"vae2\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?fit=1024%2C341\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?fit=300%2C100\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?fit=1804%2C600\" data-orig-size=\"1804,600\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae2/\" data-recalc-dims=\"1\" height=\"341\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=1024%2C341\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=1024%2C341 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=300%2C100 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=768%2C255 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=1536%2C511 1536w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=200%2C67 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?resize=690%2C229 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae2.png?w=1804 1804w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao pensar nisso por um minuto, essa falta de estrutura entre os dados codificados no espa\u00e7o latente \u00e9 bastante normal. De fato, o Autoencoder \u00e9 treinado apenas para codificar e decodificar com o m\u00ednimo de perdas poss\u00edvel, independentemente da organiza\u00e7\u00e3o do espa\u00e7o latente. Portanto, se n\u00e3o tomarmos cuidado com a defini\u00e7\u00e3o da arquitetura, \u00e9 natural que, durante o treinamento, a rede aproveite todas as possibilidades de sobreajuste para realizar sua tarefa da melhor forma poss\u00edvel \u2026 a menos que a regularizemos explicitamente!\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Defini\u00e7\u00e3o de Autoencoders Variacionais\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Portanto, para poder usar o decodificador de nosso autoencoder para fins generativos, precisamos ter certeza de que o espa\u00e7o latente \u00e9 regular o suficiente. Uma solu\u00e7\u00e3o poss\u00edvel para obter essa regularidade \u00e9 introduzir regulariza\u00e7\u00e3o expl\u00edcita durante o processo de treinamento. Assim, como mencionamos brevemente na introdu\u00e7\u00e3o deste cap\u00edtulo, um autoencoder variacional pode ser definido como um autoencoder cujo treinamento \u00e9 regularizado para evitar sobreajuste e garantir que o espa\u00e7o latente tenha boas propriedades que possibilitem processos generativos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim como um autoencoder padr\u00e3o, um autoencoder variacional \u00e9 uma arquitetura composta por um codificador e um decodificador, treinada para minimizar o erro de reconstru\u00e7\u00e3o entre os dados decodificados e os dados iniciais. No entanto, para introduzir alguma regulariza\u00e7\u00e3o do espa\u00e7o latente, procedemos a uma ligeira modifica\u00e7\u00e3o do processo de codifica\u00e7\u00e3o / decodifica\u00e7\u00e3o: em vez de codificar uma entrada como um \u00fanico ponto, a codificamos como uma distribui\u00e7\u00e3o no espa\u00e7o latente. O modelo \u00e9 treinado da seguinte maneira:\n  </span>\n </p>\n <ul style=\"text-align: justify;\">\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Primeiro, a entrada \u00e9 codificada como distribui\u00e7\u00e3o no espa\u00e7o latente.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Segundo, um ponto do espa\u00e7o latente \u00e9 amostrado a partir dessa distribui\u00e7\u00e3o.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Terceiro, o ponto amostrado \u00e9 decodificado e o erro de reconstru\u00e7\u00e3o pode ser calculado.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Finalmente, o erro de reconstru\u00e7\u00e3o \u00e9 retropropagado pela rede.\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"vae3\" class=\"aligncenter size-full wp-image-1599\" data-attachment-id=\"1599\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"vae3\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?fit=1024%2C338\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?fit=300%2C99\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?fit=1804%2C596\" data-orig-size=\"1804,596\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae3/\" data-recalc-dims=\"1\" height=\"387\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=1170%2C387\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?w=1804 1804w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=300%2C99 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=1024%2C338 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=768%2C254 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=1536%2C507 1536w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=200%2C66 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae3.png?resize=690%2C228 690w\" width=\"1170\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na pr\u00e1tica, as distribui\u00e7\u00f5es codificadas s\u00e3o escolhidas para serem normais, de modo que o codificador possa ser treinado para retornar a m\u00e9dia e a matriz de covari\u00e2ncia que descrevem esses gaussianos. A raz\u00e3o pela qual uma entrada \u00e9 codificada como uma distribui\u00e7\u00e3o com alguma vari\u00e2ncia em vez de um \u00fanico ponto \u00e9 que torna poss\u00edvel expressar muito naturalmente a regulariza\u00e7\u00e3o do espa\u00e7o latente: as distribui\u00e7\u00f5es retornadas pelo codificador s\u00e3o impostas para estarem pr\u00f3ximas a uma distribui\u00e7\u00e3o normal padr\u00e3o. Veremos na pr\u00f3xima subse\u00e7\u00e3o que garantimos dessa maneira uma regulariza\u00e7\u00e3o local e global do espa\u00e7o latente (local por causa do controle de vari\u00e2ncia e global por causa do controle m\u00e9dio).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Assim, a fun\u00e7\u00e3o de perda que \u00e9 minimizada ao treinar um VAE \u00e9 composta de um \u201ctermo de reconstru\u00e7\u00e3o\u201d (na camada final), que tende a tornar o esquema de codifica\u00e7\u00e3o-decodifica\u00e7\u00e3o o mais eficiente poss\u00edvel e um \u201ctermo de regulariza\u00e7\u00e3o\u201d (no camada latente), que tende a regularizar a organiza\u00e7\u00e3o do espa\u00e7o latente, tornando as distribui\u00e7\u00f5es retornadas pelo codificador pr\u00f3ximas a uma distribui\u00e7\u00e3o normal padr\u00e3o. Esse termo de regulariza\u00e7\u00e3o \u00e9 expresso como a diverg\u00eancia de Kulback-Leibler entre a distribui\u00e7\u00e3o retornada e uma gaussiana padr\u00e3o e ser\u00e1 mais justificado na pr\u00f3xima se\u00e7\u00e3o. Podemos notar que a diverg\u00eancia de Kullback-Leibler entre duas distribui\u00e7\u00f5es gaussianas tem uma forma fechada que pode ser expressa diretamente em termos das m\u00e9dias e das matrizes de covari\u00e2ncia das duas distribui\u00e7\u00f5es.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"vae4\" class=\"aligncenter size-full wp-image-1600\" data-attachment-id=\"1600\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"vae4\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?fit=1024%2C497\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?fit=300%2C146\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?fit=1548%2C752\" data-orig-size=\"1548,752\" data-permalink=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/vae4/\" data-recalc-dims=\"1\" height=\"568\" sizes=\"(max-width: 1170px) 100vw, 1170px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=1170%2C568\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?w=1548 1548w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=300%2C146 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=1024%2C497 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=768%2C373 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=1536%2C746 1536w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=200%2C97 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/vae4.png?resize=690%2C335 690w\" width=\"1170\"/>\n  </span>\n </p>\n <h2 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Intui\u00e7\u00f5es Sobre a Regulariza\u00e7\u00e3o\n  </span>\n </h2>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A regularidade que se espera do espa\u00e7o latente para possibilitar o processo generativo pode ser expressa por meio de duas propriedades principais: continuidade (dois pontos de fechamento no espa\u00e7o latente n\u00e3o devem fornecer dois conte\u00fados completamente diferentes uma vez decodificados) e integridade (para uma distribui\u00e7\u00e3o escolhida , um ponto amostrado no espa\u00e7o latente deve fornecer conte\u00fado \u201csignificativo\u201d depois de decodificado).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O \u00fanico fato de que os VAEs codificam entradas como distribui\u00e7\u00f5es, em vez de pontos simples, n\u00e3o \u00e9 suficiente para garantir continuidade e integridade. Sem um termo de regulariza\u00e7\u00e3o bem definido, o modelo pode aprender, a fim de minimizar seu erro de reconstru\u00e7\u00e3o, \u201cignorar\u201d o fato de que as distribui\u00e7\u00f5es s\u00e3o retornadas e se comportam quase como os autoencoders cl\u00e1ssicos (levando ao super ajuste). Para fazer isso, o codificador pode retornar distribui\u00e7\u00f5es com pequenas varia\u00e7\u00f5es (que tendem a ser distribui\u00e7\u00f5es pontuais) ou retornar distribui\u00e7\u00f5es com meios muito diferentes (que ficariam muito distantes um do outro no espa\u00e7o latente). Nos dois casos, as distribui\u00e7\u00f5es s\u00e3o usadas da maneira errada (cancelando o benef\u00edcio esperado) e a continuidade e / ou a integridade n\u00e3o s\u00e3o satisfeitas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Portanto, para evitar esses efeitos, precisamos regularizar a matriz de covari\u00e2ncia e a m\u00e9dia das distribui\u00e7\u00f5es retornadas pelo codificador. Na pr\u00e1tica, essa regulariza\u00e7\u00e3o \u00e9 feita impondo distribui\u00e7\u00f5es para estar perto de uma distribui\u00e7\u00e3o normal padr\u00e3o (centralizada e reduzida). Dessa forma, exigimos que as matrizes de covari\u00e2ncia estejam pr\u00f3ximas da identidade, impedindo distribui\u00e7\u00f5es pontuais e com a m\u00e9dia pr\u00f3xima de 0, impedindo que as distribui\u00e7\u00f5es codificadas estejam muito distantes umas das outras.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com esse termo de regulariza\u00e7\u00e3o, impedimos que o modelo codifique dados distantes no espa\u00e7o latente e incentivamos o m\u00e1ximo poss\u00edvel as distribui\u00e7\u00f5es retornadas a \u201cse sobrepor\u201d, satisfazendo dessa maneira as condi\u00e7\u00f5es de continuidade e integridade esperadas. Naturalmente, como em qualquer termo de regulariza\u00e7\u00e3o, isso tem o pre\u00e7o de um erro de reconstru\u00e7\u00e3o mais alto nos dados de treinamento. A troca entre o erro de reconstru\u00e7\u00e3o e a diverg\u00eancia de KL pode, no entanto, ser ajustada e veremos no pr\u00f3ximo cap\u00edtulo como a express\u00e3o emerge naturalmente de nossa deriva\u00e7\u00e3o formal.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para concluir, podemos observar que a continuidade e a integridade obtidas com a regulariza\u00e7\u00e3o tendem a criar um \u201cgradiente\u201d sobre as informa\u00e7\u00f5es codificadas no espa\u00e7o latente. Por exemplo, um ponto do espa\u00e7o latente que estaria a meio caminho entre as m\u00e9dias de duas distribui\u00e7\u00f5es codificadas provenientes de diferentes dados de treinamento deve ser decodificado em algo que esteja em algum lugar entre os dados que deram a primeira distribui\u00e7\u00e3o e os dados que deram a segunda distribui\u00e7\u00e3o como pode ser amostrado pelo autoencoder em ambos os casos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Essa \u00e9 uma das arquiteturas mais avan\u00e7adas de Deep Learning e para uma melhor compreens\u00e3o, precisamos da nossa amiga, a Matem\u00e1tica. No pr\u00f3ximo cap\u00edtulo traremos um pouco da Matem\u00e1tica por tr\u00e1s dos VAEs. At\u00e9 l\u00e1.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Autoencoders \u2013 Unsupervised Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Understanding Variational Autoencoders (VAEs)\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Deep inside: Autoencoders\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://medium.com/datadriveninvestor/deep-learning-different-types-of-autoencoders-41d4fa5f7570\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Deep Learning \u2014 Different Types of Autoencoders\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.icml-2011.org/papers/455_icmlpaper.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Contractive Auto-Encoders \u2013 Explicit Invariance During Feature Extraction\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.jeremyjordan.me/autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Introduction to Autoencoders\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent neural network based language model\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1579\" href=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1579\" href=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1579\" href=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1579\" href=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1579-5e0dd226c3523\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1579&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1579-5e0dd226c3523\" id=\"like-post-wrapper-140353593-1579-5e0dd226c3523\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n", "61": "<h1 class=\"entry-title\" id=\"capitulo-61\">\n Cap\u00edtulo 61 \u2013 A Matem\u00e1tica dos Variational Autoencoders (VAEs)\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   No cap\u00edtulo\n   <span style=\"text-decoration: underline;\">\n    <a href=\"http://deeplearningbook.com.br/variational-autoencoders-vaes-definicao-reducao-de-dimensionalidade-espaco-latente-e-regularizacao/\" rel=\"noopener noreferrer\" target=\"_blank\">\n     anterior\n    </a>\n   </span>\n   , fornecemos a seguinte vis\u00e3o geral intuitiva: Os VAEs s\u00e3o Autoencoders que codificam entradas como distribui\u00e7\u00f5es em vez de pontos e cuja \u201corganiza\u00e7\u00e3o\u201d do espa\u00e7o latente \u00e9 regularizada restringindo as distribui\u00e7\u00f5es retornadas pelo codificador a estarem pr\u00f3ximas de um gaussiano padr\u00e3o. Neste cap\u00edtulo, forneceremos uma vis\u00e3o matem\u00e1tica dos VAEs que nos permitir\u00e1 justificar o termo de regulariza\u00e7\u00e3o com mais rigor. Para isso, definiremos uma estrutura probabil\u00edstica clara e usaremos, em particular, a t\u00e9cnica de infer\u00eancia variacional.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A base matem\u00e1tica e estat\u00edstica por tr\u00e1s dos conceitos deste cap\u00edtulo pode ser obtida na\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"text-decoration: underline;\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </span>\n   </a>\n   . Os conceitos aqui abordados tamb\u00e9m s\u00e3o estudados\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-engineer\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Machine Learning\n    </a>\n   </span>\n   e\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Estrutura Probabil\u00edstica e Premissas\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos come\u00e7ar definindo um modelo gr\u00e1fico probabil\u00edstico para descrever nossos dados. Denotamos por x a vari\u00e1vel que representa nossos dados e assumimos que x \u00e9 gerado a partir de uma vari\u00e1vel latente z (a representa\u00e7\u00e3o codificada) que n\u00e3o \u00e9 diretamente observada. Assim, para cada ponto de dados, \u00e9 assumido o seguinte processo generativo de duas etapas:\n  </span>\n </p>\n <ul>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Primeiro, uma representa\u00e7\u00e3o latente z \u00e9 amostrada da distribui\u00e7\u00e3o anterior p(z).\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Segundo, os dados x s\u00e3o amostrados da distribui\u00e7\u00e3o de probabilidade condicional definido por p(x | z).\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Com esse modelo probabil\u00edstico em mente, podemos redefinir nossas no\u00e7\u00f5es de codificador e decodificador. De fato, ao contr\u00e1rio de um Autoencoder simples que considera codificador e decodificador determin\u00edstico, consideraremos agora vers\u00f5es probabil\u00edsticas desses dois objetos. O \u201cdecodificador probabil\u00edstico\u201d \u00e9 definido naturalmente por p(x | z), que descreve a distribui\u00e7\u00e3o da vari\u00e1vel decodificada dada a codificada, enquanto o \u201ccodificador probabil\u00edstico\u201d \u00e9 definido por p(z | x), que descreve a distribui\u00e7\u00e3o de a vari\u00e1vel codificada, dada a decodificada.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Neste ponto, j\u00e1 podemos notar que a regulariza\u00e7\u00e3o do espa\u00e7o latente que nos faltava em Autoencoders simples aparece naturalmente aqui na defini\u00e7\u00e3o do processo de gera\u00e7\u00e3o de dados: presume-se que representa\u00e7\u00f5es codificadas z no espa\u00e7o latente sigam a distribui\u00e7\u00e3o anterior p(z). Caso contr\u00e1rio, tamb\u00e9m podemos lembrar o conhecido teorema de Bayes, que faz a liga\u00e7\u00e3o entre o anterior p(z), a probabilidade p(x | z) e o posterior p(z | x):\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form1\" class=\"aligncenter size-full wp-image-1637\" data-attachment-id=\"1637\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?fit=732%2C176\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?fit=300%2C72\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?fit=732%2C176\" data-orig-size=\"732,176\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form1-12/\" data-recalc-dims=\"1\" height=\"176\" sizes=\"(max-width: 732px) 100vw, 732px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=732%2C176\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?w=732 732w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=300%2C72 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=200%2C48 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form1-1.jpg?resize=690%2C166 690w\" width=\"732\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos agora assumir que p(z) \u00e9 uma distribui\u00e7\u00e3o gaussiana padr\u00e3o e que p(x | z) \u00e9 uma distribui\u00e7\u00e3o gaussiana cuja m\u00e9dia \u00e9 definida por uma fun\u00e7\u00e3o determin\u00edstica f da vari\u00e1vel de z e cuja matriz de covari\u00e2ncia tem a forma de uma constante positiva c que multiplica a matriz de identidade I. Sup\u00f5e-se que a fun\u00e7\u00e3o f pertence a uma fam\u00edlia de fun\u00e7\u00f5es denotadas F que \u00e9 deixada n\u00e3o especificada no momento e que ser\u00e1 escolhida posteriormente. Assim, temos:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form2\" class=\"aligncenter size-full wp-image-1638\" data-attachment-id=\"1638\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?fit=894%2C156\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?fit=300%2C52\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?fit=894%2C156\" data-orig-size=\"894,156\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form2-18/\" data-recalc-dims=\"1\" height=\"156\" sizes=\"(max-width: 894px) 100vw, 894px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=894%2C156\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?w=894 894w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=300%2C52 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=768%2C134 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=200%2C35 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form2-1.jpg?resize=690%2C120 690w\" width=\"894\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Vamos considerar, por enquanto, que f est\u00e1 bem definido e fixo. Em teoria, como conhecemos p(z) e p(x | z), podemos usar o Teorema de Bayes para calcular p(z | x): este \u00e9 um problema cl\u00e1ssico de infer\u00eancia bayesiana. No entanto, esse tipo de computa\u00e7\u00e3o geralmente \u00e9 intrat\u00e1vel (por causa da integral no denominador) e requer o uso de t\u00e9cnicas de aproxima\u00e7\u00e3o, como infer\u00eancia variacional.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Nota: Aqui podemos mencionar que p(z) e p(x | z) s\u00e3o ambas distribui\u00e7\u00f5es gaussianas, implicando que p(z | x) tamb\u00e9m deve seguir uma distribui\u00e7\u00e3o gaussiana. Em teoria, poder\u00edamos \u201capenas\u201d tentar expressar a m\u00e9dia e a matriz de covari\u00e2ncia de p(z | x) com rela\u00e7\u00e3o \u00e0s m\u00e9dias e \u00e0s matrizes de covari\u00e2ncia de p(z) e p(x | z). No entanto, na pr\u00e1tica, esses valores dependem da fun\u00e7\u00e3o f que pode ser complexa e que n\u00e3o est\u00e1 definida por enquanto (mesmo que tenhamos assumido o contr\u00e1rio). Al\u00e9m disso, o uso de uma t\u00e9cnica de aproxima\u00e7\u00e3o como infer\u00eancia variacional torna a abordagem bastante geral e mais robusta a algumas mudan\u00e7as na hip\u00f3tese do modelo.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Formula\u00e7\u00e3o de Infer\u00eancia Variacional\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em Estat\u00edstica, a infer\u00eancia variacional \u00e9 uma t\u00e9cnica para aproximar distribui\u00e7\u00f5es complexas. A ideia \u00e9 definir uma fam\u00edlia de distribui\u00e7\u00e3o parametrizada (por exemplo, a fam\u00edlia de Gaussianos, cujos par\u00e2metros s\u00e3o a m\u00e9dia e a covari\u00e2ncia) e procurar a melhor aproxima\u00e7\u00e3o de nossa distribui\u00e7\u00e3o de destino entre essa fam\u00edlia. O melhor elemento da fam\u00edlia \u00e9 aquele que minimiza uma determinada medi\u00e7\u00e3o de erro de aproxima\u00e7\u00e3o (na maioria das vezes a diverg\u00eancia de Kullback-Leibler entre aproxima\u00e7\u00e3o e alvo) e \u00e9 encontrada por descida do gradiente sobre os par\u00e2metros que descrevem a fam\u00edlia. Para mais detalhes, voc\u00ea encontra um artigo sobe isso na se\u00e7\u00e3o de refer\u00eancias ao final do cap\u00edtulo.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Aqui vamos aproximar p(z | x) por uma distribui\u00e7\u00e3o gaussiana q_x(z) cuja m\u00e9dia e covari\u00e2ncia s\u00e3o definidas por duas fun\u00e7\u00f5es, g e h, do par\u00e2metro x. Essas duas fun\u00e7\u00f5es devem pertencer, respectivamente, \u00e0s fam\u00edlias de fun\u00e7\u00f5es G e H que ser\u00e3o especificadas mais tarde, mas que devem ser parametrizadas. Assim, podemos denotar:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form3\" class=\"aligncenter size-full wp-image-1639\" data-attachment-id=\"1639\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form3\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?fit=868%2C100\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?fit=300%2C35\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?fit=868%2C100\" data-orig-size=\"868,100\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form3-15/\" data-recalc-dims=\"1\" height=\"100\" sizes=\"(max-width: 868px) 100vw, 868px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=868%2C100\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?w=868 868w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=300%2C35 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=768%2C88 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=200%2C23 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form3.jpg?resize=690%2C79 690w\" width=\"868\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Portanto, definimos dessa maneira uma fam\u00edlia de candidatos \u00e0 infer\u00eancia variacional e precisamos agora encontrar a melhor aproxima\u00e7\u00e3o entre essa fam\u00edlia otimizando as fun\u00e7\u00f5es g e h (de fato, seus par\u00e2metros) para minimizar a diverg\u00eancia de Kullback-Leibler entre a aproxima\u00e7\u00e3o e o alvo p(z | x). Em outras palavras, estamos procurando a aproxima\u00e7\u00e3o ideal g* e h* de modo que:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form4\" class=\"aligncenter size-large wp-image-1640\" data-attachment-id=\"1640\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form4\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?fit=1024%2C319\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?fit=300%2C93\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?fit=1626%2C506\" data-orig-size=\"1626,506\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form4-11/\" data-recalc-dims=\"1\" height=\"319\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=1024%2C319\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=1024%2C319 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=300%2C93 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=768%2C239 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=1536%2C478 1536w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=200%2C62 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?resize=690%2C215 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form4.png?w=1626 1626w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na pen\u00faltima equa\u00e7\u00e3o, podemos observar a troca existente \u2013 ao aproximar-se do p(z | x) posterior \u2013 entre maximizar a probabilidade das \u201cobserva\u00e7\u00f5es\u201d (maximiza\u00e7\u00e3o da probabilidade logar\u00edtmica esperada para o primeiro termo) e permanecer pr\u00f3ximo \u00e0 distribui\u00e7\u00e3o anterior (minimiza\u00e7\u00e3o da diverg\u00eancia de KL entre q_x(z) e p(z), para o segundo termo). Essa troca \u00e9 natural para o problema de infer\u00eancia bayesiana e expressa o equil\u00edbrio que precisa ser encontrado entre a confian\u00e7a que temos nos dados e a confian\u00e7a que temos no passado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 agora, assumimos a fun\u00e7\u00e3o f conhecida e fixa e mostramos que, com tais premissas, podemos aproximar o p(z | x) posterior usando a t\u00e9cnica de infer\u00eancia variacional. No entanto, na pr\u00e1tica, essa fun\u00e7\u00e3o f, que define o decodificador, n\u00e3o \u00e9 conhecida e tamb\u00e9m precisa ser escolhida. Para fazer isso, lembre-se de que nosso objetivo inicial \u00e9 encontrar um esquema de codifica\u00e7\u00e3o e decodifica\u00e7\u00e3o com desempenho, cujo espa\u00e7o latente seja regular o suficiente para ser usado para fins generativos. Se a regularidade \u00e9 regida principalmente pela distribui\u00e7\u00e3o anterior assumida no espa\u00e7o latente, o desempenho do esquema geral de codifica\u00e7\u00e3o / decodifica\u00e7\u00e3o depende muito da escolha da fun\u00e7\u00e3o f. De fato, como p(z | x) pode ser aproximado (por infer\u00eancia variacional) de p(z) e p(x | z) e como p(z) \u00e9 um gaussiano padr\u00e3o simples, as duas \u00fanicas alavancas que temos \u00e0 nossa disposi\u00e7\u00e3o em nosso modelo para fazer otimiza\u00e7\u00f5es s\u00e3o o par\u00e2metro c (que define a vari\u00e2ncia da probabilidade) e a fun\u00e7\u00e3o f (que define a m\u00e9dia da probabilidade).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Portanto, vamos considerar que, como discutimos anteriormente, podemos obter para qualquer fun\u00e7\u00e3o f em F (cada uma definindo um decodificador probabil\u00edstico diferente p(x | z)) a melhor aproxima\u00e7\u00e3o de p(z | x), denotada q*_x(z) Apesar de sua natureza probabil\u00edstica, estamos procurando um esquema de codifica\u00e7\u00e3o-decodifica\u00e7\u00e3o o mais eficiente poss\u00edvel e, em seguida, queremos escolher a fun\u00e7\u00e3o f que maximize a probabilidade logar\u00edtmica esperada de x dado z quando z \u00e9 amostrado de q*_x(z) Em outras palavras, para uma dada entrada x, queremos maximizar a probabilidade de ter x\u0302 = x quando amostramos z da distribui\u00e7\u00e3o q*_x(z) e depois amostramos x\u0302 da distribui\u00e7\u00e3o p(x | z). Assim, procuramos o f* ideal para que:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form5\" class=\"aligncenter size-full wp-image-1641\" data-attachment-id=\"1641\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form5\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?fit=748%2C282\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?fit=300%2C113\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?fit=748%2C282\" data-orig-size=\"748,282\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form5-9/\" data-recalc-dims=\"1\" height=\"282\" sizes=\"(max-width: 748px) 100vw, 748px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=748%2C282\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?w=748 748w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=300%2C113 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=200%2C75 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form5.jpg?resize=690%2C260 690w\" width=\"748\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   onde q*_x (z) depende da fun\u00e7\u00e3o f e \u00e9 obtido como descrito anteriormente. Reunindo todas as pe\u00e7as, estamos procurando as aproxima\u00e7\u00f5es\n  </span>\n  <span style=\"color: #000000;\">\n   f*,\u00a0 g* e h* ideais para que:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form6\" class=\"aligncenter size-large wp-image-1642\" data-attachment-id=\"1642\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form6\" data-large-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?fit=1024%2C83\" data-medium-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?fit=300%2C24\" data-orig-file=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?fit=1280%2C104\" data-orig-size=\"1280,104\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form6-6/\" data-recalc-dims=\"1\" height=\"83\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=1024%2C83\" srcset=\"https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=1024%2C83 1024w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=300%2C24 300w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=768%2C62 768w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=200%2C16 200w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?resize=690%2C56 690w, https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form6.png?w=1280 1280w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Podemos identificar nesta fun\u00e7\u00e3o objetivo os elementos introduzidos na descri\u00e7\u00e3o intuitiva dos VAEs dados no cap\u00edtulo anterior: o erro de reconstru\u00e7\u00e3o entre x e f(z) e o termo de regulariza\u00e7\u00e3o dado pela diverg\u00eancia KL entre q_x(z) e p(z) ) (que \u00e9 um gaussiano padr\u00e3o). Tamb\u00e9m podemos notar a constante c que regula o equil\u00edbrio entre os dois termos anteriores. Quanto maior c for, mais assumimos uma alta varia\u00e7\u00e3o em torno de f(z) para o decodificador probabil\u00edstico em nosso modelo e, portanto, mais favorecemos o termo de regulariza\u00e7\u00e3o sobre o termo de reconstru\u00e7\u00e3o (e o oposto se c for baixo).\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Trazendo Redes Neurais Para o Modelo\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   At\u00e9 o momento, definimos um modelo probabil\u00edstico que depende de tr\u00eas fun\u00e7\u00f5es, f, g e h, e expressamos, usando infer\u00eancia variacional, o problema de otimiza\u00e7\u00e3o a ser resolvido para obter f*, g* e h* que ofere\u00e7am o melhor esquema de codifica\u00e7\u00e3o-decodifica\u00e7\u00e3o com este modelo. Como n\u00e3o podemos otimizar facilmente todo o espa\u00e7o de fun\u00e7\u00f5es, restringimos o dom\u00ednio de otimiza\u00e7\u00e3o e decidimos expressar f, g e h como redes neurais. Assim, F, G e H correspondem, respectivamente, \u00e0s fam\u00edlias de fun\u00e7\u00f5es definidas pelas arquiteturas de rede e a otimiza\u00e7\u00e3o \u00e9 feita sobre os par\u00e2metros dessas redes.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Na pr\u00e1tica, g e h n\u00e3o s\u00e3o definidos por duas redes completamente independentes, mas compartilham uma parte de sua arquitetura e seus pesos, de modo que temos:\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form7\" class=\"aligncenter size-large wp-image-1643\" data-attachment-id=\"1643\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form7\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?fit=1024%2C41\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?fit=300%2C12\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?fit=1046%2C42\" data-orig-size=\"1046,42\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form7-5/\" data-recalc-dims=\"1\" height=\"41\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=1024%2C41\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=1024%2C41 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=300%2C12 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=768%2C31 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=200%2C8 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?resize=690%2C28 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form7.png?w=1046 1046w\" width=\"1024\"/>\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Como isso define a matriz de covari\u00e2ncia de q_x(z), h(x) deve ser uma matriz quadrada. Entretanto, para simplificar o c\u00e1lculo e reduzir o n\u00famero de par\u00e2metros, assumimos que nossa aproxima\u00e7\u00e3o de p(z | x), q_x(z), \u00e9 uma distribui\u00e7\u00e3o gaussiana multidimensional com matriz de covari\u00e2ncia diagonal (assun\u00e7\u00e3o de independ\u00eancia de vari\u00e1veis). Com essa suposi\u00e7\u00e3o, h(x) \u00e9 simplesmente o vetor dos elementos diagonais da matriz de covari\u00e2ncia e, ent\u00e3o, tem o mesmo tamanho de g(x). No entanto, reduzimos dessa maneira a fam\u00edlia de distribui\u00e7\u00f5es que consideramos para infer\u00eancia variacional e, portanto, a aproxima\u00e7\u00e3o de p(z | x) obtida pode ser menos precisa.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form8\" class=\"aligncenter size-large wp-image-1644\" data-attachment-id=\"1644\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form8\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?fit=1024%2C474\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?fit=300%2C139\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?fit=1286%2C595\" data-orig-size=\"1286,595\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form8-5/\" data-recalc-dims=\"1\" height=\"474\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=1024%2C474\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=1024%2C474 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=300%2C139 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=768%2C355 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=200%2C93 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?resize=690%2C319 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form8.png?w=1286 1286w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ao contr\u00e1rio da parte do codificador que modela p(z | x) e para a qual consideramos um gaussiano com m\u00e9dia e covari\u00e2ncia que s\u00e3o fun\u00e7\u00f5es de x(g e h), nosso modelo assume para p(x | z) um gaussiano com covari\u00e2ncia. A fun\u00e7\u00e3o f da vari\u00e1vel z que define a m\u00e9dia desse gaussiano \u00e9 modelada por uma rede neural e pode ser representada da seguinte forma:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form9\" class=\"aligncenter size-large wp-image-1645\" data-attachment-id=\"1645\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form9\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?fit=1024%2C415\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?fit=300%2C122\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?fit=1286%2C521\" data-orig-size=\"1286,521\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form9-2/\" data-recalc-dims=\"1\" height=\"415\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=1024%2C415\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=1024%2C415 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=300%2C122 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=768%2C311 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=200%2C81 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?resize=690%2C280 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form9.png?w=1286 1286w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A arquitetura geral \u00e9 ent\u00e3o obtida concatenando o codificador e as partes do decodificador. No entanto, ainda precisamos ter muito cuidado com a maneira como coletamos amostras da distribui\u00e7\u00e3o retornada pelo codificador durante o treinamento. O processo de amostragem deve ser expresso de forma a permitir que o erro seja retropropagado pela rede. Um truque simples, chamado truque de reparametriza\u00e7\u00e3o, \u00e9 usado para tornar poss\u00edvel a descida do gradiente, apesar da amostragem aleat\u00f3ria que ocorre na metade da arquitetura e consiste em usar o fato de que se z \u00e9 uma vari\u00e1vel aleat\u00f3ria ap\u00f3s uma distribui\u00e7\u00e3o gaussiana com m\u00e9dia g(x) e com covari\u00e2ncia h(x), pode ser expresso como:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form10\" class=\"aligncenter size-full wp-image-1646\" data-attachment-id=\"1646\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form10\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?fit=680%2C118\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?fit=300%2C52\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?fit=680%2C118\" data-orig-size=\"680,118\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form10-2/\" data-recalc-dims=\"1\" height=\"118\" sizes=\"(max-width: 680px) 100vw, 680px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?resize=680%2C118\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?w=680 680w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?resize=300%2C52 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form10.jpg?resize=200%2C35 200w\" width=\"680\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form11\" class=\"aligncenter size-large wp-image-1648\" data-attachment-id=\"1648\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form11\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?fit=1024%2C388\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?fit=300%2C114\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?fit=1852%2C701\" data-orig-size=\"1852,701\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form11/\" data-recalc-dims=\"1\" height=\"388\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=1024%2C388\" srcset=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=1024%2C388 1024w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=300%2C114 300w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=768%2C291 768w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=1536%2C581 1536w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=200%2C76 200w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?resize=690%2C261 690w, https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png?w=1852 1852w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Finalmente, a fun\u00e7\u00e3o objetivo da arquitetura de Autoencoder Variacional obtida dessa maneira \u00e9 dada pela \u00faltima equa\u00e7\u00e3o da subse\u00e7\u00e3o anterior, na qual a expectativa te\u00f3rica \u00e9 substitu\u00edda por uma aproxima\u00e7\u00e3o de Monte-Carlo mais ou menos precisa que consiste, na maioria das vezes, em um sorteio \u00fanico. Assim, considerando essa aproxima\u00e7\u00e3o e denotando C = 1 / (2c), recuperamos a fun\u00e7\u00e3o de perda derivada intuitivamente na se\u00e7\u00e3o anterior, composta por um termo de reconstru\u00e7\u00e3o, um termo de regulariza\u00e7\u00e3o e uma constante para definir os pesos relativos desses dois termos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"form12\" class=\"aligncenter size-large wp-image-1649\" data-attachment-id=\"1649\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"form12\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?fit=1024%2C617\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?fit=300%2C181\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?fit=1664%2C1002\" data-orig-size=\"1664,1002\" data-permalink=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/form12/\" data-recalc-dims=\"1\" height=\"617\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=1024%2C617\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=1024%2C617 1024w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=300%2C181 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=768%2C462 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=1536%2C925 1536w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=200%2C120 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?resize=690%2C415 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/form12.png?w=1664 1664w\" width=\"1024\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Autoencoders Variacionais (VAEs) s\u00e3o Autoencoders que resolvem o problema da irregularidade do espa\u00e7o latente, fazendo com que o codificador retorne uma distribui\u00e7\u00e3o sobre o espa\u00e7o latente em vez de um \u00fanico ponto e adicionando \u00e0 fun\u00e7\u00e3o de perda um termo de regulariza\u00e7\u00e3o sobre a distribui\u00e7\u00e3o retornada para garantir uma melhor organiza\u00e7\u00e3o do espa\u00e7o latente\n  </span>\n  <span style=\"color: #000000;\">\n   assumindo um modelo probabil\u00edstico simples para descrever nossos dados, a fun\u00e7\u00e3o de perda bastante intuitiva dos VAEs, composta por um termo de reconstru\u00e7\u00e3o e um termo de regulariza\u00e7\u00e3o, pode ser cuidadosamente derivada, usando em particular a t\u00e9cnica estat\u00edstica de infer\u00eancia variacional (da\u00ed o nome Autoencoder \u201cVariacional\u201d).\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Para concluir, podemos destacar que, durante os \u00faltimos anos, as GANs se beneficiaram de muito mais contribui\u00e7\u00f5es cient\u00edficas do que os VAEs. Entre outras raz\u00f5es, o maior interesse demonstrado pela comunidade por GANs pode ser parcialmente explicado pelo maior grau de complexidade na base te\u00f3rica dos VAEs (modelo probabil\u00edstico e infer\u00eancia variacional) em compara\u00e7\u00e3o \u00e0 simplicidade do conceito de treinamento advers\u00e1rio que rege os GANs. Com este cap\u00edtulo, esperamos que tenhamos compartilhado intui\u00e7\u00f5es valiosas e fortes fundamentos te\u00f3ricos para tornar os VAEs mais acess\u00edveis aos rec\u00e9m-chegados. No entanto, agora que discutimos em profundidade os dois, resta uma pergunta \u2026 qual arquitetura voc\u00ea achou mais interessante, GANs ou VAEs?\n  </span>\n </p>\n <p>\n  <span style=\"color: #000000;\">\n   No pr\u00f3ximo cap\u00edtulo come\u00e7amos a estudar a Aprendizagem Por Refor\u00e7o! At\u00e9 l\u00e1.\n  </span>\n </p>\n <p>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Forma\u00e7\u00e3o Cientista de Dados\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Autoencoders \u2013 Unsupervised Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p>\n  <span style=\"text-decoration: underline;\">\n   <a href=\"https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29\" rel=\"noopener noreferrer\" target=\"_blank\">\n    <span style=\"color: #000000; text-decoration: underline;\">\n     Bayesian inference problem, MCMC and variational inference\n    </span>\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Understanding Variational Autoencoders (VAEs)\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep inside: Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://medium.com/datadriveninvestor/deep-learning-different-types-of-autoencoders-41d4fa5f7570\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep Learning \u2014 Different Types of Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.icml-2011.org/papers/455_icmlpaper.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Contractive Auto-Encoders \u2013 Explicit Invariance During Feature Extraction\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.jeremyjordan.me/autoencoders/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Introduction to Autoencoders\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Practical Recommendations for Gradient-Based Training of Deep Architectures\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient-Based Learning Applied to Document Recognition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Neural Networks &amp; The Backpropagation Algorithm, Explained\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Recurrent neural network based language model\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Gradient Descent For Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline;\">\n   <span style=\"color: #000000; text-decoration: underline;\">\n    <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Pattern Recognition and Machine Learning\n    </a>\n   </span>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <div class=\"sharedaddy sd-sharing-enabled\">\n   <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n    <h3 class=\"sd-title\">\n     Compartilhe isso:\n    </h3>\n    <div class=\"sd-content\">\n     <ul>\n      <li class=\"share-twitter\">\n       <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1636\" href=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Twitter(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-facebook\">\n       <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1636\" href=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Facebook(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-linkedin\">\n       <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1636\" href=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no LinkedIn(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-pinterest\">\n       <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1636\" href=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Pinterest(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-tumblr\">\n       <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no Tumblr(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-jetpack-whatsapp\">\n       <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/a-matematica-dos-variational-autoencoders-vaes/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n        <span>\n        </span>\n        <span class=\"sharing-screen-reader-text\">\n         Clique para compartilhar no WhatsApp(abre em nova janela)\n        </span>\n       </a>\n      </li>\n      <li class=\"share-end\">\n      </li>\n     </ul>\n    </div>\n   </div>\n  </div>\n  <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1636-5e0dd2292a2bf\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1636&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1636-5e0dd2292a2bf\" id=\"like-post-wrapper-140353593-1636-5e0dd2292a2bf\">\n   <h3 class=\"sd-title\">\n    Curtir isso:\n   </h3>\n   <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n    <span class=\"button\">\n     <span>\n      Curtir\n     </span>\n    </span>\n    <span class=\"loading\">\n     Carregando...\n    </span>\n   </div>\n   <span class=\"sd-text-color\">\n   </span>\n   <a class=\"sd-link-color\">\n   </a>\n  </div>\n  <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n   <h3 class=\"jp-relatedposts-headline\">\n    <em>\n     Relacionado\n    </em>\n   </h3>\n  </div>\n </p>\n</div>\n", "62": "<h1 class=\"entry-title\" id=\"capitulo-62\">\n Cap\u00edtulo 62 \u2013 O Que \u00e9 Aprendizagem Por Refor\u00e7o?\n</h1>\n<div class=\"entry-content\">\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Todas as arquiteturas de Deep Learning que estudamos at\u00e9 aqui neste livro podem ser classificadas em duas categorias de aprendizagem de m\u00e1quina (voc\u00ea j\u00e1 sabe que Deep Learning \u00e9 sub-categoria de Machine Learning, que por sua vez \u00e9 uma sub-categoria de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Intelig\u00eancia Artificial\n    </a>\n   </span>\n   ):\n  </span>\n </p>\n <ul style=\"text-align: justify;\">\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Aprendizagem Supervisionada \u2013 quando apresentamos ao algoritmo dados de entrada e as respectivas sa\u00eddas.\n   </span>\n  </li>\n  <li style=\"text-align: justify;\">\n   <span style=\"color: #000000;\">\n    Aprendizagem N\u00e3o Supervisionada \u2013 quando apresentamos somente os dados de entrada e o algoritmo descobre as sa\u00eddas.\n   </span>\n  </li>\n </ul>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Mas existe uma terceira categoria de aprendizagem, chamada de Aprendizagem Por Refor\u00e7o (ou Reinforcement Learning), muito usada em Games e Rob\u00f3tica e que vem obtendo resultados cada vez melhores. A Aprendizagem Por Refor\u00e7o \u00e9 a principal t\u00e9cnica por tr\u00e1s do AlphaGo e est\u00e1 muito bem retratada no document\u00e1rio do mesmo nome:\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.imdb.com/title/tt6700846/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     AlphaGo\n    </a>\n   </span>\n   .\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Ser\u00e3o diversos cap\u00edtulos dedicados a esta t\u00e9cnica e \u00e0 sua extens\u00e3o, o Deep Reinforcement Learning, que s\u00e3o estudados na pr\u00e1tica em\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-ii\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Deep Learning II\n    </a>\n   </span>\n   . Voc\u00ea tamb\u00e9m encontra um exemplo completo no curso gratuito\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/cursos-gratuitos-1\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Python Fundamentos Para An\u00e1lise de Dados\n    </a>\n   </span>\n   . Vamos come\u00e7ar definindo o que \u00e9 Aprendizagem Por Refor\u00e7o.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Que \u00e9 Aprendizagem Por Refor\u00e7o?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   A Aprendizagem Por Refor\u00e7o \u00e9 o treinamento de modelos de aprendizado de m\u00e1quina para tomar uma sequ\u00eancia de decis\u00f5es. O agente aprende a atingir uma meta em um ambiente incerto e potencialmente complexo. No aprendizado por refor\u00e7o, o sistema de intelig\u00eancia artificial enfrenta uma situa\u00e7\u00e3o. O computador utiliza tentativa e erro para encontrar uma solu\u00e7\u00e3o para o problema. Para que a m\u00e1quina fa\u00e7a o que o programador deseja, a intelig\u00eancia artificial recebe recompensas ou penalidades pelas a\u00e7\u00f5es que executa. Seu objetivo \u00e9 maximizar a recompensa total.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Embora o\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Cientista de Dados\n    </a>\n   </span>\n   defina a pol\u00edtica de recompensa \u2013 isto \u00e9, as regras do jogo \u2013 ele n\u00e3o d\u00e1 ao modelo nenhuma dica ou sugest\u00e3o de como resolver o jogo. Cabe ao modelo descobrir como executar a tarefa para maximizar a recompensa, come\u00e7ando com testes totalmente aleat\u00f3rios e terminando com t\u00e1ticas sofisticadas. Ao alavancar o poder da pesquisa e de muitas tentativas, o aprendizado por refor\u00e7o \u00e9 atualmente a maneira mais eficaz de sugerir a criatividade da m\u00e1quina. Ao contr\u00e1rio dos seres humanos, a intelig\u00eancia artificial pode reunir experi\u00eancia de milhares de jogos paralelos se um algoritmo de aprendizado por refor\u00e7o for executado em uma infraestrutura de computador suficientemente poderosa.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Exemplo:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O problema \u00e9 o seguinte: Temos um agente e uma recompensa, com muitos obst\u00e1culos no meio, como nesta imagem abaixo. O agente deve encontrar o melhor caminho poss\u00edvel para alcan\u00e7ar a recompensa e quando encontrar um obst\u00e1culo, deve ser penalizado (pois ele deve escolher o caminho sem obst\u00e1culos). Com a Aprendizagem Por Refor\u00e7o, podemos treinar o agente para encontrar o melhor caminho.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"rl\" class=\"aligncenter wp-image-1670 size-medium\" data-attachment-id=\"1670\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"rl\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?fit=996%2C689\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?fit=300%2C208\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?fit=996%2C689\" data-orig-size=\"996,689\" data-permalink=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/rl/\" data-recalc-dims=\"1\" height=\"208\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=300%2C208\" srcset=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=300%2C208 300w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=768%2C531 768w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=200%2C138 200w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?resize=690%2C477 690w, https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/rl.png?w=996 996w\" width=\"300\"/>\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n </h3>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Desafios do Aprendizado Por Refor\u00e7o\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O principal desafio do aprendizado por refor\u00e7o est\u00e1 na prepara\u00e7\u00e3o do ambiente de simula\u00e7\u00e3o, que depende muito da tarefa a ser executada. Quando o modelo \u00e9 treinado em jogos de Xadrez, Go ou Atari, a prepara\u00e7\u00e3o do ambiente de simula\u00e7\u00e3o \u00e9 relativamente simples. Quando se trata de construir um modelo capaz de dirigir um carro aut\u00f4nomo, a constru\u00e7\u00e3o de um simulador realista \u00e9 crucial antes de deixar o carro andar na rua. O modelo precisa descobrir como frear ou evitar uma colis\u00e3o em um ambiente seguro. Transferir o modelo do ambiente de treinamento para o mundo real \u00e9 onde as coisas ficam complicadas.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Escalar e ajustar a rede neural que controla o agente \u00e9 outro desafio. N\u00e3o h\u00e1 como se comunicar com a rede a n\u00e3o ser atrav\u00e9s do sistema de recompensas e penalidades. Isso pode levar a um esquecimento catastr\u00f3fico, em que a aquisi\u00e7\u00e3o de novos conhecimentos faz com que alguns dos antigos sejam apagados da rede. Ou seja, precisamos guardar o aprendizado na \u201cmem\u00f3ria\u201d do agente.\n   <br/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Outro desafio \u00e9 alcan\u00e7ar um \u00f3timo local \u2013 ou seja, o agente executa a tarefa como est\u00e1, mas n\u00e3o da maneira ideal ou necess\u00e1ria. Um \u201csaltador\u201d pulando como um canguru em vez de fazer o que se esperava dele com pequenos saltos \u00e9 um \u00f3timo exemplo. Por fim, existem agentes que otimizar\u00e3o o pr\u00eamio sem executar a tarefa para a qual foram projetados.\n  </span>\n </p>\n <h3 style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O Que Distingue o Aprendizado Por Refor\u00e7o do Aprendizado Profundo e do Aprendizado de M\u00e1quina?\n  </span>\n </h3>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   De fato, n\u00e3o h\u00e1 uma divis\u00e3o clara entre aprendizado de m\u00e1quina, aprendizado profundo e aprendizado por refor\u00e7o. \u00c9 como uma rela\u00e7\u00e3o paralelogramo \u2013 ret\u00e2ngulo \u2013 quadrado, em que o aprendizado de m\u00e1quina \u00e9 a categoria mais ampla e o aprendizado por refor\u00e7o \u00e9 o mais estreito.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Da mesma forma, o aprendizado por refor\u00e7o \u00e9 uma aplica\u00e7\u00e3o especializada de t\u00e9cnicas de Deep Learning e Machine Learning, projetada para resolver problemas de uma maneira espec\u00edfica.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Embora as ideias pare\u00e7am divergir, n\u00e3o h\u00e1 uma divis\u00e3o acentuada entre esses subtipos. Al\u00e9m disso, eles se mesclam nos projetos, pois os modelos s\u00e3o projetados para n\u00e3o se ater ao \u201ctipo puro\u201d, mas para executar a tarefa da maneira mais eficaz poss\u00edvel. Portanto, \u201co que distingue precisamente o aprendizado de m\u00e1quina, o aprendizado profundo e o aprendizado por refor\u00e7o\u201d \u00e9, na verdade, uma pergunta dif\u00edcil de responder. Mas vamos definir cada um deles!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <strong>\n    Aprendizado de M\u00e1quina\n   </strong>\n   \u00e9 uma forma de IA na qual os computadores t\u00eam a capacidade de melhorar progressivamente o desempenho de uma tarefa espec\u00edfica com dados, sem serem diretamente programados. Essa \u00e9 a defini\u00e7\u00e3o de\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://en.wikipedia.org/wiki/Arthur_Samuel\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n     Arthur Lee Samuel\n    </a>\n    .\n   </span>\n   Ele cunhou o termo \u201caprendizado de m\u00e1quina\u201d, do qual existem dois tipos, aprendizado de m\u00e1quina supervisionado e n\u00e3o supervisionado.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O aprendizado de m\u00e1quina supervisionado acontece quando um programador pode fornecer um r\u00f3tulo para cada entrada de treinamento no sistema de aprendizado de m\u00e1quina.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O aprendizado n\u00e3o supervisionado ocorre quando o modelo \u00e9 fornecido apenas com os dados de entrada, mas sem r\u00f3tulos expl\u00edcitos. Ele precisa pesquisar os dados e encontrar a estrutura ou os relacionamentos ocultos. O\n   <span style=\"text-decoration: underline;\">\n    <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" target=\"_blank\">\n     Cientista de Dados\n    </a>\n   </span>\n   pode n\u00e3o saber qual \u00e9 a estrutura ou o que o modelo de aprendizado de m\u00e1quina ir\u00e1 encontrar.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O\n   <strong>\n    Aprendizado Profundo\n   </strong>\n   consiste em v\u00e1rias camadas de redes neurais, projetadas para executar tarefas mais sofisticadas. A constru\u00e7\u00e3o de modelos de aprendizado profundo foi inspirada no design do c\u00e9rebro humano, mas simplificada. Os modelos de aprendizado profundo consistem em algumas camadas de rede neural que s\u00e3o, em princ\u00edpio, respons\u00e1veis por aprender gradualmente recursos mais abstratos sobre dados espec\u00edficos.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Embora as solu\u00e7\u00f5es de aprendizado profundo sejam capazes de fornecer resultados maravilhosos, em termos de escala, elas n\u00e3o s\u00e3o p\u00e1reo para o c\u00e9rebro humano. Cada camada usa o resultado de uma anterior como entrada e toda a rede \u00e9 treinada como um todo. O conceito central de criar uma rede neural artificial n\u00e3o \u00e9 novo, mas apenas recentemente o hardware moderno forneceu energia computacional suficiente para treinar efetivamente essas redes, expondo um n\u00famero suficiente de exemplos. A ado\u00e7\u00e3o estendida trouxe estruturas como TensorFlow, Keras e PyTorch, as quais tornaram a constru\u00e7\u00e3o de modelos de aprendizado de m\u00e1quina muito mais conveniente.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   O\n   <strong>\n    Aprendizado Por Refor\u00e7o\n   </strong>\n   , como declarado acima, emprega um sistema de recompensas e penalidades para obrigar o computador a resolver um problema sozinho. O envolvimento humano \u00e9 limitado \u00e0 mudan\u00e7a do ambiente e ao ajuste do sistema de recompensas e penalidades. Como o computador maximiza a recompensa, ele est\u00e1 propenso a procurar maneiras inesperadas de faz\u00ea-lo. O envolvimento humano \u00e9 focado em impedir que ele explore o sistema e motive a m\u00e1quina a executar a tarefa da maneira esperada. O aprendizado por refor\u00e7o \u00e9 \u00fatil quando n\u00e3o existe uma \u201cmaneira adequada\u201d de executar uma tarefa, mas existem regras que o modelo deve seguir para desempenhar corretamente suas tarefas. Abaixo a performance de um agente sendo treinado em um jogo cl\u00e1ssico do Atari.\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Performance inicial do agente:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"breakout_0\" class=\"aligncenter size-full wp-image-1674\" data-attachment-id=\"1674\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"breakout_0\" data-large-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?fit=160%2C210\" data-medium-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?fit=160%2C210\" data-orig-file=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?fit=160%2C210\" data-orig-size=\"160,210\" data-permalink=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/breakout_0/\" data-recalc-dims=\"1\" height=\"210\" src=\"https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_0.gif?resize=160%2C210\" width=\"160\"/>\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Ap\u00f3s 15 minutos de treinamento:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"breakout_1\" class=\"aligncenter size-full wp-image-1673\" data-attachment-id=\"1673\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"breakout_1\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?fit=160%2C210\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?fit=160%2C210\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?fit=160%2C210\" data-orig-size=\"160,210\" data-permalink=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/breakout_1/\" data-recalc-dims=\"1\" height=\"210\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_1.gif?resize=160%2C210\" width=\"160\"/>\n  </span>\n </p>\n <p style=\"text-align: center;\">\n  <span style=\"color: #000000;\">\n   Ap\u00f3s 30 minutos de treinamento:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   <img alt=\"breakout_2\" class=\"aligncenter size-full wp-image-1675\" data-attachment-id=\"1675\" data-comments-opened=\"0\" data-image-description=\"\" data-image-meta='{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}' data-image-title=\"breakout_2\" data-large-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?fit=160%2C210\" data-medium-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?fit=160%2C210\" data-orig-file=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?fit=160%2C210\" data-orig-size=\"160,210\" data-permalink=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/breakout_2/\" data-recalc-dims=\"1\" height=\"210\" src=\"https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2019/12/breakout_2.gif?resize=160%2C210\" width=\"160\"/>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Em particular, se a intelig\u00eancia artificial vai dirigir um carro ou aprender a jogar alguns cl\u00e1ssicos do Atari, pode ser considerado um marco intermedi\u00e1rio significativo. Uma aplica\u00e7\u00e3o potencial do aprendizado por refor\u00e7o em ve\u00edculos aut\u00f4nomos \u00e9 uma das aplica\u00e7\u00f5es mais trabalhadas nos dias de hoje em todo mundo. Um desenvolvedor \u00e9 incapaz de prever todas as situa\u00e7\u00f5es futuras da estrada, portanto, deixar o modelo treinar-se com um sistema de penalidades e recompensas em um ambiente variado \u00e9 possivelmente a maneira mais eficaz da IA ampliar a experi\u00eancia que possui e coleta e assim aprender a conduzir um ve\u00edculo aut\u00f4nomo sem que seja explicitamente programada para isso.\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Continuaremos no pr\u00f3ximo cap\u00edtulo!\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"color: #000000;\">\n   Refer\u00eancias:\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Intelig\u00eancia Artificial\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-analise-estatistica-para-cientistas-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o An\u00e1lise Estat\u00edstica Para Cientistas de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.datascienceacademy.com.br/pages/formacao-cientista-de-dados\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Forma\u00e7\u00e3o Cientista de Dados\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.cienciaedados.com/customizando-redes-neurais-com-funcoes-de-ativacao-alternativas/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Customizando Redes Neurais com Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Alternativas\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    What is reinforcement learning? The complete guide\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.geeksforgeeks.org/what-is-reinforcement-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Reinforcement learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://arxiv.org/pdf/1206.5533v2.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Practical Recommendations for Gradient-Based Training of Deep Architectures\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient-Based Learning Applied to Document Recognition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Neural Networks &amp; The Backpropagation Algorithm, Explained\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Recurrent neural network based language model\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E/ref=sr_1_1?ie=UTF8&amp;qid=1482130176&amp;sr=8-1&amp;keywords=The+Elements+of+Statistical+Learning%3A+Data+Mining%2C+Inference%2C+and+Prediction%2C+Second+Edition\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Gradient Descent For Machine Learning\n   </a>\n  </span>\n </p>\n <p style=\"text-align: justify;\">\n  <span style=\"text-decoration: underline; color: #000000;\">\n   <a href=\"https://www.amazon.com.br/Pattern-Recognition-Machine-Learning-Christopher/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1482130309&amp;sr=8-1&amp;keywords=Pattern+Recognition+and+Machine+Learning\" rel=\"noopener noreferrer\" style=\"color: #000000; text-decoration: underline;\" target=\"_blank\">\n    Pattern Recognition and Machine Learning\n   </a>\n  </span>\n </p>\n <div class=\"sharedaddy sd-sharing-enabled\">\n  <div class=\"robots-nocontent sd-block sd-social sd-social-icon sd-sharing\">\n   <h3 class=\"sd-title\">\n    Compartilhe isso:\n   </h3>\n   <div class=\"sd-content\">\n    <ul>\n     <li class=\"share-twitter\">\n      <a class=\"share-twitter sd-button share-icon no-text\" data-shared=\"sharing-twitter-1666\" href=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Twitter\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Twitter(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-facebook\">\n      <a class=\"share-facebook sd-button share-icon no-text\" data-shared=\"sharing-facebook-1666\" href=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Facebook\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Facebook(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-linkedin\">\n      <a class=\"share-linkedin sd-button share-icon no-text\" data-shared=\"sharing-linkedin-1666\" href=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no LinkedIn\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no LinkedIn(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-pinterest\">\n      <a class=\"share-pinterest sd-button share-icon no-text\" data-shared=\"sharing-pinterest-1666\" href=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Pinterest\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Pinterest(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-tumblr\">\n      <a class=\"share-tumblr sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=tumblr\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no Tumblr\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no Tumblr(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-jetpack-whatsapp\">\n      <a class=\"share-jetpack-whatsapp sd-button share-icon no-text\" data-shared=\"\" href=\"http://deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Clique para compartilhar no WhatsApp\">\n       <span>\n       </span>\n       <span class=\"sharing-screen-reader-text\">\n        Clique para compartilhar no WhatsApp(abre em nova janela)\n       </span>\n      </a>\n     </li>\n     <li class=\"share-end\">\n     </li>\n    </ul>\n   </div>\n  </div>\n </div>\n <div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-140353593-1666-5e0dd22bb6e3e\" data-src=\"https://widgets.wp.com/likes/#blog_id=140353593&amp;post_id=1666&amp;origin=deeplearningbook.com.br&amp;obj_id=140353593-1666-5e0dd22bb6e3e\" id=\"like-post-wrapper-140353593-1666-5e0dd22bb6e3e\">\n  <h3 class=\"sd-title\">\n   Curtir isso:\n  </h3>\n  <div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\">\n   <span class=\"button\">\n    <span>\n     Curtir\n    </span>\n   </span>\n   <span class=\"loading\">\n    Carregando...\n   </span>\n  </div>\n  <span class=\"sd-text-color\">\n  </span>\n  <a class=\"sd-link-color\">\n  </a>\n </div>\n <div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n  <h3 class=\"jp-relatedposts-headline\">\n   <em>\n    Relacionado\n   </em>\n  </h3>\n </div>\n</div>\n"}